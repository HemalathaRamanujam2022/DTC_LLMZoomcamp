{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ba3282a-6099-44b3-81c6-ba4b73d80e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bb65ff-246f-4154-90c9-3d248d06ce07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9880aca9-ae97-42f5-9aa5-37bb46448841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We generate a document id for each document. We we also use a combination of course + question,\n",
    "# but that create a few duplicate hashes where the course and question were exactly same on the FAQ.\n",
    "# Therefore we used the text field as well and extracted the first 10 characters from it.\n",
    "import hashlib\n",
    "\n",
    "def generate_document_id(doc):\n",
    "    # combined = f\"{doc['course']}-{doc['question']}\"\n",
    "    combined = f\"{doc['course']}-{doc['question']}-{doc['text'][:10]}\"\n",
    "    hash_object = hashlib.md5(combined.encode())\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    document_id = hash_hex[:8]\n",
    "    return document_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66719b38-04f3-41a8-bdd8-f33f04fe9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    doc['id'] = generate_document_id(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4672632-acc2-4c1c-96b4-d30f24598aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?',\n",
       " 'course': 'data-engineering-zoomcamp',\n",
       " 'id': '0bbf41ec'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "470b703c-29d6-4ca8-a68e-4c461b3e7a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "410f08db-2302-4c50-926c-511037b46c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashes :  defaultdict(<class 'list'>, {})\n"
     ]
    }
   ],
   "source": [
    "# This creates a default dictionary which is a list\n",
    "hashes = defaultdict(list)\n",
    "print(\"hashes : \", hashes)\n",
    "\n",
    "for doc in documents:\n",
    "    doc_id = doc['id']\n",
    "    hashes[doc_id].append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47c69c01-e952-4818-a307-94ea224ca423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(947, 948)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hashes), len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "881989e7-c297-428f-8ffc-4819341f9811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('c02e79ef', [{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\", 'section': 'General course-related questions', 'question': 'Course - When will the course start?', 'course': 'data-engineering-zoomcamp', 'id': 'c02e79ef'}]), ('1f6520ca', [{'text': 'GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites', 'section': 'General course-related questions', 'question': 'Course - What are the prerequisites for this course?', 'course': 'data-engineering-zoomcamp', 'id': '1f6520ca'}]), ('7842b56a', [{'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\", 'section': 'General course-related questions', 'question': 'Course - Can I still join the course after the start date?', 'course': 'data-engineering-zoomcamp', 'id': '7842b56a'}]), ('0bbf41ec', [{'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\", 'section': 'General course-related questions', 'question': 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?', 'course': 'data-engineering-zoomcamp', 'id': '0bbf41ec'}]), ('63394d91', [{'text': 'You can start by installing and setting up all the dependencies and requirements:\\nGoogle cloud account\\nGoogle Cloud SDK\\nPython 3 (installed with Anaconda)\\nTerraform\\nGit\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.', 'section': 'General course-related questions', 'question': 'Course - What can I do before the course starts?', 'course': 'data-engineering-zoomcamp', 'id': '63394d91'}]), ('2ed9b986', [{'text': \"There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\\nData-Engineering (Jan - Apr)\\nMLOps (May - Aug)\\nMachine Learning (Sep - Jan)\\nThere's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.\", 'section': 'General course-related questions', 'question': 'Course - how many Zoomcamps in a year?', 'course': 'data-engineering-zoomcamp', 'id': '2ed9b986'}]), ('93e2c8ed', [{'text': 'Yes. For the 2024 edition we are using Mage AI instead of Prefect and re-recorded the terraform videos, For 2023, we used Prefect instead of Airflow..', 'section': 'General course-related questions', 'question': 'Course - Is the current cohort going to be different from the previous cohort?', 'course': 'data-engineering-zoomcamp', 'id': '93e2c8ed'}]), ('a482086d', [{'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.', 'section': 'General course-related questions', 'question': 'Course - Can I follow the course after it finishes?', 'course': 'data-engineering-zoomcamp', 'id': 'a482086d'}]), ('eb56ae98', [{'text': 'Yes, the slack channel remains open and you can ask questions there. But always sDocker containers exit code w search the channel first and second, check the FAQ (this document), most likely all your questions are already answered here.\\nYou can also tag the bot @ZoomcampQABot to help you conduct the search, but don’t rely on its answers 100%, it is pretty good though.', 'section': 'General course-related questions', 'question': 'Course - Can I get support if I take the course in the self-paced mode?', 'course': 'data-engineering-zoomcamp', 'id': 'eb56ae98'}]), ('4292531b', [{'text': 'All the main videos are stored in the Main “DATA ENGINEERING” playlist (no year specified). The Github repository has also been updated to show each video with a thumbnail, that would bring you directly to the same playlist below.\\nBelow is the MAIN PLAYLIST’. And then you refer to the year specific playlist for additional videos for that year like for office hours videos etc. Also find this playlist pinned to the slack channel.\\nh\\nttps://youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&si=NspQhtZhZQs1B9F-', 'section': 'General course-related questions', 'question': 'Course - Which playlist on YouTube should I refer to?', 'course': 'data-engineering-zoomcamp', 'id': '4292531b'}]), ('ea739c65', [{'text': 'It depends on your background and previous experience with modules. It is expected to require about 5 - 15 hours per week. [source1] [source2]\\nYou can also calculate it yourself using this data and then update this answer.', 'section': 'General course-related questions', 'question': 'Course - \\u200b\\u200bHow many hours per week am I expected to spend on this  course?', 'course': 'data-engineering-zoomcamp', 'id': 'ea739c65'}]), ('cb257ee5', [{'text': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\", 'section': 'General course-related questions', 'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?', 'course': 'data-engineering-zoomcamp', 'id': 'cb257ee5'}]), ('04aa4897', [{'text': 'The zoom link is only published to instructors/presenters/TAs.\\nStudents participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\\nDon’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.', 'section': 'General course-related questions', 'question': 'Office Hours - What is the video/zoom link to the stream for the “Office Hour” or workshop sessions?', 'course': 'data-engineering-zoomcamp', 'id': '04aa4897'}]), ('9681be3b', [{'text': 'Yes! Every “Office Hours” will be recorded and available a few minutes after the live session is over; so you can view (or rewatch) whenever you want.', 'section': 'General course-related questions', 'question': 'Office Hours - I can’t attend the “Office hours” / workshop, will it be recorded?', 'course': 'data-engineering-zoomcamp', 'id': '9681be3b'}]), ('a1daf537', [{'text': 'You can find the latest and up-to-date deadlines here: https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml\\nAlso, take note of Announcements from @Au-Tomator for any extensions or other news. Or, the form may also show the updated deadline, if Instructor(s) has updated it.', 'section': 'General course-related questions', 'question': 'Homework - What are homework and project deadlines?', 'course': 'data-engineering-zoomcamp', 'id': 'a1daf537'}]), ('be5bfee4', [{'text': 'No, late submissions are not allowed. But if the form is still not closed and it’s after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page.y\\nOlder news:[source1] [source2]', 'section': 'General course-related questions', 'question': 'Homework - Are late submissions of homework allowed?', 'course': 'data-engineering-zoomcamp', 'id': 'be5bfee4'}]), ('0e424a44', [{'text': 'Answer: In short, it’s your repository on github, gitlab, bitbucket, etc\\nIn long, your repository or any other location you have your code where a reasonable person would look at it and think yes, you went through the week and exercises.', 'section': 'General course-related questions', 'question': 'Homework - What is the homework URL in the homework link?', 'course': 'data-engineering-zoomcamp', 'id': '0e424a44'}]), ('29865466', [{'text': 'After you submit your homework it will be graded based on the amount of questions in a particular homework. You can see how many points you have right on the page of the homework up top. Additionally in the leaderboard you will find the sum of all points you’ve earned - points for Homeworks, FAQs and Learning in Public. If homework is clear, others work as follows: if you submit something to FAQ, you get one point, for each learning in a public link you get one point.\\n(https://datatalks-club.slack.com/archives/C01FABYF2RG/p1706846846359379?thread_ts=1706825019.546229&cid=C01FABYF2RG)', 'section': 'General course-related questions', 'question': 'Homework and Leaderboard - what is the system for points in the course management platform?', 'course': 'data-engineering-zoomcamp', 'id': '29865466'}]), ('016d46a1', [{'text': 'When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. If you want to see what your Display name is.\\nGo to the Homework submission link →  https://courses.datatalks.club/de-zoomcamp-2024/homework/hw2 - Log in > Click on ‘Data Engineering Zoom Camp 2024’ > click on ‘Edit Course Profile’ - your display name is here, you can also change it should you wish:', 'section': 'General course-related questions', 'question': 'Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?', 'course': 'data-engineering-zoomcamp', 'id': '016d46a1'}]), ('47972cb1', [{'text': 'Yes, for simplicity (of troubleshooting against the recorded videos) and stability. [source]\\nBut Python 3.10 and 3.11 should work fine.', 'section': 'General course-related questions', 'question': 'Environment - Is Python 3.9 still the recommended version to use in 2024?', 'course': 'data-engineering-zoomcamp', 'id': '47972cb1'}]), ('ddf6c1b3', [{'text': 'You can set it up on your laptop or PC if you prefer to work locally from your laptop or PC.\\nYou might face some challenges, especially for Windows users. If you face cnd2\\nIf you prefer to work on the local machine, you may start with the week 1 Introduction to Docker and follow through.\\nHowever, if you prefer to set up a virtual machine, you may start with these first:\\nUsing GitHub Codespaces\\nSetting up the environment on a cloudV Mcodespace\\nI decided to work on a virtual machine because I have different laptops & PCs for my home & office, so I can work on this boot camp virtually anywhere.', 'section': 'General course-related questions', 'question': 'Environment - Should I use my local machine, GCP, or GitHub Codespaces for my environment?', 'course': 'data-engineering-zoomcamp', 'id': 'ddf6c1b3'}]), ('ac25d3af', [{'text': 'GitHub Codespaces offers you computing Linux resources with many pre-installed tools (Docker, Docker Compose, Python).\\nYou can also open any GitHub repository in a GitHub Codespace.', 'section': 'General course-related questions', 'question': 'Environment - Is GitHub codespaces an alternative to using cli/git bash to ingest the data and create a docker file?', 'course': 'data-engineering-zoomcamp', 'id': 'ac25d3af'}]), ('251218fc', [{'text': \"It's up to you which platform and environment you use for the course.\\nGithub codespaces or GCP VM are just possible options, but you can do the entire course from your laptop.\", 'section': 'General course-related questions', 'question': 'Environment - Do we really have to use GitHub codespaces? I already have PostgreSQL & Docker installed.', 'course': 'data-engineering-zoomcamp', 'id': '251218fc'}]), ('3c0114ce', [{'text': 'Choose the approach that aligns the most with your idea for the end project\\nOne of those should suffice. However, BigQuery, which is part of GCP, will be used, so learning that is probably a better option. Or you can set up a local environment for most of this course.', 'section': 'General course-related questions', 'question': 'Environment - Do I need both GitHub Codespaces and GCP?', 'course': 'data-engineering-zoomcamp', 'id': '3c0114ce'}]), ('f43f5fe7', [{'text': '1. To open Run command window, you can either:\\n(1-1) Use the shortcut keys: \\'Windows + R\\', or\\n(1-2) Right Click \"Start\", and click \"Run\" to open.\\n2. Registry Values Located in Registry Editor, to open it: Type \\'regedit\\' in the Run command window, and then press Enter.\\' 3. Now you can change the registry values \"Autorun\" in \"HKEY_CURRENT_USER\\\\Software\\\\Microsoft\\\\Command Processor\" from \"if exists\" to a blank.\\nAlternatively, You can simplify the solution by deleting the fingerprint saved within the known_hosts file. In Windows, this file is placed at  C:\\\\Users\\\\<your_user_name>\\\\.ssh\\\\known_host', 'section': 'General course-related questions', 'question': 'This happens when attempting to connect to a GCP VM using VSCode on a Windows machine. Changing registry value in registry editor', 'course': 'data-engineering-zoomcamp', 'id': 'f43f5fe7'}]), ('d061525d', [{'text': 'For uniformity at least, but you’re not restricted to GCP, you can use other cloud platforms like AWS if you’re comfortable with other cloud platforms, since you get every service that’s been provided by GCP in Azure and AWS or others..\\nBecause everyone has a google account, GCP has a free trial period and gives $300 in credits  to new users. Also, we are working with BigQuery, which is a part of GCP.\\nNote that to sign up for a free GCP account, you must have a valid credit card.', 'section': 'General course-related questions', 'question': 'Environment - Why are we using GCP and not other cloud providers?', 'course': 'data-engineering-zoomcamp', 'id': 'd061525d'}]), ('1cd01b2c', [{'text': 'No, if you use GCP and take advantage of their free trial.', 'section': 'General course-related questions', 'question': 'Should I pay for cloud services?', 'course': 'data-engineering-zoomcamp', 'id': '1cd01b2c'}]), ('e4a7c3b0', [{'text': 'You can do most of the course without a cloud. Almost everything we use (excluding BigQuery) can be run locally. We won’t be able to provide guidelines for some things, but most of the materials are runnable without GCP.\\nFor everything in the course, there’s a local alternative. You could even do the whole course locally.', 'section': 'General course-related questions', 'question': 'Environment - The GCP and other cloud providers are unavailable in some countries. Is it possible to provide a guide to installing a home lab?', 'course': 'data-engineering-zoomcamp', 'id': 'e4a7c3b0'}]), ('7cd1912e', [{'text': 'Yes, you can. Just remember to adapt all the information on the videos to AWS. Besides, the final capstone will be evaluated based on the task: Create a data pipeline! Develop a visualisation!\\nThe problem would be when you need help. You’d need to rely on  fellow coursemates who also use AWS (or have experience using it before), which might be in smaller numbers than those learning the course with GCP.\\nAlso see Is it possible to use x tool instead of the one tool you use?', 'section': 'General course-related questions', 'question': 'Environment - I want to use AWS. May I do that?', 'course': 'data-engineering-zoomcamp', 'id': '7cd1912e'}]), ('52393fb3', [{'text': 'We will probably have some calls during the Capstone period to clear some questions but it will be announced in advance if that happens.', 'section': 'General course-related questions', 'question': 'Besides the “Office Hour” which are the live zoom calls?', 'course': 'data-engineering-zoomcamp', 'id': '52393fb3'}]), ('10515af5', [{'text': 'We will use the same data, as the project will essentially remain the same as last year’s. The data is available here', 'section': 'General course-related questions', 'question': 'Are we still using the NYC Trip data for January 2021? Or are we using the 2022 data?', 'course': 'data-engineering-zoomcamp', 'id': '10515af5'}]), ('cdb86a97', [{'text': 'No, but we moved the 2022 stuff here', 'section': 'General course-related questions', 'question': 'Is the 2022 repo deleted?', 'course': 'data-engineering-zoomcamp', 'id': 'cdb86a97'}]), ('3e0114ad', [{'text': 'Yes, you can use any tool you want for your project.', 'section': 'General course-related questions', 'question': 'Can I use Airflow instead for my final project?', 'course': 'data-engineering-zoomcamp', 'id': '3e0114ad'}]), ('b2799574', [{'text': 'Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\\nThe course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\\nShould you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.', 'section': 'General course-related questions', 'question': 'Is it possible to use tool “X” instead of the one tool you use in the course?', 'course': 'data-engineering-zoomcamp', 'id': 'b2799574'}]), ('2f19301f', [{'text': 'Star the repo! Share it with friends if you find it useful ❣️\\nCreate a PR if you see you can improve the text or the structure of the repository.', 'section': 'General course-related questions', 'question': 'How can we contribute to the course?', 'course': 'data-engineering-zoomcamp', 'id': '2f19301f'}]), ('7c700adb', [{'text': 'Yes! Linux is ideal but technically it should not matter. Students last year used all 3 OSes successfully', 'section': 'General course-related questions', 'question': 'Environment - Is the course [Windows/mac/Linux/...] friendly?', 'course': 'data-engineering-zoomcamp', 'id': '7c700adb'}]), ('44b14808', [{'text': \"Have no idea how past cohorts got past this as I haven't read old slack messages, and no FAQ entries that I can find.\\nLater modules (module-05 & RisingWave workshop) use shell scripts in *.sh files and most Windows users not using WSL would hit a wall and cannot continue, even in git bash or MINGW64. This is why WSL environment setup is recommended from the start.\", 'section': 'General course-related questions', 'question': 'Environment - Roadblock for Windows users in modules with *.sh (shell scripts).', 'course': 'data-engineering-zoomcamp', 'id': '44b14808'}]), ('76e4baf6', [{'text': 'Yes to both! check out this document: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md', 'section': 'General course-related questions', 'question': 'Any books or additional resources you recommend?', 'course': 'data-engineering-zoomcamp', 'id': '76e4baf6'}]), ('48b533a8', [{'text': 'You will have two attempts for a project. If the first project deadline is over and you’re late or you submit the project and fail the first attempt, you have another chance to submit the project with the second attempt.', 'section': 'General course-related questions', 'question': 'Project - What is Project Attemp #1 and Project Attempt #2 exactly?', 'course': 'data-engineering-zoomcamp', 'id': '48b533a8'}]), ('954044d1', [{'text': \"The first step is to try to solve the issue on your own. Get used to solving problems and reading documentation. This will be a real life skill you need when employed. [ctrl+f] is your friend, use it! It is a universal shortcut and works in all apps/browsers.\\nWhat does the error say? There will often be a description of the error or instructions on what is needed or even how to fix it. I have even seen a link to the solution. Does it reference a specific line of your code?\\nRestart app or server/pc.\\nGoogle it, use ChatGPT, Bing AI etc.\\nIt is going to be rare that you are the first to have the problem, someone out there has posted the fly issue and likely the solution.\\nSearch using: <technology> <problem statement>. Example: pgcli error column c.relhasoids does not exist.\\nThere are often different solutions for the same problem due to variation in environments.\\nCheck the tech’s documentation. Use its search if available or use the browsers search function.\\nTry uninstall (this may remove the bad actor) and reinstall of application or reimplementation of action. Remember to restart the server/pc for reinstalls.\\nSometimes reinstalling fails to resolve the issue but works if you uninstall first.\\nPost your question to Stackoverflow. Read the Stackoverflow guide on posting good questions.\\nhttps://stackoverflow.com/help/how-to-ask\\nThis will be your real life. Ask an expert in the future (in addition to coworkers).\\nAsk in Slack\\nBefore asking a question,\\nCheck Pins (where the shortcut to the repo and this FAQ is located)\\nUse the slack app’s search function\\nUse the bot @ZoomcampQABot to do the search for you\\ncheck the FAQ (this document), use search [ctrl+f]\\nWhen asking a question, include as much information as possible:\\nWhat are you coding on? What OS?\\nWhat command did you run, which video did you follow? Etc etc\\nWhat error did you get? Does it have a line number to the “offending” code and have you check it for typos?\\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.\\nDO NOT use screenshots, especially don’t take pictures from a phone.\\nDO NOT tag instructors, it may discourage others from helping you. Copy and paste errors; if it’s long, just post it in a reply to your thread.\\nUse ``` for formatting your code.\\nUse the same thread for the conversation (that means reply to your own thread).\\nDO NOT create multiple posts to discuss the issue.\\nlearYou may create a new post if the issue reemerges down the road. Describe what has changed in the environment.\\nProvide additional information in the same thread of the steps you have taken for resolution.\\nTake a break and come back later. You will be amazed at how often you figure out the solution after letting your brain rest. Get some fresh air, workout, play a video game, watch a tv show, whatever allows your brain to not think about it for a little while or even until the next day.\\nRemember technology issues in real life sometimes take days or even weeks to resolve.\\nIf somebody helped you with your problem and it's not in the FAQ, please add it there. It will help other students.\", 'section': 'General course-related questions', 'question': 'How to troubleshoot issues', 'course': 'data-engineering-zoomcamp', 'id': '954044d1'}]), ('a820b9b3', [{'text': 'When the troubleshooting guide above does not help resolve it and you need another pair of eyeballs to spot mistakes. When asking a question, include as much information as possible:\\nWhat are you coding on? What OS?\\nWhat command did you run, which video did you follow? Etc etc\\nWhat error did you get? Does it have a line number to the “offending” code and have you check it for typos?\\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.', 'section': 'General course-related questions', 'question': 'How to ask questions', 'course': 'data-engineering-zoomcamp', 'id': 'a820b9b3'}]), ('f2945cd2', [{'text': 'After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\\nHaving this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\\nThis is also a great resource: https://dangitgit.com/', 'section': 'General course-related questions', 'question': 'How do I use Git / GitHub for this course?', 'course': 'data-engineering-zoomcamp', 'id': 'f2945cd2'}]), ('eb9d376f', [{'text': 'Error: Makefile:2: *** missing separator.  Stop.\\nSolution: Tabs in document should be converted to Tab instead of spaces. Follow this stack.', 'section': 'General course-related questions', 'question': 'VS Code: Tab using spaces', 'course': 'data-engineering-zoomcamp', 'id': 'eb9d376f'}]), ('72f25f6d', [{'text': \"If you’re running Linux on Windows Subsystem for Linux (WSL) 2, you can open HTML files from the guest (Linux) with whatever Internet Browser you have installed on the host (Windows). Just install wslu and open the page with wslview <file>, for example:\\nwslview index.html\\nYou can customise which browser to use by setting the BROWSER environment variable first. For example:\\nexport BROWSER='/mnt/c/Program Files/Firefox/firefox.exe'\", 'section': 'General course-related questions', 'question': 'Opening an HTML file with a Windows browser from Linux running on WSL', 'course': 'data-engineering-zoomcamp', 'id': '72f25f6d'}]), ('a1e59afc', [{'text': 'This tutorial shows you how to set up the Chrome Remote Desktop service on a Debian Linux virtual machine (VM) instance on Compute Engine. Chrome Remote Desktop allows you to remotely access applications with a graphical user interface.\\nTaxi Data - Yellow Taxi Trip Records downloading error, Error no or XML error webpage\\nWhen you try to download the 2021 data from TLC website, you get this error:\\nIf you click on the link, and ERROR 403: Forbidden on the terminal.\\nWe have a backup, so use it instead: https://github.com/DataTalksClub/nyc-tlc-data\\nSo the link should be https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\\nNote: Make sure to unzip the “gz” file (no, the “unzip” command won’t work for this.)\\n“gzip -d file.gz”g', 'section': 'Module 1: Docker and Terraform', 'question': 'Set up Chrome Remote Desktop for Linux on Compute Engine', 'course': 'data-engineering-zoomcamp', 'id': 'a1e59afc'}]), ('71c10610', [{'text': 'In this video, we store the data file as “output.csv”. The data file won’t store correctly if the file extension is csv.gz instead of csv. One alternative is to replace csv_name = “output.cs -v” with the file name given at the end of the URL. Notice that the URL for the yellow taxi data is: https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz where the highlighted part is the name of the file. We can parse this file name from the URL and use it as csv_name. That is, we can replace csv_name = “output.csv” with\\ncsv_name = url.split(“/”)[-1] . Then when we use csv_name to using pd.read_csv, there won’t be an issue even though the file name really has the extension csv.gz instead of csv since the pandas read_csv function can read csv.gz files directly.', 'section': 'Module 1: Docker and Terraform', 'question': 'Taxi Data - How to handle taxi data files, now that the files are available as *.csv.gz?', 'course': 'data-engineering-zoomcamp', 'id': '71c10610'}]), ('17a5aea1', [{'text': 'Yellow Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\\nGreen Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf', 'section': 'Module 1: Docker and Terraform', 'question': 'Taxi Data - Data Dictionary for NY Taxi data?', 'course': 'data-engineering-zoomcamp', 'id': '17a5aea1'}]), ('5a275db7', [{'text': 'You can unzip this downloaded parquet file, in the command line. The result is a csv file which can be imported with pandas using the pd.read_csv() shown in the videos.\\n‘’’gunzip green_tripdata_2019-09.csv.gz’’’\\nSOLUTION TO USING PARQUET FILES DIRECTLY IN PYTHON SCRIPT ingest_data.py\\nIn the def main(params) add this line\\nparquet_name= \\'output.parquet\\'\\nThen edit the code which downloads the files\\nos.system(f\"wget {url} -O {parquet_name}\")\\nConvert the download .parquet file to csv and rename as csv_name to keep it relevant to the rest of the code\\ndf = pd.read_parquet(parquet_name)\\ndf.to_csv(csv_name, index=False)', 'section': 'Module 1: Docker and Terraform', 'question': 'Taxi Data - Unzip Parquet file', 'course': 'data-engineering-zoomcamp', 'id': '5a275db7'}]), ('7ec0f9b0', [{'text': '“wget is not recognized as an internal or external command”, you need to install it.\\nOn Ubuntu, run:\\n$ sudo apt-get install wget\\nOn MacOS, the easiest way to install wget is to use Brew:\\n$ brew install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\n$ choco install wget\\nOr you can download a binary (https://gnuwin32.sourceforge.net/packages/wget.htm) and put it to any location in your PATH (e.g. C:/tools/)\\nAlso, you can following this step to install Wget on MS Windows\\n* Download the latest wget binary for windows from [eternallybored] (https://eternallybored.org/misc/wget/) (they are available as a zip with documentation, or just an exe)\\n* If you downloaded the zip, extract all (if windows built in zip utility gives an error, use [7-zip] (https://7-zip.org/)).\\n* Rename the file `wget64.exe` to `wget.exe` if necessary.\\n* Move wget.exe to your `Git\\\\mingw64\\\\bin\\\\`.\\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need to use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAlternatively, you can just paste the file URL into your web browser and download the file normally that way. You’ll want to move the resulting file into your working directory.\\nAlso recommended a look at the python library requests for the loading gz file  https://pypi.org/project/requests', 'section': 'Module 1: Docker and Terraform', 'question': 'lwget is not recognized as an internal or external command', 'course': 'data-engineering-zoomcamp', 'id': '7ec0f9b0'}]), ('bb1ba786', [{'text': 'Firstly, make sure that you add “!” before wget if you’re running your command in a Jupyter Notebook or CLI. Then, you can check one of this 2 things (from CLI):\\nUsing the Python library wget you installed with pip, try python -m wget <url>\\nWrite the usual command and add --no-check-certificate at the end. So it should be:\\n!wget <website_url> --no-check-certificate', 'section': 'Module 1: Docker and Terraform', 'question': 'wget - ERROR: cannot verify <website> certificate  (MacOS)', 'course': 'data-engineering-zoomcamp', 'id': 'bb1ba786'}]), ('2f83dbe7', [{'text': 'For those who wish to use the backslash as an escape character in Git Bash for Windows (as Alexey normally does), type in the terminal: bash.escapeChar=\\\\ (no need to include in .bashrc)', 'section': 'Module 1: Docker and Terraform', 'question': 'Git Bash - Backslash as an escape character in Git Bash for Windows', 'course': 'data-engineering-zoomcamp', 'id': '2f83dbe7'}]), ('543ff080', [{'text': 'Instruction on how to store secrets that will be avialable in GitHub  Codespaces.\\nManaging your account-specific secrets for GitHub Codespaces - GitHub Docs', 'section': 'Module 1: Docker and Terraform', 'question': 'GitHub Codespaces - How to store secrets', 'course': 'data-engineering-zoomcamp', 'id': '543ff080'}]), ('d407d65b', [{'text': \"Make sure you're able to start the Docker daemon, and check the issue immediately down below:\\nAnd don’t forget to update the wsl in powershell the  command is wsl –update\", 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Cannot connect to Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?', 'course': 'data-engineering-zoomcamp', 'id': 'd407d65b'}]), ('c9375c56', [{'text': \"As the official Docker for Windows documentation says, the Docker engine can either use the\\nHyper-V or WSL2 as its backend. However, a few constraints might apply\\nWindows 10 Pro / 11 Pro Users: \\nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\\nWindows 10 Home / 11 Home Users: \\nOn the other hand, Users of the 'Home' version do NOT have the option Hyper-V option enabled, which means, you can only get Docker up and running using the WSL2 credentials(Windows Subsystem for Linux). Url\\nYou can find the detailed instructions to do so here: rt ghttps://pureinfotech.com/install-wsl-windows-11/\\nIn case, you run into another issue while trying to install WSL2 (WslRegisterDistribution failed with error: 0x800701bc), Make sure you update the WSL2 Linux Kernel, following the guidelines here: \\n\\nhttps://github.com/microsoft/WSL/issues/5393\", 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Error during connect: In the default daemon configuration on Windows, the docker client must be run with elevated privileges to connect.: Post: \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/containers/create\" : open //./pipe/docker_engine: The system cannot find the file specified', 'course': 'data-engineering-zoomcamp', 'id': 'c9375c56'}]), ('e866156b', [{'text': 'Whenever a `docker pull is performed (either manually or by `docker-compose up`), it attempts to fetch the given image name (pgadmin4, for the example above) from a repository (dbpage).\\nIF the repository is public, the fetch and download happens without any issue whatsoever.\\nFor instance:\\ndocker pull postgres:13\\ndocker pull dpage/pgadmin4\\nBE ADVISED:\\n\\nThe Docker Images we\\'ll be using throughout the Data Engineering Zoomcamp are all public (except when or if explicitly said otherwise by the instructors or co-instructors).\\n\\nMeaning: you are NOT required to perform a docker login to fetch them. \\n\\nSo if you get the message above saying \"docker login\\': denied: requested access to the resource is denied. That is most likely due to a typo in your image name:\\n\\nFor instance:\\n$ docker pull dbpage/pgadmin4\\nWill throw that exception telling you \"repository does not exist or may require \\'docker login\\'\\nError response from daemon: pull access denied for dbpage/pgadmin4, repository does not exist or \\nmay require \\'docker login\\': denied: requested access to the resource is denied\\nBut that actually happened because the actual image is dpage/pgadmin4 and NOT dbpage/pgadmin4\\nHow to fix it:\\n$ docker pull dpage/pgadmin4\\nEXTRA NOTES:\\nIn the real world, occasionally, when you\\'re working for a company or closed organisation, the Docker image you\\'re trying to fetch might be under a private repo that your DockerHub Username was granted access to.\\nFor which cases, you must first execute:\\n$ docker login\\nFill in the details of your username and password.\\nAnd only then perform the `docker pull` against that private repository\\nWhy am I encountering a \"permission denied\" error when creating a PostgreSQL Docker container for the New York Taxi Database with a mounted volume on macOS M1?\\nIssue Description:\\nWhen attempting to run a Docker command similar to the one below:\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\mount\\npostgres:13\\nYou encounter the error message:\\ndocker: Error response from daemon: error while creating mount source path \\'/path/to/ny_taxi_postgres_data\\': chown /path/to/ny_taxi_postgres_data: permission denied.\\nSolution:\\n1- Stop Rancher Desktop:\\nIf you are using Rancher Desktop and face this issue, stop Rancher Desktop to resolve compatibility problems.\\n2- Install Docker Desktop:\\nInstall Docker Desktop, ensuring that it is properly configured and has the required permissions.\\n2-Retry Docker Command:\\nRun the Docker command again after switching to Docker Desktop. This step resolves compatibility issues on some systems.\\nNote: The issue occurred because Rancher Desktop was in use. Switching to Docker Desktop resolves compatibility problems and allows for the successful creation of PostgreSQL containers with mounted volumes for the New York Taxi Database on macOS M1.', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - docker pull dbpage', 'course': 'data-engineering-zoomcamp', 'id': 'e866156b'}]), ('16370470', [{'text': 'When I runned command to create postgre in docker container it created folder on my local machine to mount it to volume inside container. It has write and read protection and owned by user 999, so I could not delete it by simply drag to trash.  My obsidian could not started due to access error, so I had to change placement of this folder and delete old folder by this command:\\nsudo rm -r -f docker_test/\\n- where `rm` - remove, `-r` - recursively, `-f` - force, `docker_test/` - folder.', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - can’t delete local folder that mounted to docker volume', 'course': 'data-engineering-zoomcamp', 'id': '16370470'}]), ('316df755', [{'text': 'First off, make sure you\\'re running the latest version of Docker for Windows, which you can download from here. Sometimes using the menu to \"Upgrade\" doesn\\'t work (which is another clear indicator for you to uninstall, and reinstall with the latest version)\\nIf docker is stuck on starting, first try to switch containers by right clicking the docker symbol from the running programs and switch the containers from windows to linux or vice versa\\n[Windows 10 / 11 Pro Edition] The Pro Edition of Windows can run Docker either by using Hyper-V or WSL2 as its backend (Docker Engine)\\nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\\nIf you opt-in for WSL2, you can follow the same steps as detailed in the tutorial here', 'section': 'Module 1: Docker and Terraform', 'question': \"Docker - Docker won't start or is stuck in settings (Windows 10 / 11)\", 'course': 'data-engineering-zoomcamp', 'id': '316df755'}]), ('f3aa9252', [{'text': \"It is recommended by the Docker do\\n[Windows 10 / 11 Home Edition] If you're running a Home Edition, you can still make it work with WSL2 (Windows Subsystem for Linux) by following the tutorial here\\nIf even after making sure your WSL2 (or Hyper-V) is set up accordingly, Docker remains stuck, you can try the option to Reset to Factory Defaults or do a fresh install.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Should I run docker commands from the windows file system or a file system of a Linux distribution in WSL?', 'course': 'data-engineering-zoomcamp', 'id': 'f3aa9252'}]), ('a4abe7a5', [{'text': 'More info in the Docker Docs on Best Practises', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - cs to store all code in your default Linux distro to get the best out of file system performance (since Docker runs on WSL2 backend by default for Windows 10 Home / Windows 11 Home users).', 'course': 'data-engineering-zoomcamp', 'id': 'a4abe7a5'}]), ('fb930700', [{'text': 'You may have this error:\\n$ docker run -it ubuntu bash\\nthe input device is not a TTY. If you are using mintty, try prefixing the command with \\'winpty\\'\\nerror:\\nSolution:\\nUse winpty before docker command (source)\\n$ winpty docker run -it ubuntu bash\\nYou also can make an alias:\\necho \"alias docker=\\'winpty docker\\'\" >> ~/.bashrc\\nOR\\necho \"alias docker=\\'winpty docker\\'\" >> ~/.bash_profile', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - The input device is not a TTY (Docker run for Windows)', 'course': 'data-engineering-zoomcamp', 'id': 'fb930700'}]), ('aa187680', [{'text': \"You may have this error:\\nRetrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.u\\nrllib3.connection.HTTPSConnection object at 0x7efe331cf790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')':\\n/simple/pandas/\\nPossible solution might be:\\n$ winpty docker run -it --dns=8.8.8.8 --entrypoint=bash python:3.9\", 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Cannot pip install on Docker container (Windows)', 'course': 'data-engineering-zoomcamp', 'id': 'aa187680'}]), ('b000e899', [{'text': 'Even after properly running the docker script the folder is empty in the vs code  then try this (For Windows)\\nwinpty docker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"C:\\\\Users\\\\abhin\\\\dataengg\\\\DE_Project_git_connected\\\\DE_OLD\\\\week1_set_up\\\\docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\" \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nHere quoting the absolute path in  the -v parameter is solving the issue and all the files are visible in the Vs-code ny_taxi folder as shown in the video', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - ny_taxi_postgres_data is empty', 'course': 'data-engineering-zoomcamp', 'id': 'b000e899'}]), ('9c66759f', [{'text': 'Check this article for details - Setting up docker in macOS\\nFrom researching it seems this method might be out of date, it seems that since docker changed their licensing model, the above is a bit hit and miss. What worked for me was to just go to the docker website and download their dmg. Haven’t had an issue with that method.', 'section': 'Module 1: Docker and Terraform', 'question': 'dasDocker - Setting up Docker on Mac', 'course': 'data-engineering-zoomcamp', 'id': '9c66759f'}]), ('e3106e07', [{'text': '$ docker run -it\\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"admin\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"/mnt/path/to/ny_taxi_postgres_data\":\"/var/lib/postgresql/data\" \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nCCW\\nThe files belonging to this database system will be owned by user \"postgres\".\\nThis use The database cluster will be initialized with locale \"en_US.utf8\".\\nThe default databerrorase encoding has accordingly been set to \"UTF8\".\\nxt search configuration will be set to \"english\".\\nData page checksums are disabled.\\nfixing permissions on existing directory /var/lib/postgresql/data ... initdb: f\\nerror: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted  volume\\nOne way to solve this issue is to create a local docker volume and map it to postgres data directory /var/lib/postgresql/data\\nThe input dtc_postgres_volume_local must match in both commands below\\n$ docker volume create --name dtc_postgres_volume_local -d local\\n$ docker run -it\\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v dtc_postgres_volume_local:/var/lib/postgresql/data \\\\\\n-p 5432:5432\\\\\\npostgres:13\\nTo verify the above command works in (WSL2 Ubuntu 22.04, verified 2024-Jan), go to the Docker Desktop app and look under Volumes - dtc_postgres_volume_local would be listed there. The folder ny_taxi_postgres_data would however be empty, since we used an alternative config.\\nAn alternate error could be:\\ninitdb: error: directory \"/var/lib/postgresql/data\" exists but is not empty\\nIf you want to create a new database system, either remove or empthe directory \"/var/lib/postgresql/data\" or run initdb\\nwitls', 'section': 'Module 1: Docker and Terraform', 'question': '1Docker - Could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted', 'course': 'data-engineering-zoomcamp', 'id': 'e3106e07'}]), ('72229da5', [{'text': 'Mapping volumes on Windows could be tricky. The way it was done in the course video doesn’t work for everyone.\\nFirst, if yo\\nmove your data to some folder without spaces. E.g. if your code is in “C:/Users/Alexey Grigorev/git/…”, move it to “C:/git/…”\\nTry replacing the “-v” part with one of the following options:\\n-v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v //c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v /c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v //c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n--volume //driveletter/path/ny_taxi_postgres_data/:/var/lib/postgresql/data\\nwinpty docker run -it\\n-e POSTGRES_USER=\"root\"\\n-e POSTGRES_PASSWORD=\"root\"\\n-e POSTGRES_DB=\"ny_taxi\"\\n-v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-p 5432:5432\\npostgres:1\\nTry adding winpty before the whole command\\n3\\nwin\\nTry adding quotes:\\n-v \"/c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v \"//c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v “/c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v \"//c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v \"c:\\\\some\\\\path\\\\ny_taxi_postgres_data\":/var/lib/postgresql/data\\nNote:  (Window) if it automatically creates a folder called “ny_taxi_postgres_data;C” suggests you have problems with volume mapping, try deleting both folders and replacing “-v” part with other options. For me “//c/” works instead of “/c/”. And it will work by automatically creating a correct folder called “ny_taxi_postgres_data”.\\nA possible solution to this error would be to use /”$(pwd)”/ny_taxi_postgres_data:/var/lib/postgresql/data (with quotes’ position varying as in the above list).\\nYes for windows use the command it works perfectly fine\\n-v /”$(pwd)”/ny_taxi_postgres_data:/var/lib/postgresql/data\\nImportant: note how the quotes are placed.\\nIf none of these options work, you can use a volume name instead of the path:\\n-v ny_taxi_postgres_data:/var/lib/postgresql/data\\nFor Mac: You can wrap $(pwd) with quotes like the highlighted.\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"$(pwd)\"/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\nPostgres:13\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"$(pwd)\"/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nSource:https://stackoverflow.com/questions/48522615/docker-error-invalid-reference-format-repository-name-must-be-lowercase', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - invalid reference format: repository name must be lowercase (Mounting volumes with Docker on Windows)', 'course': 'data-engineering-zoomcamp', 'id': '72229da5'}]), ('58c9f99f', [{'text': 'Change the mounting path. Replace it with one of following:\\n-v /e/zoomcamp/...:/var/lib/postgresql/data\\n-v /c:/.../ny_taxi_postgres_data:/var/lib/postgresql/data\\\\ (leading slash in front of c:)', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Error response from daemon: invalid mode: \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data.', 'course': 'data-engineering-zoomcamp', 'id': '58c9f99f'}]), ('bc42139a', [{'text': 'When you run this command second time\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v <your path>:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nThe error message above could happen. That means you should not mount on the second run. This command helped me:\\nWhen you run this command second time\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-p 5432:5432 \\\\\\npostgres:13', 'section': 'Module 1: Docker and Terraform', 'question': \"Docker - Error response from daemon: error while creating buildmount source path '/run/desktop/mnt/host/c/<your path>': mkdir /run/desktop/mnt/host/c: file exists\", 'course': 'data-engineering-zoomcamp', 'id': 'bc42139a'}]), ('a146e3ee', [{'text': 'This error appeared when running the command: docker build -t taxi_ingest:v001 .\\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn’t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\\nsudo chown -R $USER dir_path\\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAdded by\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tKenan Arslanbay', 'section': 'Module 1: Docker and Terraform', 'question': \"Docker - build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.\", 'course': 'data-engineering-zoomcamp', 'id': 'a146e3ee'}]), ('593a85ba', [{'text': 'You might have installed docker via snap. Run “sudo snap status docker” to verify.\\nIf you have “error: unknown command \"status\", see \\'snap help\\'.” as a response than deinstall docker and install via the official website\\nBind for 0.0.0.0:5432 failed: port is a', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - ERRO[0000] error waiting for container: context canceled', 'course': 'data-engineering-zoomcamp', 'id': '593a85ba'}]), ('50bd1a71', [{'text': 'Found the issue in the PopOS linux. It happened because our user didn’t have authorization rights to the host folder ( which also caused folder seems empty, but it didn’t!).\\n✅Solution:\\nJust add permission for everyone to the corresponding folder\\nsudo chmod -R 777 <path_to_folder>\\nExample:\\nsudo chmod -R 777 ny_taxi_postgres_data/', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - build error checking context: can’t stat ‘/home/fhrzn/Projects/…./ny_taxi_postgres_data’', 'course': 'data-engineering-zoomcamp', 'id': '50bd1a71'}]), ('f409f751', [{'text': 'This happens on Ubuntu/Linux systems when trying to run the command to build the Docker container again.\\n$ docker build -t taxi_ingest:v001 .\\nA folder is created to host the Docker files. When the build command is executed again to rebuild the pipeline or create a new one the error is raised as there are no permissions on this new folder. Grant permissions by running this comtionmand;\\n$ sudo chmod -R 755 ny_taxi_postgres_data\\nOr use 777 if you still see problems. 755 grants write access to only the owner.', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - failed to solve with frontend dockerfile.v0: failed to read dockerfile: error from sender: open ny_taxi_postgres_data: permission denied.', 'course': 'data-engineering-zoomcamp', 'id': 'f409f751'}]), ('7d217da3', [{'text': 'Get the network name via: $ docker network ls.', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Docker network name', 'course': 'data-engineering-zoomcamp', 'id': '7d217da3'}]), ('09081824', [{'text': 'Sometimes, when you try to restart a docker image configured with a network name, the above message appears. In this case, use the following command with the appropriate container name:\\n>>> If the container is running state, use docker stop <container_name>\\n>>> then, docker rm pg-database\\nOr use docker start instead of docker run in order to restart the docker image without removing it.', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Error response from daemon: Conflict. The container name \"pg-database\" is already in use by container “xxx”.  You have to remove (or rename) that container to be able to reuse that name.', 'course': 'data-engineering-zoomcamp', 'id': '09081824'}]), ('4df80c55', [{'text': 'Typical error: sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"pgdatabase\" to address: Name or service not known\\nWhen running docker-compose up -d see which network is created and use this for the ingestions script instead of pg-network and see the name of the database to use instead of pgdatabase\\nE.g.:\\npg-network becomes 2docker_default\\nPgdatabase becomes 2docker-pgdatabase-1', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - ingestion when using docker-compose could not translate host name', 'course': 'data-engineering-zoomcamp', 'id': '4df80c55'}]), ('3aee7261', [{'text': 'terraformRun this command before starting your VM:\\nOn Intel CPU:\\nmodprobe -r kvm_intel\\nmodprobe kvm_intel nested=1\\nOn AMD CPU:\\nmodprobe -r kvm_amd\\nmodprobe kvm_amd nested=1', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Cannot install docker on MacOS/Windows 11 VM running on top of Linux (due to Nested virtualization).', 'course': 'data-engineering-zoomcamp', 'id': '3aee7261'}]), ('6497b659', [{'text': 'It’s very easy to manage your docker container, images, network and compose projects from VS Code.\\nJust install the official extension and launch it from the left side icon.\\nIt will work even if your Docker runs on WSL2, as VS Code can easily connect with your Linux.\\nDocker - How to stop a container?\\nUse the following command:\\n$ docker stop <container_id>', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - Connecting from VS Code', 'course': 'data-engineering-zoomcamp', 'id': '6497b659'}]), ('a02f2039', [{'text': \"When you see this in logs, your container with postgres is not accepting any requests, so if you attempt to connect, you'll get this error:\\nconnection failed: server closed the connection unexpectedly\\nThis probably means the server terminated abnormally before or while processing the request.\\nIn this case, you need to delete the directory with data (the one you map to the container with the -v flag) and restart the container.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - PostgreSQL Database directory appears to contain a database. Database system is shut down', 'course': 'data-engineering-zoomcamp', 'id': 'a02f2039'}]), ('c6db65aa', [{'text': 'On few versions of Ubuntu, snap command can be used to install Docker.\\nsudo snap install docker', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker not installable on Ubuntu', 'course': 'data-engineering-zoomcamp', 'id': 'c6db65aa'}]), ('f476a606', [{'text': 'error: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted  volume\\nif you have used the prev answer (just before this) and have created a local docker volume, then you need to tell the compose file about the named volume:\\nvolumes:\\ndtc_postgres_volume_local:  # Define the named volume here\\n# services mentioned in the compose file auto become part of the same network!\\nservices:\\nyour remaining code here . . .\\nnow use docker volume inspect dtc_postgres_volume_local to see the location by checking the value of Mountpoint\\nIn my case, after i ran docker compose up the mounting dir created was named ‘docker_sql_dtc_postgres_volume_local’ whereas it should have used the already existing ‘dtc_postgres_volume_local’\\nAll i did to fix this is that I renamed the existing ‘dtc_postgres_volume_local’ to ‘docker_sql_dtc_postgres_volume_local’ and removed the newly created one (just be careful when doing this)\\nrun docker compose up again and check if the table is there or not!', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - mounting error', 'course': 'data-engineering-zoomcamp', 'id': 'f476a606'}]), ('e41b100c', [{'text': 'Couldn’t translate host name to address\\nMake sure postgres database is running.\\n\\n\\u200b\\u200bUse the command to start containers in detached mode: docker-compose up -d\\n(data-engineering-zoomcamp) hw % docker compose up -d\\n[+] Running 2/2\\n⠿ Container pg-admin     Started                                                                                                                                                                      0.6s\\n⠿ Container pg-database  Started\\nTo view the containers use: docker ps.\\n(data-engineering-zoomcamp) hw % docker ps\\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                           NAMES\\nfaf05090972e   postgres:13      \"docker-entrypoint.s…\"   39 seconds ago   Up 37 seconds   0.0.0.0:5432->5432/tcp          pg-database\\n6344dcecd58f   dpage/pgadmin4   \"/entrypoint.sh\"         39 seconds ago   Up 37 seconds   443/tcp, 0.0.0.0:8080->80/tcp   pg-admin\\nhw\\nTo view logs for a container: docker logs <containerid>\\n(data-engineering-zoomcamp) hw % docker logs faf05090972e\\nPostgreSQL Database directory appears to contain a database; Skipping initialization\\n2022-01-25 05:58:45.948 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\\n2022-01-25 05:58:45.954 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\\n2022-01-25 05:58:45.984 UTC [28] LOG:  database system was interrupted; last known up at 2022-01-24 17:48:35 UTC\\n2022-01-25 05:58:48.581 UTC [28] LOG:  database system was not properly shut down; automatic recovery in\\nprogress\\n2022-01-25 05:58:48.602 UTC [28] LOG:  redo starts at 0/872A5910\\n2022-01-25 05:59:33.726 UTC [28] LOG:  invalid record length at 0/98A3C160: wanted 24, got 0\\n2022-01-25 05:59:33.726 UTC [28\\n] LOG:  redo done at 0/98A3C128\\n2022-01-25 05:59:48.051 UTC [1] LOG:  database system is ready to accept connections\\nIf docker ps doesn’t show pgdatabase running, run: docker ps -a\\nThis should show all containers, either running or stopped.\\nGet the container id for pgdatabase-1, and run', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Error translating host name to address', 'course': 'data-engineering-zoomcamp', 'id': 'e41b100c'}]), ('cd0f9300', [{'text': 'After executing `docker-compose up` - if you lose database data and are unable to successfully execute your Ingestion script (to re-populate your database) but receive the following error:\\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name /data_pgadmin:/var/lib/pgadmin\"pg-database\" to address: Name or service not known\\nDocker compose is creating its own default network since it is no longer specified in a docker execution command or file. Docker Compose will emit to logs the new network name. See the logs after executing `docker compose up` to find the network name and change the network name argument in your Ingestion script.\\nIf problems persist with pgcli, we can use HeidiSQL,usql\\nKrishna Anand', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose -  Data retention (could not translate host name \"pg-database\" to address: Name or service not known)', 'course': 'data-engineering-zoomcamp', 'id': 'cd0f9300'}]), ('7f845a1c', [{'text': 'It returns --> Error response from daemon: network 66ae65944d643fdebbc89bd0329f1409dec2c9e12248052f5f4c4be7d1bdc6a3 not found\\nTry:\\ndocker ps -a to see all the stopped & running containers\\nd to nuke all the containers\\nTry: docker-compose up -d again ports\\nOn localhost:8080 server → Unable to connect to server: could not translate host name \\'pg-database\\' to address: Name does not resolve\\nTry: new host name, best without “ - ” e.g. pgdatabase\\nAnd on docker-compose.yml, should specify docker network & specify the same network in both  containers\\nservices:\\npgdatabase:\\nimage: postgres:13\\nenvironment:\\n- POSTGRES_USER=root\\n- POSTGRES_PASSWORD=root\\n- POSTGRES_DB=ny_taxi\\nvolumes:\\n- \"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\\nports:\\n- \"5431:5432\"\\nnetworks:\\n- pg-network\\npgadmin:\\nimage: dpage/pgadmin4\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n- PGADMIN_DEFAULT_PASSWORD=root\\nports:\\n- \"8080:80\"\\nnetworks:\\n- pg-network\\nnetworks:\\npg-network:\\nname: pg-network', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Hostname does not resolve', 'course': 'data-engineering-zoomcamp', 'id': '7f845a1c'}]), ('36e54439', [{'text': 'So one common issue is when you run docker-compose on GCP, postgres won’t persist it’s data to mentioned path for example:\\nservices:\\n…\\n…\\npgadmin:\\n…\\n…\\nVolumes:\\n“./pgadmin”:/var/lib/pgadmin:wr”\\nMight not work so in this use you can use Docker Volume to make it persist, by simply changing\\nservices:\\n…\\n….\\npgadmin:\\n…\\n…\\nVolumes:\\npgadmin:/var/lib/pgadmin\\nvolumes:\\nPgadmin:', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Persist PGAdmin docker contents on GCP', 'course': 'data-engineering-zoomcamp', 'id': '36e54439'}]), ('32e8450c', [{'text': 'The docker will keep on crashing continuously\\nNot working after restart\\ndocker engine stopped\\nAnd failed to fetch extensions pop ups will on screen non-stop\\nSolution :\\nTry checking if latest version of docker is installed / Try updating the docker\\nIf Problem still persist then final solution is to reinstall docker\\n(Just have to fetch images again else no issues)', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker engine stopped_failed to fetch extensions', 'course': 'data-engineering-zoomcamp', 'id': '32e8450c'}]), ('96606db2', [{'text': 'As per the lessons,\\nPersisting pgAdmin configuration (i.e. server name) is done by adding a “volumes” section:\\nservices:\\npgdatabase:\\n[...]\\npgadmin:\\nimage: dpage/pgadmin4\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n- PGADMIN_DEFAULT_PASSWORD=root\\nvolumes:\\n- \"./pgAdmin_data:/var/lib/pgadmin/sessions:rw\"\\nports:\\n- \"8080:80\"\\nIn the example above, ”pgAdmin_data” is a folder on the host machine, and “/var/lib/pgadmin/sessions” is the session settings folder in the pgAdmin container.\\nBefore running docker-compose up on the YAML file, we also need to give the pgAdmin container access to write to the “pgAdmin_data” folder. The container runs with a username called “5050” and user group “5050”. The bash command to give access over the mounted volume is:\\nsudo chown -R 5050:5050 pgAdmin_data', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Persist PGAdmin configuration', 'course': 'data-engineering-zoomcamp', 'id': '96606db2'}]), ('0882bfac', [{'text': 'This happens if you did not create the docker group and added your user. Follow these steps from the link:\\nguides/docker-without-sudo.md at main · sindresorhus/guides · GitHub\\nAnd then press ctrl+D to log-out and log-in again. pgAdmin: Maintain state so that it remembers your previous connection\\nIf you are tired of having to setup your database connection each time that you fire up the containers, all you have to do is create a volume for pgAdmin:\\nIn your docker-compose.yaml file, enter the following into your pgAdmin declaration:\\nvolumes:\\n- type: volume\\nsource: pgadmin_data\\ntarget: /var/lib/pgadmin\\nAlso add the following to the end of the file:ls\\nvolumes:\\nPgadmin_data:', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - dial unix /var/run/docker.sock: connect: permission denied', 'course': 'data-engineering-zoomcamp', 'id': '0882bfac'}]), ('7d067f5c', [{'text': 'This is happen to me after following 1.4.1 video where we are installing docker compose in our Google Cloud VM. In my case, the docker-compose file downloaded from github named docker-compose-linux-x86_64 while it is more convenient to use docker-compose command instead. So just change the docker-compose-linux-x86_64 into docker-compose.', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - docker-compose still not available after changing .bashrc', 'course': 'data-engineering-zoomcamp', 'id': '7d067f5c'}]), ('ff352621', [{'text': 'Installing pass via ‘sudo apt install pass’ helped to solve the issue. More about this can be found here: https://github.com/moby/buildkit/issues/1078', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Error getting credentials after running docker-compose up -d', 'course': 'data-engineering-zoomcamp', 'id': 'ff352621'}]), ('2d653208', [{'text': \"For everyone who's having problem with Docker compose, getting the data in postgres and similar issues, please take care of the following:\\ncreate a new volume on docker (either using the command line or docker desktop app)\\nmake the following changes to your docker-compose.yml file (see attachment)\\nset low_memory=false when importing the csv file (df = pd.read_csv('yellow_tripdata_2021-01.csv', nrows=1000, low_memory=False))\\nuse the below function (in the upload-data.ipynb) for better tracking of your ingestion process (see attachment)\\nOrder of execution:\\n(1) open terminal in 2_docker_sql folder and run docker compose up\\n(2) ensure no other containers are running except the one you just executed (pgadmin and pgdatabase)\\n(3) open jupyter notebook and begin the data ingestion\\n(4) open pgadmin and set up a server (make sure you use the same configurations as your docker-compose.yml file like the same name (pgdatabase), port, databasename (ny_taxi) etc.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Errors pertaining to docker-compose.yml and pgadmin setup', 'course': 'data-engineering-zoomcamp', 'id': '2d653208'}]), ('f09ea61e', [{'text': 'Locate config.json file for docker (check your home directory; Users/username/.docker).\\nModify credsStore to credStore\\nSave and re-run', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker Compose up -d error getting credentials - err: exec: \"docker-credential-desktop\": executable file not found in %PATH%, out: ``', 'course': 'data-engineering-zoomcamp', 'id': 'f09ea61e'}]), ('fbd3d2bb', [{'text': 'To figure out which docker-compose you need to download from https://github.com/docker/compose/releases you can check your system with these commands:\\nuname -s  -> return Linux most likely\\nuname -m -> return \"flavor\"\\nOr try this command -\\nsudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Which docker-compose binary to use for WSL?', 'course': 'data-engineering-zoomcamp', 'id': 'fbd3d2bb'}]), ('0b014d0c', [{'text': 'If you wrote the docker-compose.yaml file exactly like the video, you might run into an error like this:dev\\nservice \"pgdatabase\" refers to undefined volume dtc_postgres_volume_local: invalid compose project\\nIn order to make it work, you need to include the volume in your docker-compose file. Just add the following:\\nvolumes:\\ndtc_postgres_volume_local:\\n(Make sure volumes are at the same level as services.)', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker-Compose - Error undefined volume in Windows/WSL', 'course': 'data-engineering-zoomcamp', 'id': '0b014d0c'}]), ('d21bff1d', [{'text': 'Error:  initdb: error: could not change permissions of directory\\nIssue: WSL and Windows do not manage permissions in the same way causing conflict if using the Windows file system rather than the WSL file system.\\nSolution: Use Docker volumes.\\nWhy: Volume is used for storage of persistent data and not for use of transferring files. A local volume is unnecessary.\\nBenefit: This resolves permission issues and allows for better management of volumes.\\nNOTE: the ‘user:’ is not necessary if using docker volumes, but is if using local drive.\\n</>  docker-compose.yaml\\nservices:\\npostgres:\\nimage: postgres:15-alpine\\ncontainer_name: postgres\\nuser: \"0:0\"\\nenvironment:\\n- POSTGRES_USER=postgres\\n- POSTGRES_PASSWORD=postgres\\n- POSTGRES_DB=ny_taxi\\nvolumes:\\n- \"pg-data:/var/lib/postgresql/data\"\\nports:\\n- \"5432:5432\"\\nnetworks:\\n- pg-network\\npgadmin:\\nimage: dpage/pgadmin4\\ncontainer_name: pgadmin\\nuser: \"${UID}:${GID}\"\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=email@some-site.com\\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\\nvolumes:\\n- \"pg-admin:/var/lib/pgadmin\"\\nports:\\n- \"8080:80\"\\nnetworks:\\n- pg-network\\nnetworks:\\npg-network:\\nname: pg-network\\nvolumes:\\npg-data:\\nname: ingest_pgdata\\npg-admin:\\nname: ingest_pgadmin', 'section': 'Module 1: Docker and Terraform', 'question': 'WSL Docker directory permissions error', 'course': 'data-engineering-zoomcamp', 'id': 'd21bff1d'}]), ('6afb7b55', [{'text': 'Cause : If Running on git bash or vm in windows pgadmin doesnt work easily LIbraries like psycopg2 and libpq ar required still the error persists.\\nSolution- I use psql instead of pgadmin totally same\\nPip install psycopg2\\ndock', 'section': 'Module 1: Docker and Terraform', 'question': 'Docker - If pgadmin is not working for Querying in Postgres Use PSQL', 'course': 'data-engineering-zoomcamp', 'id': '6afb7b55'}]), ('b51c3b82', [{'text': 'Cause:\\nIt happens because the apps are not updated. To be specific, search for any pending updates for Windows Terminal, WSL and Windows Security updates.\\nSolution\\nfor updating Windows terminal which worked for me:\\nGo to Microsoft Store.\\nGo to the library of apps installed in your system.\\nSearch for Windows terminal.\\nUpdate the app and restart your system to  see the changes.\\nFor updating the Windows security updates:\\nGo to Windows updates and check if there are any pending updates from Windows, especially security updates.\\nDo restart your system once the updates are downloaded and installed successfully.', 'section': 'Module 1: Docker and Terraform', 'question': 'WSL - Insufficient system resources exist to complete the requested service.', 'course': 'data-engineering-zoomcamp', 'id': 'b51c3b82'}]), ('326af690', [{'text': 'Up restardoting the same issue appears. Happens out of the blue on windows.\\nSolution 1: Fixing DNS Issue (credit: reddit) this worked for me personally\\nreg add \"HKLM\\\\System\\\\CurrentControlSet\\\\Services\\\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"4\" /f\\nRestart your computer and then enable it with the following\\nreg add \"HKLM\\\\System\\\\CurrentControlSet\\\\Services\\\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"2\" /f\\nRestart your OS again. It should work.\\nSolution 2: right click on running Docker icon (next to clock) and chose \"Switch to Linux containers\"\\nbash: conda: command not found\\nDatabase is uninitialized and superuser password is not specified.\\nDatabase is uninitialized and superuser password is not specified.', 'section': 'Module 1: Docker and Terraform', 'question': 'WSL - WSL integration with distro Ubuntu unexpectedly stopped with exit code 1.', 'course': 'data-engineering-zoomcamp', 'id': '326af690'}]), ('c2ec9047', [{'text': 'Issue when trying to run the GPC VM through SSH through WSL2,  probably because WSL2 isn’t looking for .ssh keys in the correct folder. My case I was trying to run this command in the terminal and getting an error\\nPC:/mnt/c/Users/User/.ssh$ ssh -i gpc [username]@[my external IP]\\nYou can try to use sudo before the command\\nSudo .ssh$ ssh -i gpc [username]@[my external IP]\\nYou can also try to cd to your folder and change the permissions for the private key SSH file.\\nchmod 600 gpc\\nIf that doesn’t work, create a .ssh folder in the home diretory of WSL2 and copy the content of windows .ssh folder to that new folder.\\ncd ~\\nmkdir .ssh\\ncp -r /mnt/c/Users/YourUsername/.ssh/* ~/.ssh/\\nYou might need to adjust the permissions of the files and folders in the .ssh directory.', 'section': 'Module 1: Docker and Terraform', 'question': 'WSL - Permissions too open at Windows', 'course': 'data-engineering-zoomcamp', 'id': 'c2ec9047'}]), ('3b711e73', [{'text': 'Such as the issue above, WSL2 may not be referencing the correct .ssh/config path from Windows. You can create a config file at the home directory of WSL2.\\ncd ~\\nmkdir .ssh\\nCreate a config file in this new .ssh/ folder referencing this folder:\\nHostName [GPC VM external IP]\\nUser [username]\\nIdentityFile ~/.ssh/[private key]', 'section': 'Module 1: Docker and Terraform', 'question': 'WSL - Could not resolve host name', 'course': 'data-engineering-zoomcamp', 'id': '3b711e73'}]), ('cfe07c9d', [{'text': 'Change TO Socket\\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi\\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - connection failed: :1), port 5432 failed: could not receive data from server: Connection refused could not send SSL negotiation packet: Connection refused', 'course': 'data-engineering-zoomcamp', 'id': 'cfe07c9d'}]), ('acf42bb8', [{'text': 'probably some installation error, check out sy', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI --help error', 'course': 'data-engineering-zoomcamp', 'id': 'acf42bb8'}]), ('176ce516', [{'text': 'In this section of the course, the 5432 port of pgsql is mapped to your computer’s 5432 port. Which means you can access the postgres database via pgcli directly from your computer.\\nSo No, you don’t need to run it inside another container. Your local system will do.', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - INKhould we run pgcli inside another docker container?', 'course': 'data-engineering-zoomcamp', 'id': '176ce516'}]), ('3e5d1e9b', [{'text': 'FATAL:  password authentication failed for user \"root\"\\nobservations: Below in bold do not forget the folder that was created ny_taxi_postgres_data\\nThis happens if you have a local Postgres installation in your computer. To mitigate this, use a different port, like 5431, when creating the docker container, as in: -p 5431: 5432\\nThen, we need to use this port when connecting to pgcli, as shown below:\\npgcli -h localhost -p 5431 -u root -d ny_taxi\\nThis will connect you to your postgres docker container, which is mapped to your host’s 5431 port (though you might choose any port of your liking as long as it is not occupied).\\nFor a more visual and detailed explanation, feel free to check the video 1.4.2 - Port Mapping and Networks in Docker\\nIf you want to debug: the following can help (on a MacOS)\\nTo find out if something is blocking your port (on a MacOS):\\nYou can use the lsof command to find out which application is using a specific port on your local machine. `lsof -i :5432`wi\\nOr list the running postgres services on your local machine with launchctl\\nTo unload the running service on your local machine (on a MacOS):\\nunload the launch agent for the PostgreSQL service, which will stop the service and free up the port  \\n`launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\\nthis one to start it again\\n`launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\\nChanging port from 5432:5432 to 5431:5432 helped me to avoid this error.', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - FATAL: password authentication failed for user \"root\" (You already have Postgres)', 'course': 'data-engineering-zoomcamp', 'id': '3e5d1e9b'}]), ('78833f32', [{'text': 'I get this error\\npgcli -h localhost -p 5432 -U root -d ny_taxi\\nTraceback (most recent call last):\\nFile \"/opt/anaconda3/bin/pgcli\", line 8, in <module>\\nsys.exit(cli())\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1128, in __call__\\nreturn self.main(*args, **kwargs)\\nFile \"/opt/anaconda3/lib/python3.9/sitYe-packages/click/core.py\", line\\n1053, in main\\nrv = self.invoke(ctx)\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1395, in invoke\\nreturn ctx.invoke(self.callback, **ctx.params)\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 754, in invoke\\nreturn __callback(*args, **kwargs)\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/pgcli/main.py\", line 880, in cli\\nos.makedirs(config_dir)\\nFile \"/opt/anaconda3/lib/python3.9/os.py\", line 225, in makedirspython\\nmkdir(name, mode)PermissionError: [Errno 13] Permission denied: \\'/Users/vray/.config/pgcli\\'\\nMake sure you install pgcli without sudo.\\nThe recommended approach is to use conda/anaconda to make sure your system python is not affected.\\nIf conda install gets stuck at \"Solving environment\" try these alternatives: https://stackoverflow.com/questions/63734508/stuck-at-solving-environment-on-anaconda', 'section': 'Module 1: Docker and Terraform', 'question': \"PGCLI - PermissionError: [Errno 13] Permission denied: '/some/path/.config/pgcli'\", 'course': 'data-engineering-zoomcamp', 'id': '78833f32'}]), ('63823f21', [{'text': 'ImportError: no pq wrapper available.\\nAttempts made:\\n- couldn\\'t import \\\\dt\\nopg \\'c\\' implementation: No module named \\'psycopg_c\\'\\n- couldn\\'t import psycopg \\'binary\\' implementation: No module named \\'psycopg_binary\\'\\n- couldn\\'t import psycopg \\'python\\' implementation: libpq library not found\\nSolution:\\nFirst, make sure your Python is set to 3.9, at least.\\nAnd the reason for that is we have had cases of \\'psycopg2-binary\\' failing to install because of an old version of Python (3.7.3). \\n\\n0. You can check your current python version with: \\n$ python -V(the V must be capital)\\n1. Based on the previous output, if you\\'ve got a 3.9, skip to Step #2\\n   Otherwispye better off with a new environment with 3.9\\n$ conda create –name de-zoomcamp python=3.9\\n$ conda activate de-zoomcamp\\n2. Next, you should be able to install the lib for postgres like this:\\n```\\n$ e\\n$ pip install psycopg2_binary\\n```\\n3. Finally, make sure you\\'re also installing pgcli, but use conda for that:\\n```\\n$ pgcli -h localhost -U root -d ny_taxisudo\\n```\\nThere, you should be good to go now!\\nAnother solution:\\nRun this\\npip install \"psycopg[binary,pool]\"', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - no pq wrapper available.', 'course': 'data-engineering-zoomcamp', 'id': '63823f21'}]), ('b36ea564', [{'text': 'If your Bash prompt is stuck on the password command for postgres\\nUse winpty:\\nwinpty pgcli -h localhost -p 5432 -u root -d ny_taxi\\nAlternatively, try using Windows terminal or terminal in VS code.\\nEditPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\\nThe error above was faced continually despite inputting the correct password\\nSolution\\nOption 1: Stop the PostgreSQL service on Windows\\nOption 2 (using WSL): Completely uninstall Protgres 12 from Windows and install postgresql-client on WSL (sudo apt install postgresql-client-common postgresql-client libpq-dev)\\nOption 3: Change the port of the docker container\\nNEW SOLUTION: 27/01/2024\\nPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\\nIf you’ve got the error above, it’s probably because you were just like me, closed the connection to the Postgres:13 image in the previous step of the tutorial, which is\\n\\ndocker run -it \\\\\\n-e POSTGRES_USER=root \\\\\\n-e POSTGRES_PASSWORD=root \\\\\\n-e POSTGRES_DB=ny_taxi \\\\\\n-v d:/git/data-engineering-zoomcamp/week_1/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nSo keep the database connected and you will be able to implement all the next steps of the tutorial.', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI -  stuck on password prompt', 'course': 'data-engineering-zoomcamp', 'id': 'b36ea564'}]), ('e2a46ce5', [{'text': 'Problem: If you have already installed pgcli but bash doesn\\'t recognize pgcli\\nOn Git bash: bash: pgcli: command not found\\nOn Windows Terminal: pgcli: The term \\'pgcli\\' is not recognized…\\nSolution: Try adding a Python path C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts to Windows PATH\\nFor details:\\nGet the location: pip list -v\\nCopy C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\n3. Replace site-packages with Scripts: C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts\\nIt can also be that you have Python installed elsewhere.\\nFor me it was under c:\\\\python310\\\\lib\\\\site-packages\\nSo I had to add c:\\\\python310\\\\lib\\\\Scripts to PATH, as shown below.\\nPut the above path in \"Path\" (or \"PATH\") in System Variables\\nReference: https://stackoverflow.com/a/68233660', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - pgcli: command not found', 'course': 'data-engineering-zoomcamp', 'id': 'e2a46ce5'}]), ('27bdbc3f', [{'text': 'In case running pgcli  locally causes issues or you do not want to install it locally you can use it running in a Docker container instead.\\nBelow the usage with values used in the videos of the course for:\\nnetwork name (docker network)\\npostgres related variables for pgcli\\nHostname\\nUsername\\nPort\\nDatabase name\\n$ docker run -it --rm --network pg-network ai2ys/dockerized-pgcli:4.0.1\\n175dd47cda07:/# pgcli -h pg-database -U root -p 5432 -d ny_taxi\\nPassword for root:\\nServer: PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1)\\nVersion: 4.0.1\\nHome: http://pgcli.com\\nroot@pg-database:ny_taxi> \\\\dt\\n+--------+------------------+-------+-------+\\n| Schema | Name             | Type  | Owner |\\n|--------+------------------+-------+-------|\\n| public | yellow_taxi_data | table | root  |\\n+--------+------------------+-------+-------+\\nSELECT 1\\nTime: 0.009s\\nroot@pg-database:ny_taxi>', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - running in a Docker container', 'course': 'data-engineering-zoomcamp', 'id': '27bdbc3f'}]), ('f7c5d8da', [{'text': 'PULocationID will not be recognized but “PULocationID” will be. This is because unquoted \"Localidentifiers are case insensitive. See docs.', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - case sensitive use “Quotations” around columns with capital letters', 'course': 'data-engineering-zoomcamp', 'id': 'f7c5d8da'}]), ('c91ad8f2', [{'text': 'When using the command `\\\\d <database name>` you get the error column `c.relhasoids does not exist`.\\nResolution:\\nUninstall pgcli\\nReinstall pgclidatabase \"ny_taxi\" does not exist\\nRestart pc', 'section': 'Module 1: Docker and Terraform', 'question': 'PGCLI - error column c.relhasoids does not exist', 'course': 'data-engineering-zoomcamp', 'id': 'c91ad8f2'}]), ('88bf31a0', [{'text': \"This happens while uploading data via the connection in jupyter notebook\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\\nThe port 5432 was taken by another postgres. We are not connecting to the port in docker, but to the port on our machine. Substitute 5431 or whatever port you mapped to for port 5432.\\nAlso if this error is still persistent , kindly check if you have a service in windows running postgres , Stopping that service will resolve the issue\", 'section': 'Module 1: Docker and Terraform', 'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"root\"', 'course': 'data-engineering-zoomcamp', 'id': '88bf31a0'}]), ('23524e6d', [{'text': 'Can happen when connecting via pgcli\\npgcli -h localhost -p 5432 -U root -d ny_taxi\\nOr while uploading data via the connection in jupyter notebook\\nengine = create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')\\nThis can happen when Postgres is already installed on your computer. Changing the port can resolve that (e.g. from 5432 to 5431).\\nTo check whether there even is a root user with the ability to login:\\nTry: docker exec -it <your_container_name> /bin/bash\\nAnd then run\\n???\\nAlso, you could change port from 5432:5432 to 5431:5432\\nOther solution that worked:\\nChanging `POSTGRES_USER=juroot` to `PGUSER=postgres`\\nBased on this: postgres with docker compose gives FATAL: role \"root\" does not exist error - Stack Overflow\\nAlso `docker compose down`, removing folder that had postgres volume, running `docker compose up` again.', 'section': 'Module 1: Docker and Terraform', 'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  role \"root\" does not exist', 'course': 'data-engineering-zoomcamp', 'id': '23524e6d'}]), ('9211bbd6', [{'text': '~\\\\anaconda3\\\\lib\\\\site-packages\\\\psycopg2\\\\__init__.py in connect(dsn, connection_factory, cursor_factory, **kwargs)\\n120\\n121     dsn = _ext.make_dsn(dsn, **kwargs)\\n--> 122     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\\n123     if cursor_factory is not None:\\n124         conn.cursor_factory = cursor_factory\\nOperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  database \"ny_taxi\" does not exist\\nMake sure postgres is running. You can check that by running `docker ps`\\n✅Solution: If you have postgres software installed on your computer before now, build your instance on a different port like 8080 instead of 5432', 'section': 'Module 1: Docker and Terraform', 'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  dodatabase \"ny_taxi\" does not exist', 'course': 'data-engineering-zoomcamp', 'id': '9211bbd6'}]), ('5db86809', [{'text': \"Issue:\\ne…\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\", 'section': 'Module 1: Docker and Terraform', 'question': \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\", 'course': 'data-engineering-zoomcamp', 'id': '5db86809'}]), ('20c604dd', [{'text': 'In the join queries, if we mention the column name directly or enclosed in single quotes it’ll throw an error says “column does not exist”.\\n✅Solution: But if we enclose the column names in double quotes then it will work', 'section': 'Module 1: Docker and Terraform', 'question': 'Postgres - \"Column does not exist\" but it actually does (Pyscopg2 error in MacBook Pro M2)', 'course': 'data-engineering-zoomcamp', 'id': '20c604dd'}]), ('b11b8c15', [{'text': 'pgAdmin has a new version. Create server dialog may not appear. Try using register-> server instead.', 'section': 'Module 1: Docker and Terraform', 'question': 'pgAdmin - Create server dialog does not appear', 'course': 'data-engineering-zoomcamp', 'id': 'b11b8c15'}]), ('a6475348', [{'text': 'Using GitHub Codespaces in the browser resulted in a blank screen after the login to pgAdmin (running in a Docker container). The terminal of the pgAdmin container was showing the following error message:\\nCSRFError: 400 Bad Request: The referrer does not match the host.\\nSolution #1:\\nAs recommended in the following issue  https://github.com/pgadmin-org/pgadmin4/issues/5432 setting the following environment variable solved it.\\nPGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\"\\nModified “docker run” command\\ndocker run --rm -it \\\\\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\\\\n-e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\\\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\\\\n-p \"8080:80\" \\\\\\n--name pgadmin \\\\\\n--network=pg-network \\\\\\ndpage/pgadmin4:8.2\\nSolution #2:\\nUsing the local installed VSCode to display GitHub Codespaces.\\nWhen using GitHub Codespaces in the locally installed VSCode (opening a Codespace or creating/starting one) this issue did not occur.', 'section': 'Module 1: Docker and Terraform', 'question': 'pgAdmin - Blank/white screen after login (browser)', 'course': 'data-engineering-zoomcamp', 'id': 'a6475348'}]), ('1ea7680e', [{'text': 'I am using a Mac Pro device and connect to the GCP Compute Engine via Remote SSH - VSCode. But when I trying to run the PgAdmin container via docker run or docker compose command, I am failed to access the pgAdmin address via my browser. I have switched to another browser, but still can not access the pgAdmin address. So I modified a little bit the configuration from the previous DE Zoomcamp repository like below and can access the pgAdmin address:\\nSolution #1:\\nModified “docker run” command\\ndocker run --rm -it \\\\\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\\\\n-e PGADMIN_DEFAULT_PASSWORD=\"pgadmin\" \\\\\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\\\\n-e PGADMIN_LISTEN_ADDRESS=0.0.0.0 \\\\\\n-e PGADMIN_LISTEN_PORT=5050 \\\\\\n-p 5050:5050 \\\\\\n--network=de-zoomcamp-network \\\\\\n--name pgadmin-container \\\\\\n--link postgres-container \\\\\\n-t dpage/pgadmin4\\nSolution #2:\\nModified docker-compose.yaml configuration (via “docker compose up” command)\\npgadmin:\\nimage: dpage/pgadmin4\\ncontainer_name: pgadmin-conntainer\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\\n- PGADMIN_CONFIG_WTF_CSRF_ENABLED=False\\n- PGADMIN_LISTEN_ADDRESS=0.0.0.0\\n- PGADMIN_LISTEN_PORT=5050\\nvolumes:\\n- \"./pgadmin_data:/var/lib/pgadmin/data\"\\nports:\\n- \"5050:5050\"\\nnetworks:\\n- de-zoomcamp-network\\ndepends_on:\\n- postgres-conntainer\\nPython - ModuleNotFoundError: No module named \\'pysqlite2\\'\\nImportError: DLL load failed while importing _sqlite3: The specified module could not be found. ModuleNotFoundError: No module named \\'pysqlite2\\'\\nThe issue seems to arise from the missing of sqlite3.dll in path \".\\\\Anaconda\\\\Dlls\\\\\".\\n✅I solved it by simply copying that .dll file from \\\\Anaconda3\\\\Library\\\\bin and put it under the path mentioned above. (if you are using anaconda)', 'section': 'Module 1: Docker and Terraform', 'question': 'pgAdmin - Can not access/open the PgAdmin address via browser', 'course': 'data-engineering-zoomcamp', 'id': '1ea7680e'}]), ('10acd478', [{'text': 'If you follow the video 1.2.2 - Ingesting NY Taxi Data to Postgres and you execute all the same\\nsteps as Alexey does, you will ingest all the data (~1.3 million rows) into the table yellow_taxi_data as expected.\\nHowever, if you try to run the whole script in the Jupyter notebook for a second time from top to bottom, you will be missing the first chunk of 100000 records. This is because there is a call to the iterator before the while loop that puts the data in the table. The while loop therefore starts by ingesting the second chunk, not the first.\\n✅Solution: remove the cell “df=next(df_iter)” that appears higher up in the notebook than the while loop. The first time w(df_iter) is called should be within the while loop.\\n📔Note: As this notebook is just used as a way to test the code, it was not intended to be run top to bottom, and the logic is tidied up in a later step when it is instead inserted into a .py file for the pipeline', 'section': 'Module 1: Docker and Terraform', 'question': 'Python - Ingestion with Jupyter notebook - missing 100000 records', 'course': 'data-engineering-zoomcamp', 'id': '10acd478'}]), ('752e8452', [{'text': '{t_end - t_start} seconds\")\\nimport pandas as pd\\ndf = pd.read_csv(\\'path/to/file.csv.gz\\', /app/ingest_data.py:1: DeprecationWarning:)\\nIf you prefer to keep the uncompressed csv (easier preview in vscode and similar), gzip files can be unzipped using gunzip (but not unzip). On a Ubuntu local or virtual machine, you may need to apt-get install gunzip first.', 'section': 'Module 1: Docker and Terraform', 'question': 'Python - Iteration csv without error', 'course': 'data-engineering-zoomcamp', 'id': '752e8452'}]), ('aa6f52b8', [{'text': \"Pandas can interpret “string” column values as “datetime” directly when reading the CSV file using “pd.read_csv” using the parameter “parse_dates”, which for example can contain a list of column names or column indices. Then the conversion afterwards is not required anymore.\\npandas.read_csv — pandas 2.1.4 documentation (pydata.org)\\nExample from week 1\\nimport pandas as pd\\ndf = pd.read_csv(\\n'yellow_tripdata_2021-01.csv',\\nnrows=100,\\nparse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\\ndf.info()\\nwhich will output\\n<class 'pandas.core.frame.DataFrame'>\\nRangeIndex: 100 entries, 0 to 99\\nData columns (total 18 columns):\\n#   Column                 Non-Null Count  Dtype\\n---  ------                 --------------  -----\\n0   VendorID               100 non-null    int64\\n1   tpep_pickup_datetime   100 non-null    datetime64[ns]\\n2   tpep_dropoff_datetime  100 non-null    datetime64[ns]\\n3   passenger_count        100 non-null    int64\\n4   trip_distance          100 non-null    float64\\n5   RatecodeID             100 non-null    int64\\n6   store_and_fwd_flag     100 non-null    object\\n7   PULocationID           100 non-null    int64\\n8   DOLocationID           100 non-null    int64\\n9   payment_type           100 non-null    int64\\n10  fare_amount            100 non-null    float64\\n11  extra                  100 non-null    float64\\n12  mta_tax                100 non-null    float64\\n13  tip_amount             100 non-null    float64\\n14  tolls_amount           100 non-null    float64\\n15  improvement_surcharge  100 non-null    float64\\n16  total_amount           100 non-null    float64\\n17  congestion_surcharge   100 non-null    float64\\ndtypes: datetime64[ns](2), float64(9), int64(6), object(1)\\nmemory usage: 14.2+ KB\", 'section': 'Module 1: Docker and Terraform', 'question': 'iPython - Pandas parsing dates with ‘read_csv’', 'course': 'data-engineering-zoomcamp', 'id': 'aa6f52b8'}]), ('3dacbb98', [{'text': 'os.system(f\"curl -LO {url} -o {csv_name}\")', 'section': 'Module 1: Docker and Terraform', 'question': 'Python - Python cant ingest data from the github link provided using curl', 'course': 'data-engineering-zoomcamp', 'id': '3dacbb98'}]), ('8b71a398', [{'text': 'When a CSV file is compressed using Gzip, it is saved with a \".csv.gz\" file extension. This file type is also known as a Gzip compressed CSV file. When you want to read a Gzip compressed CSV file using Pandas, you can use the read_csv() function, which is specifically designed to read CSV files. The read_csv() function accepts several parameters, including a file path or a file-like object. To read a Gzip compressed CSV file, you can pass the file path of the \".csv.gz\" file as an argument to the read_csv() function.\\nHere is an example of how to read a Gzip compressed CSV file using Pandas:\\ndf = pd.read_csv(\\'file.csv.gz\\'\\n, compression=\\'gzip\\'\\n, low_memory=False\\n)', 'section': 'Module 1: Docker and Terraform', 'question': 'Python - Pandas can read *.csv.gzip', 'course': 'data-engineering-zoomcamp', 'id': '8b71a398'}]), ('aa244fa0', [{'text': \"Contrary to panda’s read_csv method there’s no such easy way to iterate through and set chunksize for parquet files. We can use PyArrow (Apache Arrow Python bindings) to resolve that.\\nimport pyarrow.parquet as pq\\noutput_name = “https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet”\\nparquet_file = pq.ParquetFile(output_name)\\nparquet_size = parquet_file.metadata.num_rows\\nengine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\\ntable_name=”yellow_taxi_schema”\\n# Clear table if exists\\npq.read_table(output_name).to_pandas().head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\\n# default (and max) batch size\\nindex = 65536\\nfor i in parquet_file.iter_batches(use_threads=True):\\nt_start = time()\\nprint(f'Ingesting {index} out of {parquet_size} rows ({index / parquet_size:.0%})')\\ni.to_pandas().to_sql(name=table_name, con=engine, if_exists='append')\\nindex += 65536\\nt_end = time()\\nprint(f'\\\\t- it took %.1f seconds' % (t_end - t_start))\", 'section': 'Module 1: Docker and Terraform', 'question': 'Python - How to iterate through and ingest parquet file', 'course': 'data-engineering-zoomcamp', 'id': 'aa244fa0'}]), ('eac816d7', [{'text': 'Error raised during the jupyter notebook’s cell execution:\\nfrom sqlalchemy import create_engine.\\nSolution: Version of Python module “typing_extensions” >= 4.6.0. Can be updated by Conda or pip.', 'section': 'Module 1: Docker and Terraform', 'question': \"Python - SQLAlchemy - ImportError: cannot import name 'TypeAliasType' from 'typing_extensions'.\", 'course': 'data-engineering-zoomcamp', 'id': 'eac816d7'}]), ('d44d1c77', [{'text': 'create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \"TypeError: \\'module\\' object is not callable\"\\nSolution:\\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\\nengine = create_engine(conn_string)', 'section': 'Module 1: Docker and Terraform', 'question': \"Python - SQLALchemy - TypeError 'module' object is not callable\", 'course': 'data-engineering-zoomcamp', 'id': 'd44d1c77'}]), ('ed34766a', [{'text': \"Error raised during the jupyter notebook’s cell execution:\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\\nSolution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.\", 'section': 'Module 1: Docker and Terraform', 'question': \"Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\", 'course': 'data-engineering-zoomcamp', 'id': 'ed34766a'}]), ('fd714677', [{'text': 'Unable to add Google Cloud SDK PATH to Windows\\nWindows error: The installer is unable to automatically update your system PATH. Please add  C:\\\\tools\\\\google-cloud-sdk\\\\bin\\nif you are constantly getting this feedback. Might be that you needed to add Gitbash to your Windows path:\\nOne way of doing that is to use conda: ‘If you are not already using it\\nDownload the Anaconda Navigator\\nMake sure to check the box (add conda to the path when installing navigator: although not recommended do it anyway)\\nYou might also need to install git bash if you are not already using it(or you might need to uninstall it to reinstall it properly)\\nMake sure to check the following boxes while you install Gitbash\\nAdd a GitBash to Windows Terminal\\nUse Git and optional Unix tools from the command prompt\\nNow open up git bash and type conda init bash This should modify your bash profile\\nAdditionally, you might want to use Gitbash as your default terminal.\\nOpen your Windows terminal and go to settings, on the default profile change Windows power shell to git bash', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - Unable to add Google Cloud SDK PATH to Windows', 'course': 'data-engineering-zoomcamp', 'id': 'fd714677'}]), ('9de2c3e9', [{'text': 'It asked me to create a project. This should be done from the cloud console. So maybe we don’t need this FAQ.\\nWARNING: Project creation failed: HttpError accessing <https://cloudresourcemanager.googleapis.com/v1/projects?alt=json>: response: <{\\'vtpep_pickup_datetimeary\\': \\'Origin, X-Origin, Referer\\', \\'content-type\\': \\'application/json; charset=UTF-8\\', \\'content-encoding\\': \\'gzip\\', \\'date\\': \\'Mon, 24 Jan 2022 19:29:12 GMT\\', \\'server\\': \\'ESF\\', \\'cache-control\\': \\'private\\', \\'x-xss-protection\\': \\'0\\', \\'x-frame-options\\': \\'SAMEORIGIN\\', \\'x-content-type-options\\': \\'nosniff\\', \\'server-timing\\': \\'gfet4t7; dur=189\\', \\'alt-svc\\': \\'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"\\', \\'transfer-encoding\\': \\'chunked\\', \\'status\\': 409}>, content <{\\n\"error\": {\\n\"code\": 409,\\n\"message\": \"Requested entity alreadytpep_pickup_datetime exists\",\\n\"status\": \"ALREADY_EXISTS\"\\n}\\n}\\nFrom Stackoverflow: https://stackoverflow.com/questions/52561383/gcloud-cli-cannot-create-project-the-project-id-you-specified-is-already-in-us?rq=1\\nProject IDs are unique across all projects. That means if any user ever had a project with that ID, you cannot use it. testproject is pretty common, so it\\'s not surprising it\\'s already taken.', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - Project creation failed: HttpError accessing … Requested entity alreadytpep_pickup_datetime exists', 'course': 'data-engineering-zoomcamp', 'id': '9de2c3e9'}]), ('827dd4af', [{'text': 'If you receive the error: “Error 403: The project to be billed is associated with an absent billing account., accountDisabled” It is most likely because you did not enter YOUR project ID. The snip below is from video 1.3.2\\nThe value you enter here will be unique to each student. You can find this value on your GCP Dashboard when you login.\\nAshish Agrawal\\nAnother possibility is that you have not linked your billing account to your current project', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - The project to be billed is associated with an absent billing account', 'course': 'data-engineering-zoomcamp', 'id': '827dd4af'}]), ('a42a7e8c', [{'text': 'GCP Account Suspension Inquiry\\nIf Google refuses your credit/debit card, try another - I’ve got an issue with Kaspi (Kazakhstan) but it worked with TBC (Georgia).\\nUnfortunately, there’s small hope that support will help.\\nIt seems that Pyypl web-card should work too.', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - OR-CBAT-15 ERROR Google cloud free trial account', 'course': 'data-engineering-zoomcamp', 'id': 'a42a7e8c'}]), ('4eefdd01', [{'text': 'The ny-rides.json is your private file in Google Cloud Platform (GCP). \\n\\nAnd here’s the way to find it:\\nGCP -> Select project with your  instance -> IAM & Admin -> Service Accounts Keys tab -> add key, JSON as key type, then click create\\nNote: Once you go into Service Accounts Keys tab, click the email, then you can see the “KEYS” tab where you can add key as a JSON as its key type', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - Where can I find the “ny-rides.json” file?', 'course': 'data-engineering-zoomcamp', 'id': '4eefdd01'}]), ('0282578d', [{'text': 'In this lecture, Alexey deleted his instance in Google Cloud. Do I have to do it?\\nNope. Do not delete your instance in Google Cloud platform. Otherwise, you have to do this twice for the week 1 readings.', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - Do I need to delete my instance in Google Cloud?', 'course': 'data-engineering-zoomcamp', 'id': '0282578d'}]), ('bd3e60fd', [{'text': 'System Resource Usage:\\ntop or htop: Shows real-time information about system resource usage, including CPU, memory, and processes.\\nfree -h: Displays information about system memory usage and availability.\\ndf -h: Shows disk space usage of file systems.\\ndu -h <directory>: Displays disk usage of a specific directory.\\nRunning Processes:\\nps aux: Lists all running processes along with detailed information.\\nNetwork:\\nifconfig or ip addr show: Shows network interface configuration.\\nnetstat -tuln: Displays active network connections and listening ports.\\nHardware Information:\\nlscpu: Displays CPU information.\\nlsblk: Lists block devices (disks and partitions).\\nlshw: Lists hardware configuration.\\nUser and Permissions:\\nwho: Shows who is logged on and their activities.\\nw: Displays information about currently logged-in users and their processes.\\nPackage Management:\\napt list --installed: Lists installed packages (for Ubuntu and Debian-based systems)', 'section': 'Module 1: Docker and Terraform', 'question': 'Commands to inspect the health of your VM:', 'course': 'data-engineering-zoomcamp', 'id': 'bd3e60fd'}]), ('c4e9bc60', [{'text': 'if you’ve got the error\\n│ Error: Error updating Dataset \"projects/<your-project-id>/datasets/demo_dataset\": googleapi: Error 403: Billing has not been enabled for this project. Enable billing at https://console.cloud.google.com/billing. The default table expiration time must be less than 60 days, billingNotEnabled\\nbut you’ve set your billing account indeed, then try to disable billing for the project and enable it again. It worked for ME!', 'section': 'Module 1: Docker and Terraform', 'question': 'Billing account has not been enabled for this project. But you’ve done it indeed!', 'course': 'data-engineering-zoomcamp', 'id': 'c4e9bc60'}]), ('f10b49be', [{'text': 'for windows if you having trouble install SDK try follow these steps on the link, if you getting this error:\\nThese credentials will be used by any library that requests Application Default Credentials (ADC).\\nWARNING:\\nCannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\\nFor me:\\nI reinstalled the sdk using unzip file “install.bat”,\\nafter successfully checking gcloud version,\\nrun gcloud init to set up project before\\nyou run gcloud auth application-default login\\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md\\nGCP VM - I cannot get my Virtual Machine to start because GCP has no resources.\\nClick on your VM\\nCreate an image of your VM\\nOn the page of the image, tell GCP to create a new VM instance via the image\\nOn the settings page, change the location', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP - Windows Google Cloud SDK install issue:gcp', 'course': 'data-engineering-zoomcamp', 'id': 'f10b49be'}]), ('3184bd8b', [{'text': 'The reason this video about the GCP VM exists is that many students had problems configuring their env. You can use your own env if it works for you.\\nAnd the advantage of using your own environment is that if you are working in a Github repo where you can commit, you will be able to commit the changes that you do. In the VM the repo is cloned via HTTPS so it is not possible to directly commit, even if you are the owner of the repo.', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP VM - Is it necessary to use a GCP VM? When is it useful?', 'course': 'data-engineering-zoomcamp', 'id': '3184bd8b'}]), ('8bea4d53', [{'text': \"I am trying to create a directory but it won't let me do it\\nUser1@DESKTOP-PD6UM8A MINGW64 /\\n$ mkdir .ssh\\nmkdir: cannot create directory ‘.ssh’: Permission denied\\nYou should do it in your home directory. Should be your home (~)\\nLocal. But it seems you're trying to do it in the root folder (/). Should be your home (~)\\nLink to Video 1.4.1\", 'section': 'Module 1: Docker and Terraform', 'question': 'GCP VM - mkdir: cannot create directory ‘.ssh’: Permission denied', 'course': 'data-engineering-zoomcamp', 'id': '8bea4d53'}]), ('86d11cc0', [{'text': \"Failed to save '<file>': Unable to write file 'vscode-remote://ssh-remote+de-zoomcamp/home/<user>/data_engineering_course/week_2/airflow/dags/<file>' (NoPermissions (FileSystemError): Error: EACCES: permission denied, open '/home/<user>/data_engineering_course/week_2/airflow/dags/<file>')\\nYou need to change the owner of the files you are trying to edit via VS Code. You can run the following command to change the ownership.\\nssh\\nsudo chown -R <user> <path to your directory>\", 'section': 'Module 1: Docker and Terraform', 'question': 'GCP VM - Error while saving the file in VM via VS Code', 'course': 'data-engineering-zoomcamp', 'id': '86d11cc0'}]), ('2cb48591', [{'text': 'Question: I connected to my VM perfectly fine last week (ssh) but when I tried again this week, the connection request keeps timing out.\\n✅Answer: Start your VM. Once the VM is running, copy its External IP and paste that into your config file within the ~/.ssh folder.\\ncd ~/.ssh\\ncode config ← this opens the config file in VSCode', 'section': 'Module 1: Docker and Terraform', 'question': '. GCP VM - VM connection request timeout', 'course': 'data-engineering-zoomcamp', 'id': '2cb48591'}]), ('9523c813', [{'text': '(reference: https://serverfault.com/questions/953290/google-compute-engine-ssh-connect-to-host-ip-port-22-operation-timed-out)Go to edit your VM.\\nGo to section Automation\\nAdd Startup script\\n```\\n#!/bin/bash\\nsudo ufw allow ssh\\n```\\nStop and Start VM.', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP VM -  connect to host port 22 no route to host', 'course': 'data-engineering-zoomcamp', 'id': '9523c813'}]), ('4f8d9174', [{'text': 'You can easily forward the ports of pgAdmin, postgres and Jupyter Notebook using the built-in tools in Ubuntu and without any additional client:\\nFirst, in the VM machine, launch docker-compose up -d and jupyter notebook in the correct folder.\\nFrom the local machine, execute: ssh -i ~/.ssh/gcp -L 5432:localhost:5432 username@external_ip_of_vm\\nExecute the same command but with ports 8080 and 8888.\\nNow you can access pgAdmin on local machine in browser typing localhost:8080\\nFor Jupyter Notebook, type localhost:8888 in the browser of your local machine. If you have problems with the credentials, it is possible that you have to copy the link with the access token provided in the logs of the terminal of the VM machine when you launched the jupyter notebook command.\\nTo forward both pgAdmin and postgres use, ssh -i ~/.ssh/gcp -L 5432:localhost:5432 -L 8080:localhost:8080 modito@35.197.218.128', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP VM - Port forwarding from GCP without using VS Code', 'course': 'data-engineering-zoomcamp', 'id': '4f8d9174'}]), ('29f84a82', [{'text': 'If you are using MS VS Code and running gcloud in WSL2, when you first try to login to gcp via the gcloud cli gcloud auth application-default login, you will see a message like this, and nothing will happen\\nAnd there might be a prompt to ask if you want to open it via browser, if you click on it, it will open up a page with error message\\nSolution : you should instead hover on the long link, and ctrl + click the long link\\n\\nClick configure Trusted Domains here\\n\\nPopup will appear, pick first or second entry\\nNext time you gcloud auth, the login page should popup via default browser without issues', 'section': 'Module 1: Docker and Terraform', 'question': 'GCP gcloud + MS VS Code - gcloud auth hangs', 'course': 'data-engineering-zoomcamp', 'id': '29f84a82'}]), ('20a01fd0', [{'text': 'It is an internet connectivity error, terraform is somehow not able to access the online registry. Check your VPN/Firewall settings (or just clear cookies or restart your network). Try terraform init again after this, it should work.', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error: Failed to query available provider packages │ Could not retrieve the list of available versions for provider hashicorp/google: could not query │ provider registry for registry.terrafogorm.io/hashicorp/google: the request failed after 2 attempts, │ please try again later', 'course': 'data-engineering-zoomcamp', 'id': '20a01fd0'}]), ('5a712a20', [{'text': \"The issue was with the network. Google is not accessible in my country, I am using a VPN. And The terminal program does not automatically follow the system proxy and requires separate proxy configuration settings.I opened a Enhanced Mode in Clash, which is a VPN app, and 'terraform apply' works! So if you encounter the same issue, you can ask help for your vpn provider.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error:Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=coherent-ascent-379901\": oauth2: cannot fetch token: Post \"https://oauth2.googleapis.com/token\": dial tcp 172.217.163.42:443: i/o timeout', 'course': 'data-engineering-zoomcamp', 'id': '5a712a20'}]), ('06021091', [{'text': 'https://techcommunity.microsoft.com/t5/azure-developer-community-blog/configuring-terraform-on-windows-10-linux-sub-system/ba-p/393845', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Install for WSL', 'course': 'data-engineering-zoomcamp', 'id': '06021091'}]), ('df8ea7e8', [{'text': 'https://github.com/hashicorp/terraform/issues/14513', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error acquiring the state lock', 'course': 'data-engineering-zoomcamp', 'id': 'df8ea7e8'}]), ('1093daf5', [{'text': 'When running\\nterraform apply\\non wsl2 I\\'ve got this error:\\n│ Error: Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=<your-project-id>\": oauth2: cannot fetch token: 400 Bad Request\\n│ Response: {\"error\":\"invalid_grant\",\"error_description\":\"Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.\"}\\nIT happens because there may be time desync on your machine which affects computing JWT\\nTo fix this, run the command\\nsudo hwclock -s\\nwhich fixes your system time.\\nReference', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error 400 Bad Request.  Invalid JWT Token  on WSL.', 'course': 'data-engineering-zoomcamp', 'id': '1093daf5'}]), ('947213b1', [{'text': '│ Error: googleapi: Error 403: Access denied., forbidden\\nYour $GOOGLE_APPLICATION_CREDENTIALS might not be pointing to the correct file \\nrun = export GOOGLE_APPLICATION_CREDENTIALS=~/.gc/YOUR_JSON.json\\nAnd then = gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error 403 : Access denied', 'course': 'data-engineering-zoomcamp', 'id': '947213b1'}]), ('002d4943', [{'text': \"One service account is enough for all the services/resources you'll use in this course. After you get the file with your credentials and set your environment variable, you should be good to go.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Do I need to make another service account for terraform before I get the keys (.json file)?', 'course': 'data-engineering-zoomcamp', 'id': '002d4943'}]), ('8dc77677', [{'text': 'Here: https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Where can I find the Terraform 1.1.3 Linux (AMD 64)?', 'course': 'data-engineering-zoomcamp', 'id': '8dc77677'}]), ('29d3d343', [{'text': 'You get this error because I run the command terraform init outside the working directory, and this is wrong.You need first to navigate to the working directory that contains terraform configuration files, and and then run the command.', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Terraform initialized in an empty directory! The directory has no Terraform configuration files. You may begin working with Terraform immediately by creating Terraform configuration files.g', 'course': 'data-engineering-zoomcamp', 'id': '29d3d343'}]), ('e2095203', [{'text': 'The error:\\nError: googleapi: Error 403: Access denied., forbidden\\n│\\nand\\n│ Error: Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes.\\nFor this solution make sure to run:\\necho $GOOGLE_APPLICATION_CREDENTIALS\\necho $?\\nSolution:\\nYou have to set again the GOOGLE_APPLICATION_CREDENTIALS as Alexey did in the environment set-up video in week1:\\nexport GOOGLE_APPLICATION_CREDENTIALS=\"<path/to/your/service-account-authkeys>.json', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes', 'course': 'data-engineering-zoomcamp', 'id': 'e2095203'}]), ('22a2b9f2', [{'text': \"The error:\\nError: googleapi: Error 403: terraform-trans-campus@trans-campus-410115.iam.gserviceaccount.com does not have storage.buckets.create access to the Google Cloud project. Permission 'storage.buckets.create' denied on resource (or it may not exist)., forbidden\\nThe solution:\\nYou have to declare the project name as your Project ID, and not your Project name, available on GCP console Dashboard.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error creating Bucket: googleapi: Error 403: Permission denied to access ‘storage.buckets.create’', 'course': 'data-engineering-zoomcamp', 'id': '22a2b9f2'}]), ('5d7588f0', [{'text': 'provider \"google\" {\\nproject     = var.projectId\\ncredentials = file(\"${var.gcpkey}\")\\n#region      = var.region\\nzone = var.zone\\n}', 'section': 'Module 1: Docker and Terraform', 'question': 'To ensure the sensitivity of the credentials file, I had to spend lot of time to input that as a file.', 'course': 'data-engineering-zoomcamp', 'id': '5d7588f0'}]), ('5276a695', [{'text': 'For the HW1 I encountered this issue. The solution is\\nSELECT * FROM zones AS z WHERE z.\"Zone\" = \\'Astoria Zone\\';\\nI think columns which start with uppercase need to go between “Column”. I ran into a lot of issues like this and “ ” made it work out.\\nAddition to the above point, for me, there is no ‘Astoria Zone’, only ‘Astoria’ is existing in the dataset.\\nSELECT * FROM zones AS z WHERE z.\"Zone\" = \\'Astoria’;', 'section': 'Module 1: Docker and Terraform', 'question': \"SQL - SELECT * FROM zones_taxi WHERE Zone='Astoria Zone'; Error Column Zone doesn't exist\", 'course': 'data-engineering-zoomcamp', 'id': '5276a695'}]), ('70c159df', [{'text': 'It is inconvenient to use quotation marks all the time, so it is better to put the data to the database all in lowercase, so in Pandas after\\ndf = pd.read_csv(‘taxi+_zone_lookup.csv’)\\nAdd the row:\\ndf.columns = df.columns.str.lower()', 'section': 'Module 1: Docker and Terraform', 'question': \"SQL - SELECT Zone FROM taxi_zones Error Column Zone doesn't exist\", 'course': 'data-engineering-zoomcamp', 'id': '70c159df'}]), ('f55efcf0', [{'text': 'Solution (for mac users): os.system(f\"curl {url} --output {csv_name}\")', 'section': 'Module 1: Docker and Terraform', 'question': 'CURL - curl: (6) Could not resolve host: output.csv', 'course': 'data-engineering-zoomcamp', 'id': 'f55efcf0'}]), ('2b7a8512', [{'text': 'To resolve this, ensure that your config file is in C/User/Username/.ssh/config', 'section': 'Module 1: Docker and Terraform', 'question': 'SSH Error: ssh: Could not resolve hostname linux: Name or service not known', 'course': 'data-engineering-zoomcamp', 'id': '2b7a8512'}]), ('1cd746c4', [{'text': 'If you use Anaconda (recommended for the course), it comes with pip, so the issues is probably that the anaconda’s Python is not on the PATH.\\nAdding it to the PATH is different for each operation system.\\nFor Linux and MacOS:\\nOpen a terminal.\\nFind the path to your Anaconda installation. This is typically `~/anaconda3` or `~/opt/anaconda3`.\\nAdd Anaconda to your PATH with the command: `export PATH=\"/path/to/anaconda3/bin:$PATH\"`.\\nTo make this change permanent, add the command to your `.bashrc` (Linux) or `.bash_profile` (MacOS) file.\\nOn Windows, python and pip are in different locations (python is in the anaconda root, and pip is in Scripts). With GitBash:\\nLocate your Anaconda installation. The default path is usually `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3`.\\nDetermine the correct path format for Git Bash. Paths in Git Bash follow the Unix-style, so convert the Windows path to a Unix-style path. For example, `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3` becomes `/c/Users/[YourUsername]/Anaconda3`.\\nAdd Anaconda to your PATH with the command: `export PATH=\"/c/Users/[YourUsername]/Anaconda3/:/c/Users/[YourUsername]/Anaconda3/Scripts/$PATH\"`.\\nTo make this change permanent, add the command to your `.bashrc` file in your home directory.\\nRefresh your environment with the command: `source ~/.bashrc`.\\nFor Windows (without Git Bash):\\nRight-click on \\'This PC\\' or \\'My Computer\\' and select \\'Properties\\'.\\nClick on \\'Advanced system settings\\'.\\nIn the System Properties window, click on \\'Environment Variables\\'.\\nIn the Environment Variables window, select the \\'Path\\' variable in the \\'System variables\\' section and click \\'Edit\\'.\\nIn the Edit Environment Variable window, click \\'New\\' and add the path to your Anaconda installation (typically `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3` and C:\\\\Users\\\\[YourUsername]\\\\Anaconda3\\\\Scripts`).\\nClick \\'OK\\' in all windows to apply the changes.\\nAfter adding Anaconda to the PATH, you should be able to use `pip` from the command line. Remember to restart your terminal (or command prompt in Windows) to apply these changes.', 'section': 'Module 1: Docker and Terraform', 'question': \"'pip' is not recognized as an internal or external command, operable program or batch file.\", 'course': 'data-engineering-zoomcamp', 'id': '1cd746c4'}]), ('6d367222', [{'text': \"Resolution: You need to stop the services which is using the port.\\nRun the following:\\n```\\nsudo kill -9 `sudo lsof -t -i:<port>`\\n```\\n<port> being 8080 in this case. This will free up the port for use.\\n~ Abhijit Chakraborty\\nError: error response from daemon: cannot stop container: 1afaf8f7d52277318b71eef8f7a7f238c777045e769dd832426219d6c4b8dfb4: permission denied\\nResolution: In my case, I had to stop docker and restart the service to get it running properly\\nUse the following command:\\n```\\nsudo systemctl restart docker.socket docker.service\\n```\\n~ Abhijit Chakraborty\\nError: cannot import module psycopg2\\nResolution: Run the following command in linux:\\n```\\nsudo apt-get install libpq-dev\\npip install psycopg2\\n```\\n~ Abhijit Chakraborty\\nError: docker build Error checking context: 'can't stat '<path-to-file>'\\nResolution: This happens due to insufficient permission for docker to access a certain file within the directory which hosts the Dockerfile.\\n1. You can create a .dockerignore file and add the directory/file which you want Dockerfile to ignore while build.\\n2. If the above does not work, then put the dockerfile and corresponding script, `\\t1.py` in our case to a subfolder. and run `docker build ...`\\nfrom inside the new folder.\\n~ Abhijit Chakraborty\", 'section': 'Module 1: Docker and Terraform', 'question': 'Error: error starting userland proxy: listen tcp4 0.0.0.0:8080: bind: address already in use', 'course': 'data-engineering-zoomcamp', 'id': '6d367222'}]), ('84e601e1', [{'text': 'To get a pip-friendly requirements.txt file file from Anaconda use\\nconda install pip then `pip list –format=freeze > requirements.txt`.\\n`conda list -d > requirements.txt` will not work and `pip freeze > requirements.txt` may give odd pathing.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Anaconda to PIP', 'course': 'data-engineering-zoomcamp', 'id': '84e601e1'}]), ('4cf83cc2', [{'text': 'Prefect: https://docs.google.com/document/d/1K_LJ9RhAORQk3z4Qf_tfGQCDbu8wUWzru62IUscgiGU/edit?usp=sharing\\nAirflow: https://docs.google.com/document/d/1-BwPAsyDH_mAsn8HH5z_eNYVyBMAtawJRjHHsjEKHyY/edit?usp=sharing', 'section': 'Module 2: Workflow Orchestration', 'question': 'Where are the FAQ questions from the previous cohorts for the orchestration module?', 'course': 'data-engineering-zoomcamp', 'id': '4cf83cc2'}]), ('5adc5188', [{'text': 'Issue : Docker containers exit instantly with code 132, upon docker compose up\\nMage documentation has it listing the cause as \"older architecture\" .\\nThis might be a hardware issue, so unless you have another computer, you can\\'t solve it without purchasing a new one, so the next best solution is a VM.\\nThis is from a student running on a VirtualBox VM, Ubuntu 22.04.3 LTS, Docker version 25.0.2. So not having the context on how the vbox was spin up with (CPU, RAM, network, etc), it’s really inconclusive at this time.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Docker - 2.2.2 Configure Mage', 'course': 'data-engineering-zoomcamp', 'id': '5adc5188'}]), ('3ef0bb96', [{'text': 'This issue was occurring with Windows WSL 2\\nFor me this was because WSL 2 was not dedicating enough cpu cores to Docker.The load seems to take up at least one cpu core so I recommend dedicating at least two.\\nOpen Bash and run the following code:\\n$ cd ~\\n$ ls -la\\nLook for the .wsl config file:\\n-rw-r--r-- 1 ~1049089       31 Jan 25 12:54  .wslconfig\\nUsing a text editing tool of your choice edit or create your .wslconfig file:\\n$ nano .wslconfig\\nPaste the following into the new file/ edit the existing file in this format and save:\\n*** Note - for memory– this is the RAM on your machine you can dedicate to Docker, your situation may be different than mine ***\\n[wsl2]\\nprocessors=<Number of Processors - at least 2!> example: 4\\nmemory=<memory> example:4GB\\nExample:\\nOnce you do that run:\\n$ wsl --shutdown\\nThis shuts down WSL\\nThen Restart Docker Desktop - You should now be able to load the .csv.gz file without the error into a pandas dataframe', 'section': 'Module 2: Workflow Orchestration', 'question': 'WSL - 2.2.3 Mage - Unexpected Kernel Restarts; Kernel Running out of memory:', 'course': 'data-engineering-zoomcamp', 'id': '3ef0bb96'}]), ('a41ce360', [{'text': 'The issue and solution on the link:\\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1706817366764269?thread_ts=1706815324.993529&cid=C01FABYF2RG', 'section': 'Module 2: Workflow Orchestration', 'question': '2.2.3 Configuring Postgres', 'course': 'data-engineering-zoomcamp', 'id': 'a41ce360'}]), ('b1cf59e5', [{'text': 'Check that the POSTGRES_PORT variable in the io_config.yml  file is set to port 5432, which is the default postgres port. The POSTGRES_PORT variable is the mage container port, not the host port. Hence, there’s no need to set the POSTGRES_PORT to 5431 just because you already have a conflicting postgres installation in your host machine.', 'section': 'Module 2: Workflow Orchestration', 'question': 'MAGE - 2.2.3 OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5431 failed: Connection refused', 'course': 'data-engineering-zoomcamp', 'id': 'b1cf59e5'}]), ('f9d6f8bd', [{'text': 'You forgot to select ‘dev’ profile in the dropdown menu next to where you select ‘PostgreSQL’ in the connection drop down.', 'section': 'Module 2: Workflow Orchestration', 'question': 'MAGE - 2.2.4 executing SELECT 1; results in KeyError', 'course': 'data-engineering-zoomcamp', 'id': 'f9d6f8bd'}]), ('f3adb937', [{'text': 'If you are getting this error. Update your mage io_config.yaml file, and specify a timeout value set to 600 like this.\\nMake sure to save your changes.\\nMAGE - 2.2.4 Testing BigQuery connection using SQL 404 error:\\nNotFound: 404 Not found: Dataset ny-rides-diegogutierrez:None was not found in location northamerica-northeast1\\nIf you get this error even with all roles/permissions given to the service account check if you have ticked the box where it says “Use raw SQL”, just like the image below.', 'section': 'Module 2: Workflow Orchestration', 'question': \"MAGE -2.2.4 ConnectionError: ('Connection aborted.', TimeoutError('The write operation timed out'))\", 'course': 'data-engineering-zoomcamp', 'id': 'f3adb937'}]), ('eb3d6d36', [{'text': 'Solution: https://stackoverflow.com/questions/48056381/google-client-invalid-jwt-token-must-be-a-short-lived-token', 'section': 'Module 2: Workflow Orchestration', 'question': \"Problem: RefreshError: ('invalid_grant: Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.'})\", 'course': 'data-engineering-zoomcamp', 'id': 'eb3d6d36'}]), ('a76e1f4d', [{'text': \"Origin of Solution (Mage Slack-Channel): https://mageai.slack.com/archives/C03HTTWFEKE/p1706543947795599\\nProblem: This error can often be seen after solving the error mentioned in 2.2.4. The error can be found in Mage version 0.9.61 and is a side-effect of the update of the code for data-loader blocks.\\nNote: Mage 0.9.62 has been released, as of Feb 5 2024. Please recheck. Solution below may be obsolete\\nSolution: Using a “fixed” version of the docker container\\nPull updated docker image from docker-hub\\nmageai/mageaidocker pull:alpha\\nUpdate docker-compose.yaml\\nversion: '3'\\nservices:\\nmagic:\\nimage: mageai/mageai:alpha  <--- instead of “latest”-tag\\ndocker-compose up\\nThe original Error is still present, but the SQL-query will return the desired result:\\n--------------------------------------------------------------------------------------\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage - 2.2.4 IndexError: list index out of range', 'course': 'data-engineering-zoomcamp', 'id': 'a76e1f4d'}]), ('934facf8', [{'text': 'Add\\nif not path.parent.is_dir():\\npath.parent.mkdir(parents=True)\\npath = Path(path).as_posix()\\nsee:\\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1675774214591809?thread_ts=1675768839.028879&cid=C01FABYF2RG', 'section': 'Module 2: Workflow Orchestration', 'question': '2.2.6 OSError: Cannot save file into a non-existent directory: \\'..\\\\\\\\..\\\\\\\\data\\\\\\\\yellow\\'\\\\n\")', 'course': 'data-engineering-zoomcamp', 'id': '934facf8'}]), ('a2c7b59f', [{'text': 'The video DE Zoomcamp 2.2.7 is missing  the actual deployment of Mage using Terraform to GCP. The steps for the deployment were not covered in the video.\\nI successfully deployed it and wanted to share some key points:\\nIn variables.tf, set the project_id default value to your GCP project ID.\\nEnable the Cloud Filestore API:\\nVisit the Google Cloud Console.to\\nNavigate to \"APIs & Services\" > \"Library.\"\\nSearch for \"Cloud Filestore API.\"\\nClick on the API and enable it.\\nTo perform the deployment:\\nterraform init\\nterraform apply\\nPlease note that during the terraform apply step, Terraform will prompt you to enter the PostgreSQL password. After that, it will ask for confirmation to proceed with the deployment. Review the changes, type \\'yes\\' when prompted, and press Enter.', 'section': 'Module 2: Workflow Orchestration', 'question': 'GCP - 2.2.7d Deploying Mage to GCP', 'course': 'data-engineering-zoomcamp', 'id': 'a2c7b59f'}]), ('997d4aaa', [{'text': 'If you want to rune multiple docker containers from different directories. Then make sure to change the port mappings in the docker-compose.yml file.\\nports:\\n- 8088:6789\\nThe 8088 port in above case is hostport, where mage will run on your local machine. You can customize this as long as the port is available. If you are running on VM, make sure to forward the port too. You need to keep the container port to 6789 as this is the port where mage is running.\\nGCP - 2.2.7d Deploying Mage to Google Cloud\\nWhile terraforming all the resources inside a VM created in GCS the following error is shown.\\nError log:\\nmodule.lb-http.google_compute_backend_service.default[\"default\"]: Creating...\\n╷\\n│ Error: Error creating GlobalAddress: googleapi: Error 403: Request had insufficient authentication scopes.\\n│ Details:\\n│ [\\n│   {\\n│     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\\n│     \"domain\": \"googleapis.com\",\\n│     \"metadatas\": {\\n│       \"method\": \"compute.beta.GlobalAddressesService.Insert\",\\n│       \"service\": \"compute.googleapis.com\"\\n│     },\\n│     \"reason\": \"ACCESS_TOKEN_SCOPE_INSUFFICIENT\"\\n│   }\\n│ ]\\n│\\n│ More details:\\n│ Reason: insufficientPermissions, Message: Insufficient Permission\\nThis error might happen when you are using a VM inside GCS. To use the Google APIs from a GCP virtual machine you need to add the cloud platform scope (\"https://www.googleapis.com/auth/cloud-platform\") to your VM when it is created.\\nSince ours is already created you can just stop it and change the permissions. You can do it in the console, just go to \"EDIT\", g99o all the way down until you find \"Cloud API access scopes\". There you can \"Allow full access to all Cloud APIs\". I did this and all went smoothly generating all the resources needed. Hope it helps if you encounter this same error.\\nResources: https://stackoverflow.com/questions/35928534/403-request-had-insufficient-authentication-scopes-during-gcloud-container-clu', 'section': 'Module 2: Workflow Orchestration', 'question': 'Ruuning Multiple Mage instances in Docker from different directories', 'course': 'data-engineering-zoomcamp', 'id': '997d4aaa'}]), ('bc269b95', [{'text': 'If you are on the free trial account on GCP you will face this issue when trying to deploy the infrastructures with terraform. This service is not available for this kind of account.\\nThe solution I found was to delete the load_balancer.tf file and to comment or delete the rows that differentiate it on the main.tf file. After this just do terraform destroy to delete any infrastructure created on the fail attempts and re-run the terraform apply.\\nCode on main.tf to comment/delete:\\nLine 166, 167, 168', 'section': 'Module 2: Workflow Orchestration', 'question': 'GCP - 2.2.7d Load Balancer Problem (Security Policies quota)', 'course': 'data-engineering-zoomcamp', 'id': 'bc269b95'}]), ('10ea342e', [{'text': \"If you get the following error\\nYou have to edit variables.tf on the gcp folder, set your project-id and region and zones properly. Then, run terraform apply again.\\nYou can find correct regions/zones here: https://cloud.google.com/compute/docs/regions-zones\\nDeploying MAGE to GCP  with Terraform via the VM (2.2.7)\\nFYI - It can take up to 20 minutes to deploy the MAGE Terraform files if you are using a GCP Virtual Machine. It is normal, so don’t interrupt the process or think it’s taking too long. If you have, make sure you run a terraform destroy before trying again as you will have likely partially created resources which will cause errors next time you run `terraform apply`.\\n`terraform destroy` may not completely delete partial resources - go to Google Cloud Console and use the search bar at the top to search for the ‘app.name’ you declared in your variables.tf file; this will list all resources with that name - make sure you delete them all before running `terraform apply` again.\\nWhy are my GCP free credits going so fast? MAGE .tf files - Terraform Destroy not destroying all Resources\\nI checked my GCP billing last night & the MAGE Terraform IaC didn't destroy a GCP Resource called Filestore as ‘mage-data-prep- it has been costing £5.01 of my free credits each day  I now have £151 left - Alexey has assured me that This amount WILL BE SUFFICIENT funds to finish the course. Note to anyone who had issues deploying the MAGE terraform code: check your billing account to see what you're being charged for (main menu - billing) (even if it's your free credits) and run a search for 'mage-data-prep' in the top bar just to be sure that your resources have been destroyed - if any come up delete them.\", 'section': 'Module 2: Workflow Orchestration', 'question': 'GCP - 2.2.7d Part 2 - Getting error when you run terraform apply', 'course': 'data-engineering-zoomcamp', 'id': '10ea342e'}]), ('4bd23594', [{'text': '```\\n│ Error: Error creating Connector: googleapi: Error 403: Permission \\'vpcaccess.connectors.create\\' denied on resource \\'//vpcaccess.googleapis.com/projects/<ommit>/locations/us-west1\\' (or it may not exist).\\n│ Details:\\n│ [\\n│   {\\n│     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\\n│     \"domain\": \"vpcaccess.googleapis.com\",\\n│     \"metadata\": {\\n│       \"permission\": \"vpcaccess.connectors.create\",\\n│       \"resource\": \"projects/<ommit>/locations/us-west1\"\\n│     },\\n│     \"reason\": \"IAM_PERMISSION_DENIED\"\\n│   }\\n│ ]\\n│\\n│   with google_vpc_access_connector.connector,\\n│   on fs.tf line 19, in resource \"google_vpc_access_connector\" \"connector\":\\n│   19: resource \"google_vpc_access_connector\" \"connector\" {\\n│\\n```\\nSolution: Add Serverless VPC Access Admin to Service Account.\\nLine 148', 'section': 'Module 2: Workflow Orchestration', 'question': \"Question: Permission 'vpcaccess.connectors.create'\", 'course': 'data-engineering-zoomcamp', 'id': '4bd23594'}]), ('b0d48cd7', [{'text': 'Git won’t push an empty folder to GitHub, so if you put a file in that folder and then push, then you should be good to go.\\nOr - in your code- make the folder if it doesn’t exist using Pathlib as shown here: https://stackoverflow.com/a/273227/4590385.\\nFor some reason, when using github storage, the relative path for writing locally no longer works. Try using two separate paths, one full path for the local write, and the original relative path for GCS bucket upload.', 'section': 'Module 2: Workflow Orchestration', 'question': \"File Path: Cannot save file into a non-existent directory: 'data/green'\", 'course': 'data-engineering-zoomcamp', 'id': 'b0d48cd7'}]), ('70a37f2c', [{'text': 'The green dataset contains lpep_pickup_datetime while the yellow contains tpep_pickup_datetime. Modify the script(s) depending on  the dataset as required.', 'section': 'Module 2: Workflow Orchestration', 'question': 'No column name lpep_pickup_datetime / tpep_pickup_datetime', 'course': 'data-engineering-zoomcamp', 'id': '70a37f2c'}]), ('8ab78bee', [{'text': 'pd.read_csv\\ndf_iter = pd.read_csv(dataset_url, iterator=True, chunksize=100000)\\nThe data needs to be appended to the parquet file using the fastparquet engine\\ndf.to_parquet(path, compression=\"gzip\", engine=\\'fastparquet\\', append=True)', 'section': 'Module 2: Workflow Orchestration', 'question': 'Process to download the VSC using Pandas is killed right away', 'course': 'data-engineering-zoomcamp', 'id': '8ab78bee'}]), ('54c6db2f', [{'text': 'denied: requested access to the resource is denied\\nThis can happen when you\\nHaven\\'t logged in properly to Docker Desktop (use docker login -u \"myusername\")\\nHave used the wrong username when pushing to docker images. Use the same one as your username and as the one you build on\\ndocker image build -t <myusername>/<imagename>:<tag>\\ndocker image push <myusername>/<imagename>:<tag>', 'section': 'Module 2: Workflow Orchestration', 'question': 'Push to docker image failure', 'course': 'data-engineering-zoomcamp', 'id': '54c6db2f'}]), ('c5b998f3', [{'text': \"16:21:35.607 | INFO    | Flow run 'singing-malkoha' - Executing 'write_bq-b366772c-0' immediately...\\nKilled\\nSolution:  You probably are running out of memory on your VM and need to add more.  For example, if you have 8 gigs of RAM on your VM, you may want to expand that to 16 gigs.\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Flow script fails with “killed” message:', 'course': 'data-engineering-zoomcamp', 'id': 'c5b998f3'}]), ('eec29536', [{'text': 'After playing around with prefect for a while this can happen.\\nSsh to your VM and run sudo du -h --block-size=G | sort -n -r | head -n 30 to see which directory needs the most space.\\nMost likely it will be …/.prefect/storage, where your cached flows are stored. You can delete older flows from there. You also have to delete the corresponding flow in the UI, otherwise it will throw you an error, when you try to run your next flow.\\nSSL Certificate Verify: (I got it when trying to run flows on MAC): urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]\\npip install certifi\\n/Applications/Python\\\\ {ver}/Install\\\\ Certificates.command\\nor\\nrunning the “Install Certificate.command” inside of the python{ver} folder', 'section': 'Module 2: Workflow Orchestration', 'question': 'GCP VM: Disk Space is full', 'course': 'data-engineering-zoomcamp', 'id': 'eec29536'}]), ('727e5a69', [{'text': 'It means your container consumed all available RAM allocated to it. It can happen in particular when working on Question#3 in the homework as the dataset is relatively large and containers eat a lot of memory in general.\\nI would recommend restarting your computer and only starting the necessary processes to run the container. If that doesn’t work, allocate more resources to docker. If also that doesn’t work because your workstation is a potato, you can use an online compute environment service like GitPod, which is free under under 50 hours / month of use.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Docker: container crashed with status code 137.', 'course': 'data-engineering-zoomcamp', 'id': '727e5a69'}]), ('da899638', [{'text': 'In Q3 there was a task to run the etl script from web to GCS. The problem was, it wasn’t really an ETL straight from web to GCS, but it was actually a web to local storage to local memory to GCS over network ETL. Yellow data is about 100 MB each per month compressed and ~700 MB after uncompressed on memory\\nThis leads to a problem where i either got a network type error because my not so good 3rd world internet or i got my WSL2 crashed/hanged because out of memory error and/or 100% resource usage hang.\\nSolution:\\nif you have a lot of time at hand, try compressing it to parquet and writing it to GCS with the timeout argument set to a really high number (the default os 60 seconds)\\nthe yellow taxi data for feb 2019 is about 100MB as parquet file\\ngcp_cloud_storage_bucket_block.upload_from_path(\\nfrom_path=f\"{path}\",\\nto_path=path,\\ntimeout=600\\n)', 'section': 'Module 2: Workflow Orchestration', 'question': 'Timeout due to slow upload internet', 'course': 'data-engineering-zoomcamp', 'id': 'da899638'}]), ('dde58c8f', [{'text': 'This error occurs when you try to re-run the export block, of the transformed green_taxi data to PostgreSQL.\\nWhat you’ll need to do is to drop the table using SQL in Mage (screenshot below).\\nYou should be able to re-run the block successfully after dropping the table.', 'section': 'Module 2: Workflow Orchestration', 'question': 'UndefinedColumn: column \"ratecode_id\", \"rate_code_id\" “vendor_id”, “pu_location_id”, “do_location_id” of relation \"green_taxi\" does not exist - Export transformed green_taxi data to PostgreSQL', 'course': 'data-engineering-zoomcamp', 'id': 'dde58c8f'}]), ('207be93b', [{'text': 'SettingWithCopyWarning:\\nA value is trying to be set on a copy of a slice from a DataFrame.\\nUse the data.loc[] = value syntax instead of df[] = value to ensure that the new column is being assigned to the original dataframe instead of a copy of a dataframe or a series.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Homework - Q3 SettingWithCopyWarning Error:', 'course': 'data-engineering-zoomcamp', 'id': '207be93b'}]), ('f0617e65', [{'text': 'CSV Files are very big in nyc data, so we instead of using Pandas/Python kernel , we can try Pyspark Kernel\\nDocumentation of Mage for using pyspark kernel: https://docs.mage.ai/integrations/spark-pyspark\\n?', 'section': 'Module 2: Workflow Orchestration', 'question': 'Since I was using slow laptop, and we have so big csv files, I used pyspark kernel in mage instead of python, How to do it?', 'course': 'data-engineering-zoomcamp', 'id': 'f0617e65'}]), ('6290a1a6', [{'text': 'So we will first delete the connection between blocks then we can remove the connection.', 'section': 'Module 2: Workflow Orchestration', 'question': 'I got an error when I was deleting  BLOCK IN A PIPELINE', 'course': 'data-engineering-zoomcamp', 'id': '6290a1a6'}]), ('5a06248c', [{'text': 'While Editing the Pipeline Name It throws permission denied error.\\n(Work around)In that case proceed with the work and save later on revisit it will let you edit.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage UI won’t let you edit the Pipeline name?', 'course': 'data-engineering-zoomcamp', 'id': '5a06248c'}]), ('c46a2e9e', [{'text': 'Solution n°1 if you want to download everything :\\n```\\nimport pyarrow as pa\\nimport pyarrow.parquet as pq\\nfrom pyarrow.fs import GcsFileSystem\\n…\\n@data_loader\\ndef load_data(*args, **kwargs):\\n    bucket_name = YOUR_BUCKET_NAME_HERE\\'\\n    blob_prefix = \\'PATH / TO / WHERE / THE / PARTITIONS / ARE\\'\\n    root_path = f\"{bucket_name}/{blob_prefix}\"\\npa_table = pq.read_table(\\n        source=root_path,\\n        filesystem=GcsFileSystem(),        \\n    )\\n\\n    return pa_table.to_pandas()\\nSolution n°2 if you want to download only some dates :\\n@data_loader\\ndef load_data(*args, **kwargs):\\ngcs = pa.fs.GcsFileSystem()\\nbucket_name = \\'YOUR_BUCKET_NAME_HERE\\'\\nblob_prefix = \\'\\'PATH / TO / WHERE / THE / PARTITIONS / ARE\\'\\'\\nroot_path = f\"{bucket_name}/{blob_prefix}\"\\npa_dataset = pq.ParquetDataset(\\npath_or_paths=root_path,\\nfilesystem=gcs,\\nfilters=[(\\'lpep_pickup_date\\', \\'>=\\', \\'2020-10-01\\'), (\\'lpep_pickup_date\\', \\'<=\\', \\'2020-10-31\\')]\\n)\\nreturn pa_dataset.read().to_pandas()\\n# More information about the pq.Parquet.Dataset : Encapsulates details of reading a complete Parquet dataset possibly consisting of multiple files and partitions in subdirectories. Documentation here :\\nhttps://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html#pyarrow.parquet.ParquetDataset\\nERROR: UndefinedColumn: column \"vendor_id\" of relation \"green_taxi\" does not exist\\nTwo possible solutions both of them work in the same way.\\nOpen up a Data Loader connect using SQL - RUN the command \\n`DROP TABLE mage.green_taxi`\\nElse, Open up a Data Extractor of SQL  - increase the rows to above the number of rows in the dataframe (you can find that in the bottom of the transformer block) change the Write Policy to `Replace` and run the SELECT statement', 'section': 'Module 2: Workflow Orchestration', 'question': 'How do I make Mage load the partitioned files that we created on 2.2.4, to load them into BigQuery ?', 'course': 'data-engineering-zoomcamp', 'id': 'c46a2e9e'}]), ('0513ab8a', [{'text': \"All mage files are in your /home/src/folder where you saved your credentials.json so you should be able to access them locally. You will see a folder for ‘Pipelines’,  'data loaders', 'data transformers' & 'data exporters' - inside these will be the .py or .sql files for the blocks you created in your pipeline.\\nRight click & ‘download’ the pipeline itself to your local machine (which gives you metadata, pycache and other files)\\nAs above, download each .py/.sql file that corresponds to each block you created for the pipeline. You'll find these under 'data loaders', 'data transformers' 'data exporters'\\nMove the downloaded files to your GitHub repo folder & commit your changes.\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Git - What Files Should I Submit for Homework 2 & How do I get them out of MAGE:', 'course': 'data-engineering-zoomcamp', 'id': '0513ab8a'}]), ('a9385356', [{'text': 'Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\\nMove the contents of the .gitignore file in your main .gitignore.\\nUse the terminal to cd into the Mage folder and:\\nrun “git remote remove origin” to de-couple the Mage repo,\\nrun “rm -rf .git” to delete local git files,\\nrun “git add .” to add the current folder as changes to stage, commit and push.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Git - How do I include the files in the Mage repo (including exercise files and homework) in a personal copy of the Data Engineering Zoomcamp repo?', 'course': 'data-engineering-zoomcamp', 'id': 'a9385356'}]), ('c30468c0', [{'text': \"When try to add three assertions:\\nvendor_id is one of the existing values in the column (currently)\\npassenger_count is greater than 0\\ntrip_distance is greater than 0\\nto test_output, I got ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). Below is my code:\\ndata_filter = (data['passenger_count'] > 0) and (data['trip_distance'] > 0)\\nAfter looking for solutions at Stackoverflow, I found great discussion about it. So I changed my code into:\\ndata_filter = (data['passenger_count'] > 0) & (data['trip_distance'] > 0)\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Got ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()', 'course': 'data-engineering-zoomcamp', 'id': 'c30468c0'}]), ('305aead7', [{'text': 'This happened when I just booted up my PC, continuing from the progress I was doing from yesterday.\\nAfter cd-ing into your directory, and running docker compose up , the web interface for the Mage shows, but the files that I had yesterday was gone.\\nIf your files are gone, go ahead and close the web interface, and properly shutting down the mage docker compose by doing Ctrl + C once. Try running it again. This worked for me more than once (yes the issue persisted with my PC twice)\\nAlso, you should check if you’re in the correct repository before doing docker compose up . This was discussed in the Slack #course-data-engineering channel', 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage AI Files are Gone/disappearing', 'course': 'data-engineering-zoomcamp', 'id': '305aead7'}]), ('77410975', [{'text': 'The above errors due to “ at the trailing side and it need to be modified with ‘ quotes at both ends\\nKrishna Anand', 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage - Errors in io.config.yaml file', 'course': 'data-engineering-zoomcamp', 'id': '77410975'}]), ('0952abde', [{'text': \"Problem: The following error occurs when attempting to export data from Mage to a GCS bucket using pyarrow suggesting Mage doesn’t have the necessary permissions to access the specified GCP credentials .json file.\\nArrowException: Unknown error: google::cloud::Status(UNKNOWN: Permanent error GetBucketMetadata: Could not create a OAuth2 access token to authenticate the request. The request was not sent, as such an access token is required to complete the request successfully. Learn more about Google Cloud authentication at https://cloud.google.com/docs/authentication. The underlying error message was: Cannot open credentials file /home/src/...\\nSolution: Inside the Mage app:\\nCreate a credentials folder (e.g. gcp-creds) within the magic-zoomcamp folder\\nIn the credentials folder create a .json key file (e.g. mage-gcp-creds.json)\\nCopy/paste GCP service account credentials into the .json key file and save\\nUpdate code to point to this file. E.g.\\nenviron['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/src/magic-zoomcamp/gcp-creds/mage-gcp-creds.json'\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage - ArrowException Cannot open credentials file', 'course': 'data-engineering-zoomcamp', 'id': '0952abde'}]), ('7c4326eb', [{'text': \"Oserror: google::cloud::status(unavailable: retry policy exhausted getbucketmetadata: could not create a OAuth2 access token to authenticate the request. the request was not sent, as such an access token is required to complete the request successfully. learn more about google cloud authentication at https://cloud.google.com/docs/authentication. the underlying error message was: performwork() - curl error [6]=couldn't resolve host name)\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage - OSError', 'course': 'data-engineering-zoomcamp', 'id': '7c4326eb'}]), ('a1fc1a14', [{'text': \"Problem: The following error occurs when attempting to export data from Mage to a GCS bucket. Assigned service account doesn’t have the necessary permissions access Google Cloud Storage Bucket\\nPermissionError: [Errno 13] google::cloud::Status(PERMISSION_DENIED: Permanent error GetBucketMetadata:... .iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist). error_info={reason=forbidden, domain=global, metadata={http_status_code=403}}). Detail: [errno 13] Permission denied\\nSolution: Add Cloud Storage Admin role to the service account:\\nGo to project in Google Cloud Console>IAM & Admin>IAM\\nClick Edit principal (pencil symbol) to the right of the service account you are using\\nClick + ADD ANOTHER ROLE\\nSelect Cloud Storage>Storage Admin\\nClick Save\", 'section': 'Module 2: Workflow Orchestration', 'question': 'Mage - PermissionError service account does not have storage.buckets.get access to the Google Cloud Storage bucket', 'course': 'data-engineering-zoomcamp', 'id': 'a1fc1a14'}]), ('6d67fba9', [{'text': '1. Make sure your pyspark script is ready to be send to Dataproc cluster\\n2. Create a Dataproc Cluster in GCP Console\\n3. Make sure to edit the service account and add new role - Dataproc Editor\\n4. Copy the python script ./notebooks/pyspark_script.py and place it under GCS bucket path\\n5. Make sure gcloud cli is installed either in Mage manually or  via your Dockerfile and docker-compose files. This is needed to let Mage access google Dataproc and the script it needs to execute. Refer - Installing the latest gcloud CLI\\n6. Use the Bigquery/Dataproc script mentioned here - https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/code/cloud.md . Use Mage to trigger the query', 'section': 'Module 3: Data Warehousing', 'question': 'Trigger Dataproc from Mage', 'course': 'data-engineering-zoomcamp', 'id': '6d67fba9'}]), ('06876291', [{'text': 'A:\\n1 solution) Add -Y flag, so that apt-get automatically agrees to install additional packages\\n2) Use python ZipFile package, which is included in all modern python distributions', 'section': 'Module 3: Data Warehousing', 'question': 'Docker-compose takes infinitely long to install zip unzip packages for linux, which are required to unpack datasets', 'course': 'data-engineering-zoomcamp', 'id': '06876291'}]), ('690ba010', [{'text': 'Make sure to use Nullable dataTypes, such as Int64 when appliable.', 'section': 'Module 3: Data Warehousing', 'question': 'GCS Bucket - error when writing data from web to GCS:', 'course': 'data-engineering-zoomcamp', 'id': '690ba010'}]), ('b6fdd91d', [{'text': 'Ultimately, when trying to ingest data into a BigQuery table, all files within a given directory must have the same schema.\\nWhen dealing for example with the FHV Datasets from 2019, however (see image below), one can see that the files for \\'2019-05\\', and 2019-06, have the columns \"PUlocationID\" and \"DOlocationID\" as Integers, while for the period of \\'2019-01\\' through \\'2019-04\\', the same column is defined as FLOAT.\\nSo while importing these files as parquet to BigQuery, the first one will be used to define the schema of the table, while all files following that will be used to append data on the existing table. Which means, they must all follow the very same schema of the file that created the table.\\nSo, in order to prevent errors like that, make sure to enforce the data types for the columns on the DataFrame before you serialize/upload them to BigQuery. Like this:\\npd.read_csv(\"path_or_url\").astype({\\n\\t\"col1_name\": \"datatype\",\\t\\n\\t\"col2_name\": \"datatype\",\\t\\n\\t...\\t\\t\\t\\t\\t\\n\\t\"colN_name\": \"datatype\" \\t\\n})', 'section': 'Module 3: Data Warehousing', 'question': \"GCS Bucket - Failed to create table: Error while reading data, error message: Parquet column 'XYZ' has type INT which does not match the target cpp_type DOUBLE. File: gs://path/to/some/blob.parquet\", 'course': 'data-engineering-zoomcamp', 'id': 'b6fdd91d'}]), ('155aa868', [{'text': \"If you receive the error gzip.BadGzipFile: Not a gzipped file (b'\\\\n\\\\n'), this is because you have specified the wrong URL to the FHV dataset. Make sure to use https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/{dataset_file}.csv.gz\\nEmphasising the ‘/releases/download’ part of the URL.\", 'section': 'Module 3: Data Warehousing', 'question': 'GCS Bucket - Fix Error when importing FHV data to GCS', 'course': 'data-engineering-zoomcamp', 'id': '155aa868'}]), ('e78cf960', [{'text': 'Krishna Anand', 'section': 'Module 3: Data Warehousing', 'question': 'GCS Bucket - Load Data From URL list in to GCP Bucket', 'course': 'data-engineering-zoomcamp', 'id': 'e78cf960'}]), ('9afa1f74', [{'text': 'Check the Schema\\nYou might have a wrong formatting\\nTry to upload the CSV.GZ files without formatting or going through pandas via wget\\nSee this Slack conversation for helpful tips', 'section': 'Module 3: Data Warehousing', 'question': 'GCS Bucket - I query my dataset and get a Bad character (ASCII 0) error?', 'course': 'data-engineering-zoomcamp', 'id': '9afa1f74'}]), ('fac138a7', [{'text': 'Run the following command to check if “BigQuery Command Line Tool” is installed or not: gcloud components list\\nYou can also use bq.cmd instead of bq to make it work.', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - “bq: command not found”', 'course': 'data-engineering-zoomcamp', 'id': 'fac138a7'}]), ('0174dde5', [{'text': 'Use big queries carefully,\\nI created by bigquery dataset on an account where my free trial was exhausted, and got a bill of $80.\\nUse big query in free credits and destroy all the datasets after creation.\\nCheck your Billing daily! Especially if you’ve spinned up a VM.', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Caution in using bigquery:no', 'course': 'data-engineering-zoomcamp', 'id': '0174dde5'}]), ('1023ee65', [{'text': 'Be careful when you create your resources on GCP, all of them have to share the same Region in order to allow load data from GCS Bucket to BigQuery. If you forgot it when you created them, you can create a new dataset on BigQuery using the same Region which you used on your GCS Bucket.\\nThis means that your GCS Bucket and the BigQuery dataset are placed in different regions. You have to create a new dataset inside BigQuery in the same region with your GCS bucket and store the data in the newly created dataset.', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Cannot read and write in different locations: source: EU, destination: US - Loading data from GCS into BigQuery (different Region):', 'course': 'data-engineering-zoomcamp', 'id': '1023ee65'}]), ('effd2bfa', [{'text': \"Make sure to create the BigQuery dataset in the very same location that you've created the GCS Bucket. For instance, if your GCS Bucket was created in `us-central1`, then BigQuery dataset must be created in the same region (us-central1, in this example)\", 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Cannot read and write in different locations: source: <REGION_HERE>, destination: <ANOTHER_REGION_HERE>', 'course': 'data-engineering-zoomcamp', 'id': 'effd2bfa'}]), ('5b55273c', [{'text': 'By the way, this isn’t a problem/solution, but a useful hint:\\nPlease, remember to save your progress in BigQuery SQL Editor.\\nI was almost finishing the homework, when my Chrome Tab froze and I had to reload it. Then I lost my entire SQL script.\\nSave your script from time to time. Just click on the button at the top bar. Your saved file will be available on the left panel.\\nAlternatively, you can copy paste your queries into an .sql file in your preferred editor (Notepad++, VS Code, etc.). Using the .sql extension will provide convenient color formatting.', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Remember to save your queries', 'course': 'data-engineering-zoomcamp', 'id': '5b55273c'}]), ('1835bfe0', [{'text': 'Ans :  While real-time analytics might not be explicitly mentioned, BigQuery has real-time data streaming capabilities, allowing for potential integration in future project iterations.', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Can I use BigQuery for real-time analytics in this project?', 'course': 'data-engineering-zoomcamp', 'id': '1835bfe0'}]), ('04656af5', [{'text': \"could not parse 'pickup_datetime' as timestamp for field pickup_datetime (position 2)\\nThis error is caused by invalid data in the timestamp column. A way to identify the problem is to define the schema from the external table using string datatype. This enables the queries to work at which point we can filter out the invalid rows from the import to the materialised table and insert the fields with the timestamp data type.\", 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Unable to load data from external tables into a materialized table in BigQuery due to an invalid timestamp error that are added while appending data to the file in Google Cloud Storage', 'course': 'data-engineering-zoomcamp', 'id': '04656af5'}]), ('2d6536d3', [{'text': 'Background:\\n`pd.read_parquet`\\n`pd.to_datetime`\\n`pq.write_to_dataset`\\nReference:\\nhttps://stackoverflow.com/questions/48314880/are-parquet-file-created-with-pyarrow-vs-pyspark-compatible\\nhttps://stackoverflow.com/questions/57798479/editing-parquet-files-with-python-causes-errors-to-datetime-format\\nhttps://www.reddit.com/r/bigquery/comments/16aoq0u/parquet_timestamp_to_bq_coming_across_as_int/?share_id=YXqCs5Jl6hQcw-kg6-VgF&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1\\nSolution:\\nAdd `use_deprecated_int96_timestamps=True` to `pq.write_to_dataset` function, like below\\npq.write_to_dataset(\\ntable,\\nroot_path=root_path,\\nfilesystem=gcs,\\nuse_deprecated_int96_timestamps=True\\n# Write timestamps to INT96 Parquet format\\n)', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Error Message in BigQuery: annotated as a valid Timestamp, please annotate it as TimestampType(MICROS) or TimestampType(MILLIS)', 'course': 'data-engineering-zoomcamp', 'id': '2d6536d3'}]), ('0516ccbe', [{'text': 'Solution:\\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage use PyArrow to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won\\'t be converted to timestamp when loaded by BigQuery later on.\\nimport pyarrow as pa\\nimport pyarrow.parquet as pq\\nimport os\\nif \\'data_exporter\\' not in globals():\\nfrom mage_ai.data_preparation.decorators import data_exporter\\n# Replace with the location of your service account key JSON file.\\nos.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'/home/src/personal-gcp.json\\'\\nbucket_name = \"<YOUR_BUCKET_NAME>\"\\nobject_key = \\'nyc_taxi_data_2022.parquet\\'\\nwhere = f\\'{bucket_name}/{object_key}\\'\\n@data_exporter\\ndef export_data(data, *args, **kwargs):\\ntable = pa.Table.from_pandas(data, preserve_index=False)\\ngcs = pa.fs.GcsFileSystem()\\npq.write_table(\\ntable,\\nwhere,\\n# Convert integer columns in Epoch milliseconds\\n# to Timestamp columns in microseconds (\\'us\\') so\\n# they can be loaded into BigQuery with the right\\n# data type\\ncoerce_timestamps=\\'us\\',\\nfilesystem=gcs\\n)\\nSolution 2:\\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage, provide PyArrow with explicit schema to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won\\'t be converted to timestamp when loaded by BigQuery later on.\\nschema = pa.schema([\\n(\\'vendor_id\\', pa.int64()),\\n(\\'lpep_pickup_datetime\\', pa.timestamp(\\'ns\\')),\\n(\\'lpep_dropoff_datetime\\', pa.timestamp(\\'ns\\')),\\n(\\'store_and_fwd_flag\\', pa.string()),\\n(\\'ratecode_id\\', pa.int64()),\\n(\\'pu_location_id\\', pa.int64()),\\n(\\'do_location_id\\', pa.int64()),\\n(\\'passenger_count\\', pa.int64()),\\n(\\'trip_distance\\', pa.float64()),\\n(\\'fare_amount\\', pa.float64()),\\n(\\'extra\\', pa.float64()),\\n(\\'mta_tax\\', pa.float64()),\\n(\\'tip_amount\\', pa.float64()),\\n(\\'tolls_amount\\', pa.float64()),\\n(\\'improvement_surcharge\\', pa.float64()),\\n(\\'total_amount\\', pa.float64()),\\n(\\'payment_type\\', pa.int64()),\\n(\\'trip_type\\', pa.int64()),\\n(\\'congestion_surcharge\\', pa.float64()),\\n(\\'lpep_pickup_month\\', pa.int64())\\n])\\ntable = pa.Table.from_pandas(data, schema=schema)', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Datetime columns in Parquet files created from Pandas show up as integer columns in BigQuery', 'course': 'data-engineering-zoomcamp', 'id': '0516ccbe'}]), ('6052513d', [{'text': 'Reference:\\nhttps://cloud.google.com/bigquery/docs/external-data-cloud-storage\\nSolution:\\nfrom google.cloud import bigquery\\n# Set table_id to the ID of the table to create\\ntable_id = f\"{project_id}.{dataset_name}.{table_name}\"\\n# Construct a BigQuery client object\\nclient = bigquery.Client()\\n# Set the external source format of your table\\nexternal_source_format = \"PARQUET\"\\n# Set the source_uris to point to your data in Google Cloud\\nsource_uris = [ f\\'gs://{bucket_name}/{object_key}/*\\']\\n# Create ExternalConfig object with external source format\\nexternal_config = bigquery.ExternalConfig(external_source_format)\\n# Set source_uris that point to your data in Google Cloud\\nexternal_config.source_uris = source_uris\\nexternal_config.autodetect = True\\ntable = bigquery.Table(table_id)\\n# Set the external data configuration of the table\\ntable.external_data_configuration = external_config\\ntable = client.create_table(table)  # Make an API request.\\nprint(f\\'Created table with external source: {table_id}\\')\\nprint(f\\'Format: {table.external_data_configuration.source_format}\\')', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Create External Table using Python', 'course': 'data-engineering-zoomcamp', 'id': '6052513d'}]), ('7a71fa2c', [{'text': 'Reference:\\nhttps://stackoverflow.com/questions/60941726/can-bigquery-api-overwrite-existing-table-view-with-create-table-tables-inser\\nSolution:\\nCombine with “Create External Table using Python”, use it before “client.create_table” function.\\ndef tableExists(tableID, client):\\n\"\"\"\\nCheck if a table already exists using the tableID.\\nreturn : (Boolean)\\n\"\"\"\\ntry:\\ntable = client.get_table(tableID)\\nreturn True\\nexcept Exception as e: # NotFound:\\nreturn False', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Check BigQuery Table Exist And Delete', 'course': 'data-engineering-zoomcamp', 'id': '7a71fa2c'}]), ('f83d9435', [{'text': 'To avoid this error you can upload data from Google Cloud Storage to BigQuery through BigQuery Cloud Shell using the command:\\n$ bq load  --autodetect --allow_quoted_newlines --source_format=CSV dataset_name.table_name \"gs://dtc-data-lake-bucketname/fhv/fhv_tripdata_2019-*.csv.gz\"', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Error: Missing close double quote (\") character', 'course': 'data-engineering-zoomcamp', 'id': 'f83d9435'}]), ('dbf65e11', [{'text': 'Solution: This problem arises if your gcs and bigquery storage is in different regions.\\nOne potential way to solve it:\\nGo to your google cloud bucket and check the region in field named “Location”\\nNow in bigquery, click on three dot icon near your project name and select create dataset.\\nIn region filed choose the same regions as you saw in your google cloud bucket', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Cannot read and write in different locations: source: asia-south2, destination: US', 'course': 'data-engineering-zoomcamp', 'id': 'dbf65e11'}]), ('c489266b', [{'text': 'There are multiple benefits of using Cloud Functions to automate tasks in Google Cloud.\\nUse below Cloud Function python script to load files directly to BigQuery. Use your project id, dataset id & table id as defined by you.\\nimport tempfile\\nimport requests\\nimport logging\\nfrom google.cloud import bigquery\\ndef hello_world(request):\\n# table_id = <project_id.dataset_id.table_id>\\ntable_id = \\'de-zoomcap-project.dezoomcamp.fhv-2019\\'\\n# Create a new BigQuery client\\nclient = bigquery.Client()\\nfor month in range(4, 13):\\n# Define the schema for the data in the CSV.gz files\\nurl = \\'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-{:02d}.csv.gz\\'.format(month)\\n# Download the CSV.gz file from Github\\nresponse = requests.get(url)\\n# Create new table if loading first month data else append\\nwrite_disposition_string = \"WRITE_APPEND\" if month > 1 else \"WRITE_TRUNCATE\"\\n# Defining LoadJobConfig with schema of table to prevent it from changing with every table\\njob_config = bigquery.LoadJobConfig(\\nschema=[\\nbigquery.SchemaField(\"dispatching_base_num\", \"STRING\"),\\nbigquery.SchemaField(\"pickup_datetime\", \"TIMESTAMP\"),\\nbigquery.SchemaField(\"dropOff_datetime\", \"TIMESTAMP\"),\\nbigquery.SchemaField(\"PUlocationID\", \"STRING\"),\\nbigquery.SchemaField(\"DOlocationID\", \"STRING\"),\\nbigquery.SchemaField(\"SR_Flag\", \"STRING\"),\\nbigquery.SchemaField(\"Affiliated_base_number\", \"STRING\"),\\n],\\nskip_leading_rows=1,\\nwrite_disposition=write_disposition_string,\\nautodetect=True,\\nsource_format=\"CSV\",\\n)\\n# Load the data into BigQuery\\n# Create a temporary file to prevent the exception- AttributeError: \\'bytes\\' object has no attribute \\'tell\\'\"\\nwith tempfile.NamedTemporaryFile() as f:\\nf.write(response.content)\\nf.seek(0)\\njob = client.load_table_from_file(\\nf,\\ntable_id,\\nlocation=\"US\",\\njob_config=job_config,\\n)\\njob.result()\\nlogging.info(\"Data for month %d successfully loaded into table %s.\", month, table_id)\\nreturn \\'Data loaded into table {}.\\'.format(table_id)', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - Tip: Using Cloud Function to read csv.gz files from github directly to BigQuery in Google Cloud:', 'course': 'data-engineering-zoomcamp', 'id': 'c489266b'}]), ('ebd63566', [{'text': 'You need to uncheck cache preferences in query settings', 'section': 'Module 3: Data Warehousing', 'question': 'GCP BQ - When querying two different tables external and materialized you get the same result when count(distinct(*))', 'course': 'data-engineering-zoomcamp', 'id': 'ebd63566'}]), ('f7252f17', [{'text': 'Problem: When you inject data into GCS using Pandas, there is a chance that some dataset has missing values on  DOlocationID and PUlocationID. Pandas by default will cast these columns as float data type, causing inconsistent data type between parquet in GCS and schema defined in big query. You will see something like this:\\nSolution:\\nFix the data type issue in data pipeline\\nBefore injecting data into GCS, use astype and Int64 (which is different from int64 and accept both missing value and integer exist in the column) to cast the columns.\\nSomething like:\\ndf[\"PUlocationID\"] = df.PUlocationID.astype(\"Int64\")\\ndf[\"DOlocationID\"] = df.DOlocationID.astype(\"Int64\")\\nNOTE: It is best to define the data type of all the columns in the Transformation section of the ETL pipeline before loading to BigQuery', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ - How to handle type error from big query and parquet data?', 'course': 'data-engineering-zoomcamp', 'id': 'f7252f17'}]), ('47a43bb0', [{'text': 'Problem occurs when misplacing content after fro``m clause in BigQuery SQLs.\\nCheck to remove any extra apaces or any other symbols, keep in lowercases, digits and dashes only', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ - Invalid project ID . Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project', 'course': 'data-engineering-zoomcamp', 'id': '47a43bb0'}]), ('f3f13def', [{'text': 'No. Based on the documentation for Bigquery, it does not support more than 1 column to be partitioned.\\n[source]', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ - Does BigQuery support multiple columns partition?', 'course': 'data-engineering-zoomcamp', 'id': 'f3f13def'}]), ('4fd37712', [{'text': 'Error Message:\\nPARTITION BY expression must be DATE(<timestamp_column>), DATE(<datetime_column>), DATETIME_TRUNC(<datetime_column>, DAY/HOUR/MONTH/YEAR), a DATE column, TIMESTAMP_TRUNC(<timestamp_column>, DAY/HOUR/MONTH/YEAR), DATE_TRUNC(<date_column>, MONTH/YEAR), or RANGE_BUCKET(<int64_column>, GENERATE_ARRAY(<int64_value>, <int64_value>[, <int64_value>]))\\nSolution:\\nConvert the column to datetime first.\\ndf[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\\ndf[\"dropOff_datetime\"] = pd.to_datetime(df[\"dropOff_datetime\"])', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ - DATE() Error in BigQuery', 'course': 'data-engineering-zoomcamp', 'id': '4fd37712'}]), ('8abeca36', [{'text': 'Native tables are tables where the data is stored in BigQuery.  External tables store the data outside BigQuery, with BigQuery storing metadata about that external table.\\nResources:\\nhttps://cloud.google.com/bigquery/docs/external-tables\\nhttps://cloud.google.com/bigquery/docs/tables-intro', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ - Native tables vs External tables in BigQuery?', 'course': 'data-engineering-zoomcamp', 'id': '8abeca36'}]), ('16c16ff9', [{'text': 'Issue: Tried running command to export ML model from BQ to GCS from Week 3\\nbq --project_id taxi-rides-ny extract -m nytaxi.tip_model gs://taxi_ml_model/tip_model\\nIt is failing on following error:\\nBigQuery error in extract operation: Error processing job Not found: Dataset was not found in location US\\nI verified the BQ data set and gcs bucket are in the same region- us-west1. Not sure how it gets location US. I couldn’t find the solution yet.\\nSolution:  Please enter correct project_id and gcs_bucket folder address. My gcs_bucket folder address is\\ngs://dtc_data_lake_optimum-airfoil-376815/tip_model', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ ML - Unable to run command (shown in video) to export ML model from BQ to GCS', 'course': 'data-engineering-zoomcamp', 'id': '16c16ff9'}]), ('c65d8fd9', [{'text': \"To solve this error mention the location = US when creating the dim_zones table\\n{{ config(\\nmaterialized='table',\\nlocation='US'\\n) }}\\nJust Update this part to solve the issue and run the dim_zones again and then run the fact_trips\", 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Dim_zones.sql Dataset was not found in location US When Running fact_trips.sql', 'course': 'data-engineering-zoomcamp', 'id': 'c65d8fd9'}]), ('c1a95536', [{'text': 'Solution: proceed with setting up serving_dir on your computer as in the extract_model.md file. Then instead of\\ndocker pull tensorflow/serving\\nuse\\ndocker pull emacski/tensorflow-serving\\nThen\\ndocker run -p 8500:8500 -p 8501:8501 --mount type=bind,source=`pwd`/serving_dir/tip_model,target=/models/tip_model -e MODEL_NAME=tip_model -t emacski/tensorflow-serving\\nThen run the curl command as written, and you should get a prediction.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'GCP BQ ML - Export ML model to make predictions does not work for MacBook with Apple M1 chip (arm architecture).', 'course': 'data-engineering-zoomcamp', 'id': 'c1a95536'}]), ('bba0da04', [{'text': 'Try deleting data you’ve saved to your VM locally during ETLs\\nKill processes related to deleted files\\nDownload ncdu and look for large files (pay particular attention to files related to Prefect)\\nIf you delete any files related to Prefect, eliminate caching from your flow code', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'VMs - What do I do if my VM runs out of space?', 'course': 'data-engineering-zoomcamp', 'id': 'bba0da04'}]), ('a2120335', [{'text': \"Ans: What they mean is that they don't want you to do anything more than that. You should load the files into the bucket and create an external table based on those files (but nothing like cleaning the data and putting it in parquet format)\", 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': \"Homework - What does it mean “Stop with loading the files into a bucket.' Stop with loading the files into a bucket?”\", 'course': 'data-engineering-zoomcamp', 'id': 'a2120335'}]), ('a4ba2478', [{'text': 'If for whatever reason you try to read parquets directly from nyc.gov’s cloudfront into pandas, you might run into this error:\\npyarrow.lib.ArrowInvalid: Casting from timestamp[us] to timestamp[ns] would result in out of bounds\\nCause:\\nthere is one errant data record where the dropOff_datetime was set to year 3019 instead of 2019.\\npandas uses “timestamp[ns]” (as noted above), and int64 only allows a ~580 year range, centered on 2000. See `pd.Timestamp.max` and `pd.Timestamp.min`\\nThis becomes out of bounds when pandas tries to read it because 3019 > 2300 (approx value of pd.Timestamp.Max\\nFix:\\nUse pyarrow to read it:\\nimport pyarrow.parquet as pq df = pq.read_table(\\'fhv_tripdata_2019-02.parquet\\').to_pandas(safe=False)\\nHowever this results in weird timestamps for the offending record\\nRead the datetime columns separately using pq.read_table\\n\\ntable = pq.read_table(‘taxi.parquet’)\\ndatetimes = [‘list of datetime column names’]\\ndf_dts = pd.DataFrame()\\nfor col in datetimes:\\ndf_dts[col] = pd.to_datetime(table .column(col), errors=\\'coerce\\')\\n\\nThe `errors=’coerce’` parameter will convert the out of bounds timestamps into either the max or the min\\nUse parquet.compute.filter to remove the offending rows\\n\\nimport pyarrow.compute as pc\\ntable = pq.read_table(\"‘taxi.parquet\")\\ndf = table.filter(\\npc.less_equal(table[\"dropOff_datetime\"], pa.scalar(pd.Timestamp.max))\\n).to_pandas()', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Homework - Reading parquets from nyc.gov directly into pandas returns Out of bounds error', 'course': 'data-engineering-zoomcamp', 'id': 'a4ba2478'}]), ('74c361fe', [{'text': 'Answer: The 2022 NYC taxi data parquet files are available for each month separately. Therefore, you need to add all 12 files to your GCS bucket and then refer to them using the URIs option when creating an external table in BigQuery. You can use the wildcard \"*\" to refer to all 12 files using a single string.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Question: for homework 3 , we need all 12 parquet files for green taxi 2022 right ?', 'course': 'data-engineering-zoomcamp', 'id': '74c361fe'}]), ('b9b3ef9f', [{'text': 'This can help avoid schema issues in the homework. \\nDownload files locally and use the ‘upload files’ button in GCS at the desired path. You can upload many files at once. You can also choose to upload a folder.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Homework - Uploading files to GCS via GUI', 'course': 'data-engineering-zoomcamp', 'id': 'b9b3ef9f'}]), ('009ac612', [{'text': 'Ans: Take a careful look at the format of the dates in the question.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Homework - Qn 5: The partitioned/clustered table isn’t giving me the prediction I expected', 'course': 'data-engineering-zoomcamp', 'id': '009ac612'}]), ('68815ec2', [{'text': 'Many people aren’t getting an exact match, but are very close to one of the options. As per Alexey said to choose the closest option.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Homework - Qn 6: Did anyone get an exact match for one of the options given in Module 3 homework Q6?', 'course': 'data-engineering-zoomcamp', 'id': '68815ec2'}]), ('c8ad08b3', [{'text': 'UnicodeDecodeError: \\'utf-8\\' codec can\\'t decode byte 0xa0 in position 41721: invalid start byte\\nSolution:\\nStep 1: When reading the data from the web into the pandas dataframe mention the encoding as follows:\\npd.read_csv(dataset_url, low_memory=False, encoding=\\'latin1\\')\\nStep 2: When writing the dataframe from the local system to GCS as a csv mention the encoding as follows:\\ndf.to_csv(path_on_gsc, compression=\"gzip\", encoding=\\'utf-8\\')\\nAlternative: use pd.read_parquet(url)', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Python - invalid start byte Error Message', 'course': 'data-engineering-zoomcamp', 'id': 'c8ad08b3'}]), ('d68b433f', [{'text': 'A generator is a function in python that returns an iterator using the yield keyword.\\nA generator is a special type of iterable, similar to a list or a tuple, but with a crucial difference. Instead of creating and storing all the values in memory at once, a generator generates values on-the-fly as you iterate over it. This makes generators memory-efficient, particularly when dealing with large datasets.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Python - Generators in python', 'course': 'data-engineering-zoomcamp', 'id': 'd68b433f'}]), ('e265ee5a', [{'text': 'The read_parquet function supports a list of files as an argument. The list of files will be merged into a single result table.', 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Python - Easiest way to read multiple files at the same time?', 'course': 'data-engineering-zoomcamp', 'id': 'e265ee5a'}]), ('0e7dfddc', [{'text': \"Incorrect:\\ndf['DOlocationID'] = pd.to_numeric(df['DOlocationID'], downcast=integer) or\\ndf['DOlocationID'] = df['DOlocationID'].astype(int)\\nCorrect:\\ndf['DOlocationID'] = df['DOlocationID'].astype('Int64')\", 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': \"Python - These won't work. You need to make sure you use Int64:\", 'course': 'data-engineering-zoomcamp', 'id': '0e7dfddc'}]), ('0a059700', [{'text': \"ValueError: Path /Users/kt/.prefect/storage/44ccce0813ed4f24ab2d3783de7a9c3a does not exist.\\nRemove ```cache_key_fn=task_input_hash ``` as it’s in argument in your function & run your flow again.\\nNote: catche key is beneficial if you happen to run the code multiple times, it won't repeat the process which you have finished running in the previous run.  That means, if you have this ```cache_key``` in your initial run, this might cause the error.\", 'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\", 'question': 'Prefect - Error on Running Prefect Flow to Load data to GCS', 'course': 'data-engineering-zoomcamp', 'id': '0a059700'}]), ('feca7402', [{'text': '@task\\ndef download_file(url: str, file_path: str):\\nresponse = requests.get(url)\\nopen(file_path, \"wb\").write(response.content)\\nreturn file_path\\n@flow\\ndef extract_from_web() -> None:\\nfile_path = download_file(url=f\\'{url-filename}.csv.gz\\',file_path=f\\'{filename}.csv.gz\\')', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Prefect - Tip: Downloading csv.gz from a url in a prefect environment (sample snippet).', 'course': 'data-engineering-zoomcamp', 'id': 'feca7402'}]), ('1f519b1a', [{'text': 'Update the seed column types in the dbt_project.yaml file\\nfor using double : float\\nfor using int : numeric\\nDBT Cloud production error: prod dataset not available in location EU\\nProblem: I am trying to deploy my DBT  models to production, using DBT Cloud. The data should live in BigQuery.  The dataset location is EU.  However, when I am running the model in production, a prod dataset is being create in BigQuery with a location US and the dbt invoke build is failing giving me \"ERROR 404: porject.dataset:prod not available in location EU\". I tried different ways to fix this. I am not sure if there is a more simple solution then creating my project or buckets in location US. Hope anyone can help here.\\nNote: Everything is working fine in development mode, the issue is just happening when scheduling and running job in production\\nSolution: I created the prod dataset manually in BQ and specified EU, then I ran the job.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'If you are getting not found in location us error.', 'course': 'data-engineering-zoomcamp', 'id': '1f519b1a'}]), ('43c454c7', [{'text': 'Error: This project does not have a development environment configured. Please create a development environment and configure your development credentials to use the dbt IDE.\\nThe error itself tells us how to solve this issue, the guide is here. And from videos @1:42 and also slack chat', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Setup - No development environment', 'course': 'data-engineering-zoomcamp', 'id': '43c454c7'}]), ('d7ad69da', [{'text': \"Runtime Error\\ndbt was unable to connect to the specified database.\\nThe database returned the following error:\\n>Database Error\\nAccess Denied: Project <project_name>: User does not have bigquery.jobs.create permission in project <project_name>.\\nCheck your database credentials and try again. For more information, visit:\\nhttps://docs.getdbt.com/docs/configure-your-profile\\nSteps to resolve error in Google Cloud:\\n1. Navigate to IAM & Admin and select IAM\\n2. Click Grant Access if your newly created dbt service account isn't listed\\n3. In New principals field, add your service account\\n4. Select a Role and search for BigQuery Job User to add\\n5. Go back to dbt cloud project setup and Test your connection\\n6. Note: Also add BigQuery Data Owner, Storage Object Admin, & Storage Admin to prevent permission issues later in the course\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Setup - Connecting dbt Cloud with BigQuery Error', 'course': 'data-engineering-zoomcamp', 'id': 'd7ad69da'}]), ('03fdb780', [{'text': 'error: This dbt Cloud run was cancelled because a valid dbt project was not found. Please check that the repository contains a proper dbt_project.yml config file. If your dbt project is located in a subdirectory of the connected repository, be sure to specify its location on the Project settings page in dbt Cloud', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Dbt build error', 'course': 'data-engineering-zoomcamp', 'id': '03fdb780'}]), ('9c85f3aa', [{'text': \"Error: Failed to clone repository.\\ngit clone git@github.com:DataTalksClub/data-engineering-zoomcamp.git /usr/src/develop/…\\nCloning into '/usr/src/develop/...\\nWarning: Permanently added 'github.com,140.82.114.4' (ECDSA) to the list of known hosts.\\ngit@github.com: Permission denied (publickey).\\nfatal: Could not read from remote repository.\\nIssue: You don’t have permissions to write to DataTalksClub/data-engineering-zoomcamp.git\\nSolution 1: Clone the repository and use this forked repo, which contains your github username. Then, proceed to specify the path, as in:\\n[your github username]/data-engineering-zoomcamp.git\\nSolution 2: create a fresh repo for dbt-lessons. We’d need to do branching and PRs in this lesson, so it might be a good idea to also not mess up your whole other repo. Then you don’t have to create a subfolder for the dbt project files\\nSolution 3: Use https link\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Setup - Failed to clone repository.', 'course': 'data-engineering-zoomcamp', 'id': '9c85f3aa'}]), ('63026349', [{'text': \"Solution:\\nCheck if you’re on the Developer Plan. As per the prerequisites, you'll need to be enrolled in the Team Plan or Enterprise Plan to set up a CI Job in dbt Cloud.\\nSo If you're on the Developer Plan, you'll need to upgrade to utilise CI Jobs.\\nNote from another user: I’m in the Team Plan (trial period) but the option is still disabled. What worked for me instead was this. It works for the Developer (free) plan.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'dbt job - Triggered by pull requests is disabled when I try to create a new Continuous Integration job in dbt cloud.', 'course': 'data-engineering-zoomcamp', 'id': '63026349'}]), ('6ba02f77', [{'text': 'Issue: If the DBT cloud IDE loading indefinitely then giving you this error\\nSolution: check the dbt_cloud_setup.md  file and make a SSH Key and use gitclone to import repo into dbt project, copy and paste deploy key back in your repo setting.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Setup - Your IDE session was unable to start. Please contact support.', 'course': 'data-engineering-zoomcamp', 'id': '6ba02f77'}]), ('8b14286c', [{'text': 'Issue: If you don’t define the column format while converting from csv to parquet Python will “choose” based on the first rows.\\n✅Solution: Defined the schema while running web_to_gcp.py pipeline.\\nSebastian adapted the script:\\nhttps://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py\\nNeed a quick change to make the file work with gz files, added the following lines (and don’t forget to delete the file at the end of each iteration of the loop to avoid any problem of disk space)\\nfile_name_gz = f\"{service}_tripdata_{year}-{month}.csv.gz\"\\nopen(file_name_gz, \\'wb\\').write(r.content)\\nos.system(f\"gzip -d {file_name_gz}\")\\nos.system(f\"rm {file_name_init}.*\")\\nSame ERROR - When running dbt run for fact_trips.sql, the task failed with error:\\n“Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64”\\n开启屏幕阅读器支持\\n要启用屏幕阅读器支持，请按Ctrl+Alt+Z。要了解键盘快捷键，请按Ctrl+斜杠。\\n查找和替换\\nReason: Parquet files have their own schema. Some parquet files for green data have records with decimals in ehail_fee column.\\nThere are some possible fixes:\\nDrop ehail_feel column since it is not really used. For instance when creating a partitioned table from the external table in BigQuery\\nSELECT * EXCEPT (ehail_fee) FROM…\\nModify stg_green_tripdata.sql model using this line cast(0 as numeric) as ehail_fee.\\nModify Airflow dag to make the conversion and avoid the error.\\npv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types = {\\'ehail_fee\\': \\'float64\\'}))\\nSame type of ERROR - parquet files with different data types - Fix it with pandas\\nHere is another possibility that could be interesting:\\nYou can specify the dtypes when importing the file from csv to a dataframe with pandas\\npd.from_csv(..., dtype=type_dict)\\nOne obstacle is that the regular int64 pandas use (I think this is from the numpy library) does not accept null values (NaN, not a number). But you can use the pandas Int64 instead, notice capital ‘I’. The type_dict is a python dictionary mapping the column names to the dtypes.\\nSources:\\nhttps://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\\nNullable integer data type — pandas 1.5.3 documentation', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT - I am having problems with columns datatype while running DBT/BigQuery', 'course': 'data-engineering-zoomcamp', 'id': '8b14286c'}]), ('14a876ea', [{'text': 'If the provided URL isn’t working for you (https://nyc-tlc.s3.amazonaws.com/trip+data/):\\nWe can use the GitHub CLI to easily download the needed trip data from https://github.com/DataTalksClub/nyc-tlc-data, and manually upload to a GCS bucket.\\nInstructions on how to download the CLI here: https://github.com/cli/cli\\nCommands to use:\\ngh auth login\\ngh release list -R DataTalksClub/nyc-tlc-data\\ngh release download yellow -R DataTalksClub/nyc-tlc-data\\ngh release download green -R DataTalksClub/nyc-tlc-data\\netc.\\nNow you can upload the files to a GCS bucket using the GUI.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Ingestion: When attempting to use the provided quick script to load trip data into GCS, you receive error Access Denied from the S3 bucket', 'course': 'data-engineering-zoomcamp', 'id': '14a876ea'}]), ('1cf5be74', [{'text': \"R: This conversion is needed for the question 3 of homework, in order to process files for fhv data. The error is:\\npyarrow.lib.ArrowInvalid: CSV parse error: Expected 7 columns, got 1: B02765\\nCause: Some random line breaks in this particular file.\\nFixed by opening a bash in the container executing the dag and manually running the following command that deletes all \\\\n not preceded by \\\\r.\\nperl -i -pe 's/(?<!\\\\r)\\\\n/\\\\1/g' fhv_tripdata_2020-01.csv\\nAfter that, clear the failed task in Airflow to force re-execution.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Ingestion - Error thrown by format_to_parquet_task when converting fhv_tripdata_2020-01.csv using Airflow', 'course': 'data-engineering-zoomcamp', 'id': '1cf5be74'}]), ('315ac3cc', [{'text': 'I initially followed data-engineering-zoomcamp/03-data-warehouse/extras/web_to_gcs.py at main · DataTalksClub/data-engineering-bootcamp (github.com)\\nBut it was taking forever for the yellow trip data and when I tried to download and upload the parquet files directly to GCS, that works fine but when creating the Bigquery table, there was a schema inconsistency issue\\nThen I found another hack shared in the slack which was suggested by Victoria.\\n[Optional] Hack for loading data to BigQuery for Week 4 - YouTube\\nPlease watch until the end as there is few schema changes required to be done', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Hack to load yellow and green trip data for 2019 and 2020', 'course': 'data-engineering-zoomcamp', 'id': '315ac3cc'}]), ('c5c3beba', [{'text': '“gs\\\\storage_link\\\\*.parquet” need to be added in destination folder', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Move many files (more than one) from Google cloud storage bucket to Big query', 'course': 'data-engineering-zoomcamp', 'id': 'c5c3beba'}]), ('f19be91b', [{'text': 'One common cause experienced is lack of space after running prefect several times. When running prefect, check the folder ‘.prefect/storage’ and delete the logs now and then to avoid the problem.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'GCP VM - All of sudden ssh stopped working for my VM after my last restart', 'course': 'data-engineering-zoomcamp', 'id': 'f19be91b'}]), ('33db7dc7', [{'text': 'You can try to do this steps:', 'section': 'Module 4: analytics engineering with dbt', 'question': 'GCP VM - If you have lost SSH access to your machine due to lack of space. Permission denied (publickey)', 'course': 'data-engineering-zoomcamp', 'id': '33db7dc7'}]), ('67ef8f87', [{'text': 'R: Go to BigQuery, and check the location of BOTH\\nThe source dataset (trips_data_all), and\\nThe schema you’re trying to write to (name should be \\tdbt_<first initial><last name> (if you didn’t change the default settings at the end when setting up your project))\\nLikely, your source data will be in your region, but the write location will be a multi-regional location (US in this example). Delete these datasets, and recreate them with your specified region and the correct naming format.\\nAlternatively, instead of removing datasets, you can specify the single-region location you are using. E.g. instead of ‘location: US’, specify the region, so ‘location: US-east1’. See this Github comment for more detail. Additionally please see this post of Sandy\\nIn DBT cloud you can actually specify the location using the following steps:\\nGPo to your profile page (top right drop-down --> profile)\\nThen go to under Credentials --> Analytics (you may have customised this name)\\nClick on Bigquery >\\nHit Edit\\nUpdate your location, you may need to re-upload your service account JSON to re-fetch your private key, and save. (NOTE: be sure to exactly copy the region BigQuery specifies your dataset is in.)', 'section': 'Module 4: analytics engineering with dbt', 'question': '404 Not found: Dataset eighth-zenith-372015:trip_data_all was not found in location us-west1', 'course': 'data-engineering-zoomcamp', 'id': '67ef8f87'}]), ('6acf2e77', [{'text': 'Error: `dbt_utils.surrogate_key` has been replaced by `dbt_utils.generate_surrogate_key`\\nFix:\\nReplace dbt_utils.surrogate_key  with dbt_utils.generate_surrogate_key in stg_green_tripdata.sql\\nWhen executing dbt run after fact_trips.sql has been created, the task failed with error:\\nR: “Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.”\\n1. Fixed by adding the Storage Object Viewer role to the service account in use in BigQuery.\\n2. Add the related roles to the service account in use in GCS.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'When executing dbt run after installing dbt-utils latest version i.e., 1.0.0 warning has generated', 'course': 'data-engineering-zoomcamp', 'id': '6acf2e77'}]), ('18430f10', [{'text': 'You need to create packages.yml file in main project directory and add packages’ meta data:\\npackages:\\n- package: dbt-labs/dbt_utils\\nversion: 0.8.0\\nAfter creating file run:\\nAnd hit enter.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'When You are getting error dbt_utils not found', 'course': 'data-engineering-zoomcamp', 'id': '18430f10'}]), ('afb7a40a', [{'text': \"Ensure you properly format your yml file. Check the build logs if the run was completed successfully. You can expand the command history console (where you type the --vars '{'is_test_run': 'false'}')  and click on any stage’s logs to expand and read errors messages or warnings.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Lineage is currently unavailable. Check that your project does not contain compilation errors or contact support if this error persists.', 'course': 'data-engineering-zoomcamp', 'id': 'afb7a40a'}]), ('d6a5b80e', [{'text': \"Make sure you use:\\ndbt run --var ‘is_test_run: false’ or\\ndbt build --var ‘is_test_run: false’\\n(watch out for formatted text from this document: re-type the single quotes). If that does not work, use --vars '{'is_test_run': 'false'}' with each phrase separately quoted.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Build - Why do my Fact_trips only contain a few days of data?', 'course': 'data-engineering-zoomcamp', 'id': 'd6a5b80e'}]), ('de426d2f', [{'text': 'Check if you specified if_exists argument correctly when writing data from GCS to BigQuery. When I wrote my automated flow for each month of the years 2019 and 2020 for green and yellow data I had specified if_exists=\"replace\" while I was experimenting with the flow setup. Once you want to run the flow for all months in 2019 and 2020 make sure to set if_exists=\"append\"\\nif_exists=\"replace\" will replace the whole table with only the month data that you are writing into BigQuery in that one iteration -> you end up with only one month in BigQuery (the last one you inserted)\\nif_exists=\"append\" will append the new monthly data -> you end up with data from all months', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Build - Why do my fact_trips only contain one month of data?', 'course': 'data-engineering-zoomcamp', 'id': 'de426d2f'}]), ('354f0e10', [{'text': \"R: After the second SELECT, change this line:\\ndate_trunc('month', pickup_datetime) as revenue_month,\\nTo this line:\\ndate_trunc(pickup_datetime, month) as revenue_month,\\nMake sure that “month” isn’t surrounded by quotes!\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'BigQuery returns an error when I try to run the dm_monthly_zone_revenue.sql model.', 'course': 'data-engineering-zoomcamp', 'id': '354f0e10'}]), ('98fae8d0', [{'text': 'For this instead:\\n{{ dbt_utils.generate_surrogate_key([ \\n     field_a, \\n     field_b, \\n     field_c,\\n     …,\\n     field_z\\n]) }}', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Replace: \\n{{ dbt_utils.surrogate_key([ \\n     field_a, \\n     field_b, \\n     field_c,\\n     …,\\n     field_z     \\n]) }}', 'course': 'data-engineering-zoomcamp', 'id': '98fae8d0'}]), ('cb678fde', [{'text': 'Remove the dataset from BigQuery which was created by dbt and run dbt run again so that it will recreate the dataset in BigQuery with the correct location', 'section': 'Module 4: analytics engineering with dbt', 'question': 'I changed location in dbt, but dbt run still gives me an error', 'course': 'data-engineering-zoomcamp', 'id': 'cb678fde'}]), ('39bfb043', [{'text': 'Remove the dataset from BigQuery created by dbt and run again (with test disabled) to ensure the dataset created has all the rows.\\nDBT - Why am I getting a new dataset after running my CI/CD Job? / What is this new dbt dataset in BigQuery?\\nAnswer: when you create the CI/CD job, under ‘Compare Changes against an environment (Deferral) make sure that you select ‘ No; do not defer to another environment’ - otherwise dbt won’t merge your dev models into production models; it will create a new environment called ‘dbt_cloud_pr_number of pull request’', 'section': 'Module 4: analytics engineering with dbt', 'question': 'I ran dbt run without specifying variable which gave me a table of 100 rows. I ran again with the variable value specified but my table still has 100 rows in BQ.', 'course': 'data-engineering-zoomcamp', 'id': '39bfb043'}]), ('351a078a', [{'text': \"Vic created three different datasets in the videos.. dbt_<name> was used for development and you used a production dataset for the production environment. What was the use for the staging dataset?\\nR: Staging, as the name suggests, is like an intermediate between the raw datasets and the fact and dim tables, which are the finished product, so to speak. You'll notice that the datasets in staging are materialised as views and not tables.\\nVic didn't use it for the project, you just need to create production and dbt_name + trips_data_all that you had already.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Why do we need the Staging dataset?', 'course': 'data-engineering-zoomcamp', 'id': '351a078a'}]), ('61da1919', [{'text': 'Try removing the “network: host” line in docker-compose.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT Docs Served but Not Accessible via Browser', 'course': 'data-engineering-zoomcamp', 'id': '61da1919'}]), ('6528c6ae', [{'text': 'Go to Account settings >> Project >> Analytics >> Click on your connection >> go all the way down to Location and type in the GCP location just as displayed in GCP (e.g. europe-west6). You might need to reupload your GCP key.\\nDelete your dataset in GBQ\\nRebuild project: dbt build\\nNewly built dataset should be in the correct location', 'section': 'Module 4: analytics engineering with dbt', 'question': 'BigQuery adapter: 404 Not found: Dataset was not found in location europe-west6', 'course': 'data-engineering-zoomcamp', 'id': '6528c6ae'}]), ('c0d3a2e8', [{'text': 'Create a new branch to edit. More on this can be found here in the dbt docs.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Dbt+git - Main branch is “read-only”', 'course': 'data-engineering-zoomcamp', 'id': 'c0d3a2e8'}]), ('859a97c5', [{'text': 'Create a new branch for development, then you can merge it to the main branch\\nCreate a new branch and switch to this branch. It allows you to make changes. Then you can commit and push the changes to the “main” branch.', 'section': 'Module 4: analytics engineering with dbt', 'question': \"Dbt+git - It appears that I can't edit the files because I'm in read-only mode. Does anyone know how I can change that?\", 'course': 'data-engineering-zoomcamp', 'id': '859a97c5'}]), ('32469a2d', [{'text': \"Error:\\nTriggered by pull requests\\nThis feature is only available for dbt repositories connected through dbt Cloud's native integration with Github, Gitlab, or Azure DevOps\\nSolution: Contrary to the guide on DTC repo, don’t use the Git Clone option. Use the Github one instead. Step-by-step guide to UN-LINK Git Clone and RE-LINK with Github in the next entry below\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Dbt deploy + Git CI - cannot create CI checks job for deployment to Production. See more discussion in slack chat', 'course': 'data-engineering-zoomcamp', 'id': '32469a2d'}]), ('c599b3a0', [{'text': 'If you’re trying to configure CI with Github and on the job’s options you can’t see Run on Pull Requests? on triggers, you have to reconnect with Github using native connection instead clone by SSH. Follow these steps:\\nOn Profile Settings > Linked Accounts connect your Github account with dbt project allowing the permissions asked. More info at https://docs.getdbt.com/docs/collaborate/git/connect-gith\\nDisconnect your current Github’s configuration from Account Settings > Projects (analytics) > Github connection. At the bottom left appears the button Disconnect, press it.\\nOnce we have confirmed the change, we can configure it again. This time, choose Github and it will appear in all repositories which you have allowed to work with dbt. Select your repository and it’s ready.\\nGo to the Deploy > job configuration’s page and go down until Triggers and now you can see the option Run on Pull Requests:', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Dbt deploy + Git CI - Unable to configure Continuous Integration (CI) with Github', 'course': 'data-engineering-zoomcamp', 'id': 'c599b3a0'}]), ('179df18d', [{'text': \"If you're following video DE Zoomcamp 4.3.1 - Building the First DBT Models, you may have encountered an issue at 14:25 where the Lineage graph isn't displayed and a Compilation Error occurs, as shown in the attached image. Don't worry - a quick fix for this is to simply save your schema.yml file. Once you've done this, you should be able to view your Lineage graph without any further issues.\", 'section': 'Module 4: analytics engineering with dbt', 'question': \"Compilation Error (Model 'model.my_new_project.stg_green_tripdata' (models/staging/stg_green_tripdata.sql) depends on a source named 'staging.green_trip_external' which was not found)\", 'course': 'data-engineering-zoomcamp', 'id': '179df18d'}]), ('1ce1a275', [{'text': '> in macro test_accepted_values (tests/generic/builtin.sql)\\n> called by test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)\\nRemember that you have to add to dbt_project.yml the vars:\\nvars:\\npayment_type_values: [1, 2, 3, 4, 5, 6]', 'section': 'Module 4: analytics engineering with dbt', 'question': \"'NoneType' object is not iterable\", 'course': 'data-engineering-zoomcamp', 'id': '1ce1a275'}]), ('b529b0bc', [{'text': \"You will face this issue if you copied and pasted the exact macro directly from data-engineering-zoomcamp repo.\\nBigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]; reason: invalidQuery, location: query, message: No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]')\\nWhat you’d have to do is to change the data type of the numbers (1, 2, 3 etc.) to text by inserting ‘’, as the initial ‘payment_type’ data type should be string (Note: I extracted and loaded the green trips data using Google BQ Marketplace)\\n{#\\nThis macro returns the description of the payment_type\\n#}\\n{% macro get_payment_type_description(payment_type) -%}\\ncase {{ payment_type }}\\nwhen '1' then 'Credit card'\\nwhen '2' then 'Cash'\\nwhen '3' then 'No charge'\\nwhen '4' then 'Dispute'\\nwhen '5' then 'Unknown'\\nwhen '6' then 'Voided trip'\\nend\\n{%- endmacro %}\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'dbt macro errors with get_payment_type_description(payment_type)', 'course': 'data-engineering-zoomcamp', 'id': 'b529b0bc'}]), ('2e51a111', [{'text': 'The dbt error  log contains a link to BigQuery. When you follow it you will see your query and the problematic line will be highlighted.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Troubleshooting in dbt:', 'course': 'data-engineering-zoomcamp', 'id': '2e51a111'}]), ('6e1a0834', [{'text': 'It is a default behaviour of dbt to append custom schema to initial schema. To override this behaviour simply create a macro named “generate_schema_name.sql”:\\n{% macro generate_schema_name(custom_schema_name, node) -%}\\n{%- set default_schema = target.schema -%}\\n{%- if custom_schema_name is none -%}\\n{{ default_schema }}\\n{%- else -%}\\n{{ custom_schema_name | trim }}\\n{%- endif -%}\\n{%- endmacro %}\\nNow you can override default custom schema in “dbt_project.yml”:', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Why changing the target schema to “marts” actually creates a schema named “dbt_marts” instead?', 'course': 'data-engineering-zoomcamp', 'id': '6e1a0834'}]), ('a8657e65', [{'text': 'There is a project setting which allows you to set `Project subdirectory` in dbt cloud:', 'section': 'Module 4: analytics engineering with dbt', 'question': 'How to set subdirectory of the github repository as the dbt project root', 'course': 'data-engineering-zoomcamp', 'id': 'a8657e65'}]), ('2678d8c2', [{'text': \"Remember that you should modify accordingly your .sql models, to read from existing table names in BigQuery/postgres db\\nExample: select * from {{ source('staging',<your table name in the database>') }}\", 'section': 'Module 4: analytics engineering with dbt', 'question': \"Compilation Error : Model 'model.XXX' (models/<model_path>/XXX.sql) depends on a source named '<a table name>' which was not found\", 'course': 'data-engineering-zoomcamp', 'id': '2678d8c2'}]), ('aa85c6ae', [{'text': 'Make sure that you create a pull request from your Development branch to the Production branch (main by default). After that, check in your ‘seeds’ folder if the seed file is inside it.\\nAnother thing to check is your .gitignore file. Make sure that the .csv extension is not included.', 'section': 'Module 4: analytics engineering with dbt', 'question': \"Compilation Error : Model '<model_name>' (<model_path>) depends on a node named '<seed_name>' which was not found   (Production Environment)\", 'course': 'data-engineering-zoomcamp', 'id': 'aa85c6ae'}]), ('de06929d', [{'text': '1. Go to your dbt cloud service account\\n1. Adding the  [Storage Object Admin,Storage Admin] role in addition tco BigQuery Admin.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'When executing dbt run after using fhv_tripdata as an external table: you get “Access Denied: BigQuery BigQuery: Permission denied”', 'course': 'data-engineering-zoomcamp', 'id': 'de06929d'}]), ('b087fa95', [{'text': 'Problem: when injecting data to bigquery, you may face the type error. This is because pandas by default will parse integer columns with missing value as float type.\\nSolution:\\nOne way to solve this problem is to specify/ cast data type Int64 during the data transformation stage.\\nHowever, you may be lazy to type all the int columns. If that is the case, you can simply use convert_dtypes to infer the data type\\n# Make pandas to infer correct data type (as pandas parse int with missing as float)\\ndf.fillna(-999999, inplace=True)\\ndf = df.convert_dtypes()\\ndf = df.replace(-999999, None)', 'section': 'Module 4: analytics engineering with dbt', 'question': 'How to automatically infer the column data type (pandas missing value issues)?', 'course': 'data-engineering-zoomcamp', 'id': 'b087fa95'}]), ('3c41892d', [{'text': 'Seed files loaded from directory with name ‘seed’, that’s why you should rename dir with name ‘data’ to ‘seed’', 'section': 'Module 4: analytics engineering with dbt', 'question': 'When loading github repo raise exception that ‘taxi_zone_lookup’ not found', 'course': 'data-engineering-zoomcamp', 'id': '3c41892d'}]), ('4842f3e8', [{'text': 'Check the .gitignore file and make sure you don’t have *.csv in it\\n\\nDbt error 404 was not found in location\\nMy specific error:\\nRuntime Error in rpc request (from remote system.sql) 404 Not found: Table dtc-de-0315:trips_data_all.green_tripdata_partitioned was not found in location europe-west6 Location: europe-west6 Job ID: 168ee9bd-07cd-4ca4-9ee0-4f6b0f33897c\\nMake sure all of your datasets have the correct region and not a generalised region:\\nEurope-west6 as opposed to EU\\n\\nMatch this in dbt settings:\\ndbt -> projects -> optional settings -> manually set location to match', 'section': 'Module 4: analytics engineering with dbt', 'question': '‘taxi_zone_lookup’ not found', 'course': 'data-engineering-zoomcamp', 'id': '4842f3e8'}]), ('5eaf61fe', [{'text': \"The easiest way to avoid these errors is by ingesting the relevant data in a .csv.gz file type. Then, do:\\nCREATE OR REPLACE EXTERNAL TABLE `dtc-de.trips_data_all.fhv_tripdata`\\nOPTIONS (\\nformat = 'CSV',\\nuris = ['gs://dtc_data_lake_dtc-de-updated/data/fhv_all/fhv_tripdata_2019-*.csv.gz']\\n);\\nAs an example. You should no longer have any data type issues for week 4.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Data type errors when ingesting with parquet files', 'course': 'data-engineering-zoomcamp', 'id': '5eaf61fe'}]), ('8ed36cea', [{'text': 'This is due to the way the deduplication is done in the two staging files.\\nSolution: add order by in the partition by part of both staging files. Keep adding columns to order by until the number of rows in the fact_trips table is consistent when re-running the fact_trips model.\\nExplanation (a bit convoluted, feel free to clarify, correct etc.)\\nWe partition by vendor id and pickup_datetime and choose the first row (rn=1) from all these partitions. These partitions are not ordered, so every time we run this, the first row might be a different one. Since the first row is different between runs, it might or might not contain an unknown borough. Then, in the fact_trips model we will discard a different number of rows when we discard all values with an unknown borough.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Inconsistent number of rows when re-running fact_trips model', 'course': 'data-engineering-zoomcamp', 'id': '8ed36cea'}]), ('46aebc79', [{'text': 'If you encounter data type error on trip_type column, it may due to some nan values that isn’t null in bigquery.\\nSolution: try casting it to FLOAT datatype instead of NUMERIC', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Data Type Error when running fact table', 'course': 'data-engineering-zoomcamp', 'id': '46aebc79'}]), ('e2d2bc58', [{'text': \"This error could result if you are using some select * query without mentioning the name of table for ex:\\nwith dim_zones as (\\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`dim_zones`\\nwhere borough != 'Unknown'\\n),\\nfhv as (\\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`stg_fhv_tripdata`\\n)\\nselect * from fhv\\ninner join dim_zones as pickup_zone\\non fhv.PUlocationID = pickup_zone.locationid\\ninner join dim_zones as dropoff_zone\\non fhv.DOlocationID = dropoff_zone.locationid\\n);\\nTo resolve just replace use : select fhv.* from fhv\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'CREATE TABLE has columns with duplicate name locationid.', 'course': 'data-engineering-zoomcamp', 'id': 'e2d2bc58'}]), ('137aab88', [{'text': 'Some ehail fees are null and casting them to integer gives Bad int64 value: 0.0 error,\\nSolution:\\nUsing safe_cast returns NULL instead of throwing an error. So use safe_cast from dbt_utils function in the jinja code for casting into integer as follows:\\n{{ dbt_utils.safe_cast(\\'ehail_fee\\',  api.Column.translate_type(\"integer\"))}} as ehail_fee,\\nCan also just use safe_cast(ehail_fee as integer) without relying on dbt_utils.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Bad int64 value: 0.0 error', 'course': 'data-engineering-zoomcamp', 'id': '137aab88'}]), ('a260e651', [{'text': \"You might encounter this when building the fact_trips.sql model. The issue may be with the payment_type_description field.\\nUsing safe_cast as above, would cause the entire field to become null. A better approach is to drop the offending decimal place, then cast to integer.\\ncast(replace({{ payment_type }},'.0','') as integer)\\nBad int64 value: 1.0 error (again)\\n\\nI found that there are more columns causing the bad INT64: ratecodeid and trip_type on Green_tripdata table.\\nYou can use the queries below to address them:\\nCAST(\\nREGEXP_REPLACE(CAST(rate_code AS STRING), r'\\\\.0', '') AS INT64\\n) AS ratecodeid,\\nCAST(\\nCASE\\nWHEN REGEXP_CONTAINS(CAST(trip_type AS STRING), r'\\\\.\\\\d+') THEN NULL\\nELSE CAST(trip_type AS INT64)\\nEND AS INT64\\n) AS trip_type,\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Bad int64 value: 2.0/1.0 error', 'course': 'data-engineering-zoomcamp', 'id': 'a260e651'}]), ('da8d9fcc', [{'text': 'The two solution above don’t work for me - I used the line below in `stg_green_trips.sql` to replace the original ehail_fee line:\\n`{{ dbt.safe_cast(\\'ehail_fee\\',  api.Column.translate_type(\"numeric\"))}} as ehail_fee,`', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT - Error on building fact_trips.sql: Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64. File: gs://<gcs bucket>/<table>/green_taxi_2019-01.parquet\")', 'course': 'data-engineering-zoomcamp', 'id': 'da8d9fcc'}]), ('2314e3c4', [{'text': \"Remember to add a space between the variable and the value. Otherwise, it won't be interpreted as a dictionary.\\nIt should be:\\ndbt run --var 'is_test_run: false'\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'The - vars argument must be a YAML dictionary, but was of type str', 'course': 'data-engineering-zoomcamp', 'id': '2314e3c4'}]), ('e7bdbba6', [{'text': \"You don't need to change the environment type. If you are following the videos, you are creating a Production Deployment, so the only available option is the correct one.'\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Not able to change Environment Type as it is greyed out and inaccessible', 'course': 'data-engineering-zoomcamp', 'id': 'e7bdbba6'}]), ('52cccade', [{'text': 'Database Error in model stg_yellow_tripdata (models/staging/stg_yellow_tripdata.sql)\\nAccess Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.\\ncompiled Code at target/run/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql\\nIn my case, I was set up in a different branch, so always check the branch you are working on. Change the 04-analytics-engineering/taxi_rides_ny/models/staging/schema.yml file in the\\nsources:\\n- name: staging\\ndatabase: your_database_name\\nIf this error will continue when running dbt job, As for changing the branch for your job, you can use the ‘Custom Branch’ settings in your dbt Cloud environment. This allows you to run your job on a different branch than the default one (usually main). To do this, you need to:\\nGo to an environment and select Settings to edit it\\nSelect Only run on a custom branch in General settings\\nEnter the name of your custom branch (e.g. HW)\\nClick Save\\nCould not parse the dbt project. please check that the repository contains a valid dbt project\\nRunning the Environment on the master branch causes this error, you must activate “Only run on a custom branch” checkbox and specify the branch you are  working when Environment is setup.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Access Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.', 'course': 'data-engineering-zoomcamp', 'id': '52cccade'}]), ('11a814ea', [{'text': 'Change to main branch, make a pull request from the development branch.\\nNote: this will take you to github.\\nApprove the merging and rerun you job, it would work as planned now', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Made change to your modelling files and commit the your development branch, but Job still runs on old file?', 'course': 'data-engineering-zoomcamp', 'id': '11a814ea'}]), ('0d1e02d5', [{'text': 'Before you can develop some data model on dbt, you should create development environment and set some parameter on it. After the model being developed, we should also create deployment environment to create and run some jobs.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Setup - I’ve set Github and Bigquery to dbt successfully. Why nothing showed in my Develop tab?', 'course': 'data-engineering-zoomcamp', 'id': '0d1e02d5'}]), ('0a0cc4c3', [{'text': 'Error Message:\\nInvestigate Sentry error: ProtocolError \"Invalid input ConnectionInputs.SEND_HEADERS in state ConnectionState.CLOSED\"\\nSolution:\\nreference\\nRun it again because it happens sometimes. Or wait a few minutes, it will continue.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Prefect Agent retrieving runs from queue sometimes fails with httpx.LocalProtocolError', 'course': 'data-engineering-zoomcamp', 'id': '0a0cc4c3'}]), ('cb912983', [{'text': \"My taxi data was loaded into gcs with etl_web_to_gcs.py script that converts csv data into parquet. Then I placed raw data trips into external tables and when I executed dbt run I got an error message: Parquet column 'passenger_count' has type INT64 which does not match the target cpp_type DOUBLE. It is because several columns in files have different formats of data.\\nWhen I added df[col] = df[col].astype('Int64') transformation to the columns: passenger_count, payment_type, RatecodeID, VendorID, trip_type it went ok. Several people also faced this error and more about it you can read on the slack channel.\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'BigQuery returns an error when i try to run ‘dbt run’:', 'course': 'data-engineering-zoomcamp', 'id': 'cb912983'}]), ('2d4e434f', [{'text': 'Use the syntax below instead if the code in the tutorial is not working.\\ndbt run --select stg_green_tripdata --vars \\'{\"is_test_run\": false}\\'', 'section': 'Module 4: analytics engineering with dbt', 'question': \"Running dbt run --models stg_green_tripdata --var 'is_test_run: false' is not returning anything:\", 'course': 'data-engineering-zoomcamp', 'id': '2d4e434f'}]), ('bb6655b9', [{'text': \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\", 'section': 'Module 4: analytics engineering with dbt', 'question': \"DBT - Error: No module named 'pytz' while setting up dbt with docker\", 'course': 'data-engineering-zoomcamp', 'id': 'bb6655b9'}]), ('fc2eb036', [{'text': \"If you have problems editing dbt_project.yml when using Docker after ‘docker-compose run dbt-bq-dtc init’, to change profile ‘taxi_rides_ny’ to 'bq-dbt-workshop’, just run:\\nsudo chown -R username path\\nDBT - Internal Error: Profile should not be None if loading is completed\\nWhen  running dbt debug, change the directory to the newly created subdirectory (e.g: the newly created `taxi_rides_ny` directory, which contains the dbt project).\", 'section': 'Module 4: analytics engineering with dbt', 'question': '\\u200b\\u200bVS Code: NoPermissions (FileSystemError): Error: EACCES: permission denied (linux)', 'course': 'data-engineering-zoomcamp', 'id': 'fc2eb036'}]), ('25daead9', [{'text': 'When running a query on BigQuery sometimes could appear a this table is not on the specified location error.\\nFor this problem there is not a straightforward solution, you need to dig a little, but the problem could be one of these:\\nCheck the locations of your bucket, datasets and tables. Make sure they are all on the same one.\\nChange the query settings to the location you are in: on the query window select more -> query settings -> select the location\\nCheck if all the paths you are using in your query to your tables are correct: you can click on the table -> details -> and copy the path.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Google Cloud BigQuery Location Problems', 'course': 'data-engineering-zoomcamp', 'id': '25daead9'}]), ('2221d75e', [{'text': 'This happens because we have moved the dbt project to another directory on our repo.\\nOr might be that you’re on a different branch than is expected to be merged from / to.\\nSolution:\\nGo to the projects window on dbt cloud -> settings -> edit -> and add directory (the extra path to the dbt project)\\nFor example:\\n/week5/taxi_rides_ny\\nMake sure your file explorer path and this Project settings path matches and there’s no files waiting to be committed to github if you’re running the job to deploy to PROD.\\nAnd that you had setup the PROD environment to check in the main branch, or whichever you specified.\\nIn the picture below, I had set it to ella2024 to be checked as “production-ready” by the “freshness” check mark at the PROD environment settings. So each time I merge a branch from something else into ella2024 and then trigger the PR, the CI check job would kick-in. But we still do need to Merge and close the PR manually, I believe, that part is not automated.\\nYou set up the PROD custom branch (if not default main) in the Environment setup screen.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT Deploy - This dbt Cloud run was cancelled because a valid dbt project was not found.', 'course': 'data-engineering-zoomcamp', 'id': '2221d75e'}]), ('94524a9d', [{'text': 'When you are creating the pull request and running the CI, dbt is creating a new schema on BIgQuery. By default that new schema will be created on ‘US’ location, if you have your dataset, schemas and tables on ‘EU’ that will generate an error and the pull request will not be accepted. To change that location to ‘EU’ on the connection to BigQuery from dbt we need to add the location ‘EU’ on the connection optional settings:\\nDbt -> project -> settings -> connection BIgQuery -> OPtional Settings -> Location -> EU', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT Deploy + CI - Location Problems on BigQuery', 'course': 'data-engineering-zoomcamp', 'id': '94524a9d'}]), ('1f1ecbb7', [{'text': 'When running trying to run the dbt project on prod there is some things you need to do and check on your own:\\nFirst Make the pull request and Merge the branch into the main.\\nMake sure you have the latest version, if you made changes to the repo in another place.\\nCheck if the dbt_project.yml file is accessible to the project, if not check this solution (Dbt: This dbt Cloud run was cancelled because a valid dbt project was not found.).\\nCheck if the name you gave to the dataset on BigQuery is the same you put on the dataset spot on the production environment created on dbt cloud.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT Deploy - Error When trying to run the dbt project on Prod', 'course': 'data-engineering-zoomcamp', 'id': '1f1ecbb7'}]), ('c5af32ab', [{'text': 'In the step in this video (DE Zoomcamp 4.3.1 - Build the First dbt Models), after creating `stg_green_tripdata.sql` and clicking `build`, I encountered an error saying dataset not found in location EU. The default location for dbt Bigquery is the US, so when generating the new Bigquery schema for dbt, unless specified, the schema locates in the US.\\nSolution:\\nTurns out I forgot to specify Location to be `EU` when adding connection details.\\nDevelop -> Configure Cloud CLI -> Projects -> taxi_rides_ny -> (connection) Bigquery -> Edit -> Location (Optional) -> type `EU` -> Save', 'section': 'Module 4: analytics engineering with dbt', 'question': 'DBT - Error: “404 Not found: Dataset <dataset_name>:<dbt_schema_name> was not found in location EU” after building from stg_green_tripdata.sql', 'course': 'data-engineering-zoomcamp', 'id': 'c5af32ab'}]), ('1e6b7da1', [{'text': 'Issue: If you’re having problems loading the FHV_20?? data from the github repo into GCS and then into BQ (input file not of type parquet), you need to do two things. First, append the URL Template link with ‘?raw=true’ like so:\\nURL_TEMPLATE = URL_PREFIX + \"/fhv_tripdata_{{ execution_date.strftime(\\\\\\'%Y-%m\\\\\\') }}.parquet?raw=true\"\\nSecond, update make sure the URL_PREFIX is set to the following value:\\n\\nURL_PREFIX = \"https://github.com/alexeygrigorev/datasets/blob/master/nyc-tlc/fhv\"\\nIt is critical that you use this link with the keyword blob. If your link has ‘tree’ here, replace it. Everything else can stay the same, including the curl -sSLf command. ‘', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Homework - Ingesting FHV_20?? data', 'course': 'data-engineering-zoomcamp', 'id': '1e6b7da1'}]), ('259481c4', [{'text': 'I found out that the easies way to upload datasets form github for the homework is utilising this script git_csv_to_gcs.py. Thank you Lidia!!\\nIt is similar to a script that Alexey provided us in 03-data-warehouse/extras/web_to_gcs.py', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Homework - Ingesting NYC TLC Data', 'course': 'data-engineering-zoomcamp', 'id': '259481c4'}]), ('edbae698', [{'text': 'If you have to securely put your credentials for a project and, probably, push it to a git repository then the best option is to use an environment variable\\nFor example for web_to_gcs.py or git_csv_to_gcs.py we have to set these variables:\\nGOOGLE_APPLICATION_CREDENTIALS\\nGCP_GCS_BUCKET\\nThe easises option to do it  is to use .env  (dotenv).\\nInstall it and add a few lines of code that inject these variables for your project\\npip install python-dotenv\\nfrom dotenv import load_dotenv\\nimport os\\n# Load environment variables from .env file\\nload_dotenv()\\n# Now you can access environment variables like GCP_GCS_BUCKET and GOOGLE_APPLICATION_CREDENTIALS\\ncredentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\\nBUCKET = os.environ.get(\"GCP_GCS_BUCKET\")', 'section': 'Module 4: analytics engineering with dbt', 'question': 'How to set environment variable easily for any credentials', 'course': 'data-engineering-zoomcamp', 'id': 'edbae698'}]), ('67217f4c', [{'text': \"If you uploaded manually the fvh 2019 csv files, you may face errors regarding date types. Try to create an the external table in bigquery but define the pickup_datetime and dropoff_datetime to be strings\\nCREATE OR REPLACE EXTERNAL TABLE `gcp_project.trips_data_all.fhv_tripdata`  (\\ndispatching_base_num STRING,\\npickup_datetime STRING,\\ndropoff_datetime STRING,\\nPUlocationID STRING,\\nDOlocationID STRING,\\nSR_Flag STRING,\\nAffiliated_base_number STRING\\n)\\nOPTIONS (\\nformat = 'csv',\\nuris = ['gs://bucket/*.csv']\\n);\\nThen when creating the fhv core model in dbt, use TIMESTAMP(CAST(()) to ensure it first parses as a string and then convert it to timestamp.\\nwith fhv_tripdata as (\\nselect * from {{ ref('stg_fhv_tripdata') }}\\n),\\ndim_zones as (\\nselect * from {{ ref('dim_zones') }}\\nwhere borough != 'Unknown'\\n)\\nselect fhv_tripdata.dispatching_base_num,\\nTIMESTAMP(CAST(fhv_tripdata.pickup_datetime AS STRING)) AS pickup_datetime,\\nTIMESTAMP(CAST(fhv_tripdata.dropoff_datetime AS STRING)) AS dropoff_datetime,\", 'section': 'Module 4: analytics engineering with dbt', 'question': \"Invalid date types after Ingesting FHV data through CSV files: Could not parse 'pickup_datetime' as a timestamp\", 'course': 'data-engineering-zoomcamp', 'id': '67217f4c'}]), ('2aadd232', [{'text': \"If you uploaded manually the fvh 2019 parquet files manually after downloading from https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-*.parquet you may face errors regarding date types while loading the data in a landing table (say fhv_tripdata). Try to create an the external table with the schema defines as following and load each month in a loop.\\n-----Correct load with schema defination----will not throw error----------------------\\nCREATE OR REPLACE EXTERNAL TABLE `dw-bigquery-week-3.trips_data_all.external_tlc_fhv_trips_2019` (\\ndispatching_base_num STRING,\\npickup_datetime TIMESTAMP,\\ndropoff_datetime TIMESTAMP,\\nPUlocationID FLOAT64,\\nDOlocationID FLOAT64,\\nSR_Flag FLOAT64,\\nAffiliated_base_number STRING\\n)\\nOPTIONS (\\nformat = 'PARQUET',\\nuris = ['gs://project id/fhv_2019_8.parquet']\\n);\\nCan Also USE  uris = ['gs://project id/fhv_2019_*.parquet'] (THIS WILL remove the need for the loop and can be done for all month in single RUN )\\n– THANKYOU FOR THIS –\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'Invalid data types after Ingesting FHV data through parquet files: Could not parse SR_Flag as Float64,Couldn’t parse datetime column as timestamp,couldn’t handle NULL values in PULocationID,DOLocationID', 'course': 'data-engineering-zoomcamp', 'id': '2aadd232'}]), ('adcd914a', [{'text': 'When accessing Looker Studio through the Google Cloud Project console, you may be prompted to subscribe to the Pro version and receive the following errors:\\nInstead, navigate to https://lookerstudio.google.com/navigation/reporting which will take you to the free version.', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Google Looker Studio - you have used up your 30-day trial', 'course': 'data-engineering-zoomcamp', 'id': 'adcd914a'}]), ('bbf094b3', [{'text': 'Ans: Dbt provides a mechanism called \"ref\" to manage dependencies between models. By referencing other models using the \"ref\" keyword in SQL, dbt automatically understands the dependencies and ensures the correct execution order.\\nLoading FHV Data goes into slumber using Mage?\\nTry loading the data using jupyter notebooks in a local environment. There might be bandwidth issues with Mage.\\nLoad the data into a pandas dataframe using the urls, make necessary transformations, upload the gcp bucket / alternatively download the parquet/csv files locally and then upload to GCP manually.\\nRegion Mismatch in DBT and BigQuery\\nIf you are using the datasets copied into BigQuery from BigQuery public datasets, the region will be set as US by default and hence it is much easier to set your dbt profile location as US while transforming the tables and views. \\nYou can change the location as follows:', 'section': 'Module 4: analytics engineering with dbt', 'question': 'How does dbt handle dependencies between models?', 'course': 'data-engineering-zoomcamp', 'id': 'bbf094b3'}]), ('2fdc5057', [{'text': \"Use the PostgreSQL COPY FROM feature that is compatible with csv files\\nCOPY table_name [ ( column_name [, ...] ) ]\\nFROM { 'filename' | PROGRAM 'command' | STDIN }\\n[ [ WITH ] ( option [, ...] ) ]\\n[ WHERE condition ]\", 'section': 'Module 4: analytics engineering with dbt', 'question': 'What is the fastest way to upload taxi data to dbt-postgres?', 'course': 'data-engineering-zoomcamp', 'id': '2fdc5057'}]), ('95e302f7', [{'text': 'Update the line:\\nWith:', 'section': 'Module 5: pyspark', 'question': 'When configuring the profiles.yml file for dbt-postgres with jinja templates with environment variables, I\\'m getting \"Credentials in profile \"PROFILE_NAME\", target: \\'dev\\', invalid: \\'5432\\'is not of type \\'integer\\'', 'course': 'data-engineering-zoomcamp', 'id': '95e302f7'}]), ('1ac2c13c', [{'text': 'Install SDKMAN:\\ncurl -s \"https://get.sdkman.io\" | bash\\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\\nUsing SDKMAN, install Java 11 and Spark 3.3.2:\\nsdk install java 11.0.22-tem\\nsdk install spark 3.3.2\\nOpen a new terminal or run the following in the same shell:\\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\\nVerify the locations and versions of Java and Spark that were installed:\\necho $JAVA_HOME\\njava -version\\necho $SPARK_HOME\\nspark-submit --version', 'section': 'Module 5: pyspark', 'question': 'Setting up Java and Spark (with PySpark) on Linux (Alternative option using SDKMAN)', 'course': 'data-engineering-zoomcamp', 'id': '1ac2c13c'}]), ('5cc0e4d9', [{'text': 'If you’re seriously struggling to set things up \"locally\" (here locally meaning non/partly-managed environment like own laptop, a VM or Codespaces) you can use the following guide to use Spark in Google Colab:\\nhttps://medium.com/gitconnected/launch-spark-on-google-colab-and-connect-to-sparkui-342cad19b304\\nStarter notebook:\\nhttps://github.com/aaalexlit/medium_articles/blob/main/Spark_in_Colab.ipynb\\nIt’s advisable to spend some time setting things up locally rather than jumping right into this solution.', 'section': 'Module 5: pyspark', 'question': 'PySpark - Setting Spark up in Google Colab', 'course': 'data-engineering-zoomcamp', 'id': '5cc0e4d9'}]), ('17090545', [{'text': 'If after installing Java (either jdk or openjdk), Hadoop and Spark, and setting the corresponding environment variables you find the following error when spark-shell is run at CMD:\\njava.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c947bc5) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed\\nmodule @0x3c947bc5\\nSolution: Java 17 or 19 is not supported by Spark. Spark 3.x: requires Java 8/11/16. Install Java 11 from the website provided in the windows.md setup file.', 'section': 'Module 5: pyspark', 'question': 'Spark-shell: unable to load native-hadoop library for platform - Windows', 'course': 'data-engineering-zoomcamp', 'id': '17090545'}]), ('d17e30c6', [{'text': 'I found this error while executing the user defined function in Spark (crazy_stuff_udf). I am working on Windows and using conda. After following the setup instructions, I found that the PYSPARK_PYTHON environment variable was not set correctly, given that conda has different python paths for each environment.\\nSolution:\\npip install findspark on the command line inside proper environment\\nAdd to the top of the script\\nimport findspark\\nfindspark.init()', 'section': 'Module 5: pyspark', 'question': 'PySpark - Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.', 'course': 'data-engineering-zoomcamp', 'id': 'd17e30c6'}]), ('1520b5bc', [{'text': 'This is because Python 3.11 has some inconsistencies with such an old version of Spark. The solution is a downgrade in the Python version. Python 3.9 using a conda environment takes care of it. Or install newer PySpark >= 3.5.1 works for me (Ella) [source].', 'section': 'Module 5: pyspark', 'question': 'PySpark - TypeError: code() argument 13 must be str, not int  , while executing `import pyspark`  (Windows/ Spark 3.0.3 - Python 3.11)', 'course': 'data-engineering-zoomcamp', 'id': '1520b5bc'}]), ('e86ca928', [{'text': 'If anyone is a Pythonista or becoming one (which you will essentially be one along this journey), and desires to have all python dependencies under same virtual environment (e.g. conda) as done with prefect and previous exercises, simply follow these steps\\nInstall OpenJDK 11,\\non MacOS: $ brew install java11\\nAdd export PATH=\"/opt/homebrew/opt/openjdk@11/bin:$PATH\"\\nto ~/.bashrc or ~/zshrc\\nActivate working environment (by pipenv / poetry / conda)\\nRun $ pip install pyspark\\nWork with exercises as normal\\nAll default commands of spark will be also available at shell session under activated enviroment.\\nHope this can help!\\nP.s. you won’t need findspark to firstly initialize.\\nPy4J - Py4JJavaError: An error occurred while calling (...)  java.net.ConnectException: Connection refused: no further information;\\nIf you\\'re getting `Py4JavaError` with a generic root cause, such as the described above (Connection refused: no further information). You\\'re most likely using incompatible versions of the JDK or Python with Spark.\\nAs of the current latest Spark version (3.5.0), it supports JDK 8 / 11 / 17. All of which can be easily installed with SDKMan! on macOS or Linux environments\\n\\n$ sdk install java 17.0.10-librca\\n$ sdk install spark 3.5.0\\n$ sdk install hadoop 3.3.5\\nAs PySpark 3.5.0 supports Python 3.8+ make sure you\\'re setting up your virtualenv with either 3.8 / 3.9 / 3.10 / 3.11 (Most importantly avoid using 3.12 for now as not all libs in the data-science/engineering ecosystem are fully package for that)\\n\\n\\n$ conda create -n ENV_NAME python=3.11\\n$ conda activate ENV_NAME\\n$ pip install pyspark==3.5.0\\nThis setup makes installing `findspark` and the likes of it unnecessary. Happy coding.\\nPy4J - Py4JJavaError: An error occurred while calling o54.parquet. Or any kind of Py4JJavaError that show up after run df.write.parquet(\\'zones\\')(On window)\\nThis assume you already correctly set up the PATH in the nano ~/.bashrc\\nHere my\\nexport JAVA_HOME=\"/c/tools/jdk-11.0.21\"\\nexport PATH=\"${JAVA_HOME}/bin:${PATH}\"\\nexport HADOOP_HOME=\"/c/tools/hadoop-3.2.0\"\\nexport PATH=\"${HADOOP_HOME}/bin:${PATH}\"\\nexport SPARK_HOME=\"/c/tools/spark-3.3.2-bin-hadoop3\"\\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"\\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\\nexport PYTHONPATH=\"${SPARK_HOME}spark-3.5.1-bin-hadoop3py4j-0.10.9.5-src.zip:$PYTHONPATH\"\\nYou also need to add environment variables correctly which paths to java jdk, spark and hadoop through\\nGo to Stephenlaye2/winutils3.3.0: winutils.exe hadoop.dll and hdfs.dll binaries for hadoop windows (github.com), download the right winutils for hadoop-3.2.0. Then create a new folder,bin and put every thing in side to make a /c/tools/hadoop-3.2.0/bin(You might not need to do this, but after testing it without the /bin I could not make it to work)\\nThen follow the solution in this video: How To Resolve Issue with Writing DataFrame to Local File | winutils | msvcp100.dll (youtube.com)\\nRemember to restart IDE and computer, After the error An error occurred while calling o54.parquet.  is fixed but new errors like o31.parquet. Or o35.parquet. appear.', 'section': 'Module 5: pyspark', 'question': 'Java+Spark - Easy setup with miniconda env (worked on MacOS)', 'course': 'data-engineering-zoomcamp', 'id': 'e86ca928'}]), ('3b5b4eb3', [{'text': 'After installing all including pyspark (and it is successfully imported), but then running this script on the jupyter notebook\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder \\\\\\n.master(\"local[*]\") \\\\\\n.appName(\\'test\\') \\\\\\n.getOrCreate()\\ndf = spark.read \\\\\\n.option(\"header\", \"true\") \\\\\\n.csv(\\'taxi+_zone_lookup.csv\\')\\ndf.show()\\nit gives the error:\\nRuntimeError: Java gateway process exited before sending its port number\\n✅The solution (for me) was:\\npip install findspark on the command line and then\\nAdd\\nimport findspark\\nfindspark.init()\\nto the top of the script.\\nAnother possible solution is:\\nCheck that pyspark is pointing to the correct location.\\nRun pyspark.__file__. It should be list /home/<your user name>/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/__init__.py if you followed the videos.\\nIf it is pointing to your python site-packages remove the pyspark directory there and check that you have added the correct exports to you .bashrc file and that there are not any other exports which might supersede the ones provided in the course content.\\nTo add to the solution above, if the errors persist in regards to setting the correct path for spark,  an alternative solution for permanent path setting solve the error is  to set environment variables on system and user environment variables following this tutorial: Install Apache PySpark on Windows PC | Apache Spark Installation Guide\\nOnce everything is installed, skip to 7:14 to set up environment variables. This allows for the environment variables to be set permanently.', 'section': 'Module 5: pyspark', 'question': 'lsRuntimeError: Java gateway process exited before sending its port number', 'course': 'data-engineering-zoomcamp', 'id': '3b5b4eb3'}]), ('489c366f', [{'text': 'Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\nThe solution which worked for me(use following in jupyter notebook) :\\n!pip install findspark\\nimport findspark\\nfindspark.init()\\nThereafter , import pyspark and create spark contex<<t as usual\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\nFilter based on conditions based on multiple columns\\nfrom pyspark.sql.functions import col\\nnew_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\\nKrishna Anand', 'section': 'Module 5: pyspark', 'question': 'Module Not Found Error in Jupyter Notebook .', 'course': 'data-engineering-zoomcamp', 'id': '489c366f'}]), ('59381b15', [{'text': 'You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\\nAdditionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.', 'section': 'Module 5: pyspark', 'question': \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\", 'course': 'data-engineering-zoomcamp', 'id': '59381b15'}]), ('220b1cf3', [{'text': 'If below does not work, then download the latest available py4j version with\\nconda install -c conda-forge py4j\\nTake care of the latest version number in the website to replace appropriately.\\nNow add\\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\\nexport PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH\"\\nin your  .bashrc file.', 'section': 'Module 5: pyspark', 'question': \"Py4J Error - ModuleNotFoundError: No module named 'py4j' (Solve with latest version)\", 'course': 'data-engineering-zoomcamp', 'id': '220b1cf3'}]), ('d970a0da', [{'text': 'Even after we have exported our paths correctly you may find that  even though Jupyter is installed you might not have Jupyter Noteboopgak for one reason or another. Full instructions are found here (for my walkthrough) or here (where I got the original instructions from) but are included below. These instructions include setting up a virtual environment (handy if you are on your own machine doing this and not a VM):\\nFull steps:\\nUpdate and upgrade packages:\\nsudo apt update && sudo apt -y upgrade\\nInstall Python:\\nsudo apt install python3-pip python3-dev\\nInstall Python virtualenv:\\nsudo -H pip3 install --upgrade pip\\nsudo -H pip3 install virtualenv\\nCreate a Python Virtual Environment:\\nmkdir notebook\\ncd notebook\\nvirtualenv jupyterenv\\nsource jupyterenv/bin/activate\\nInstall Jupyter Notebook:\\npip install jupyter\\nRun Jupyter Notebook:\\njupyter notebook', 'section': 'Module 5: pyspark', 'question': 'Exception: Jupyter command `jupyter-notebook` not found.', 'course': 'data-engineering-zoomcamp', 'id': 'd970a0da'}]), ('5fa98bd0', [{'text': 'Code executed:\\ndf = spark.read.parquet(pq_path)\\n… some operations on df …\\ndf.write.parquet(pq_path, mode=\"overwrite\")\\njava.io.FileNotFoundException: File file:/home/xxx/code/data/pq/fhvhv/2021/02/part-00021-523f9ad5-14af-4332-9434-bdcb0831f2b7-c000.snappy.parquet does not exist\\nThe problem is that Sparks performs lazy transformations, so the actual action that trigger the job is df.write, which does delete the parquet files that is trying to read (mode=”overwrite”)\\n✅Solution: Write to a different directorydf\\ndf.write.parquet(pq_path_temp, mode=\"overwrite\")', 'section': 'Module 5: pyspark', 'question': 'Error java.io.FileNotFoundException', 'course': 'data-engineering-zoomcamp', 'id': '5fa98bd0'}]), ('ce508f3c', [{'text': 'You need to create the Hadoop /bin directory manually and add the downloaded files in there, since the shell script provided for Windows installation just puts them in /c/tools/hadoop-3.2.0/ .', 'section': 'Module 5: pyspark', 'question': 'Hadoop - FileNotFoundException: Hadoop bin directory does not exist , when trying to write (Windows)', 'course': 'data-engineering-zoomcamp', 'id': 'ce508f3c'}]), ('b7b9487d', [{'text': 'Actually Spark SQL is one independent “type” of SQL - Spark SQL.\\nThe several SQL providers are very similar:\\nSELECT [attributes]\\nFROM [table]\\nWHERE [filter]\\nGROUP BY [grouping attributes]\\nHAVING [filtering the groups]\\nORDER BY [attribute to order]\\n(INNER/FULL/LEFT/RIGHT) JOIN [table2]\\nON [attributes table joining table2] (...)\\nWhat differs the most between several SQL providers are built-in functions.\\nFor Built-in Spark SQL function check this link: https://spark.apache.org/docs/latest/api/sql/index.html\\nExtra information on SPARK SQL :\\nhttps://databricks.com/glossary/what-is-spark-sql#:~:text=Spark%20SQL%20is%20a%20Spark,on%20existing%20deployments%20and%20data.', 'section': 'Module 5: pyspark', 'question': 'Which type of SQL is used in Spark? Postgres? MySQL? SQL Server?', 'course': 'data-engineering-zoomcamp', 'id': 'b7b9487d'}]), ('a74de125', [{'text': \"✅Solution: I had two notebooks running, and the one I wanted to look at had opened a port on localhost:4041.\\nIf a port is in use, then Spark uses the next available port number. It can be even 4044. Clean up after yourself when a port does not work or a container does not run.\\nYou can run spark.sparkContext.uiWebUrl\\nand result will be some like\\n'http://172.19.10.61:4041'\", 'section': 'Module 5: pyspark', 'question': 'The spark viewer on localhost:4040 was not showing the current run', 'course': 'data-engineering-zoomcamp', 'id': 'a74de125'}]), ('e5270303', [{'text': '✅Solution: replace Java Developer Kit 11 with Java Developer Kit 8.\\nJava - RuntimeError: Java gateway process exited before sending its port number\\nShows java_home is not set on the notebook log\\nhttps://sparkbyexamples.com/pyspark/pyspark-exception-java-gateway-process-exited-before-sending-the-driver-its-port-number/\\nhttps://twitter.com/drkrishnaanand/status/1765423415878463839', 'section': 'Module 5: pyspark', 'question': 'Java - java.lang.NoSuchMethodError: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner Error during repartition call (conda pyspark installation)', 'course': 'data-engineering-zoomcamp', 'id': 'e5270303'}]), ('cabe8a5b', [{'text': '✅I got it working using `gcs-connector-hadoop-2.2.5-shaded.jar` and Spark 3.1\\nI also added the google_credentials.json and .p12 to auth with gcs. These files are downloadable from GCP Service account.\\nTo create the SparkSession:\\nspark = SparkSession.builder.master(\\'local[*]\\') \\\\\\n.appName(\\'spark-read-from-bigquery\\') \\\\\\n.config(\\'BigQueryProjectId\\',\\'razor-project-xxxxxxx) \\\\\\n.config(\\'BigQueryDatasetLocation\\',\\'de_final_data\\') \\\\\\n.config(\\'parentProject\\',\\'razor-project-xxxxxxx) \\\\\\n.config(\"google.cloud.auth.service.account.enable\", \"true\") \\\\\\n.config(\"credentialsFile\", \"google_credentials.json\") \\\\\\n.config(\"GcpJsonKeyFile\", \"google_credentials.json\") \\\\\\n.config(\"spark.driver.memory\", \"4g\") \\\\\\n.config(\"spark.executor.memory\", \"2g\") \\\\\\n.config(\"spark.memory.offHeap.enabled\",True) \\\\\\n.config(\"spark.memory.offHeap.size\",\"5g\") \\\\\\n.config(\\'google.cloud.auth.service.account.json.keyfile\\', \"google_credentials.json\") \\\\\\n.config(\"fs.gs.project.id\", \"razor-project-xxxxxxx\") \\\\\\n.config(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\\\\n.config(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\\\\n.getOrCreate()', 'section': 'Module 5: pyspark', 'question': 'Spark fails when reading from BigQuery and using `.show()` on `SELECT` queries', 'course': 'data-engineering-zoomcamp', 'id': 'cabe8a5b'}]), ('e3c0f777', [{'text': 'While creating a SparkSession using the config spark.jars.packages as com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\\nspark = SparkSession.builder.master(\\'local\\').appName(\\'bq\\').config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\").getOrCreate()\\nautomatically downloads the required dependency jars and configures the connector, removing the need to manage this dependency. More details available here', 'section': 'Module 5: pyspark', 'question': 'Spark BigQuery connector Automatic configuration', 'course': 'data-engineering-zoomcamp', 'id': 'e3c0f777'}]), ('50c009ef', [{'text': 'Link to Slack Thread : has anyone figured out how to read from GCP data lake instead of downloading all the taxi data again?\\nThere’s a few extra steps to go into reading from GCS with PySpark\\n1.)  IMPORTANT: Download the Cloud Storage connector for Hadoop here: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters\\nAs the name implies, this .jar file is what essentially connects PySpark with your GCS\\n2.) Move the .jar file to your Spark file directory. I installed Spark using homebrew on my MacOS machine and I had to create a /jars directory under \"/opt/homebrew/Cellar/apache-spark/3.2.1/ (where my spark dir is located)\\n3.) In your Python script, there are a few extra classes you’ll have to import:\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.conf import SparkConf\\nfrom pyspark.context import SparkContext\\n4.) You must set up your configurations before building your SparkSession. Here’s my code snippet:\\nconf = SparkConf() \\\\\\n.setMaster(\\'local[*]\\') \\\\\\n.setAppName(\\'test\\') \\\\\\n.set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") \\\\\\n.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\\\\n.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\\nsc = SparkContext(conf=conf)\\nsc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\\nsc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\\n5.) Once you run that, build your SparkSession with the new parameters we’d just instantiated in the previous step:\\nspark = SparkSession.builder \\\\\\n.config(conf=sc.getConf()) \\\\\\n.getOrCreate()\\n6.) Finally, you’re able to read your files straight from GCS!\\ndf_green = spark.read.parquet(\"gs://{BUCKET}/green/202*/\")', 'section': 'Module 5: pyspark', 'question': 'Spark Cloud Storage connector', 'course': 'data-engineering-zoomcamp', 'id': '50c009ef'}]), ('3fe85b16', [{'text': 'from pyarrow.parquet import ParquetFile\\npf = ParquetFile(\\'fhvhv_tripdata_2021-01.parquet\\')\\n#pyarrow builds tables, not dataframes\\ntbl_small = next(pf.iter_batches(batch_size = 1000))\\n#this function converts the table to a dataframe of manageable size\\ndf = tbl_small.to_pandas()\\nAlternatively without PyArrow:\\ndf = spark.read.parquet(\\'fhvhv_tripdata_2021-01.parquet\\')\\ndf1 = df.sort(\\'DOLocationID\\').limit(1000)\\npdf = df1.select(\"*\").toPandas()\\ngcsu', 'section': 'Module 5: pyspark', 'question': 'How can I read a small number of rows from the parquet file directly?', 'course': 'data-engineering-zoomcamp', 'id': '3fe85b16'}]), ('0fe0c76a', [{'text': 'Probably you’ll encounter this if you followed the video ‘5.3.1 - First Look at Spark/PySpark’ and used the parquet file from the TLC website (csv was used in the video).\\nWhen defining the schema, the PULocation and DOLocationID are defined as IntegerType. This will cause an error because the Parquet file is INT64 and you’ll get an error like:\\nParquet column cannot be converted in file [...] Column [...] Expected: int, Found: INT64\\nChange the schema definition from IntegerType to LongType and it should work', 'section': 'Module 5: pyspark', 'question': 'DataType error when creating Spark DataFrame with a specified schema?', 'course': 'data-engineering-zoomcamp', 'id': '0fe0c76a'}]), ('18c5bafe', [{'text': 'df_finalx=df_finalw.select([col(x).alias(x.replace(\" \",\"\")) for x in df_finalw.columns])\\nKrishna Anand', 'section': 'Module 5: pyspark', 'question': 'Remove white spaces from column names in Pyspark', 'course': 'data-engineering-zoomcamp', 'id': '18c5bafe'}]), ('59e86b40', [{'text': 'This error comes up on the Spark video 5.3.1 - First Look at Spark/PySpark,\\nbecause as at the creation of the video, 2021 data was the most recent which utilised csv files but as at now its parquet.\\nSo when you run the command spark.createDataFrame(df1_pandas).show(),\\nYou get the Attribute error. This is caused by the pandas version 2.0.0 which seems incompatible with Spark 3.3.2, so to fix it you have to downgrade pandas to 1.5.3 using the command pip install -U pandas==1.5.3\\nAnother option is adding the following after importing pandas, if one does not want to downgrade pandas version (source) :\\npd.DataFrame.iteritems = pd.DataFrame.items\\nNote that this problem is solved with Spark versions from 3.4.1', 'section': 'Module 5: pyspark', 'question': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\", 'course': 'data-engineering-zoomcamp', 'id': '59e86b40'}]), ('1ac3ea8f', [{'text': 'Another alternative is to install pandas 2.0.1 (it worked well as at the time of writing this), and it is compatible with Pyspark 3.5.1. Make sure to add or edit your environment variable like this:\\nexport SPARK_HOME=\"${HOME}/spark/spark-3.5.1-bin-hadoop3\"\\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"', 'section': 'Module 5: pyspark', 'question': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\", 'course': 'data-engineering-zoomcamp', 'id': '1ac3ea8f'}]), ('e04529ac', [{'text': 'Open a CMD terminal in administrator mode\\ncd %SPARK_HOME%\\nStart a master node: bin\\\\spark-class org.apache.spark.deploy.master.Master\\nStart a worker node: bin\\\\spark-class org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> --host <IP_ADDR>\\nbin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host <IP_ADDR>\\nspark://<master_ip>:<port>: copy the address from the previous command, in my case it was spark://localhost:7077\\nUse --host <IP_ADDR> if you want to run the worker on a different machine. For now leave it empty.\\nNow you can access Spark UI through localhost:8080\\nHomework for Module 5:\\nDo not refer to the homework file located under /05-batch/code/. The correct file is located under\\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/05-batch/homework.md', 'section': 'Module 5: pyspark', 'question': 'Spark Standalone Mode on Windows', 'course': 'data-engineering-zoomcamp', 'id': 'e04529ac'}]), ('a602a7f8', [{'text': 'You can either type the export command every time you run a new session, add it to the .bashrc/ which you can find in /home or run this command at the beginning of your homebook:\\nimport findspark\\nfindspark.init()', 'section': 'Module 5: pyspark', 'question': 'Export PYTHONPATH command in linux is temporary', 'course': 'data-engineering-zoomcamp', 'id': 'a602a7f8'}]), ('9336ce2c', [{'text': 'I solved this issue: unzip the file with:\\nf\\nbefore creating head.csv', 'section': 'Module 5: pyspark', 'question': 'Compressed file ended before the end-of-stream marker was reached', 'course': 'data-engineering-zoomcamp', 'id': '9336ce2c'}]), ('bac4e0f7', [{'text': 'In the code along from Video 5.3.3 Alexey downloads the CSV files from the NYT website and gzips them in their bash script. If we now (2023) follow along but download the data from the GH course Repo, it will already be zippes as csv.gz files. Therefore we zip it again if we follow the code from the video exactly. This then leads to gibberish outcome when we then try to cat the contents or count the lines with zcat, because the file is zipped twitch and zcat only unzips it once.\\n✅solution: do not gzip the files downloaded from the course repo. Just wget them and save them as they are as csv.gz files. Then the zcat command and the showSchema command will also work\\nURL=\"${URL_PREFIX}/${TAXI_TYPE}/${TAXI_TYPE}_tripdata_${YEAR}-${FMONTH}.csv.gz\"\\nLOCAL_PREFIX=\"data/raw/${TAXI_TYPE}/${YEAR}/${FMONTH}\"\\nLOCAL_FILE=\"${TAXI_TYPE}_tripdata_${YEAR}_${FMONTH}.csv.gz\"\\nLOCAL_PATH=\"${LOCAL_PREFIX}/${LOCAL_FILE}\"\\necho \"downloading ${URL} to ${LOCAL_PATH}\"\\nmkdir -p ${LOCAL_PREFIX}\\nwget ${URL} -O ${LOCAL_PATH}\\necho \"compressing ${LOCAL_PATH}\"\\n# gzip ${LOCAL_PATH} <- uncomment this line', 'section': 'Module 5: pyspark', 'question': 'Compression Error: zcat output is gibberish, seems like still compressed', 'course': 'data-engineering-zoomcamp', 'id': 'bac4e0f7'}]), ('13dad632', [{'text': 'Occurred while running : spark.createDataFrame(df_pandas).show()\\nThis error is usually due to the python version, since spark till date of 2 march 2023 doesn’t support python 3.11, try creating a new env with python version 3.8 and then run this command.\\nOn the virtual machine, you can create a conda environment (here called myenv) with python 3.10 installed:\\nconda create -n myenv python=3.10 anaconda\\nThen you must run conda activate myenv to run python 3.10. Otherwise you’ll still be running version 3.11. You can deactivate by typing conda deactivate.', 'section': 'Module 5: pyspark', 'question': 'PicklingError: Could not serialise object: IndexError: tuple index out of range.', 'course': 'data-engineering-zoomcamp', 'id': '13dad632'}]), ('ddc3c75b', [{'text': 'Make sure you have your credentials of your GCP in your VM under the location defined in the script.', 'section': 'Module 5: pyspark', 'question': 'Connecting from local Spark to GCS - Spark does not find my google credentials as shown in the video?', 'course': 'data-engineering-zoomcamp', 'id': 'ddc3c75b'}]), ('095b667f', [{'text': 'To run spark in docker setup\\n1. Build bitnami spark docker\\na. clone bitnami repo using command\\ngit clone https://github.com/bitnami/containers.git\\n(tested on commit 9cef8b892d29c04f8a271a644341c8222790c992)\\nb. edit file `bitnami/spark/3.3/debian-11/Dockerfile` and update java and spark version as following\\n\"python-3.10.10-2-linux-${OS_ARCH}-debian-11\" \\\\\\n\"java-17.0.5-8-3-linux-${OS_ARCH}-debian-11\" \\\\\\nreference: https://github.com/bitnami/containers/issues/13409\\nc. build docker image by navigating to above directory and running docker build command\\nnavigate cd bitnami/spark/3.3/debian-11/\\nbuild command docker build -t spark:3.3-java-17 .\\n2. run docker compose\\nusing following file\\n```yaml docker-compose.yml\\nversion: \\'2\\'\\nservices:\\nspark:\\nimage: spark:3.3-java-17\\nenvironment:\\n- SPARK_MODE=master\\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n- SPARK_RPC_ENCRYPTION_ENABLED=no\\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n- SPARK_SSL_ENABLED=no\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8080:8080\\'\\n- \\'7077:7077\\'\\nspark-worker:\\nimage: spark:3.3-java-17\\nenvironment:\\n- SPARK_MODE=worker\\n- SPARK_MASTER_URL=spark://spark:7077\\n- SPARK_WORKER_MEMORY=1G\\n- SPARK_WORKER_CORES=1\\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n- SPARK_RPC_ENCRYPTION_ENABLED=no\\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n- SPARK_SSL_ENABLED=no\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8081:8081\\'\\nspark-nb:\\nimage: jupyter/pyspark-notebook:java-17.0.5\\nenvironment:\\n- SPARK_MASTER_URL=spark://spark:7077\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8888:8888\\'\\n- \\'4040:4040\\'\\n```\\nrun command to deploy docker compose\\ndocker-compose up\\nAccess jupyter notebook using link logged in docker compose logs\\nSpark master url is spark://spark:7077', 'section': 'Module 5: pyspark', 'question': 'Spark docker-compose setup', 'course': 'data-engineering-zoomcamp', 'id': '095b667f'}]), ('56a67c23', [{'text': 'To do this\\npip install gcsfs,\\nThereafter copy the uri path to the file and use \\ndf = pandas.read_csc(gs://path)', 'section': 'Module 5: pyspark', 'question': 'How do you read data stored in gcs on pandas with your local computer?', 'course': 'data-engineering-zoomcamp', 'id': '56a67c23'}]), ('7fed7813', [{'text': 'Error:\\nspark.createDataFrame(df_pandas).schema\\nTypeError: field Affiliated_base_number: Can not merge type <class \\'pyspark.sql.types.StringType\\'> and <class \\'pyspark.sql.types.DoubleType\\'>\\nSolution:\\nAffiliated_base_number is a mix of letters and numbers (you can check this with a preview of the table), so it cannot be set to DoubleType (only for double-precision numbers). The suitable type would be StringType. Spark  inferSchema is more accurate than Pandas infer type method in this case. You can set it to  true  while reading the csv, so you don’t have to take out any data from your dataset. Something like this can help:\\ndf = spark.read \\\\\\n.options(\\nheader = \"true\", \\\\\\ninferSchema = \"true\", \\\\\\n) \\\\\\n.csv(\\'path/to/your/csv/file/\\')\\nSolution B:\\nIt\\'s because some rows in the affiliated_base_number are null and therefore it is assigned the datatype String and this cannot be converted to type Double. So if you really want to convert this pandas df to a pyspark df only take the  rows from the pandas df that are not null in the \\'Affiliated_base_number\\' column. Then you will be able to apply the pyspark function createDataFrame.\\n# Only take rows that have no null values\\npandas_df= pandas_df[pandas_df.notnull().all(1)]', 'section': 'Module 5: pyspark', 'question': 'TypeError when using spark.createDataFrame function on a pandas df', 'course': 'data-engineering-zoomcamp', 'id': '7fed7813'}]), ('a0e7e259', [{'text': 'Default executor memory is 1gb. This error appeared when working with the homework dataset.\\nError: MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\\nScaling row group sizes to 95.00% for 8 writers\\nSolution:\\nIncrease the memory of the executor when creating the Spark session like this:\\nRemember to restart the Jupyter session (ie. close the Spark session) or the config won’t take effect.', 'section': 'Module 5: pyspark', 'question': 'MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory', 'course': 'data-engineering-zoomcamp', 'id': 'a0e7e259'}]), ('4ca14331', [{'text': 'Change the working directory to the spark directory:\\nif you have setup up your SPARK_HOME variable, use the following;\\ncd %SPARK_HOME%\\nif not, use the following;\\ncd <path to spark installation>\\nCreating a Local Spark Cluster\\nTo start Spark Master:\\nbin\\\\spark-class org.apache.spark.deploy.master.Master --host localhost\\nStarting up a cluster:\\nbin\\\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost', 'section': 'Module 5: pyspark', 'question': 'How to spark standalone cluster is run on windows OS', 'course': 'data-engineering-zoomcamp', 'id': '4ca14331'}]), ('6fdd09eb', [{'text': 'I added PYTHONPATH, JAVA_HOME and SPARK_HOME to ~/.bashrc, import pyspark worked ok in iPython in terminal, but couldn’t be found in .ipynb opened in VS Code\\nAfter adding new lines to ~/.bashrc, need to restart the shell to activate the new lines, do either\\nsource ~/.bashrc\\nexec bash\\nInstead of configuring paths in ~/.bashrc, I created .env file in the root of my workspace:', 'section': 'Module 5: pyspark', 'question': 'Env variables set in ~/.bashrc are not loaded to Jupyter in VS Code', 'course': 'data-engineering-zoomcamp', 'id': '6fdd09eb'}]), ('64bfb2c3', [{'text': 'I don’t use visual studio, so I did it the old fashioned way: ssh -L 8888:localhost:8888 <my user>@<VM IP> (replace user and IP with the ones used by the GCP VM, e.g. : ssh -L 8888:localhost:8888 myuser@34.140.188.1', 'section': 'Module 5: pyspark', 'question': 'How to port forward outside VS Code', 'course': 'data-engineering-zoomcamp', 'id': '64bfb2c3'}]), ('33dd4516', [{'text': 'If you are doing wc -l fhvhv_tripdata_2021-01.csv.gz  with the gzip file as the file argument, you will get a different result, obviously! Since the file is compressed.\\nUnzip the file and then do wc -l fhvhv_tripdata_2021-01.csv to get the right results.', 'section': 'Module 5: pyspark', 'question': '“wc -l” is giving a different result then shown in the video', 'course': 'data-engineering-zoomcamp', 'id': '33dd4516'}]), ('504b8570', [{'text': 'when trying to:\\nURL=\"spark://$HOSTNAME:7077\"\\nspark-submit \\\\\\n--master=\"{$URL}\" \\\\\\n06_spark_sql.py \\\\\\n--input_green=data/pq/green/2021/*/ \\\\\\n--input_yellow=data/pq/yellow/2021/*/ \\\\\\n--output=data/report-2021\\nand you get errors like the following (SUMMARIZED):\\nWARN Utils: Your hostname, <HOSTNAME> resolves to a loopback address..\\nWARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address Setting default log level to \"WARN\".\\nException in thread \"main\" org.apache.spark.SparkException: Master must either be yarn or start with spark, mesos, k8s, or local at …\\nTry replacing --master=\"{$URL}\"\\nwith --master=$URL (edited)\\nExtra edit for spark version 3.4.2 - if encountering:\\n`Error: Unrecognized option: --master=`\\n→ Replace `--master=\"{$URL}\"` with  `--master \"${URL}\"`', 'section': 'Module 5: pyspark', 'question': '`spark-submit` errors', 'course': 'data-engineering-zoomcamp', 'id': '504b8570'}]), ('42e933c5', [{'text': 'If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\\nFor Windows, create a new User Variable “HADOOP_HOME” that points to your Hadoop directory. Then add “%HADOOP_HOME%\\\\bin” to the PATH variable.\\nAdditional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io', 'section': 'Module 5: pyspark', 'question': 'Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z', 'course': 'data-engineering-zoomcamp', 'id': '42e933c5'}]), ('fe9240b0', [{'text': \"Change the hadoop version to 3.0.1.Replace all the files in the local hadoop bin folder with the files in this repo:  winutils/hadoop-3.0.1/bin at master · cdarlint/winutils (github.com)\\nIf this does not work try to change other versions found in this repository.\\nFor more information please see this link: This version of %1 is not compatible with the version of Windows you're running · Issue #20 · cdarlint/winutils (github.com)\", 'section': 'Module 5: pyspark', 'question': 'Java.io.IOException. Cannot run program “C:\\\\hadoop\\\\bin\\\\winutils.exe”. CreateProcess error=216, This version of 1% is not compatible with the version of Windows you are using.', 'course': 'data-engineering-zoomcamp', 'id': 'fe9240b0'}]), ('c0a46e5d', [{'text': 'Fix is to set the flag like the error states. Get your project ID from your dashboard and set it like so:\\ngcloud dataproc jobs submit pyspark \\\\\\n--cluster=my_cluster \\\\\\n--region=us-central1 \\\\\\n--project=my-dtc-project-1010101 \\\\\\ngs://my-dtc-bucket-id/code/06_spark_sql.py\\n-- \\\\\\n…', 'section': 'Module 5: pyspark', 'question': 'Dataproc - ERROR: (gcloud.dataproc.jobs.submit.pyspark) The required property [project] is not currently set. It can be set on a per-command basis by re-running your command with the [--project] flag.', 'course': 'data-engineering-zoomcamp', 'id': 'c0a46e5d'}]), ('943c2466', [{'text': 'Go to %SPARK_HOME%\\\\bin\\nRun spark-class org.apache.spark.deploy.master.Master to run the master. This will give you a URL of the form spark://ip:port\\nRun spark-class org.apache.spark.deploy.worker.Worker spark://ip:port to run the worker. Make sure you use the URL you obtained in step 2.\\nCreate a new Jupyter notebook:\\nspark = SparkSession.builder \\\\\\n.master(\"spark://{ip}:7077\") \\\\\\n.appName(\\'test\\') \\\\\\n.getOrCreate()\\nCheck on Spark UI the master, worker and app.', 'section': 'Module 5: pyspark', 'question': 'Run Local Cluster Spark in Windows 10 with CMD', 'course': 'data-engineering-zoomcamp', 'id': '943c2466'}]), ('f41ef231', [{'text': 'This occurs because you are not logged in “gcloud auth login” and maybe the project id is not settled. Then type in a terminal:\\ngcloud auth login\\nThis will open a tab in the browser, accept the terms, after that close the tab if you want. Then set the project is like:\\ngcloud config set project <YOUR PROJECT_ID>\\nThen you can run the command to upload the pq dir to a GCS Bucket:\\ngsutil -m cp -r pq/ <YOUR URI from gsutil>/pq', 'section': 'Module 5: pyspark', 'question': \"lServiceException: 401 Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket. Permission 'storage.objects.list' denied on resource (or it may not exist).\", 'course': 'data-engineering-zoomcamp', 'id': 'f41ef231'}]), ('6b26d73c', [{'text': \"When submit a job, it might throw an error about Java in log panel within Dataproc. I changed the Versioning Control when I created a cluster, so it means that I delete the cluster and created a new one, and instead of choosing Debian-Hadoop-Spark, I switch to Ubuntu 20.02-Hadoop3.3-Spark3.3 for Versioning Control feature, the main reason to choose this is because I have the same Ubuntu version in mi laptop, I tried to find documentation to sustent this but unfortunately I couldn't nevertheless it works for me.\", 'section': 'Module 5: pyspark', 'question': 'py4j.protocol.Py4JJavaError  GCP', 'course': 'data-engineering-zoomcamp', 'id': '6b26d73c'}]), ('830e2936', [{'text': \"Use both repartition and coalesce, like so:\\ndf = df.repartition(6)\\ndf = df.coalesce(6)\\ndf.write.parquet('fhv/2019/10', mode='overwrite')\", 'section': 'Module 5: pyspark', 'question': 'Repartition the Dataframe to 6 partitions using df.repartition(6) - got 8 partitions instead', 'course': 'data-engineering-zoomcamp', 'id': '830e2936'}]), ('02007b7c', [{'text': \"Possible solution - Try to forward the port using ssh cli instead of vs code.\\nRun > “ssh -L <local port>:<VM host/ip>:<VM port> <ssh hostname>”\\nssh hostname is the name you specified in the ~/.ssh/config file\\nIn case of Jupyter Notebook run\\n“ssh -L 8888:localhost:8888 gcp-vm”\\nfrom your local machine’s cli.\\nNOTE: If you logout from the session, the connection would break. Also while creating the spark session notice the block's log because sometimes it fails to run at 4040 and then switches to 4041.\\n~Abhijit Chakrborty: If you are having trouble accessing localhost ports from GCP VM consider adding the forwarding instructions to .ssh/config file as following:\\n```\\nHost <hostname>\\nHostname <external-gcp-ip>\\nUser xxxx\\nIdentityFile yyyy\\nLocalForward 8888 localhost:8888\\nLocalForward 8080 localhost:8080\\nLocalForward 5432 localhost:5432\\nLocalForward 4040 localhost:4040\\n```\\nThis should automatically forward all ports and will enable accessing localhost ports.\", 'section': 'Module 5: pyspark', 'question': 'Jupyter Notebook or SparkUI not loading properly at localhost after port forwarding from VS code?', 'course': 'data-engineering-zoomcamp', 'id': '02007b7c'}]), ('1ebb9a47', [{'text': '~ Abhijit Chakraborty\\n`sdk list java`  to check for available java sdk versions.\\n`sdk install java 11.0.22-amzn`  as  java-11.0.22-amzn was available for my codespace.\\nclick on Y if prompted to change the default java version.\\nCheck for java version using `java -version `.\\nIf working fine great, else `sdk default java 11.0.22-amzn` or whatever version you have installed.', 'section': 'Module 5: pyspark', 'question': 'Installing Java 11 on codespaces', 'course': 'data-engineering-zoomcamp', 'id': '1ebb9a47'}]), ('80125745', [{'text': 'Sometimes while creating a dataproc cluster on GCP, the following error is encountered.\\nSolution: As mentioned here, sometimes there might not be enough resources in the given region to allocate the request. Usually, gets freed up in a bit and one can create a cluster. – abhirup ghosh\\nSolution 2:  Changing the type of boot-disk from PD-Balanced to PD-Standard, in terraform, helped solve the problem.- Sundara Kumar Padmanabhan', 'section': 'Module 5: pyspark', 'question': \"Error: Insufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 470.0.\", 'course': 'data-engineering-zoomcamp', 'id': '80125745'}]), ('f01df45b', [{'text': \"Pyspark converts the difference of two TimestampType values to Python's native datetime.timedelta object. The timedelta object only stores the duration in terms of days, seconds, and microseconds. Each of the three units of time must be manually converted into hours in order to express the total duration between the two timestamps using only hours.\\nAnother way for achieving this is using the datediff (sql function). It receives this parameters\\nUpper Date: the closest date you have. For example dropoff_datetime\\nLower Date: the farthest date you have.  For example pickup_datetime\\nAnd the result is returned in terms of days, so you could multiply the result for 24 in order to get the hours.\", 'section': 'Module 5: pyspark', 'question': 'Homework - how to convert the time difference of two timestamps to hours', 'course': 'data-engineering-zoomcamp', 'id': 'f01df45b'}]), ('06014eec', [{'text': 'This version combination worked for me:\\nPySpark = 3.3.2\\nPandas = 1.5.3\\n\\nIf it still has an error,', 'section': 'Module 5: pyspark', 'question': 'PicklingError: Could not serialize object: IndexError: tuple index out of range', 'course': 'data-engineering-zoomcamp', 'id': '06014eec'}]), ('54653ca9', [{'text': \"Run this before SparkSession\\nimport os\\nimport sys\\nos.environ['PYSPARK_PYTHON'] = sys.executable\\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\", 'section': 'Module 5: pyspark', 'question': 'Py4JJavaError: An error occurred while calling o180.showString. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.', 'course': 'data-engineering-zoomcamp', 'id': '54653ca9'}]), ('f95304db', [{'text': \"import os\\nimport sys\\nos.environ['PYSPARK_PYTHON'] = sys.executable\\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\\nDataproc Pricing: https://cloud.google.com/dataproc/pricing#on_gke_pricing\", 'section': 'Module 5: pyspark', 'question': 'RuntimeError: Python in worker has different version 3.11 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.', 'course': 'data-engineering-zoomcamp', 'id': 'f95304db'}]), ('591df4e6', [{'text': 'Ans: No, you can submit a job to DataProc from your local computer by installing gsutil (https://cloud.google.com/storage/docs/gsutil_install) and configuring it. Then, you can execute the following command from your local computer.\\ngcloud dataproc jobs submit pyspark \\\\\\n--cluster=de-zoomcamp-cluster \\\\\\n--region=europe-west6 \\\\\\ngs://dtc_data_lake_de-zoomcamp-nytaxi/code/06_spark_sql.py \\\\\\n-- \\\\\\n--input_green=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/green/2020/*/ \\\\\\n--input_yellow=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/yellow/2020/*/ \\\\\\n--output=gs://dtc_data_lake_de-zoomcamp-nytaxi/report-2020 (edited)', 'section': 'Module 5: pyspark', 'question': 'Dataproc Qn: Is it essential to have a VM on GCP for running Dataproc and submitting jobs ?', 'course': 'data-engineering-zoomcamp', 'id': '591df4e6'}]), ('5cb7f597', [{'text': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\\nthis is because the method inside the pyspark refers to a package that has been already deprecated\\n(https://stackoverflow.com/questions/76404811/attributeerror-dataframe-object-has-no-attribute-iteritems)\\nYou can do this code below, which is mentioned in the stackoverflow link above:\\nQ: DE Zoomcamp 5.6.3 - Setting up a Dataproc Cluster I cannot create a cluster and get this message. I tried many times as the FAQ said, but it didn't work. What can I do?\\nError\\nInsufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 250.0.\\nRequest ID: 17942272465025572271\\nA: The master and worker nodes are allocated a maximum of 250 GB of memory combined. In the configuration section, adhere to the following specifications:\\nMaster Node:\\nMachine type: n2-standard-2\\nPrimary disk size: 85 GB\\nWorker Node:\\nNumber of worker nodes: 2\\nMachine type: n2-standard-2\\nPrimary disk size: 80 GB\\nYou can allocate up to 82.5 GB memory for worker nodes, keeping in mind that the total memory allocated across all nodes cannot exceed 250 GB.\", 'section': 'Module 5: pyspark', 'question': 'In module 5.3.1, trying to run spark.createDataFrame(df_pandas).show() returns error', 'course': 'data-engineering-zoomcamp', 'id': '5cb7f597'}]), ('c5de1f96', [{'text': 'The MacOS setup instruction (https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/macos.md#installing-java) for setting the JAVA_HOME environment variable is for Intel-based Macs which have a default install location at /usr/local/. If you have an Apple Silicon mac, you will have to set JAVA_HOME to /opt/homebrew/, specifically in your .bashrc or .zshrc:\\nexport JAVA_HOME=\"/opt/homebrew/opt/openjdk/bin\"\\nexport PATH=\"$JAVA_HOME:$PATH\"\\nConfirm that your path was correctly set by running the command: which java\\nYou should expect to see the output:\\n/opt/homebrew/opt/openjdk/bin/java\\nReference: https://docs.brew.sh/Installation', 'section': 'Module 6: streaming with kafka', 'question': 'Setting JAVA_HOME with Homebrew on Apple Silicon', 'course': 'data-engineering-zoomcamp', 'id': 'c5de1f96'}]), ('70ac8e80', [{'text': 'Check Docker Compose File:\\nEnsure that your docker-compose.yaml file is correctly configured with the necessary details for the \"control-center\" service. Check the service name, image name, ports, volumes, environment variables, and any other configurations required for the container to start.\\nOn Mac OSX 12.2.1 (Monterey) I could not start the kafka control center. I opened Docker Desktop and saw docker images still running from week 4, which I did not see when I typed “docker ps.” I deleted them in docker desktop and then had no problem starting up the kafka environment.', 'section': 'Module 6: streaming with kafka', 'question': 'Could not start docker image “control-center” from the docker-compose.yaml file.', 'course': 'data-engineering-zoomcamp', 'id': '70ac8e80'}]), ('f6551ffb', [{'text': \"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\nTo create a virtual env and install packages (run only once)\\npython -m venv env\\nsource env/bin/activate\\npip install -r ../requirements.txt\\nTo activate it (you'll need to run it every time you need the virtual env):\\nsource env/bin/activate\\nTo deactivate it:\\ndeactivate\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\", 'section': 'Module 6: streaming with kafka', 'question': 'Module “kafka” not found when trying to run producer.py', 'course': 'data-engineering-zoomcamp', 'id': 'f6551ffb'}]), ('0ec021de', [{'text': 'ImportError: DLL load failed while importing cimpl: The specified module could not be found\\nVerify Python Version:\\nMake sure you are using a compatible version of Python with the Avro library. Check the Python version and compatibility requirements specified by the Avro library documentation.\\n... you may have to load librdkafka-5d2e2910.dll in the code. Add this before importing avro:\\nfrom ctypes import CDLL\\nCDLL(\"C:\\\\\\\\Users\\\\\\\\YOUR_USER_NAME\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\dtcde\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\confluent_kafka.libs\\\\librdkafka-5d2e2910.dll\")\\nIt seems that the error may occur depending on the OS and python version installed.\\nALTERNATIVE:\\nImportError: DLL load failed while importing cimpl\\n✅SOLUTION: $env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1 in Powershell.\\nYou need to set this DLL manually in Conda Env.\\nSource: https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2', 'section': 'Module 6: streaming with kafka', 'question': 'Error importing cimpl dll when running avro examples', 'course': 'data-engineering-zoomcamp', 'id': '0ec021de'}]), ('1edd4630', [{'text': \"✅SOLUTION: pip install confluent-kafka[avro].\\nFor some reason, Conda also doesn't include this when installing confluent-kafka via pip.\\nMore sources on Anaconda and confluent-kafka issues:\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/590\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/1221\\nhttps://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka\", 'section': 'Module 6: streaming with kafka', 'question': \"ModuleNotFoundError: No module named 'avro'\", 'course': 'data-engineering-zoomcamp', 'id': '1edd4630'}]), ('4664ae28', [{'text': 'If you get an error while running the command python3 stream.py worker\\nRun pip uninstall kafka-python\\nThen run pip install kafka-python==1.4.6\\nWhat is the use of  Redpanda ?\\nRedpanda: Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles.\\nRedpanda is a powerful, yet simple, and cost-efficient streaming data platform that is compatible with Kafka® APIs while eliminating Kafka complexity.', 'section': 'Module 6: streaming with kafka', 'question': 'Error while running python3 stream.py worker', 'course': 'data-engineering-zoomcamp', 'id': '4664ae28'}]), ('676e1b76', [{'text': 'Got this error because the docker container memory was exhausted. The dta file was upto 800MB but my docker container does not have enough memory to handle that.\\nSolution was to load the file in chunks with Pandas, then create multiple parquet files for each dat file I was processing. This worked smoothly and the issue was resolved.', 'section': 'Module 6: streaming with kafka', 'question': 'Negsignal:SIGKILL while converting dta files to parquet format', 'course': 'data-engineering-zoomcamp', 'id': '676e1b76'}]), ('a3c84279', [{'text': 'Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv', 'section': 'Module 6: streaming with kafka', 'question': 'data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing', 'course': 'data-engineering-zoomcamp', 'id': 'a3c84279'}]), ('119c917d', [{'text': 'tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.', 'section': 'Module 6: streaming with kafka', 'question': 'Kafka- python videos have low audio and hard to follow up', 'course': 'data-engineering-zoomcamp', 'id': '119c917d'}]), ('f1284c1f', [{'text': 'If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.', 'section': 'Module 6: streaming with kafka', 'question': 'kafka.errors.NoBrokersAvailable: NoBrokersAvailable', 'course': 'data-engineering-zoomcamp', 'id': 'f1284c1f'}]), ('49a7db28', [{'text': 'Ankush said we can focus on horizontal scaling option.\\n“think of scaling in terms of scaling from consumer end. Or consuming message via horizontal scaling”', 'section': 'Module 6: streaming with kafka', 'question': 'Kafka homwork Q3, there are options that support scaling concept more than the others:', 'course': 'data-engineering-zoomcamp', 'id': '49a7db28'}]), ('196cb0f2', [{'text': 'If you get this error, know that you have not built your sparks and juypter images. This images aren’t readily available on dockerHub.\\nIn the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose', 'section': 'Module 6: streaming with kafka', 'question': \"How to fix docker compose error: Error response from daemon: pull access denied for spark-3.3.1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\", 'course': 'data-engineering-zoomcamp', 'id': '196cb0f2'}]), ('1e50eab7', [{'text': 'Run this command in terminal in the same directory (/docker/spark):\\nchmod +x build.sh', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ./build.sh: Permission denied Error', 'course': 'data-engineering-zoomcamp', 'id': '1e50eab7'}]), ('a7a6d0d7', [{'text': 'Restarting all services worked for me:\\ndocker-compose down\\ndocker-compose up', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ‘KafkaTimeoutError: Failed to update metadata after 60.0 secs.’ when running stream-example/producer.py', 'course': 'data-engineering-zoomcamp', 'id': 'a7a6d0d7'}]), ('0996213a', [{'text': 'While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\\n…\\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\\n…\\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\\n…\\nSolution:\\nDowngrade your local PySpark to 3.3.1 (same as Dockerfile)\\nThe reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\\nSolution 2:\\nCheck what Spark version your local machine has\\npyspark –version\\nspark-submit –version\\nAdd your version to SPARK_VERSION in build.sh', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.', 'course': 'data-engineering-zoomcamp', 'id': '0996213a'}]), ('311bf368', [{'text': 'Start a new terminal\\nRun: docker ps\\nCopy the CONTAINER ID of the spark-master container\\nRun: docker exec -it <spark_master_container_id> bash\\nRun: cat logs/spark-master.out\\nCheck for the log when the error happened\\nGoogle the error message from there', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails', 'course': 'data-engineering-zoomcamp', 'id': '311bf368'}]), ('c1551650', [{'text': 'Make sure your java version is 11 or 8.\\nCheck your version by:\\njava --version\\nCheck all your versions by:\\n/usr/libexec/java_home -V\\nIf you already have got java 11 but just not selected as default, select the specific version by:\\nexport JAVA_HOME=$(/usr/libexec/java_home -v 11.0.22)\\n(or other version of 11)', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: ./spark-submit.sh streaming.py Error: py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.', 'course': 'data-engineering-zoomcamp', 'id': 'c1551650'}]), ('f9b673cf', [{'text': 'In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \"java-kafka-rides\"\\narchiveClassifier = \\'\\'\\n}\\nAnd then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build', 'course': 'data-engineering-zoomcamp', 'id': 'f9b673cf'}]), ('5479dce2', [{'text': 'confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py', 'course': 'data-engineering-zoomcamp', 'id': '5479dce2'}]), ('02cf2317', [{'text': 'In the project directory, run:\\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: How to run producer/consumer/kstreams/etc in terminal', 'course': 'data-engineering-zoomcamp', 'id': '02cf2317'}]), ('947c07a6', [{'text': 'For example, when running JsonConsumer.java, got:\\nConsuming form kafka started\\nRESULTS:::0\\nRESULTS:::0\\nRESULTS:::0\\nOr when running JsonProducer.java, got:\\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\nSolution:\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent', 'course': 'data-engineering-zoomcamp', 'id': '947c07a6'}]), ('bea22953', [{'text': 'Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn’t see it at first and had to do some fixes.\\nSolution:\\n(Source)\\nVS Code\\n→ Explorer (first icon on the left navigation bar)\\n→ JAVA PROJECTS (bottom collapsable)\\n→  icon next in the rightmost position to JAVA PROJECTS\\n→  clean Workspace\\n→ Confirm by clicking Reload and Delete\\nNow you will be able to see the triangle icon next to each test like what you normally see in python tests.\\nE.g.:\\nYou can also add classes and packages in this window instead of creating files in the project directory', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: Tests are not picked up in VSCode', 'course': 'data-engineering-zoomcamp', 'id': 'bea22953'}]), ('a1603359', [{'text': 'In Confluent Cloud:\\nEnvironment → default (or whatever you named your environment as) → The right navigation bar →  “Stream Governance API” →  The URL under “Endpoint”\\nAnd create credentials from Credentials section below it', 'section': 'Module 6: streaming with kafka', 'question': 'Confluent Kafka: Where can I find schema registry URL?', 'course': 'data-engineering-zoomcamp', 'id': 'a1603359'}]), ('a85a6a91', [{'text': 'You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.', 'section': 'Module 6: streaming with kafka', 'question': 'How do I check compatibility of local and container Spark versions?', 'course': 'data-engineering-zoomcamp', 'id': 'a85a6a91'}]), ('343864f5', [{'text': 'According to https://github.com/dpkp/kafka-python/\\n“DUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING”\\nUse pip install kafka-python-ng instead', 'section': 'Project', 'question': 'How to fix the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\"?', 'course': 'data-engineering-zoomcamp', 'id': '343864f5'}]), ('6cb3b4a9', [{'text': 'Each submitted project will be evaluated by 3 (three) randomly assigned students that have also submitted the project.\\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here.', 'section': 'Project', 'question': 'How is my capstone project going to be evaluated?', 'course': 'data-engineering-zoomcamp', 'id': '6cb3b4a9'}]), ('5959ea3c', [{'text': 'There is only ONE project for this Zoomcamp. You do not need to submit or create two projects. There are simply TWO chances to pass the course. You can use the Second Attempt if you a) fail the first attempt b) do not have the time due to other engagements such as holiday or sickness etc. to enter your project into the first attempt.', 'section': 'Project', 'question': 'Project 1 & Project 2', 'course': 'data-engineering-zoomcamp', 'id': '5959ea3c'}]), ('202af70b', [{'text': 'See a list of datasets here: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_7_project/datasets.md', 'section': 'Project', 'question': 'Does anyone know nice and relatively large datasets?', 'course': 'data-engineering-zoomcamp', 'id': '202af70b'}]), ('f2705fe7', [{'text': 'You need to redefine the python environment variable to that of your user account', 'section': 'Project', 'question': 'How to run python as start up script?', 'course': 'data-engineering-zoomcamp', 'id': 'f2705fe7'}]), ('74f412c4', [{'text': 'Initiate a Spark Session\\nspark = (SparkSession\\n.builder\\n.appName(app_name)\\n.master(master=master)\\n.getOrCreate())\\nspark.streams.resetTerminated()\\nquery1 = spark\\n.readStream\\n…\\n…\\n.load()\\nquery2 = spark\\n.readStream\\n…\\n…\\n.load()\\nquery3 = spark\\n.readStream\\n…\\n…\\n.load()\\nquery1.start()\\nquery2.start()\\nquery3.start()\\nspark.streams.awaitAnyTermination() #waits for any one of the query to receive kill signal or error failure. This is asynchronous\\n# On the contrary query3.start().awaitTermination() is a blocking ex call. Works well when we are reading only from one topic.', 'section': 'Project', 'question': 'Spark Streaming - How do I read from multiple topics in the same Spark Session', 'course': 'data-engineering-zoomcamp', 'id': '74f412c4'}]), ('5214eb93', [{'text': 'Transformed data can be moved in to azure blob storage and then it can be moved in to azure SQL DB, instead of moving directly from databricks to Azure SQL DB.', 'section': 'Project', 'question': 'Data Transformation from Databricks to Azure SQL DB', 'course': 'data-engineering-zoomcamp', 'id': '5214eb93'}]), ('3cfd16a7', [{'text': 'The trial dbt account provides access to dbt API. Job will still be needed to be added manually. Airflow will run the job using a python operator calling the API. You will need to provide api key, job id, etc. (be careful not committing it to Github).\\nDetailed explanation here: https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment\\nSource code example here: https://github.com/sungchun12/airflow-toolkit/blob/95d40ac76122de337e1b1cdc8eed35ba1c3051ed/dags/examples/dbt_cloud_example.py', 'section': 'Project', 'question': 'Orchestrating dbt with Airflow', 'course': 'data-engineering-zoomcamp', 'id': '3cfd16a7'}]), ('a7cecdf9', [{'text': 'https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dataproc/index.html\\nhttps://airflow.apache.org/docs/apache-airflow-providers-google/stable/_modules/airflow/providers/google/cloud/operators/dataproc.html\\nGive the following roles to you service account:\\nDataProc Administrator\\nService Account User (explanation here)\\nUse DataprocSubmitPySparkJobOperator, DataprocDeleteClusterOperator and  DataprocCreateClusterOperator.\\nWhen using  DataprocSubmitPySparkJobOperator, do not forget to add:\\ndataproc_jars = [\"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.24.0.jar\"]\\nBecause DataProc does not already have the BigQuery Connector.', 'section': 'Project', 'question': 'Orchestrating DataProc with Airflow', 'course': 'data-engineering-zoomcamp', 'id': 'a7cecdf9'}]), ('2aad1011', [{'text': 'You can trigger your dbt job in Mage pipeline. For this get your dbt cloud api key under settings/Api tokens/personal tokens. Add it safely to  your .env\\nFor example\\ndbt_api_trigger=dbt_**\\nNavigate to job page and find api trigger  link\\nThen create a custom mage Python block with a simple http request like here\\nfrom dotenv import load_dotenv\\nfrom pathlib import Path\\ndotenv_path = Path(\\'/home/src/.env\\')\\nload_dotenv(dotenv_path=dotenv_path)\\ndbt_api_trigger= os.getenv(dbt_api_trigger)\\nurl = f\"https://cloud.getdbt.com/api/v2/accounts/{dbt_account_id}/jobs/<job_id>/run/\"\\nheaders = {\\n        \"Authorization\": f\"Token {dbt_api_trigger}\",\\n        \"Content-Type\": \"application/json\" }\\nbody = {\\n        \"cause\": \"Triggered via API\"\\n    }\\n    response = requests.post(url, headers=headers, json=body)\\nvoila! You triggered dbt job form your mage pipeline.', 'section': 'Project', 'question': 'Orchestrating dbt cloud with Mage', 'course': 'data-engineering-zoomcamp', 'id': '2aad1011'}]), ('cb478996', [{'text': \"The slack thread : thttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1677678161866999\\nThe question is that sometimes even if you take plenty of effort to document every single step, and we can't even sure if the person doing the peer review will be able to follow-up, so how this criteria will be evaluated?\\nAlex clarifies: “Ideally yes, you should try to re-run everything. But I understand that not everyone has time to do it, so if you check the code by looking at it and try to spot errors, places with missing instructions and so on - then it's already great”\", 'section': 'Project', 'question': 'Project evaluation - Reproducibility', 'course': 'data-engineering-zoomcamp', 'id': 'cb478996'}]), ('b4ef8ca7', [{'text': 'The key valut in Azure cloud is used to store credentials or passwords or secrets of different tech stack used in Azure. For example if u do not want to expose the password in SQL database, then we can save the password under a given name and use them in other Azure stack.', 'section': 'Project', 'question': 'Key Vault in Azure cloud stack', 'course': 'data-engineering-zoomcamp', 'id': 'b4ef8ca7'}]), ('8e74f943', [{'text': 'You can get the version of py4j from inside docker using this command\\ndocker exec -it --user airflow airflow-airflow-scheduler-1 bash -c \"ls /opt/spark/python/lib\"', 'section': 'Project', 'question': \"Spark docker - `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\", 'course': 'data-engineering-zoomcamp', 'id': '8e74f943'}]), ('a73ed357', [{'text': 'Either use conda or pip for managing venv, using both of them together will cause incompatibility.\\nIf you’re using conda, install psycopg2 using the conda-forge channel, which may handle the architecture compatibility automatically\\nconda install -c conda-forge psycopg2\\nIf pip, do the normal install\\npip install psycopg2', 'section': 'Project', 'question': 'psycopg2 complains of incompatible environment e.g x86 instead of amd', 'course': 'data-engineering-zoomcamp', 'id': 'a73ed357'}]), ('d5b6ef5d', [{'text': 'This is not a FAQ but more of an advice if you want to set up dbt locally, I did it in the following way:\\nI had the postgres instance from week 2 (year 2024) up (the docker-compose)\\nmkdir dbt\\nvi dbt/profiles.yml\\nAnd here I attached this content (only the required fields) and replaced them with the proper values (for instance mine where in the .env file of the folder of week 2 docker stuff)\\ncd dbt && git clone https://github.com/dbt-labs/dbt-starter-project\\nmkdir project && cd project && mv dbt-starter-project/* .\\nMake sure that you align the profile name in profiles.yml with the dbt_project.yml file\\nAdd this line anywhere on the dbt_project.yml file:\\nconfig-version: 2\\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres ls\\nIf you have trouble run\\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres debug', 'section': 'Project', 'question': 'Setting up dbt locally with Docker and Postgres', 'course': 'data-engineering-zoomcamp', 'id': 'd5b6ef5d'}]), ('b406d90e', [{'text': 'The following line should be included in pyspark configuration\\n# Example initialization of SparkSession variable\\nspark = (SparkSession.builder\\n.master(...)\\n.appName(...)\\n# Add the following configuration\\n.config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-3.5-bigquery:0.37.0\")\\n)', 'section': 'Project', 'question': 'How to connect Pyspark with BigQuery?', 'course': 'data-engineering-zoomcamp', 'id': 'b406d90e'}]), ('0002ab8b', [{'text': 'Install the astronomer-cosmos package as a dependency. (see Terraform example).\\nMake a new folder, dbt/, inside the dags/ folder of your Composer GCP bucket and copy paste your dbt-core project there. (see example)\\nEnsure your profiles.yml is configured to authenticate with a service account key. (see BigQuery example)\\nCreate a new DAG using the DbtTaskGroup class and a ProfileConfig specifying a profiles_yml_filepath that points to the location of your JSON key file. (see example)\\nYour dbt lineage graph should now appear as tasks inside a task group like this:', 'section': 'Course Management Form for Homeworks', 'question': 'How to run a dbt-core project as an Airflow Task Group on Google Cloud Composer using a service account JSON key', 'course': 'data-engineering-zoomcamp', 'id': '0002ab8b'}]), ('138b55c7', [{'text': 'The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname, or your real name, if you prefer. Your entry on the Leaderboard is the one highlighted in teal(?) / light green (?).\\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\\nThe \"Display on Leaderboard\" option indicates whether you want your name to be listed on the course leaderboard.\\nQuestion: Is it possible to create external tables in BigQuery using URLs, such as those from the NY Taxi data website?\\nAnswer: Not really, only Bigtable, Cloud Storage, and Google Drive are supported data stores.', 'section': 'Workshop 1 - dlthub', 'question': 'Edit Course Profile.', 'course': 'data-engineering-zoomcamp', 'id': '138b55c7'}]), ('154d7705', [{'text': \"Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If you’re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).\", 'section': 'Workshop 1 - dlthub', 'question': 'How do I install the necessary dependencies to run the code?', 'course': 'data-engineering-zoomcamp', 'id': '154d7705'}]), ('f96517d9', [{'text': 'If you are running Jupyter Notebook on a fresh new Codespace or in local machine with a new Virtual Environment, you will need this package to run the starter Jupyter Notebook offered by the teacher. Execute this:\\npip install jupyter', 'section': 'Workshop 1 - dlthub', 'question': 'Other packages needed but not listed', 'course': 'data-engineering-zoomcamp', 'id': 'f96517d9'}]), ('773587dd', [{'text': 'Alternatively, you can switch to in-file storage with:', 'section': 'Workshop 1 - dlthub', 'question': 'How can I use DuckDB In-Memory database with dlt ?', 'course': 'data-engineering-zoomcamp', 'id': '773587dd'}]), ('73aff710', [{'text': 'After loading, you should have a total of 8 records, and ID 3 should have age 33\\nQuestion: Calculate the sum of ages of all the people loaded as described above\\nThe sum of all eight records\\' respective ages is too big to be in the choices. You need to first filter out the people whose occupation is equal to None in order to get an answer that is close to or present in the given choices. 😃\\n----------------------------------------------------------------------------------------\\nFIXED = use a raw string and keep the file:/// at the start of your file path\\nI\\'m having an issue with the dlt workshop notebook. The \\'Load to Parquet file\\' section specifically. No matter what I change the file path to, it\\'s still saving the dlt files directly to my C drive.\\n# Set the bucket_url. We can also use a local folder\\nos.environ[\\'DESTINATION__FILESYSTEM__BUCKET_URL\\'] = r\\'file:///content/.dlt/my_folder\\'\\nurl = \"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\\n# Define your pipeline\\npipeline = dlt.pipeline(\\npipeline_name=\\'my_pipeline\\',\\ndestination=\\'filesystem\\',\\ndataset_name=\\'mydata\\'\\n)\\n# Run the pipeline with the generator we created earlier.\\nload_info = pipeline.run(stream_download_jsonl(url), table_name=\"users\", loader_file_format=\"parquet\")\\nprint(load_info)\\n# Get a list of all Parquet files in the specified folder\\nparquet_files = glob.glob(\\'/content/.dlt/my_folder/mydata/users/*.parquet\\')\\n# show parquet files\\nfor file in parquet_files:\\nprint(file)', 'section': 'Workshop 2 - RisingWave', 'question': 'Homework - dlt Exercise 3 - Merge a generator concerns', 'course': 'data-engineering-zoomcamp', 'id': '73aff710'}]), ('0728ca67', [{'text': 'Check the contents of the repository with ls - the command.sh file should be in the root folder\\nIf it is not, verify that you had cloned the correct repository - https://github.com/risingwavelabs/risingwave-data-talks-workshop-2024-03-04', 'section': 'Workshop 2 - RisingWave', 'question': 'command.sh Error - source: no such file or directory: command.sh', 'course': 'data-engineering-zoomcamp', 'id': '0728ca67'}]), ('49a51e24', [{'text': \"psql is a command line tool that is installed alongside PostgreSQL DB, but since we've always been running PostgreSQL in a container, you've only got `pgcli`, which lacks the feature to run a sql script into the DB. Besides, having a command line for each database flavor you'll have to deal with as a Data Professional is far from ideal.\\nSo, instead, you can use usql. Check the docs for details on how to install for your OS. On macOS, it supports `homebrew`, and on Windows, it supports scoop.\\nSo, to run the taxi_trips.sql script with usql:\", 'section': 'Workshop 2 - RisingWave', 'question': 'psql - command not found: psql (alternative install)', 'course': 'data-engineering-zoomcamp', 'id': '49a51e24'}]), ('f0d552a7', [{'text': 'If you encounter this error and are certain that you have docker compose installed, but typically run it as docker compose without the hyphen, then consider editing command.sh file by removing the hyphen from ‘docker-compose’. Example:\\nstart-cluster() {\\ndocker compose -f docker/docker-compose.yml up -d\\n}', 'section': 'Workshop 2 - RisingWave', 'question': 'Setup - source command.sh - error: “docker-compose” not found', 'course': 'data-engineering-zoomcamp', 'id': 'f0d552a7'}]), ('9c750080', [{'text': 'ERROR: The Compose file \\'./docker/docker-compose.yml\\' is invalid because:\\nInvalid top-level property \"x-image\". Valid top-level sections for this Compose file are: version, services, networks, volumes, secrets, configs, and extensions starting with \"x-\".\\nYou might be seeing this error because you\\'re using the wrong Compose file version. Either specify a supported version (e.g \"2.2\" or \"3.3\") and place your service definitions under the `services` key, or omit the `version` key and place your service definitions at the root of the file to use version 1.\\nFor more on the Compose file format versions, see https://docs.docker.com/compose/compose-file/\\nIf you encounter the above error and have docker-compose installed, try updating your version of docker-compose. At the time of reporting this issue (March 17 2024), Ubuntu does not seem to support a docker-compose version high enough to run the required docker images. If you have this error and are on a Ubuntu machine, consider starting a VM with a Debian machine or look for an alternative way to download docker-compose at the latest version on your machine.', 'section': 'Workshop 2 - RisingWave', 'question': 'Setup - start-cluster error: Invalid top-level property x-image', 'course': 'data-engineering-zoomcamp', 'id': '9c750080'}]), ('6f4998e6', [{'text': 'Ans: [source] Yes, it is so that we can observe the changes as we’re working on the queries in real-time. The script is changing the date timestamp to the current time, so our queries with the now()filter would work. Open another terminal tab to copy+paste the queries while the stream-kafka script is running in the background.\\nNoel: I have recently increased this up to 100 at a time, you may pull the latest changes from the repository.', 'section': 'Workshop 2 - RisingWave', 'question': 'stream-kafka Qn: Is it expected that the records are being ingested 10 at a time?', 'course': 'data-engineering-zoomcamp', 'id': '6f4998e6'}]), ('97170587', [{'text': 'Ans: No, it is not.', 'section': 'Workshop 2 - RisingWave', 'question': 'Setup - Qn: Is kafka install required for the RisingWave workshop? [source]', 'course': 'data-engineering-zoomcamp', 'id': '97170587'}]), ('4def6541', [{'text': 'Ans: about 7GB free for all the containers to be provisioned and then the psql still needs to run and ingest the taxi data, so maybe 10gb in total?', 'section': 'Workshop 2 - RisingWave', 'question': 'Setup - Qn: How much free disk space should we have? [source]', 'course': 'data-engineering-zoomcamp', 'id': '4def6541'}]), ('66e117dd', [{'text': 'Replace psycopg2==2.9.9 with psycopg2-binary in the requirements.txt file [source] [another]\\nWhen you open another terminal to run the psql, remember to do the source command.sh step for each terminal session\\n---------------------------------------------------------------------------------------------', 'section': 'Workshop 2 - RisingWave', 'question': 'Psycopg2 - issues when running stream-kafka script', 'course': 'data-engineering-zoomcamp', 'id': '66e117dd'}]), ('94fd2476', [{'text': \"If you’re using an Anaconda installation:\\nCd home/\\nConda install gcc\\nSource back to your RisingWave Venv - source .venv/bin/activate\\nPip install psycopg2-binary\\nPip install -r requirements.txt\\nFor some reason this worked - the Conda base doesn’t have the GCC installed - (GNU Compiler Collection) a compiler system that supports various programming languages. Without this the it fails to install pyproject.toml-based projects\\n“It's possible that in your specific environment, the gcc installation was required at the system level rather than within the virtual environment. This can happen if the build process for psycopg2 tries to access system-level dependencies during installation.\\nInstalling gcc in your main Python installation (Conda) would make it available system-wide, allowing any Python environment to access it when necessary for building packages.”\\ngcc stands for GNU Compiler Collection. It is a compiler system developed by the GNU Project that supports various programming languages, including C, C++, Objective-C, and Fortran.\\nGCC is widely used for compiling source code written in these languages into executable programs or libraries. It's a key tool in the software development process, particularly in the compilation stage where source code is translated into machine code that can be executed by a computer's processor.\\nIn addition to compiling source code, GCC also provides various optimization options, debugging support, and extensive documentation, making it a powerful and versatile tool for developers across different platforms and architectures.\\n—-----------------------------------------------------------------------------------\", 'section': 'Workshop 2 - RisingWave', 'question': 'Psycopg2 - `Could not build wheels for psycopg2, which is required to install pyproject.toml-based projects`', 'course': 'data-engineering-zoomcamp', 'id': '94fd2476'}]), ('70d83d78', [{'text': \"Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\\nUse the git bash terminal in windows.\\nActivate python venv from git bash: source .venv/Scripts/activate\\nModify the seed_kafka.py file: in the first line, replace python3 with python.\\nNow from git bash, run the seed-kafka cmd. It should work now.\\nAdditional Notes:\\nYou can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\\nThe equivalent of source commands.sh  in Powershell is . .\\\\commands.sh from the workshop directory.\\nHope this can save you from some trouble in case you're doing this workshop on Windows like I am.\\n—--------------------------------------------------------------------------------------\", 'section': 'Workshop 2 - RisingWave', 'question': 'Psycopg2 InternalError: Failed to run the query - when running the seed-kafka command after initial setup.', 'course': 'data-engineering-zoomcamp', 'id': '70d83d78'}]), ('accb7285', [{'text': 'In case the script gets stuck on\\n%3|1709652240.100|FAIL|rdkafka#producer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)gre\\nafter trying to load the trip data, check the logs of the message_queue container in docker. If it keeps restarting with Could not initialize seastar: std::runtime_error (insufficient physical memory: needed 4294967296 available 4067422208)  as the last message, then go to the docker-compose file in the docker folder of the project and change the ‘memory’ command for the message_queue service for some lower value.\\nSolution: lower the memory allocation of the service “message_queue” in your docker-compose file from 4GB. If you have the “insufficient physical memory” error message (try 3GB)\\nIssue: Running psql -f risingwave-sql/table/trip_data.sql after starting services with ‘default’ values using docker-compose up gives the error  “psql:risingwave-sql/table/trip_data.sql:61: ERROR:  syntax error at or near \".\" LINE 60:       properties.bootstrap.server=\\'message_queue:29092\\'”\\nSolution: Make sure you have run source commands.sh in each terminal window', 'section': 'Workshop 2 - RisingWave', 'question': 'Running stream-kafka script gets stuck on a loop with Connection Refused', 'course': 'data-engineering-zoomcamp', 'id': 'accb7285'}]), ('cbca4495', [{'text': 'Use seed-kafka instead of stream-kafka to get a static set of results.', 'section': 'Workshop 2 - RisingWave', 'question': 'For the homework questions is there a specific number of records that have to be processed to obtain the final answer?', 'course': 'data-engineering-zoomcamp', 'id': 'cbca4495'}]), ('78fce6ad', [{'text': 'It is best to use the order by and limit clause on the query to the materialized view instead of the materialized view creation in order to guarantee consistent results\\nHomework - The answers in the homework do not match the provided options: You must follow the following steps: 1. clean-cluster 2. docker volume prune and use seed-kafka instead of stream-kafka. Ensure that the number of records is 100K.', 'section': 'Workshop 2 - RisingWave', 'question': 'Homework - Materialized view does not guarantee order by warning', 'course': 'data-engineering-zoomcamp', 'id': '78fce6ad'}]), ('68842c02', [{'text': 'For this workshop, and if you are following the view from Noel (2024) this requires you to install postgres to use it on your terminal. Found this steps (commands) to get it done [source]:\\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\\nsudo sh -c \\'echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list\\'\\nsudo apt update\\napt install postgresql postgresql-contrib\\n(comment): now let’s check the service for postgresql\\nservice postgresql status\\n(comment) If down: use the next command\\nservice postgresql start\\n(comment) And your are done', 'section': 'Workshop 2 - RisingWave', 'question': 'How to install postgress on Linux like OS', 'course': 'data-engineering-zoomcamp', 'id': '68842c02'}]), ('71b1984b', [{'text': 'Refer to the solution given in the first solution here:\\nhttps://stackoverflow.com/questions/24683221/xdg-open-no-method-available-even-after-installing-xdg-utils\\nInstead of w3m use any other browser of your choice.\\nIt is just trying to open the index.html file. Which you can do from your File Explorer/Finder. If you’re on wsl try using explorer.exe index.html', 'section': 'Workshop 2 - RisingWave', 'question': 'Unable to Open Dashboard as xdg-open doesn’t open any browser', 'course': 'data-engineering-zoomcamp', 'id': '71b1984b'}]), ('d452b490', [{'text': 'Example Error:\\nWhen attempting to execute a Python script named seed-kafka.py or server.py with the following shebang line specifying Python 3 as the interpreter:\\nUsers may encounter the following error in a Unix-like environment:\\nThis error indicates that there is a problem with the Python interpreter path specified in the shebang line. The presence of the \\\\r character suggests that the script was edited or created in a Windows environment, causing the interpreter path to be incorrect when executed in Unix-like environments.\\n2 Solutions:\\nEither one or the other\\nUpdate Shebang Line:\\nVerify Python Interpreter Path: Use the which python3 command to determine the path to the Python 3 interpreter available in the current environment.\\nUpdate Shebang Line: Open the script file in a text editor. Modify the shebang line to point to the correct Python interpreter path found in the previous step. Ensure that the shebang line is consistent with the Python interpreter path in the execution environment.\\nExample Shebang Line:\\nReplace /usr/bin/env python3 with the correct Python interpreter path found using which python3.\\nConvert Line Endings:\\nUse the dos2unix command-line tool to convert the line endings of the script from Windows-style to Unix-style.\\nThis removes the extraneous carriage return characters (\\\\r), resolving issues related to unexpected tokens and ensuring compatibility with Unix-like environments.\\nExample Command:', 'section': 'Workshop 2 - RisingWave', 'question': 'Resolving Python Interpreter Path Inconsistencies in Unix-like Environments', 'course': 'data-engineering-zoomcamp', 'id': 'd452b490'}]), ('707cae8f', [{'text': 'Ans : Windowing in streaming SQL involves defining a time-based or row-based boundary for data processing. It allows you to analyze and aggregate data over specific time intervals or based on the number of events received, providing a way to manage and organize streaming data for analysis.', 'section': 'Workshop 2 - RisingWave', 'question': 'How does windowing work in Sql?', 'course': 'data-engineering-zoomcamp', 'id': '707cae8f'}]), ('ffbf3311', [{'text': 'Python 3.12.1, is not compatible with kafka-python-2.0.2. Therefore, instead of running \"pip install kafka-python\", you can resolve the issue by using \"pip install git+https://github.com/dpkp/kafka-python.git\". If you have already installed kafka-python, you need to run \"pip uninstall kafka-python\" before executing \"pip install git+https://github.com/dpkp/kafka-python.git\" to resolve the compatibility issue.\\nQ:In the Mage pipeline, individual blocks run successfully. However, when executing the pipeline as a whole, some blocks fail.\\nA: I have the following key-value pair in io_config.yaml file configured but still Mage blocks failed to generate OAuth and authenticate with GCP: GOOGLE_SERVICE_ACC_KEY_FILEPATH: \"{{ env_var(\\'GCP_CREDENTIALS\\') }}\". The GCP_CREDENTIALS variable holds the full path to the service account key\\'s JSON file. Adding the following line within the failed code block resolved the issue: os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = os.environ.get(\\'GCP_CREDENTIALS\\').\\nThis occurs because the path to profiles.yml is not correctly specified. You can rectify this by:\\n“export DBT_PROFILES_DBT=path/to/profiles.yml”\\nEg., /home/src/magic-zoomcamp/dbt/project_name/\\nDo the similar for DBT_PROJECT_DIR if getting similar issue with dbt_project.yml.\\nOnce DIRs are set,:\\n“dbt debug –config-dir”\\nThis would update your paths. To maintain same path across sessions, use the path variables in your .env file.\\nTo add triggers in mage pipelines via CLI, you can create a trigger of type API, and copy the API links.\\nEg. link: http://localhost:6789/api/pipeline_schedules/10/pipeline_runs/f3a1a4228fc64cfd85295b668c93f3b2\\nThen create a trigger.py as such:\\nimport os\\nimport requests\\nclass MageTrigger:\\nOPTIONS = {\\n\"<pipeline_name>\": {\\n\"trigger_id\": 10,\\n\"key\": \"f3a1a4228fc64cfd85295b668c93f3b2\"\\n}\\n}\\n@staticmethod\\ndef trigger_pipeline(pipeline_name, variables=None):\\ntrigger_id = MageTrigger.OPTIONS[pipeline_name][\"trigger_id\"]\\nkey = MageTrigger.OPTIONS[pipeline_name][\"key\"]\\nendpoint = f\"http://localhost:6789/api/pipeline_schedules/{trigger_id}/pipeline_runs/{key}\"\\nheaders = {\\'Content-Type\\': \\'application/json\\'}\\npayload = {}\\nif variables is not None:\\npayload[\\'pipeline_run\\'] = {\\'variables\\': variables}\\nresponse = requests.post(endpoint, headers=headers, json=payload)\\nreturn response\\nMageTrigger.trigger_pipeline(\"<pipeline_name>\")\\nFinally, after the mage server is up an running, simply this command:\\npython trigger.py from mage directory in terminal.\\nCan I do data partitioning & clustering run by dbt pipeline, or I would need to do this manually in BigQuery afterwards?\\nYou can use this configuration in your DBT model:\\n{\\n\"field\": \"<field name>\",\\n\"data_type\": \"<timestamp | date | datetime | int64>\",\\n\"granularity\": \"<hour | day | month | year>\"\\n# Only required if data_type is \"int64\"\\n\"range\": {\\n\"start\": <int>,\\n\"end\": <int>,\\n\"interval\": <int>\\n}\\n}\\nand for clustering\\n{{\\nconfig(\\nmaterialized = \"table\",\\ncluster_by = \"order_id\",\\n)\\n}}\\nmore details in: https://docs.getdbt.com/reference/resource-configs/bigquery-configs', 'section': 'Triggers in Mage via CLI', 'question': 'Encountering the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\" when running \"from kafka import KafkaProducer\" in Jupyter Notebook for Module 6 Homework?', 'course': 'data-engineering-zoomcamp', 'id': 'ffbf3311'}]), ('3916f4a9', [{'text': 'Docker Commands\\n# Create a Docker Image from a base image\\nDocker run -it ubuntu bash\\n#List docker images\\nDocker images list\\n#List  Running containers\\nDocker ps -a\\n#List with full container ids\\nDocker ps -a --no-trunc\\n#Add onto existing image to create new image\\nDocker commit -a <User_Name> -m \"Message\" container_id New_Image_Name\\n# Create a Docker Image with an entrypoint from a base image\\nDocker run -it --entry_point=bash python:3.11\\n#Attach to a stopped container\\nDocker start -ai <Container_Name>\\n#Attach to a running container\\ndocker exec -it <Container_ID> bash\\n#copying from host to container\\nDocker cp <SRC_PATH/file> <containerid>:<dest_path>\\n#copying from container to host\\nDocker cp <containerid>:<Srct_path> <Dest Path on host/file>\\n#Create an image from a docker file\\nDocker build -t <Image_Name> <Location of Dockerfile>\\n#DockerFile Options and best practices\\nhttps://devopscube.com/build-docker-image/\\n#Docker delete all images forcefully\\ndocker rmi -f $(docker images -aq)\\n#Docker delete all containers forcefully\\ndocker rm -f $(docker ps -qa)\\n#docker compose creation\\nhttps://www.composerize.com/\\nGCP Commands\\n1.     Create SSH Keys\\n2.     Added to the Settings of Compute Engine VM Instance\\n3.     SSH-ed into the VM Instance with a config similar to following\\nHost my-website.com\\nHostName my-website.com\\nUser my-user\\nIdentityFile ~/.ssh/id_rsa\\n4.     Installed Anaconda by installing the sh file through bash <Anaconda.sh>\\n5.     Install Docker after\\na.     Sudo apt-get update\\nb.     Sudo apt-get docker\\n6.     To run Docker without SUDO permissions\\na.     https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md\\n7.     Google cloud remote copy\\na.     gcloud compute scp LOCAL_FILE_PATHVM_NAME:REMOTE_DIR\\nInstall GCP Cloud SDK on Docker Machine\\nhttps://stackoverflow.com/questions/23247943/trouble-installing-google-cloud-sdk-in-ubuntu\\nsudo apt-get install apt-transport-https ca-certificates gnupg && echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\"| sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list&& curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && sudo apt-get update && sudo apt-get install google-cloud-sdk && sudo apt-get install google-cloud-sdk-app-engine-java && sudo apt-get install google-cloud-sdk-app-engine-python && gcloud init\\nAnaconda Commands\\n#Activate environment\\nConda Activate <environment_name>\\n#DeActivate environment\\nConda DeActivate <environment_name>\\n#Start iterm without conda environment\\nconda config --set auto_activate_base false\\n# Using Conda forge as default (Community driven packaging recipes and solutions)\\nhttps://conda-forge.org/docs/user/introduction.html\\nconda --version\\nconda update conda\\nconda config --add channels conda-forge\\nconda config --set channel_priority strict\\n#Using Libmamba as Solver\\nconda install pgcli  --solver=libmamba\\nLinux/MAC Commands\\nStarting and Stopping Services on Linux\\n●  \\tsudo systemctl start postgresql\\n●  \\tsudo systemctl stop postgresql\\nStarting and Stopping Services on MAC\\n●      launchctl start postgresql\\n●      launchctl stop postgresql\\nIdentifying processes listening to a Port across MAC/Linux\\nsudo lsof -i -P -n | grep LISTEN\\n$ sudo netstat -tulpn | grep LISTEN\\n$ sudo ss -tulpn | grep LISTEN\\n$ sudo lsof -i:22 ## see a specific port such as 22 ##\\n$ sudo nmap -sTU -O IP-address-Here\\nInstalling a package on Debian\\nsudo apt install <packagename>\\nListing all package on Debian\\nDpkg -l | grep <packagename>\\nUnInstalling a package on Debian\\nSudo apt remove <packagename>\\nSudo apt autoclean  && sudo apt autoremove\\nList all Processes on Debian/Ubuntu\\nPs -aux\\napt-get update && apt-get install procps\\napt-get install iproute2 for ss -tulpn\\n#Postgres Install\\nsudo sh -c \\'echo \"deb https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list\\'\\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\\nsudo apt-get update\\nsudo apt-get -y install postgresql\\n#Changing Postgresql port to 5432\\n- sudo service postgresql stop - sed -e \\'s/^port.*/port = 5432/\\' /etc/postgresql/10/main/postgresql.conf > postgresql.conf\\n- sudo chown postgres postgresql.conf\\n- sudo mv postgresql.conf /etc/postgresql/10/main\\n- sudo systemctl restart postgresql', 'section': 'Triggers in Mage via CLI', 'question': 'Basic Commands', 'course': 'data-engineering-zoomcamp', 'id': '3916f4a9'}]), ('0227b872', [{'text': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork', 'section': 'General course-related questions', 'question': 'How do I sign up?', 'course': 'machine-learning-zoomcamp', 'id': '0227b872'}]), ('39fda9f0', [{'text': 'The course videos are pre-recorded, you can start watching the course right now.\\nWe will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\\nYou can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.', 'section': 'General course-related questions', 'question': 'Is it going to be live? When?', 'course': 'machine-learning-zoomcamp', 'id': '39fda9f0'}]), ('5170565b', [{'text': 'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.', 'section': 'General course-related questions', 'question': 'What if I miss a session?', 'course': 'machine-learning-zoomcamp', 'id': '5170565b'}]), ('ecca790c', [{'text': \"The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.: https://mlbookcamp.com/article/python\\nFor example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results.\", 'section': 'General course-related questions', 'question': 'How much theory will you cover?', 'course': 'machine-learning-zoomcamp', 'id': 'ecca790c'}]), ('c25b3de4', [{'text': \"Yes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code.\\nHere are some interesting videos covering linear algebra that you can already watch: ML Zoomcamp 1.8 - Linear Algebra Refresher from Alexey Grigorev or the excellent playlist from 3Blue1Brown Vectors | Chapter 1, Essence of linear algebra. Never hesitate to ask the community for help if you have any question.\\n(Mélanie Fouesnard)\", 'section': 'General course-related questions', 'question': \"I don't know math. Can I take the course?\", 'course': 'machine-learning-zoomcamp', 'id': 'c25b3de4'}]), ('6ba259b1', [{'text': \"The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.\\nIf you unsubscribed from our newsletter, you won't get course related updates too.\\nBut don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.\", 'section': 'General course-related questions', 'question': \"I filled the form, but haven't received a confirmation email. Is it normal?\", 'course': 'machine-learning-zoomcamp', 'id': '6ba259b1'}]), ('67e2fd13', [{'text': 'Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)', 'section': 'General course-related questions', 'question': 'How long is the course?', 'course': 'machine-learning-zoomcamp', 'id': '67e2fd13'}]), ('a6897e8c', [{'text': 'Around ~10 hours per week. Timur Kamaliev did a detailed analysis of how much time students of the previous cohort needed to spend on different modules and projects. Full article', 'section': 'General course-related questions', 'question': 'How much time do I need for this course?', 'course': 'machine-learning-zoomcamp', 'id': 'a6897e8c'}]), ('2eba08e3', [{'text': 'Yes, if you finish at least 2 out of 3 projects and review 3 peers’ Projects by the deadline, you will get a certificate. This is what it looks like: link. There’s also a version without a robot: link.', 'section': 'General course-related questions', 'question': 'Will I get a certificate?', 'course': 'machine-learning-zoomcamp', 'id': '2eba08e3'}]), ('1d644223', [{'text': \"Yes, it's possible. See the previous answer.\", 'section': 'General course-related questions', 'question': 'Will I get a certificate if I missed the midterm project?', 'course': 'machine-learning-zoomcamp', 'id': '1d644223'}]), ('14890cd2', [{'text': 'Check this article. If you know everything in this article, you know enough. If you don’t, read the article and join the coursIntroduction to Pythone too :)\\nIntroduction to Python – Machine Learning Bookcamp\\nYou can follow this English course from the OpenClassrooms e-learning platform, which is free and covers the python basics for data analysis: Learn Python Basics for Data Analysis - OpenClassrooms . It is important to know some basics such as: how to run a Jupyter notebook, how to import libraries (and what libraries are), how to declare a variable (and what variables are) and some important operations regarding data analysis.\\n(Mélanie Fouesnard)', 'section': 'General course-related questions', 'question': 'How much Python should I know?', 'course': 'machine-learning-zoomcamp', 'id': '14890cd2'}]), ('a4fad482', [{'text': 'For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud (we use Saturn cloud but can be anything else).\\n(Rileen Sinha; based on response by Alexey on Slack)', 'section': 'General course-related questions', 'question': \"Any particular hardware requirements for the course or everything is mostly cloud? TIA! Couldn't really find this in the FAQ.\", 'course': 'machine-learning-zoomcamp', 'id': 'a4fad482'}]), ('34b7fd35', [{'text': 'Here is an article that worked for me: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/', 'section': 'General course-related questions', 'question': 'How to setup TensorFlow with GPU support on Ubuntu?', 'course': 'machine-learning-zoomcamp', 'id': '34b7fd35'}]), ('4930aa19', [{'text': \"Here’s how you join a in Slack: https://slack.com/help/articles/205239967-Join-a-channel\\nClick “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.\\nBrowse the list of public channels in your workspace, or use the search bar to search by channel name or description.\\nSelect a channel from the list to view it.\\nClick Join Channel.\\nDo we need to provide the GitHub link to only our code corresponding to the homework questions?\\nYes. You are required to provide the URL to your repo in order to receive a grade\", 'section': 'General course-related questions', 'question': 'I’m new to Slack and can’t find the course channel. Where is it?', 'course': 'machine-learning-zoomcamp', 'id': '4930aa19'}]), ('ee58a693', [{'text': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.', 'section': 'General course-related questions', 'question': 'The course has already started. Can I still join it?', 'course': 'machine-learning-zoomcamp', 'id': 'ee58a693'}]), ('636f55d5', [{'text': 'The course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).', 'section': 'General course-related questions', 'question': 'When does the next iteration start?', 'course': 'machine-learning-zoomcamp', 'id': '636f55d5'}]), ('c839b764', [{'text': 'No, it’s not possible. The form is closed after the due date. But don’t worry, homework is not mandatory for finishing the course.', 'section': 'General course-related questions', 'question': 'Can I submit the homework after the due date?', 'course': 'machine-learning-zoomcamp', 'id': 'c839b764'}]), ('0a278fb2', [{'text': 'Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\\nClick on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nOr you can just use this link: http://mlzoomcamp.com/#syllabus', 'section': 'General course-related questions', 'question': 'I just joined. What should I do next? How can I access course materials?', 'course': 'machine-learning-zoomcamp', 'id': '0a278fb2'}]), ('8de4fefd', [{'text': 'For the 2023 cohort, you can see the deadlines here (it’s taken from the 2023 cohort page)', 'section': 'General course-related questions', 'question': 'What are the deadlines in this course?', 'course': 'machine-learning-zoomcamp', 'id': '8de4fefd'}]), ('94e86808', [{'text': 'There’s not much difference. There was one special module (BentoML) in the previous iteration of the course, but the rest of the modules are the same as in 2022. The homework this year is different.', 'section': 'General course-related questions', 'question': 'What’s the difference between the previous iteration of the course (2022) and this one (2023)?', 'course': 'machine-learning-zoomcamp', 'id': '94e86808'}]), ('e7ba6b8a', [{'text': 'We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\\nIf you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.', 'section': 'General course-related questions', 'question': 'The course videos are from the previous iteration. Will you release new ones or we’ll use the videos from 2021?', 'course': 'machine-learning-zoomcamp', 'id': 'e7ba6b8a'}]), ('f7bc2f65', [{'text': 'When you post about what you learned from the course on your social media pages, use the tag #mlzoomcamp. When you submit your homework, there’s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).\\nFor posting the learning in public links, you get extra scores. But the number of scores is limited to 7 points: if you put more than 7 links in your homework form, you’ll get only 7 points.\\nThe same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week, see Alexey’s reply. (~ ellacharmed)\\nFor midterms/capstones, the awarded points are doubled as the duration is longer. So for projects the points are capped at 14 for 14 URLs.', 'section': 'General course-related questions', 'question': 'Submitting learning in public links', 'course': 'machine-learning-zoomcamp', 'id': 'f7bc2f65'}]), ('ae52a907', [{'text': \"You can create your own github repository for the course with your notes, homework, projects, etc.\\nThen fork the original course repo and add a link under the 'Community Notes' section to the notes that are in your own repo.\\nAfter that's done, create a pull request to sync your fork with the original course repo.\\n(By Wesley Barreto)\", 'section': 'General course-related questions', 'question': 'Adding community notes', 'course': 'machine-learning-zoomcamp', 'id': 'ae52a907'}]), ('dab5a24a', [{'text': \"Leaderboard Links:\\n2023 - https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml\\n2022 - https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml\\nPython Code:\\nfrom hashlib import sha1\\ndef compute_hash(email):\\nreturn sha1(email.lower().encode('utf-8')).hexdigest()\\nYou need to call the function as follows:\\nprint(compute_hash('YOUR_EMAIL_HERE'))\\nThe quotes are required to denote that your email is a string.\\n(By Wesley Barreto)\\nYou can also use this website directly by entering your email: http://www.sha1-online.com. Then, you just have to copy and paste your hashed email in the “research” bar of the leaderboard to get your scores.\\n(Mélanie Fouesnard)\", 'section': '1. Introduction to Machine Learning', 'question': 'Computing the hash for the leaderboard and project review', 'course': 'machine-learning-zoomcamp', 'id': 'dab5a24a'}]), ('49f9bda9', [{'text': 'If you get “wget is not recognized as an internal or external command”, you need to install it.\\nOn Ubuntu, run\\nsudo apt-get install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\nchoco install wget\\nOr you can download a binary from here and put it to any location in your PATH (e.g. C:/tools/)\\nOn Mac, the easiest way to install wget is to use brew.\\nBrew install wget\\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need eeeto use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAnd then in your python code, for example in your jupyter notebook, use:\\nimport wget\\nwget.download(\"URL\")\\nThis should download whatever is at the URL in the same directory as your code.\\n(Memoona Tahira)\\nAlternatively, you can read a CSV file from a URL directly with pandas:\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\ndf = pd.read_csv(url)\\nValid URL schemes include http, ftp, s3, gs, and file.\\nIn some cases you might need to bypass https checks:\\nimport ssl\\nssl._create_default_https_context = ssl._create_unverified_context\\nOr you can use the built-in Python functionality for downloading the files:\\nimport urllib.request\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\nUrllib.request.urlretrieve() is a standard Python library function available on all devices and platforms. URL requests and URL data retrieval are done with the urllib.request module.\\nThe urlretrieve() function allows you to download files from URLs and save them locally. Python programs use it to download files from the internet.\\nOn any Python-enabled device or platform, you can use the urllib.request.urlretrieve() function to download the file.\\n(Mohammad Emad Sharifi)', 'section': '1. Introduction to Machine Learning', 'question': 'wget is not recognized as an internal or external command', 'course': 'machine-learning-zoomcamp', 'id': '49f9bda9'}]), ('d44de7d1', [{'text': 'You can use\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nTo download the data too. The exclamation mark !, lets you execute shell commands inside your notebooks. This works generally for shell commands such as ls, cp, mkdir, mv etc . . .\\nFor instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:\\n!mkdir -p ../data/\\n!mv housing.csv ../data/', 'section': '1. Introduction to Machine Learning', 'question': 'Retrieving csv inside notebook', 'course': 'machine-learning-zoomcamp', 'id': 'd44de7d1'}]), ('314ebe32', [{'text': '(Tyler Simpson)', 'section': '1. Introduction to Machine Learning', 'question': 'Windows WSL and VS Code\\nIf you have a Windows 11 device and would like to use the built in WSL to access linux you can use the Microsoft Learn link Set up a WSL development environment | Microsoft Learn. To connect this to VS Code download the Microsoft verified VS Code extension ‘WSL’ this will allow you to remotely connect to your WSL Ubuntu instance as if it was a virtual machine.', 'course': 'machine-learning-zoomcamp', 'id': '314ebe32'}]), ('98cff602', [{'text': 'This is my first time using Github to upload a code. I was getting the below error message when I type\\ngit push -u origin master:\\nerror: src refspec master does not match any\\nerror: failed to push some refs to \\'https://github.com/XXXXXX/1st-Homework.git\\'\\nSolution:\\nThe error message got fixed by running below commands:\\ngit commit -m \"initial commit\"\\ngit push origin main\\nIf this is your first time to use Github, you will find a great & straightforward tutorial in this link https://dennisivy.com/github-quickstart\\n(Asia Saeed)\\nYou can also use the “upload file” functionality from GitHub for that\\nIf you write your code on Google colab you can also directly share it on your Github.\\n(By Pranab Sarma)', 'section': '1. Introduction to Machine Learning', 'question': 'Uploading the homework to Github', 'course': 'machine-learning-zoomcamp', 'id': '98cff602'}]), ('54ec0de4', [{'text': \"I'm trying to invert the matrix but I got error that the matrix is singular matrix\\nThe singular matrix error is caused by the fact that not every matrix can be inverted. In particular, in the homework it happens because you have to pay close attention when dealing with multiplication (the method .dot) since multiplication is not commutative! X.dot(Y) is not necessarily equal to Y.dot(X), so respect the order otherwise you get the wrong matrix.\", 'section': '1. Introduction to Machine Learning', 'question': 'Singular Matrix Error', 'course': 'machine-learning-zoomcamp', 'id': '54ec0de4'}]), ('f81f4ecb', [{'text': 'I have a problem with my terminal. Command\\nconda create -n ml-zoomcamp python=3.9\\ndoesn’t work. Any of 3.8/ 3.9 / 3.10 should be all fine\\nIf you’re on Windows and just installed Anaconda, you can use Anaconda’s own terminal called “Anaconda Prompt”.\\nIf you don’t have Anaconda or Miniconda, you should install it first\\n(Tatyana Mardvilko)', 'section': '1. Introduction to Machine Learning', 'question': 'Conda is not an internal command', 'course': 'machine-learning-zoomcamp', 'id': 'f81f4ecb'}]), ('be760b92', [{'text': 'How do I read the dataset with Pandas in Windows?\\nI used the code below but not working\\ndf = pd.read_csv(\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\nUnlike Linux/Mac OS, Windows uses the backslash (\\\\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the \\'\\\\\\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, \"\\\\n\" to add a new line or \"\\\\t\" to add spaces, etc. To avoid the issue we just need to add \"r\" before the file path and Python will treat it as a literal string (not an escape sequence).\\nHere’s how we should be loading the file instead:\\ndf = pd.read_csv(r\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\n(Muhammad Awon)', 'section': '1. Introduction to Machine Learning', 'question': 'Read-in the File in Windows OS', 'course': 'machine-learning-zoomcamp', 'id': 'be760b92'}]), ('a2cfa1c9', [{'text': 'Type the following command:\\ngit config -l | grep url\\nThe output should look like this:\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\nChange this to the following format and make sure the change is reflected using command in step 1:\\ngit remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n(Added by Dheeraj Karra)', 'section': '1. Introduction to Machine Learning', 'question': \"'403 Forbidden' error message when you try to push to a GitHub repository\", 'course': 'machine-learning-zoomcamp', 'id': 'a2cfa1c9'}]), ('7b907071', [{'text': \"I had a problem when I tried to push my code from Git Bash:\\nremote: Support for password authentication was removed on August 13, 2021.\\nremote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\\nfatal: Authentication failed for 'https://github.com/username\\nSolution:\\nCreate a personal access token from your github account and use it when you make a push of your last changes.\\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\\nBruno Bedón\", 'section': '1. Introduction to Machine Learning', 'question': \"Fatal: Authentication failed for 'https://github.com/username\", 'course': 'machine-learning-zoomcamp', 'id': '7b907071'}]), ('fc2e0a61', [{'text': \"In Kaggle, when you are trying to !wget a dataset from github (or any other public repository/location), you get the following error:\\nGetting  this error while trying to import data- !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.\\nwget: unable to resolve host address 'raw.githubusercontent.com'\\nSolution:\\nIn your Kaggle notebook settings, turn on the Internet for your session. It's on the settings panel, on the right hand side of the Kaggle screen. You'll be asked to verify your phone number so Kaggle knows you are not a bot.\", 'section': '1. Introduction to Machine Learning', 'question': \"wget: unable to resolve host address 'raw.githubusercontent.com'\", 'course': 'machine-learning-zoomcamp', 'id': 'fc2e0a61'}]), ('d43e5742', [{'text': 'I found this video quite helpful: Creating Virtual Environment for Python from VS Code\\n[Native Jupiter Notebooks support in VS Code] In VS Code you can also have a native Jupiter Notebooks support, i.e. you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled + run a ‘jupyter notebook ‘ command from a remote machine + have a remote connection configured in .ssh/config (as Alexey’s video suggests) - VS Code can execute remote Jupyter Notebooks files on a remote server from your local machine: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\\n[Git support from VS Code] You can work with Github from VSCode - staging and commits are easy from the VS Code’s UI:  https://code.visualstudio.com/docs/sourcecontrol/overview\\n(Added by Ivan Brigida)', 'section': '1. Introduction to Machine Learning', 'question': 'Setting up an environment using VS Code', 'course': 'machine-learning-zoomcamp', 'id': 'd43e5742'}]), ('32bc0538', [{'text': 'With regards to creating an environment for the project, do we need to run the command \"conda create -n .......\" and \"conda activate ml-zoomcamp\" everytime we open vs code to work on the project?\\nAnswer:\\n\"conda create -n ....\" is just run the first time to create the environment. Once created, you just need to run \"conda activate ml-zoomcamp\" whenever you want to use it.\\n(Added by Wesley Barreto)\\nconda env export > environment.yml will also allow you to reproduce your existing environment in a YAML file.  You can then recreate it with conda env create -f environment.yml', 'section': '1. Introduction to Machine Learning', 'question': 'Conda Environment Setup', 'course': 'machine-learning-zoomcamp', 'id': '32bc0538'}]), ('b6730228', [{'text': \"I was doing Question 7 from Week1 Homework and with step6: Invert XTX, I created the inverse. Now, an inverse when multiplied by the original matrix should return in an Identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:\\nInverse * Original:\\n[[ 1.00000000e+00 -1.38777878e-16]\\n[ 3.16968674e-13  1.00000000e+00]]\\nSolution:\\nIt's because floating point math doesn't work well on computers as shown here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken\\n(Added by Wesley Barreto)\", 'section': '1. Introduction to Machine Learning', 'question': 'Floating Point Precision', 'course': 'machine-learning-zoomcamp', 'id': 'b6730228'}]), ('3ce9bbb8', [{'text': 'Answer:\\nIt prints the information about the dataset like:\\nIndex datatype\\nNo. of entries\\nColumn information with not-null count and datatype\\nMemory usage by dataset\\nWe use it as:\\ndf.info()\\n(Added by Aadarsha Shrestha & Emoghena Itakpe)', 'section': '1. Introduction to Machine Learning', 'question': 'What does pandas.DataFrame.info() do?', 'course': 'machine-learning-zoomcamp', 'id': '3ce9bbb8'}]), ('4e584d06', [{'text': \"Pandas and numpy libraries are not being imported\\nNameError: name 'np' is not defined\\nNameError: name 'pd' is not defined\\nIf you're using numpy or pandas, make sure you use the first few lines before anything else.\\nimport pandas as pd\\nimport numpy as np\\nAdded by Manuel Alejandro Aponte\", 'section': '1. Introduction to Machine Learning', 'question': \"NameError: name 'np' is not defined\", 'course': 'machine-learning-zoomcamp', 'id': '4e584d06'}]), ('ff4da2b6', [{'text': \"What if there were hundreds of columns? How do you get the columns only with numeric or object data in a more concise way?\\ndf.select_dtypes(include=np.number).columns.tolist()\\ndf.select_dtypes(include='object').columns.tolist()\\nAdded by Gregory Morris\", 'section': '1. Introduction to Machine Learning', 'question': 'How to select column by dtype', 'course': 'machine-learning-zoomcamp', 'id': 'ff4da2b6'}]), ('58c1c168', [{'text': 'There are many ways to identify the shape of dataset, one of them is using .shape attribute!\\ndf.shape\\ndf.shape[0] # for identify the number of rows\\ndf.shape[1] # for identify the number of columns\\nAdded by Radikal Lukafiardi', 'section': '1. Introduction to Machine Learning', 'question': 'How to identify the shape of dataset in Pandas', 'course': 'machine-learning-zoomcamp', 'id': '58c1c168'}]), ('96076a1a', [{'text': 'First of all use np.dot for matrix multiplication. When you compute matrix-matrix multiplication you should understand that order of multiplying is crucial and affects the result of the multiplication!\\nDimension Mismatch\\nTo perform matrix multiplication, the number of columns in the 1st matrix should match the number of rows in the 2nd matrix. You can rearrange the order to make sure that this satisfies the condition.\\nAdded by Leah Gotladera', 'section': '1. Introduction to Machine Learning', 'question': 'How to avoid Value errors with array shapes in homework?', 'course': 'machine-learning-zoomcamp', 'id': '96076a1a'}]), ('3218389a', [{'text': 'You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.\\nThis method is called imputing - when you have NaN/ null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns.\\nAdded by Anneysha Sarkar', 'section': '1. Introduction to Machine Learning', 'question': 'Question 5: How and why do we replace the NaN values with average of the column?', 'course': 'machine-learning-zoomcamp', 'id': '3218389a'}]), ('183a1c90', [{'text': 'In Question 7 we are asked to calculate\\nThe initial problem  can be solved by this, where a Matrix X is multiplied by some unknown weights w resulting in the target y.\\nAdditional reading and videos:\\nOrdinary least squares\\nMultiple Linear Regression in Matrix Form\\nPseudoinverse Solution to OLS\\nAdded by Sylvia Schmitt\\nwith commends from Dmytro Durach', 'section': '1. Introduction to Machine Learning', 'question': 'Question 7: Mathematical formula for linear regression', 'course': 'machine-learning-zoomcamp', 'id': '183a1c90'}]), ('f0bc1c19', [{'text': 'This is most likely that you interchanged the first step of the multiplication\\nYou used  instead of\\nAdded by Emmanuel Ikpesu', 'section': '1. Introduction to Machine Learning', 'question': 'Question 7: FINAL MULTIPLICATION not having 5 column', 'course': 'machine-learning-zoomcamp', 'id': 'f0bc1c19'}]), ('735e6c78', [{'text': 'Note, that matrix multiplication (matrix-matrix, matrix-vector multiplication) can be written as * operator in some sources, but performed as @ operator or np.matmul() via numpy. * operator performs element-wise multiplication (Hadamard product).\\nnumpy.dot() or ndarray.dot() can be used, but for matrix-matrix multiplication @ or np.matmul() is preferred (as per numpy doc).\\nIf multiplying by a scalar numpy.multiply() or * is preferred.\\nAdded by Andrii Larkin', 'section': '1. Introduction to Machine Learning', 'question': 'Question 7: Multiplication operators.', 'course': 'machine-learning-zoomcamp', 'id': '735e6c78'}]), ('b8ca1cd3', [{'text': 'If you face an error kind of ImportError: cannot import name \\'contextfilter\\' from \\'jinja2\\' (anaconda\\\\lib\\\\site-packages\\\\jinja2\\\\__init__.py) when launching a new notebook for a brand new environment.\\nSwitch to the main environment and run \"pip install nbconvert --upgrade\".\\nAdded by George Chizhmak', 'section': '1. Introduction to Machine Learning', 'question': 'Error launching Jupyter notebook', 'course': 'machine-learning-zoomcamp', 'id': 'b8ca1cd3'}]), ('efdb235f', [{'text': 'If you face this situation and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. Then try again', 'section': '1. Introduction to Machine Learning', 'question': 'wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv hangs on MacOS Ventura M1', 'course': 'machine-learning-zoomcamp', 'id': 'efdb235f'}]), ('355348f0', [{'text': \"Wget doesn't ship with macOS, so there are other alternatives to use.\\nNo worries, we got curl:\\nexample:\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nExplanations:\\ncurl: a utility for retrieving information from the internet.\\n-o: Tell it to store the result as a file.\\nfilename: You choose the file's name.\\nLinks: Put the web address (URL) here, and cURL will extract data from it and save it under the name you provide.\\nMore about it at:\\nCurl Documentation\\nAdded by David Espejo\", 'section': '1. Introduction to Machine Learning', 'question': 'In case you are using mac os and having trouble with WGET', 'course': 'machine-learning-zoomcamp', 'id': '355348f0'}]), ('67afabf5', [{'text': \"You can use round() function or f-strings\\nround(number, 4)  - this will round number up to 4 decimal places\\nprint(f'Average mark for the Homework is {avg:.3f}') - using F string\\nAlso there is pandas.Series. round idf you need to round values in the whole Series\\nPlease check the documentation\\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\\nAdded by Olga Rudakova\", 'section': '2. Machine Learning for Regression', 'question': 'How to output only a certain number of decimal places', 'course': 'machine-learning-zoomcamp', 'id': '67afabf5'}]), ('50d737e7', [{'text': 'Here are the crucial links for this Week 2 that starts September 18, 2023\\nAsk questions for Live Sessions: https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions\\nCalendar for weekly meetings: https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1\\nWeek 2 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md\\nSubmit HW Week 2: https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform (also available at the bottom of the above link)\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 2.X --- https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12\\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n~~Nukta Bhatia~~', 'section': '2. Machine Learning for Regression', 'question': 'How do I get started with Week 2?', 'course': 'machine-learning-zoomcamp', 'id': '50d737e7'}]), ('bbc0fca3', [{'text': 'We can use histogram:\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n# Load the data\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\ndf = pd.read_csv(url)\\n# EDA\\nsns.histplot(df[\\'median_house_value\\'], kde=False)\\nplt.show()\\nOR ceck skewness and describe:\\nprint(df[\\'median_house_value\\'].describe())\\n# Calculate the skewness of the \\'median_house_value\\' variable\\nskewness = df[\\'median_house_value\\'].skew()\\n# Print the skewness value\\nprint(\"Skewness of \\'median_house_value\\':\", skewness)\\n(Mohammad Emad Sharifi)', 'section': '2. Machine Learning for Regression', 'question': 'Checking long tail of data', 'course': 'machine-learning-zoomcamp', 'id': 'bbc0fca3'}]), ('6f3bdd20', [{'text': 'It’s possible that when you follow the videos, you’ll get a Singular Matrix error. We will explain why it happens in the Regularization video. Don’t worry, it’s normal that you have it.\\nYou can also have an error because you did the inverse of X once in your code and you’re doing it a second time.\\n(Added by Cécile Guillot)', 'section': '2. Machine Learning for Regression', 'question': 'LinAlgError: Singular matrix', 'course': 'machine-learning-zoomcamp', 'id': '6f3bdd20'}]), ('27c2d90a', [{'text': 'You can find a detailed description of the dataset ere https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\\nKS', 'section': '2. Machine Learning for Regression', 'question': 'California housing dataset', 'course': 'machine-learning-zoomcamp', 'id': '27c2d90a'}]), ('88e9600a', [{'text': 'I was using for loops to apply rmse to list of y_val and y_pred. But the resulting rmse is all nan.\\nI found out that the problem was when my data reached the mean step after squaring the error in the rmse function. Turned out there were nan in the array, then I traced the problem back to where I first started to split the data: I had only use fillna(0) on the train data, not on the validation and test data. So the problem was fixed after I applied fillna(0) to all the dataset (train, val, test). Voila, my for loops to get rmse from all the seed values work now.\\nAdded by Sasmito Yudha Husada', 'section': '2. Machine Learning for Regression', 'question': 'Getting NaNs after applying .mean()', 'course': 'machine-learning-zoomcamp', 'id': '88e9600a'}]), ('d59d8df7', [{'text': 'Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?\\nOnly if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable.\\nThis can help to understand skewness and how it can be applied to the distribution of your data set.\\nhttps://en.wikipedia.org/wiki/Skewness\\nPastor Soto', 'section': '2. Machine Learning for Regression', 'question': 'Target variable transformation', 'course': 'machine-learning-zoomcamp', 'id': 'd59d8df7'}]), ('0b3eaf92', [{'text': 'The dataset can be read directly to pandas dataframe from the github link using the technique shown below\\ndfh=pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\nKrishna Anand', 'section': '2. Machine Learning for Regression', 'question': 'Reading the dataset directly from github', 'course': 'machine-learning-zoomcamp', 'id': '0b3eaf92'}]), ('8fe56032', [{'text': \"For users of kaggle notebooks, the dataset can be loaded through widget using the below command. Please remember that ! before wget is essential\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nOnce the dataset is loaded to the kaggle notebook server, it can be read through the below pandas command\\ndf = pd.read_csv('housing.csv')\\nHarish Balasundaram\", 'section': '2. Machine Learning for Regression', 'question': 'Loading the dataset directly through Kaggle Notebooks', 'course': 'machine-learning-zoomcamp', 'id': '8fe56032'}]), ('af833e0a', [{'text': 'We can filter a dataset by using its values as below.\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\nYou can use | for ‘OR’, and & for ‘AND’\\nAlternative:\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\nRadikal Lukafiardi', 'section': '2. Machine Learning for Regression', 'question': 'Filter a dataset by using its values', 'course': 'machine-learning-zoomcamp', 'id': 'af833e0a'}]), ('8d209d6d', [{'text': 'Above users showed how to load the dataset directly from github. Here is another useful way of doing this using the `requests` library:\\n# Get data for homework\\nimport requests\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\nif response.status_code == 200:\\nwith open(\\'housing.csv\\', \\'wb\\') as file:\\nfile.write(response.content)\\nelse:\\nprint(\"Download failed.\")\\nTyler Simpson', 'section': '2. Machine Learning for Regression', 'question': 'Alternative way to load the data using requests', 'course': 'machine-learning-zoomcamp', 'id': '8d209d6d'}]), ('0bc4c3da', [{'text': 'When creating a duplicate of your dataframe by doing the following:\\nX_train = df_train\\nX_val = df_val\\nYou’re still referencing the original variable, this is called a shallow copy. You can make sure that no references are attaching both variables and still keep the copy of the data do the following to create a deep copy:\\nX_train = df_train.copy()\\nX_val = df_val.copy()\\nAdded by Ixchel García', 'section': '2. Machine Learning for Regression', 'question': 'Null column is appearing even if I applied .fillna()', 'course': 'machine-learning-zoomcamp', 'id': '0bc4c3da'}]), ('c0ee2665', [{'text': 'Yes, you can. Here we implement it ourselves to better understand how it works, but later we will only rely on Scikit-Learn’s functions. If you want to start using it earlier — feel free to do it', 'section': '2. Machine Learning for Regression', 'question': 'Can I use Scikit-Learn’s train_test_split for this week?', 'course': 'machine-learning-zoomcamp', 'id': 'c0ee2665'}]), ('3f60871d', [{'text': 'Yes, you can. We will also do that next week, so don’t worry, you will learn how to do it.', 'section': '2. Machine Learning for Regression', 'question': 'Can I use LinearRegression from Scikit-Learn for this week?', 'course': 'machine-learning-zoomcamp', 'id': '3f60871d'}]), ('f30217a7', [{'text': 'What are equivalents in Scikit-Learn for the linear regression with and without regularization used in week 2.\\nCorresponding function for model without regularization:\\nsklearn.linear_model.LinearRegression\\nCorresponding function for model with regularization:\\nsklearn.linear_model.Ridge\\nThe linear model from Scikit-Learn are explained  here:\\nhttps://scikit-learn.org/stable/modules/linear_model.html\\nAdded by Sylvia Schmitt', 'section': '2. Machine Learning for Regression', 'question': 'Corresponding Scikit-Learn functions for Linear Regression (with and without Regularization)', 'course': 'machine-learning-zoomcamp', 'id': 'f30217a7'}]), ('91fc573d', [{'text': '`r` is a regularization parameter.\\nIt’s similar to `alpha` in sklearn.Ridge(), as both control the \"strength\" of regularization (increasing both will lead to stronger regularization), but mathematically not quite, here\\'s how both are used:\\nsklearn.Ridge()\\n||y - Xw||^2_2 + alpha * ||w||^2_2\\nlesson’s notebook (`train_linear_regression_reg` function)\\nXTX = XTX + r * np.eye(XTX.shape[0])\\n`r` adds “noise” to the main diagonal to prevent multicollinearity, which “breaks” finding inverse matrix.', 'section': '2. Machine Learning for Regression', 'question': 'Question 4: what is `r`, is it the same as `alpha` in sklearn.Ridge()?', 'course': 'machine-learning-zoomcamp', 'id': '91fc573d'}]), ('fe3139f6', [{'text': 'Q: “In lesson 2.8 why is y_pred different from y? After all, we trained X_train to get the weights that when multiplied by X_train should give exactly y, or?”\\nA: linear regression is a pretty simple model, it neither can nor should fit 100% (nor any other model, as this would be the sign of overfitting). This picture might illustrate some intuition behind this, imagine X is a single feature:\\nAs our model is linear, how would you draw a line to fit all the \"dots\"?\\nYou could \"fit\" all the \"dots\" on this pic using something like scipy.optimize.curve_fit (non-linear least squares) if you wanted to, but imagine how it would perform on previously unseen data.\\nAdded by Andrii Larkin', 'section': '2. Machine Learning for Regression', 'question': 'Why linear regression doesn’t provide a “perfect” fit?', 'course': 'machine-learning-zoomcamp', 'id': 'fe3139f6'}]), ('48aac030', [{'text': 'One of the questions on the homework calls for using a random seed of 42. When using 42, all my missing values ended up in my training dataframe and not my validation or test dataframes, why is that?\\nThe purpose of the seed value is to randomly generate the proportion split. Using a seed of 42 ensures that all learners are on the same page by getting the same behavior (in this case, all missing values ending up in the training dataframe). If using a different seed value (e.g. 9), missing values will then appear in all other dataframes.', 'section': '2. Machine Learning for Regression', 'question': 'Random seed 42', 'course': 'machine-learning-zoomcamp', 'id': '48aac030'}]), ('28321bc2', [{'text': 'It is possible to do the shuffling of the dataset with the pandas built-in function pandas.DataFrame.sample.The complete dataset can be shuffled including resetting the index with the following commands:\\nSetting frac=1 will result in returning a shuffled version of the complete Dataset.\\nSetting random_state=seed will result in the same randomization as used in the course resources.\\ndf_shuffled = df.sample(frac=1, random_state=seed)\\ndf_shuffled.reset_index(drop=True, inplace=True)\\nAdded by Sylvia Schmitt', 'section': '2. Machine Learning for Regression', 'question': 'Shuffling the initial dataset using pandas built-in function', 'course': 'machine-learning-zoomcamp', 'id': '28321bc2'}]), ('edb92d22', [{'text': 'That’s normal. We all have different environments: our computers have different versions of OS and different versions of libraries — even different versions of Python.\\nIf it’s the case, just select the option that’s closest to your answer', 'section': '2. Machine Learning for Regression', 'question': \"The answer I get for one of the homework questions doesn't match any of the options. What should I do?\", 'course': 'machine-learning-zoomcamp', 'id': 'edb92d22'}]), ('f488ce85', [{'text': \"In question 3 of HW02 it is mentioned: ‘For computing the mean, use the training only’. What does that mean?\\nIt means that you should use only the training data set for computing the mean, not validation or  test data set. This is how you can calculate the mean\\ndf_train['column_name'].mean( )\\nAnother option:\\ndf_train[‘column_name’].describe()\\n(Bhaskar Sarma)\", 'section': '2. Machine Learning for Regression', 'question': 'Meaning of mean in homework 2, question 3', 'course': 'machine-learning-zoomcamp', 'id': 'f488ce85'}]), ('bf395099', [{'text': 'When the target variable has a long tail distribution, like in prices, with a wide range, you can transform the target variable with np.log1p() method, but be aware if your target variable has negative values, this method will not work', 'section': '2. Machine Learning for Regression', 'question': 'When should we transform the target variable to logarithm distribution?', 'course': 'machine-learning-zoomcamp', 'id': 'bf395099'}]), ('01cd3b35', [{'text': 'If we try to perform an arithmetic operation between 2 arrays of different shapes or different dimensions, it throws an error like operands could not be broadcast together with shapes. There are some scenarios when broadcasting can occur and when it fails.\\nIf this happens sometimes we can use * operator instead of dot() method to solve the issue. So that the error is solved and also we get the dot product.\\n(Santhosh Kumar)', 'section': '2. Machine Learning for Regression', 'question': 'ValueError: shapes not aligned', 'course': 'machine-learning-zoomcamp', 'id': '01cd3b35'}]), ('5551c92e', [{'text': 'Copy of a dataframe is made with X_copy = X.copy().\\nThis is called creating a deep copy.  Otherwise it will keep changing the original dataframe if used like this: X_copy = X.\\nAny changes to X_copy will reflect back to X. This is not a real copy, instead it is a “view”.\\n(Memoona Tahira)', 'section': '2. Machine Learning for Regression', 'question': 'How to copy a dataframe without changing the original dataframe?', 'course': 'machine-learning-zoomcamp', 'id': '5551c92e'}]), ('94f928d2', [{'text': 'One of the most important characteristics of the normal distribution is that mean=median=mode, this means that the most popular value, the mean of the distribution and 50% of the sample are under the same value, this is equivalent to say that the area under the curve (black) is the same on the left and on the right. The long tail (red curve) is the result of having a few observations with high values, now the behaviour of the distribution changes, first of all, the area is different on each side and now the mean, median and mode are different. As a consequence, the mean is no longer representative, the range is larger than before and the probability of being on the left or on the right is not the same.\\n(Tatiana Dávila)', 'section': '2. Machine Learning for Regression', 'question': 'What does ‘long tail’ mean?', 'course': 'machine-learning-zoomcamp', 'id': '94f928d2'}]), ('266faa6d', [{'text': 'In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. [Wikipedia] The formula to calculate standard deviation is:\\n(Aadarsha Shrestha)', 'section': '2. Machine Learning for Regression', 'question': 'What is standard deviation?', 'course': 'machine-learning-zoomcamp', 'id': '266faa6d'}]), ('c21f99f5', [{'text': 'The application of regularization depends on the specific situation and problem. It is recommended to consider it when training machine learning models, especially with small datasets or complex models, to prevent overfitting. However, its necessity varies depending on the data quality and size. Evaluate each case individually to determine if it is needed.\\n(Daniel Muñoz Viveros)', 'section': '2. Machine Learning for Regression', 'question': 'Do we need to apply regularization techniques always? Or only in certain scenarios?', 'course': 'machine-learning-zoomcamp', 'id': 'c21f99f5'}]), ('13702957', [{'text': 'As it speeds up the development:\\nprepare_df(initial_df, seed, fill_na_type)  - that prepared all 3 dataframes and 3 y_vectors. Fillna() can be done before the initial_df is split.\\nOf course, you can reuse other functions: rmse() and train_linear_regression(X,y,r) from the class notebook\\n(Ivan Brigida)', 'section': '2. Machine Learning for Regression', 'question': 'Shortcut: define functions for faster execution', 'course': 'machine-learning-zoomcamp', 'id': '13702957'}]), ('7cd652c5', [{'text': 'If we have a list or series of data for example x = [1,2,3,4,5]. We can use pandas to find the standard deviation. We can pass our list into panda series and call standard deviation directly on the series pandas.Series(x).std().\\n(Quinn Avila)', 'section': '2. Machine Learning for Regression', 'question': 'How to use pandas to find standard deviation', 'course': 'machine-learning-zoomcamp', 'id': '7cd652c5'}]), ('e1f93d10', [{'text': 'Numpy and Pandas packages use different equations to compute the standard deviation. Numpy uses  population standard deviation, whereas pandas uses sample standard deviation by default.\\nNumpy\\nPandas\\npandas default standard deviation is computed using one degree of freedom. You can change degree in of freedom in NumPy to change this to unbiased estimator by using ddof parameter:\\nimport numpy as np\\nnp.std(df.weight, ddof=1)\\nThe result will be similar if we change the dof = 1 in numpy\\n(Harish Balasundaram)', 'section': '2. Machine Learning for Regression', 'question': 'Standard Deviation Differences in Numpy and Pandas', 'course': 'machine-learning-zoomcamp', 'id': 'e1f93d10'}]), ('36b9d1b7', [{'text': \"In pandas you can use built in Pandas function names std() to get standard deviation. For example\\ndf['column_name'].std() to get standard deviation of that column.\\ndf[['column_1', 'column_2']].std() to get standard deviation of multiple columns.\\n(Khurram Majeed)\", 'section': '2. Machine Learning for Regression', 'question': 'Standard deviation using Pandas built in Function', 'course': 'machine-learning-zoomcamp', 'id': '36b9d1b7'}]), ('3c8b32a1', [{'text': 'Use ‘pandas.concat’ function (https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to combine two dataframes. To combine two numpy arrays use numpy.concatenate (https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) function. So the code would be as follows:\\ndf_train_combined = pd.concat([df_train, df_val])\\ny_train = np.concatenate((y_train, y_val), axis=0)\\n(George Chizhmak)', 'section': '2. Machine Learning for Regression', 'question': 'How to combine train and validation datasets', 'course': 'machine-learning-zoomcamp', 'id': '3c8b32a1'}]), ('05fb3a16', [{'text': 'The Root Mean Squared Error (RMSE) is one of the primary metrics to evaluate the performance of a regression model. It calculates the average deviation between the model\\'s predicted values and the actual observed values, offering insight into the model\\'s ability to accurately forecast the target variable. To calculate RMSE score:\\nLibraries needed\\nimport numpy as np\\nfrom sklearn.metrics import mean_squared_error\\nmse = mean_squared_error(actual_values, predicted_values)\\nrmse = np.sqrt(mse)\\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\\n(Aminat Abolade)', 'section': '2. Machine Learning for Regression', 'question': 'Understanding RMSE and how to calculate RMSE score', 'course': 'machine-learning-zoomcamp', 'id': '05fb3a16'}]), ('225506b9', [{'text': 'If you would like to use multiple conditions as an example below you will get the error. The correct syntax for OR is |, and for AND is &\\n(Olga Rudakova)\\n–', 'section': '2. Machine Learning for Regression', 'question': 'What syntax use in Pandas for multiple conditions using logical AND and OR', 'course': 'machine-learning-zoomcamp', 'id': '225506b9'}]), ('bd4a1395', [{'text': 'I found this video pretty usual for understanding how we got the normal form with linear regression Normal Equation Derivation for Regression', 'section': '2. Machine Learning for Regression', 'question': 'Deep dive into normal equation for regression', 'course': 'machine-learning-zoomcamp', 'id': 'bd4a1395'}]), ('81b8e8d0', [{'text': '(Hrithik Kumar Advani)', 'section': '2. Machine Learning for Regression', 'question': 'Useful Resource for Missing Data Treatment\\nhttps://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python/notebook', 'course': 'machine-learning-zoomcamp', 'id': '81b8e8d0'}]), ('a7f6a33c', [{'text': 'The instruction for applying log transformation to the ‘median_house_value’ variable is provided before Q3 in the homework for Week-2 under the ‘Prepare and split the dataset’ heading.\\nHowever, this instruction is absent in the subsequent questions of the homework, and I got stuck with Q5 for a long time, trying to figure out why my RMSE was so huge, when it clicked to me that I forgot to apply log transformation to the target variable. Please remember to apply log transformation to the target variable for each question.\\n(Added by Soham Mundhada)', 'section': '2. Machine Learning for Regression', 'question': 'Caution for applying log transformation in Week-2 2023 cohort homework', 'course': 'machine-learning-zoomcamp', 'id': 'a7f6a33c'}]), ('129b4ac0', [{'text': 'Version 0.24.2 and Python 3.8.11\\n(Added by Diego Giraldo)', 'section': '3. Machine Learning for Classification', 'question': 'What sklearn version is Alexey using in the youtube videos?', 'course': 'machine-learning-zoomcamp', 'id': '129b4ac0'}]), ('b8cca8b7', [{'text': 'Week 3 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/03-classification/homework.md\\nSubmit HW Week 3: https://docs.google.com/forms/d/e/1FAIpQLSeXS3pqsv_smRkYmVx-7g6KIZDnG29g2s7pdHo-ASKNqtfRFQ/viewform\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 3.X --- https://www.youtube.com/watch?v=0Zw04wdeTQo&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=29\\n~~Nukta Bhatia~~', 'section': '3. Machine Learning for Classification', 'question': 'How do I get started with Week 3?', 'course': 'machine-learning-zoomcamp', 'id': 'b8cca8b7'}]), ('1091b10f', [{'text': \"The error message “could not convert string to float: ‘Nissan’” typically occurs when a machine learning model or function is expecting numerical input, but receives a string instead. In this case, it seems like the model is trying to convert the car brand ‘Nissan’ into a numerical value, which isn’t possible.\\nTo resolve this issue, you can encode categorical variables like car brands into numerical values. One common method is one-hot encoding, which creates new binary columns for each category/label present in the original column.\\nHere’s an example of how you can perform one-hot encoding using pandas:\\nimport pandas as pd\\n# Assuming 'data' is your DataFrame and 'brand' is the column with car brands\\ndata_encoded = pd.get_dummies(data, columns=['brand'])\\nIn this code, pd.get_dummies() creates a new DataFrame where the ‘brand’ column is replaced with binary columns for each brand (e.g., ‘brand_Nissan’, ‘brand_Toyota’, etc.). Each row in the DataFrame has a 1 in the column that corresponds to its brand and 0 in all other brand columns.\\n-Mohammad Emad Sharifi-\", 'section': '3. Machine Learning for Classification', 'question': \"Could not convert string to float:’Nissan’rt string to float: 'Nissan'\", 'course': 'machine-learning-zoomcamp', 'id': '1091b10f'}]), ('0c7715a1', [{'text': 'Solution: Mutual Information score calculates the relationship between categorical variables or discrete variables. So in the homework, because the target which is median_house_value is continuous, we had to change it to binary format which in other words, makes its values discrete as either 0 or 1. If we allowed it to remain in the continuous variable format, the mutual information score could be calculated, but the algorithm would have to divide the continuous variables into bins and that would be highly subjective. That is why continuous variables are not used for mutual information score calculation.\\n—Odimegwu David—-', 'section': '3. Machine Learning for Classification', 'question': 'Why did we change the targets to binary format when calculating mutual information score in the homework?', 'course': 'machine-learning-zoomcamp', 'id': '0c7715a1'}]), ('d2043cf5', [{'text': \"Q2 asks about correlation matrix and converting median_house_value from numeric to binary. Just to make sure here we are only dealing with df_train not df_train_full, right? As the question explicitly mentions the train dataset.\\nYes. I think it is only on df_train. The reason behind this is that df_train_full also contains the validation dataset, so at this stage we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data.\\nPastor Soto\", 'section': '3. Machine Learning for Classification', 'question': 'What data should we use for correlation matrix', 'course': 'machine-learning-zoomcamp', 'id': 'd2043cf5'}]), ('44d22817', [{'text': \"The background of any dataframe can be colored (not only the correlation matrix) based on the numerical values the dataframe contains by using the method pandas.io.formats.style.Styler.background_graident.\\nHere an example on how to color the correlation matrix. A color map of choice can get passed, here ‘viridis’ is used.\\n# ensure to have only numerical values in the dataframe before calling 'corr'\\ncorr_mat = df_numerical_only.corr()\\ncorr_mat.style.background_gradient(cmap='viridis')\\nHere is an example of how the coloring will look like using a dataframe containing random values and applying “background_gradient” to it.\\nnp.random.seed = 3\\ndf_random = pd.DataFrame(data=np.random.random(3*3).reshape(3,3))\\ndf_random.style.background_gradient(cmap='viridis')\\nAdded by Sylvia Schmitt\", 'section': '3. Machine Learning for Classification', 'question': 'Coloring the background of the pandas.DataFrame.corr correlation matrix directly', 'course': 'machine-learning-zoomcamp', 'id': '44d22817'}]), ('1f76dbeb', [{'text': 'data_corr = pd.DataFrame(data_num.corr().round(3).abs().unstack().sort_values(ascending=False))\\ndata_corr.head(10)\\nAdded by Harish Balasundaram\\nYou can also use seaborn to create a heatmap with the correlation. The code for doing that:\\nsns.heatmap(df[numerical_features].corr(),\\nannot=True,\\nsquare=True,\\nfmt=\".2g\",\\ncmap=\"crest\")\\nAdded by Cecile Guillot\\nYou can refine your heatmap and plot only a triangle, with a blue to red color gradient, that will show every correlation between your numerical variables without redundant information with this function:\\nWhich outputs, in the case of churn dataset:\\n(Mélanie Fouesnard)', 'section': '3. Machine Learning for Classification', 'question': 'Identifying highly correlated feature pairs easily through unstack', 'course': 'machine-learning-zoomcamp', 'id': '1f76dbeb'}]), ('b8071a54', [{'text': \"Should we perform EDA on the base of train or train+validation or train+validation+test dataset?\\nIt's indeed good practice to only rely on the train dataset for EDA. Including validation might be okay. But we aren't supposed to touch the test dataset, even just looking at it isn't a good idea. We indeed pretend that this is the future unseen data\\nAlena Kniazeva\", 'section': '3. Machine Learning for Classification', 'question': 'What data should be used for EDA?', 'course': 'machine-learning-zoomcamp', 'id': 'b8071a54'}]), ('b8da9037', [{'text': 'Validation dataset helps to validate models and prediction on unseen data. This helps get an estimate on its performance on fresh data. It helps optimize the model.\\nEdidiong Esu\\nBelow is an extract of Alexey\\'s book explaining this point. Hope is useful\\nWhen we apply the fit method, this method is looking at the content of the df_train dictionaries we are passing to the DictVectorizer instance, and fit is figuring out (training) how to map the values of these dictionaries. If categorical, applies one-hot encoding, if numerical it will leave it as it is.\\nWith this context, if we apply the fit to the validation model, we are \"giving the answers\" and we are not letting the \"fit\" do its job for data that we haven\\'t seen. By not applying the fit to the validation model we can know how well it was trained.\\nBelow is an extract of Alexey\\'s book explaining this point.\\nHumberto Rodriguez\\nThere is no need to initialize another instance of dictvectorizer after fitting it on the train set as it will overwrite what it learnt from being fit on the train data.\\nThe correct way is to fit_transform the train set, and only transform the validation and test sets.\\nMemoona Tahira', 'section': '3. Machine Learning for Classification', 'question': 'Fitting DictVectorizer on validation', 'course': 'machine-learning-zoomcamp', 'id': 'b8da9037'}]), ('467e0cec', [{'text': 'For Q5 in homework, should we calculate the smallest difference in accuracy in real values (i.e. -0.001 is less than -0.0002) or in absolute values (i.e. 0.0002 is less than 0.001)?\\nWe should select the “smallest” difference, and not the “lowest”, meaning we should reason in absolute values.\\nIf the difference is negative, it means that the model actually became better when we removed the feature.', 'section': '3. Machine Learning for Classification', 'question': 'Feature elimination', 'course': 'machine-learning-zoomcamp', 'id': '467e0cec'}]), ('b69f32f6', [{'text': \"Instead use the method “.get_feature_names_out()” from DictVectorizer function and the warning will be resolved , but we need not worry about the waning as there won't be any warning\\nSanthosh Kumar\", 'section': '3. Machine Learning for Classification', 'question': 'FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2', 'course': 'machine-learning-zoomcamp', 'id': 'b69f32f6'}]), ('3b3b1989', [{'text': 'Fitting the logistic regression takes a long time / kernel crashes when calling predict() with the fitted model.\\nMake sure that the target variable for the logistic regression is binary.\\nKonrad Muehlberg', 'section': '3. Machine Learning for Classification', 'question': 'Logistic regression crashing Jupyter kernel', 'course': 'machine-learning-zoomcamp', 'id': '3b3b1989'}]), ('eb5771a0', [{'text': 'Ridge regression is a linear regression technique used to mitigate the problem of multicollinearity (when independent variables are highly correlated) and prevent overfitting in predictive modeling. It adds a regularization term to the linear regression cost function, penalizing large coefficients.\\nsag Solver: The sag solver stands for \"Stochastic Average Gradient.\" It\\'s particularly suitable for large datasets, as it optimizes the regularization term using stochastic gradient descent (SGD). sag can be faster than some other solvers for large datasets.\\nAlpha: The alpha parameter  controls the strength of the regularization in Ridge regression. A higher alpha value leads to stronger regularization, which means the model will have smaller coefficient values, reducing the risk of overfitting.\\nfrom sklearn.linear_model import Ridge\\nridge = Ridge(alpha=alpha, solver=\\'sag\\', random_state=42)\\nridge.fit(X_train, y_train)\\nAminat Abolade', 'section': '3. Machine Learning for Classification', 'question': 'Understanding Ridge', 'course': 'machine-learning-zoomcamp', 'id': 'eb5771a0'}]), ('bca10281', [{'text': 'DictVectorizer(sparse=True) produces CSR format, which is both more memory efficient and converges better during fit(). Basically it stores non-zero values and indices instead of adding a column for each class of each feature (models of cars produced 900+ columns alone in the current task).\\nUsing “sparse” format like on the picture above, both via pandas.get_dummies() and DictVectorizer(sparse=False) - is slower (around 6-8min for Q6 task - Linear/Ridge Regression) for high amount of classes (like models of cars for eg) and gives a bit “worse” results in both Logistic and Linear/Ridge Regression, while also producing convergence warnings for Linear/Ridge Regression.\\nLarkin Andrii', 'section': '3. Machine Learning for Classification', 'question': 'pandas.get_dummies() and DictVectorizer(sparse=False) produce the same type of one-hot encodings:', 'course': 'machine-learning-zoomcamp', 'id': 'bca10281'}]), ('34a8edb0', [{'text': 'Ridge with sag solver requires feature to be of the same scale. You may get the following warning: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\\nPlay with different scalers. See notebook-scaling-ohe.ipynb\\nDmytro Durach\\n(Oscar Garcia)  Use a StandardScaler for the numeric fields and OneHotEncoder (sparce = False) for the categorical features.  This help with the warning. Separate the features (num/cat) without using the encoder first and see if that helps.', 'section': '3. Machine Learning for Classification', 'question': 'Convergence Problems in W3Q6', 'course': 'machine-learning-zoomcamp', 'id': '34a8edb0'}]), ('f625307b', [{'text': \"When encountering convergence errors during the training of a Ridge regression model, consider the following steps:\\nFeature Normalization: Normalize your numerical features using techniques like MinMaxScaler or StandardScaler. This ensures that numerical features are on a \\tsimilar scale, preventing convergence issues.\\nCategorical Feature Encoding: If your dataset includes categorical features, apply \\tcategorical encoding techniques such as OneHotEncoder (OHE) to \\tconvert them into a numerical format. OHE is commonly used to represent categorical variables as binary vectors, making them compatible with regression models like Ridge.\\nCombine Features: After \\tnormalizing numerical features and encoding categorical features using OneHotEncoder, combine them to form a single feature matrix (X_train). This combined dataset serves as the input for training the Ridge regression model.\\nBy following these steps, you can address convergence errors and enhance the stability of your Ridge model training process. It's important to note that the choice of encoding method, such as OneHotEncoder, is appropriate for handling categorical features in this context.\\nYou can find an example here.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tOsman Ali\", 'section': '3. Machine Learning for Classification', 'question': 'Dealing with Convergence in Week 3 q6', 'course': 'machine-learning-zoomcamp', 'id': 'f625307b'}]), ('7fa98526', [{'text': 'A sparse matrix is more memory-efficient because it only stores the non-zero values and their positions in memory. This is particularly useful when working with large datasets with many zero or missing values.\\nThe default DictVectorizer configuration is a sparse matrix. For week3 Q6 using the default sparse is an interesting option because of the size of the matrix. Training the model was also more performant and didn’t give an error message like dense mode.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tQuinn Avila', 'section': '3. Machine Learning for Classification', 'question': 'Sparse matrix compared dense matrix', 'course': 'machine-learning-zoomcamp', 'id': '7fa98526'}]), ('0807f0f3', [{'text': 'The warnings on the jupyter notebooks can be disabled/ avoided with the following comments:\\nImport warnings\\nwarnings.filterwarnings(“ignore”)\\nKrishna Anand', 'section': '3. Machine Learning for Classification', 'question': 'How  to Disable/avoid Warnings in Jupyter Notebooks', 'course': 'machine-learning-zoomcamp', 'id': '0807f0f3'}]), ('6d0fb418', [{'text': 'Question: Regarding RMSE, how do we decide on the correct score to choose? In the study group discussion    about week two homework, all of us got it wrong and one person had the lowest score selected as well.\\nAnswer: You need to find RMSE for each alpha. If RMSE scores  are equal, you will select the lowest alpha.\\nAsia Saeed', 'section': '3. Machine Learning for Classification', 'question': 'How to select the alpha parameter in Q6', 'course': 'machine-learning-zoomcamp', 'id': '6d0fb418'}]), ('fbda1f40', [{'text': 'Question: Could you please help me with HW3 Q3: \"Calculate the mutual information score with the (binarized) price for the categorical variable that we have. Use the training set only.\" What is the second variable that we need to use to calculate the mutual information score?\\nAnswer: You need to calculate the mutual info score between the binarized price (above_average) variable & ocean_proximity, the only original categorical variable in the dataset.\\nAsia Saeed', 'section': '3. Machine Learning for Classification', 'question': 'Second variable that we need to use to calculate the mutual information score', 'course': 'machine-learning-zoomcamp', 'id': 'fbda1f40'}]), ('0f88b7ac', [{'text': 'Do we need to train the model only with the features: total_rooms, total_bedrooms, population and households? or with all the available features and then pop once at a time each of the previous features and train the model to make the accuracy comparison?\\nYou need to create a list of all features in this question and evaluate the model one time to obtain the accuracy, this will be the original accuracy, and then remove one feature each time, and in each time, train the model, find the accuracy and the difference between the original accuracy and the found accuracy. Finally, find out which feature has the smallest absolute accuracy difference.\\nWhile calculating differences between accuracy scores while training on the whole model, versus dropping one feature at a time and comparing its accuracy to the model to judge impact of the feature on the accuracy of the model, do we take the smallest difference or smallest absolute difference?\\nSince order of subtraction between the two accuracy scores can result in a negative number, we will take its absolute value as we are interested in the smallest value difference, not the lowest difference value. Case in point, if difference is -4 and -2, the smallest difference is abs(-2), and not abs(-4)', 'section': '3. Machine Learning for Classification', 'question': 'Features for homework Q5', 'course': 'machine-learning-zoomcamp', 'id': '0f88b7ac'}]), ('9ffcc895', [{'text': 'Both work in similar ways. That is, to convert categorical features to numerical variables for use in training the model. But the difference lies in the input. OneHotEncoder uses an array as input while DictVectorizer uses a dictionary.\\nBoth will produce the same result. But when we use OneHotEncoder, features are sorted alphabetically. When you use DictVectorizer you stack features that you want.\\nTanya Mard', 'section': '3. Machine Learning for Classification', 'question': 'What is the difference between OneHotEncoder and DictVectorizer?', 'course': 'machine-learning-zoomcamp', 'id': '9ffcc895'}]), ('94a3b2fb', [{'text': 'They are basically the same. There are some key differences with regards to their input/output types, handling of missing values, etc, but they are both techniques to one-hot-encode categorical variables with identical results. The biggest difference is get_dummies are a convenient choice when you are working with Pandas Dataframes, while if you are building a scikit-learn-based machine learning pipeline and need to handle categorical data as part of that pipeline, OneHotEncoder is a more suitable choice. [Abhirup Ghosh]', 'section': '3. Machine Learning for Classification', 'question': 'What is the difference between pandas get_dummies and sklearn OnehotEncoder?', 'course': 'machine-learning-zoomcamp', 'id': '94a3b2fb'}]), ('fb9a45d8', [{'text': \"For the test_train_split question on week 3's homework, are we supposed to use 42 as the random_state in both splits or only the 1st one?\\nAnswer: for both splits random_state = 42 should be used\\n(Bhaskar Sarma)\", 'section': '3. Machine Learning for Classification', 'question': 'Use of random seed in HW3', 'course': 'machine-learning-zoomcamp', 'id': 'fb9a45d8'}]), ('e31051f7', [{'text': 'Should correlation be calculated after splitting or before splitting. And lastly I know how to find the correlation but how do i find the two most correlated features.\\nAnswer: Correlation matrix of your train dataset. Thus, after splitting. Two most correlated features are the ones having the highest correlation coefficient in terms of absolute values.', 'section': '3. Machine Learning for Classification', 'question': 'Correlation before or after splitting the data', 'course': 'machine-learning-zoomcamp', 'id': 'e31051f7'}]), ('493b7b59', [{'text': 'Make sure that the features used in ridge regression model are only NUMERICAL ones not categorical.\\nDrop all categorical features first before proceeding.\\n(Aileah Gotladera)\\nWhile it is True that ridge regression accepts only numerical values, the categorical ones can be useful for your model. You have to transform them using one-hot encoding before training the model. To avoid the error of non convergence, put sparse=True when doing so.\\n(Erjon)', 'section': '3. Machine Learning for Classification', 'question': 'Features in Ridge Regression Model', 'course': 'machine-learning-zoomcamp', 'id': '493b7b59'}]), ('4a55c510', [{'text': \"You need to use all features. and price for target. Don't include the average variable we created before.\\nIf you use DictVectorizer then make sure to use sparce=True to avoid convergence errors\\nI also used StandardScalar for numerical variable you can try running with or without this\\n(Peter Pan)\", 'section': '3. Machine Learning for Classification', 'question': 'Handling Column Information for Homework 3 Question 6', 'course': 'machine-learning-zoomcamp', 'id': '4a55c510'}]), ('3ca0b489', [{'text': 'Use sklearn.preprocessing encoders and scalers, e.g. OneHotEncoder, OrdinalEncoder, and StandardScaler.', 'section': '3. Machine Learning for Classification', 'question': 'Transforming Non-Numerical Columns into Numerical Columns', 'course': 'machine-learning-zoomcamp', 'id': '3ca0b489'}]), ('690d97f1', [{'text': 'These both methods receive the dictionary as an input. While the DictVectorizer will store the big vocabulary and takes more memory. FeatureHasher create a vectors with predefined length. They are both used for categorical features.\\nWhen you have a high cardinality for categorical features better to use FeatureHasher. If you want to preserve feature names in transformed data and have a small number of unique values is DictVectorizer. But your choice will dependence on your data.\\nYou can read more by follow the link https://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html\\nOlga Rudakova', 'section': '3. Machine Learning for Classification', 'question': 'What is the better option FeatureHasher or DictVectorizer', 'course': 'machine-learning-zoomcamp', 'id': '690d97f1'}]), ('eb5a25cb', [{'text': '(Question by Connie S.)\\nThe reason it\\'s good/recommended practice to do it after splitting is to avoid data leakage - you don\\'t want any data from the test set influencing the training stage (similarly from the validation stage in the initial training). See e.g. scikit-learn documentation on \"Common pitfalls and recommended practices\": https://scikit-learn.org/stable/common_pitfalls.html\\nAnswered/added by Rileen Sinha', 'section': '3. Machine Learning for Classification', 'question': \"Isn't it easier to use DictVertorizer or get dummies before splitting the data into train/val/test? Is there a reason we wouldn't do this? Or is it the same either way?\", 'course': 'machine-learning-zoomcamp', 'id': 'eb5a25cb'}]), ('6d9e0a6f', [{'text': 'If you are getting 1.0 as accuracy then there is a possibility you have overfitted the model. Dropping the column msrp/price can help you solve this issue.\\nAdded by Akshar Goyal', 'section': '3. Machine Learning for Classification', 'question': 'HW3Q4 I am getting 1.0 as accuracy. Should I use the closest option?', 'course': 'machine-learning-zoomcamp', 'id': '6d9e0a6f'}]), ('618ad97a', [{'text': 'We can use sklearn & numpy packages to calculate Root Mean Squared Error\\nfrom sklearn.metrics import mean_squared_error\\nimport numpy as np\\nRmse = np.sqrt(mean_squared_error(y_pred, y_val/ytest)\\nAdded by Radikal Lukafiardi\\nYou can also refer to Alexey’s notebook for Week 2:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb\\nwhich includes the following code:\\ndef rmse(y, y_pred):\\nerror = y_pred - y\\nmse = (error ** 2).mean()\\nreturn np.sqrt(mse)\\n(added by Rileen Sinha)', 'section': '3. Machine Learning for Classification', 'question': 'How to calculate Root Mean Squared Error?', 'course': 'machine-learning-zoomcamp', 'id': '618ad97a'}]), ('683495d2', [{'text': 'The solution is to use “get_feature_names_out” instead. See details: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\\nGeorge Chizhmak', 'section': '3. Machine Learning for Classification', 'question': \"AttributeError: 'DictVectorizer' object has no attribute 'get_feature_names'\", 'course': 'machine-learning-zoomcamp', 'id': '683495d2'}]), ('dc1897b5', [{'text': 'To use RMSE without math or numpy, ‘sklearn.metrics’ has a mean_squared_error function with a squared kwarg (defaults to True). Setting squared to False will return the RMSE.\\nfrom sklearn.metrics import mean_squared_error\\nrms = mean_squared_error(y_actual, y_predicted, squared=False)\\nSee details: https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python\\nAhmed Okka', 'section': '3. Machine Learning for Classification', 'question': 'Root Mean Squared Error', 'course': 'machine-learning-zoomcamp', 'id': 'dc1897b5'}]), ('826098f2', [{'text': 'This article explains different encoding techniques used https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02\\nHrithik Kumar Advani', 'section': '3. Machine Learning for Classification', 'question': 'Encoding Techniques', 'course': 'machine-learning-zoomcamp', 'id': '826098f2'}]), ('821dfc08', [{'text': \"I got this error multiple times here is the code:\\n“accuracy_score(y_val, y_pred >= 0.5)”\\nTypeError: 'numpy.float64' object is not callable\\nI solve it using\\nfrom sklearn import metrics\\nmetrics.accuracy_score(y_train, y_pred>= 0.5)\\nOMAR Wael\", 'section': '4. Evaluation Metrics for Classification', 'question': 'Error in use of accuracy_score from sklearn in jupyter (sometimes)', 'course': 'machine-learning-zoomcamp', 'id': '821dfc08'}]), ('27c8d5da', [{'text': 'Week 4 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/04-evaluation/homework.md\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYouTube Link: 4.X --- https://www.youtube.com/watch?v=gmg5jw1bM8A&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=40\\nSci-Kit Learn on Evaluation:\\nhttps://scikit-learn.org/stable/model_selection.html\\n~~Nukta Bhatia~~', 'section': '4. Evaluation Metrics for Classification', 'question': 'How do I get started with Week 4?', 'course': 'machine-learning-zoomcamp', 'id': '27c8d5da'}]), ('a52d4739', [{'text': 'https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696475675887119\\nMetrics can be used on a series or a dataframe\\n~~Ella Sahnan~~', 'section': '4. Evaluation Metrics for Classification', 'question': 'Using a variable to score', 'course': 'machine-learning-zoomcamp', 'id': 'a52d4739'}]), ('dc55359c', [{'text': 'Ie particularly in module-04 homework Qn2 vs Qn5. https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696760905214979\\nRefer to the sklearn docs, random_state is to ensure the “randomness” that is used to shuffle dataset is reproducible, and it usually requires both random_state and shuffle params to be set accordingly.\\n~~Ella Sahnan~~', 'section': '4. Evaluation Metrics for Classification', 'question': 'Why do we sometimes use random_state and not at other times?', 'course': 'machine-learning-zoomcamp', 'id': 'dc55359c'}]), ('2ab49e43', [{'text': 'How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\\nUse classification_report from sklearn. For more info check here.\\nAbhishek N', 'section': '4. Evaluation Metrics for Classification', 'question': 'How to get all classification metrics?', 'course': 'machine-learning-zoomcamp', 'id': '2ab49e43'}]), ('b431e7eb', [{'text': 'I am getting multiple thresholds with the same F1 score, does this indicate I am doing something wrong or is there a method for choosing? I would assume just pick the lowest?\\nChoose the one closest to any of the options\\nAdded by Azeez Enitan Edunwale\\nYou can always use scikit-learn (or other standard libraries/packages) to verify results obtained using your own code, e.g. you can use  “classification_report” (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to obtain precision, recall and F1-score.\\nAdded by Rileen Sinha', 'section': '4. Evaluation Metrics for Classification', 'question': 'Multiple thresholds for Q4', 'course': 'machine-learning-zoomcamp', 'id': 'b431e7eb'}]), ('c5fdeba9', [{'text': \"Solution description: duplicating the\\ndf.churn = (df.churn == 'yes').astype(int)\\nThis is causing you to have only 0's in your churn column. In fact, match with the error you are getting:  ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.\\nIt is telling us that it only contains 0's.\\nDelete one of the below cells and you will get the accuracy\\nHumberto Rodriguez\", 'section': '4. Evaluation Metrics for Classification', 'question': 'ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0', 'course': 'machine-learning-zoomcamp', 'id': 'c5fdeba9'}]), ('b8c9eaf1', [{'text': 'Use Yellowbrick. Yellowbrick in a library that combines scikit-learn with matplotlib to produce visualizations for your models. It produces colorful classification reports.\\nKrishna Annad', 'section': '4. Evaluation Metrics for Classification', 'question': 'Method to get beautiful classification report', 'course': 'machine-learning-zoomcamp', 'id': 'b8c9eaf1'}]), ('c54058a1', [{'text': 'That’s fine, use the closest option', 'section': '4. Evaluation Metrics for Classification', 'question': 'I’m not getting the exact result in homework', 'course': 'machine-learning-zoomcamp', 'id': 'c54058a1'}]), ('b4b85c4b', [{'text': 'Check the solutions from the 2021 iteration of the course. You should use roc_auc_score.', 'section': '4. Evaluation Metrics for Classification', 'question': 'Use AUC to evaluate feature importance of numerical variables', 'course': 'machine-learning-zoomcamp', 'id': 'b4b85c4b'}]), ('7d40f6f6', [{'text': 'When calculating the ROC AUC score using sklearn.metrics.roc_auc_score the function expects two parameters “y_true” and “y_score”. So for each numerical value in the dataframe it will be passed as the “y_score” to the function and the target variable will get passed a “y_true” each time.\\nSylvia Schmitt', 'section': '4. Evaluation Metrics for Classification', 'question': 'Help with understanding: “For each numerical value, use it as score and compute AUC”', 'course': 'machine-learning-zoomcamp', 'id': '7d40f6f6'}]), ('f5dc446c', [{'text': 'You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards, as you did in Question 2.\\nDiego Giraldo', 'section': '4. Evaluation Metrics for Classification', 'question': 'What dataset should I use to compute the metrics in Question 3', 'course': 'machine-learning-zoomcamp', 'id': 'f5dc446c'}]), ('d30fc29d', [{'text': \"What does this line do?\\nKFold(n_splits=n_splits, shuffle=True, random_state=1)\\nIf I do it inside the loop [0.01, 0.1, 1, 10] or outside the loop in Q6, HW04 it doesn't make any difference to my answers. I am wondering why and what is the right way, although it doesn't make a difference!\\nDid you try using a different random_state? From my understanding, KFold just makes N (which is equal to n_splits) separate pairs of datasets (train+val).\\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\\nIn my case changing random state changed results\\n(Arthur Minakhmetov)\\nChanging the random state makes a difference in my case too, but not whether it is inside or outside the for loop. I think I have got the answer. kFold = KFold(n_splits=n_splits, shuffle = True, random_state = 1)  is just a generator object and it contains only the information n_splits, shuffle and random_state. The k-fold splitting actually happens in the next for loop for train_idx, val_idx in kFold.split(df_full_train): . So it doesn't matter where we generate the object, before or after the first loop. It will generate the same information. But from the programming point of view, it is better to do it before the loop. No point doing it again and again inside the loop\\n(Bhaskar Sarma)\\nIn case of KFold(n_splits=n_splits, shuffle=True, random_state=1) and C= [0.01, 0.1, 1, 10], it is better to loop through the different values of Cs as the video explained. I had separate train() and predict() functions, which were reused after dividing the dataset via KFold. The model ran about 10 minutes and provided a good score.\\n(Ani Mkrtumyan)\", 'section': '4. Evaluation Metrics for Classification', 'question': 'What does KFold do?', 'course': 'machine-learning-zoomcamp', 'id': 'd30fc29d'}]), ('8eca9f73', [{'text': \"I’m getting “ValueError: multi_class must be in ('ovo', 'ovr')” when using roc_auc_score to evaluate feature importance of numerical variables in question 1.\\nI was getting this error because I was passing the parameters to roc_auc_score incorrectly (df_train[col] , y_train) . The correct way is to pass the parameters in this way: roc_auc_score(y_train, df_train[col])\\nAsia Saeed\", 'section': '4. Evaluation Metrics for Classification', 'question': \"ValueError: multi_class must be in ('ovo', 'ovr')\", 'course': 'machine-learning-zoomcamp', 'id': '8eca9f73'}]), ('7b9eb7f7', [{'text': 'from tqdm.auto import tqdm\\nTqdm - terminal progress bar\\nKrishna Anand', 'section': '4. Evaluation Metrics for Classification', 'question': 'Monitoring Wait times and progress of the code execution can be done with:', 'course': 'machine-learning-zoomcamp', 'id': '7b9eb7f7'}]), ('c4aaeed9', [{'text': 'Inverting or negating variables with ROC AUC scores less than the threshold is a valuable technique to improve feature importance and model performance when dealing with negatively correlated features. It helps ensure that the direction of the correlation aligns with the expectations of most machine learning algorithms.\\nAileah Gotladera', 'section': '4. Evaluation Metrics for Classification', 'question': 'What is the use of inverting or negating the variables less than the threshold?', 'course': 'machine-learning-zoomcamp', 'id': 'c4aaeed9'}]), ('3af31e2a', [{'text': 'In case of using predict(X) for this task we are getting the binary classification predictions which are 0 and 1. This may lead to incorrect evaluation values.\\nThe solution is to use predict_proba(X)[:,1], where we get the probability that the value belongs to one of the classes.\\nVladimir Yesipov\\nPredict_proba shows probailites per class.\\nAni Mkrtumyan', 'section': '4. Evaluation Metrics for Classification', 'question': 'Difference between predict(X) and predict_proba(X)[:, 1]', 'course': 'machine-learning-zoomcamp', 'id': '3af31e2a'}]), ('746342ff', [{'text': 'For churn/not churn predictions, I need help to interpret the following scenario please, what is happening when:\\nThe threshold is 1.0\\nFPR is 0.0\\nAnd TPR is 0.0\\nWhen the threshold is 1.0, the condition for belonging to the positive class (churn class) is g(x)>=1.0 But g(x) is a sigmoid function for a binary classification problem. It has values between 0 and 1. This function  never becomes equal to outermost values, i.e. 0 and 1.\\nThat is why there is no object, for which churn-condition could be satisfied. And that is why there is no any positive  (churn) predicted value (neither true positive, nor false positive), if threshold is equal to 1.0\\nAlena Kniazeva', 'section': '4. Evaluation Metrics for Classification', 'question': 'Why are FPR and TPR equal to 0.0, when threshold = 1.0?', 'course': 'machine-learning-zoomcamp', 'id': '746342ff'}]), ('bda2c9b3', [{'text': \"Matplotlib has a cool method to annotate where you could provide an X,Y point and annotate with an arrow and text. For example this will show an arrow pointing to the x,y point optimal threshold.\\nplt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\\\\nOptimal F1 Score: {optimal_f1_score:.2f}',\\nxy=(optimal_threshold, optimal_f1_score),\\nxytext=(0.3, 0.5),\\ntextcoords='axes fraction',\\narrowprops=dict(facecolor='black', shrink=0.05))\\nQuinn Avila\", 'section': '4. Evaluation Metrics for Classification', 'question': 'How can I annotate a graph?', 'course': 'machine-learning-zoomcamp', 'id': 'bda2c9b3'}]), ('41521c92', [{'text': \"It's a complex and abstract topic and it requires some time to understand. You can move on without fully understanding the concept.\\nNonetheless, it might be useful for you to rewatch the video, or even watch videos/lectures/notes by other people on this topic, as the ROC AUC is one of the most important metrics used in Binary Classification models.\", 'section': '4. Evaluation Metrics for Classification', 'question': 'I didn’t fully understand the ROC curve. Can I move on?', 'course': 'machine-learning-zoomcamp', 'id': '41521c92'}]), ('25481ce5', [{'text': 'One main reason behind that, is the way of splitting data. For example, we want to split data into train/validation/test with the ratios 60%/20%/20% respectively.\\nAlthough the following two options end up with the same ratio, the data itself is a bit different and not 100% matching in each case.\\n1)\\ndf_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\\ndf_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\\n2)\\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\\nTherefore, I would recommend using the second method which is more consistent with the lessons and thus the homeworks.\\nIbraheem Taha', 'section': '4. Evaluation Metrics for Classification', 'question': 'Why do I have different values of accuracy than the options in the homework?', 'course': 'machine-learning-zoomcamp', 'id': '25481ce5'}]), ('1427d567', [{'text': 'You can find the intercept between these two curves using numpy diff (https://numpy.org/doc/stable/reference/generated/numpy.diff.html ) and sign (https://numpy.org/doc/stable/reference/generated/numpy.sign.html):\\nI suppose here that you have your df_scores ready with your three columns ‘threshold’, ‘precision’ and ‘recall’:\\nYou want to know at which index (or indices) you have your intercept between precision and recall (namely: where the sign of the difference between precision and recall changes):\\nidx = np.argwhere(\\nnp.diff(\\nnp.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"]))\\n)\\n).flatten()\\nYou can print the result to easily read it:\\nprint(\\nf\"The precision and recall curves intersect at a threshold equal to {df_scores.loc[idx][\\'threshold\\']}.\"\\n)\\n(Mélanie Fouesnard)', 'section': '4. Evaluation Metrics for Classification', 'question': 'How to find the intercept between precision and recall curves by using numpy?', 'course': 'machine-learning-zoomcamp', 'id': '1427d567'}]), ('76c91dfb', [{'text': \"In the demonstration video, we are shown how to calculate the precision and recall manually. You can use the Scikit Learn library to calculate the confusion matrix. precision, recall, f1_score without having to first define true positive, true negative, false positive, and false negative.\\nfrom sklearn.metrics import precision_score, recall_score, f1_score\\nprecision_score(y_true, y_pred, average='binary')\\nrecall_score(y_true, y_pred, average='binary')\\nf1_score(y_true, y_pred, average='binary')\\nRadikal Lukafiardi\", 'section': '4. Evaluation Metrics for Classification', 'question': 'Compute Recall, Precision, and F1 Score using scikit-learn library', 'course': 'machine-learning-zoomcamp', 'id': '76c91dfb'}]), ('e4dd91cf', [{'text': 'Cross-validation evaluates the performance of a model and chooses the best hyperparameters. Cross-validation does this by splitting the dataset into multiple parts (folds), typically 5 or 10. It then trains and evaluates your model multiple times, each time using a different fold as the validation set and the remaining folds as the training set.\\n\"C\" is a hyperparameter that is typically associated with regularization in models like Support Vector Machines (SVM) and logistic regression.\\nSmaller \"C\" values: They introduce more regularization, which means the model will try to find a simpler decision boundary, potentially underfitting the data. This is because it penalizes the misclassification of training examples more severely.\\nLarger \"C\" values: They reduce the regularization effect, allowing the model to fit the training data more closely, potentially overfitting. This is because it penalizes misclassification less severely, allowing the model to prioritize getting training examples correct.\\nAminat Abolade', 'section': '4. Evaluation Metrics for Classification', 'question': 'Why do we use cross validation?', 'course': 'machine-learning-zoomcamp', 'id': 'e4dd91cf'}]), ('cc53ae94', [{'text': \"Model evaluation metrics can be easily computed using off the shelf calculations available in scikit learn library. This saves a lot of time and more precise compared to our own calculations from the scratch using numpy and pandas libraries.\\nfrom sklearn.metrics import (accuracy_score,\\nprecision_score,\\nrecall_score,\\nf1_score,\\nroc_auc_score\\n)\\naccuracy = accuracy_score(y_val, y_pred)\\nprecision = precision_score(y_val, y_pred)\\nrecall = recall_score(y_val, y_pred)\\nf1 = f1_score(y_val, y_pred)\\nroc_auc = roc_auc_score(y_val, y_pred)\\nprint(f'Accuracy: {accuracy}')\\nprint(f'Precision: {precision}')\\nprint(f'Recall: {recall}')\\nprint(f'F1-Score: {f1}')\\nprint(f'ROC AUC: {roc_auc}')\\n(Harish Balasundaram)\", 'section': '4. Evaluation Metrics for Classification', 'question': 'Evaluate the Model using scikit learn metrics', 'course': 'machine-learning-zoomcamp', 'id': 'cc53ae94'}]), ('403bbdd8', [{'text': 'Scikit-learn offers another way: precision_recall_fscore_support\\nExample:\\nfrom sklearn.metrics import precision_recall_fscore_support\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n(Gopakumar Gopinathan)', 'section': '4. Evaluation Metrics for Classification', 'question': 'Are there other ways to compute Precision, Recall and F1 score?', 'course': 'machine-learning-zoomcamp', 'id': '403bbdd8'}]), ('7c68ace0', [{'text': '- ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\\n- The reason for this recommendation is that ROC curves present an optimistic picture of the model on datasets with a class imbalance.\\n-This is because of the use of true negatives in the False Positive Rate in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve.\\n- If the proportion of positive to negative instances changes in a test set, the ROC curves will not change. Metrics such as accuracy, precision, lift and F scores use values from both columns of the confusion matrix. As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not. ROC graphs are based upon TP rate and FP rate, in which each dimension is a strict columnar ratio, so cannot give an accurate picture of performance when there is class imbalance.\\n(Anudeep Vanjavakam)', 'section': '4. Evaluation Metrics for Classification', 'question': 'When do I use ROC vs Precision-Recall curves?', 'course': 'machine-learning-zoomcamp', 'id': '7c68ace0'}]), ('147577f5', [{'text': 'You can use roc_auc_score function from sklearn.metrics module and pass the vector of the target variable (‘above_average’) as the first argument and the vector of feature values as the second one. This function will return AUC score for the feature that was passed as a second argument.\\n(Denys Soloviov)', 'section': '4. Evaluation Metrics for Classification', 'question': 'How to evaluate feature importance for numerical variables with AUC?', 'course': 'machine-learning-zoomcamp', 'id': '147577f5'}]), ('d3ffb802', [{'text': 'Precision-recall curve, and thus the score, explicitly depends on the ratio  of positive to negative test cases. This means that comparison of the F-score across different problems with differing class ratios is problematic. One way to address this issue is to use a standard class ratio  when making such comparisons.\\n(George Chizhmak)', 'section': '4. Evaluation Metrics for Classification', 'question': 'Dependence of the F-score on class imbalance', 'course': 'machine-learning-zoomcamp', 'id': 'd3ffb802'}]), ('cc04d27a', [{'text': \"We can import precision_recall_curve from scikit-learn and plot the graph as follows:\\nfrom sklearn.metrics import precision_recall_curve\\nprecision, recall, thresholds = precision_recall_curve(y_val, y_predict)\\nplt.plot(thresholds, precision[:-1], label='Precision')\\nplt.plot(thresholds, recall[:-1], label='Recall')\\nplt.legend()\\nHrithik Kumar Advani\", 'section': '4. Evaluation Metrics for Classification', 'question': 'Quick way to plot Precision-Recall Curve', 'course': 'machine-learning-zoomcamp', 'id': 'cc04d27a'}]), ('927b5e09', [{'text': 'For multiclass classification it is important to keep class balance when you split the data set. In this case Stratified k-fold returns folds that contains approximately the sme percentage of samples of each classes.\\nPlease check the realisation in sk-learn library:\\nhttps://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold\\nOlga Rudakova', 'section': '5. Deploying Machine Learning Models', 'question': 'What is Stratified k-fold?', 'course': 'machine-learning-zoomcamp', 'id': '927b5e09'}]), ('d22efea7', [{'text': 'Week 5 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/05-deployment/homework.md\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nHW 3 Solution: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/03-classification/homework_3.ipynb\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYouTube Link: 5.X --- https://www.youtube.com/watch?v=agIFak9A3m8&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=49\\n~~~ Nukta Bhatia ~~~', 'section': '5. Deploying Machine Learning Models', 'question': 'How do I get started with Week 5?', 'course': 'machine-learning-zoomcamp', 'id': 'd22efea7'}]), ('d1409f67', [{'text': 'While weeks 1-4 can relatively easily be followed and the associated homework completed with just about any default environment / local setup, week 5 introduces several layers of abstraction and dependencies.\\nIt is advised to prepare your “homework environment” with a cloud provider of your choice. A thorough step-by-step guide for doing so for an AWS EC2 instance is provided in an introductory video taken from the MLOPS course here:\\nhttps://www.youtube.com/watch?v=IXSiYkP23zo\\nNote that (only) small AWS instances can be run for free, and that larger ones will be billed hourly based on usage (but can and should be stopped when not in use).\\nAlternative ways are sketched here:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/01-intro/06-environment.md', 'section': '5. Deploying Machine Learning Models', 'question': 'Errors related to the default environment: WSL, Ubuntu, proper Python version, installing pipenv etc.', 'course': 'machine-learning-zoomcamp', 'id': 'd1409f67'}]), ('e07759e9', [{'text': \"You’ll need a kaggle account\\nGo to settings, API and click `Create New Token`. This will download a `kaggle.json` file which contains your `username` and `key` information\\nIn the same location as your Jupyter NB, place the `kaggle.json` file\\nRun `!chmod 600 <ENTER YOUR FILEPATH>/kaggle.json`\\nMake sure to import os via `import os` and then run:\\nos.environ['KAGGLE_CONFIG_DIR'] = <STRING OF YOUR FILE PATH>\\nFinally you can run directly in your NB: `!kaggle datasets download -d kapturovalexander/bank-credit-scoring`\\nAnd then you can unzip the file and access the CSV via: `!unzip -o bank-credit-scoring.zip`\\n>>> Michael Fronda <<<\", 'section': '5. Deploying Machine Learning Models', 'question': 'How to download CSV data via Jupyter NB and the Kaggle API, for one seamless experience', 'course': 'machine-learning-zoomcamp', 'id': 'e07759e9'}]), ('620fb76e', [{'text': 'Cd .. (go back)\\nLs (see current folders)\\nCd ‘path’/ (go to this path)\\nPwd (home)\\nCat “file name’ --edit txt file in ubuntu\\nAileah Gotladera', 'section': '5. Deploying Machine Learning Models', 'question': 'Basic Ubuntu Commands:', 'course': 'machine-learning-zoomcamp', 'id': '620fb76e'}]), ('957280d8', [{'text': 'Open terminal and type the code below to check the version on your laptop\\npython3 --version\\nFor windows,\\nVisit the official python website at  https://www.python.org/downloads/ to download the python version you need for installation\\nRun the installer and  ensure to check the box that says “Add Python to PATH” during installation and complete the installation by following the prompts\\nOr\\nFor Python 3,\\nOpen your command prompt or terminal and run the following command:\\npip install --upgrade python\\nAminat Abolade', 'section': '5. Deploying Machine Learning Models', 'question': 'Installing and updating to the python version 3.10 and higher', 'course': 'machine-learning-zoomcamp', 'id': '957280d8'}]), ('185096ad', [{'text': 'It is quite simple, and you can follow these instructions here:\\nhttps://www.youtube.com/watch?v=qYlgUDKKK5A&ab_channel=NeuralNine\\nMake sure that you have “Virtual Machine Platform” feature activated in your Windows “Features”. To do that, search “features” in the research bar and see if the checkbox is selected. You also need to make sure that your system (in the bios) is able to virtualize. This is usually the case.\\nIn the Microsoft Store: look for ‘Ubuntu’ or ‘Debian’ (or any linux distribution you want) and install it\\nOnce it is downloaded, open the app and choose a username and a password (secured one). When you type your password, nothing will show in the window, which is normal: the writing is invisible.\\nYou are now inside of your linux system. You can test some commands such as “pwd”. You are not in your Windows system.\\nTo go to your windows system: you need to go back two times with cd ../.. And then go to the “mnt” directory with cd mnt. If you list here your files, you will see your disks. You can move to the desired folder, for example here I moved to the ML_Zoomcamp folder:\\nPython should be already installed but you can check it by running sudo apt install python3 command.\\nYou can make your actual folder your default folder when you open your Ubuntu terminal with this command : echo \"cd ../../mnt/your/folder/path\" >> ~/.bashrc\\nYou can disable bell sounds (when you type something that does not exist for example) by modifying the inputrc file with this command: sudo vim /etc/inputrc\\nYou have to uncomment the set bell-style none line -> to do that, press the “i” keyboard letter (for insert) and go with your keyboard to this line. Delete the # and then press the Escape keyboard touch and finally press “:wq” to write (it saves your modifications) then quit.\\nYou can check that your modifications are taken into account by opening a new terminal (you can pin it to your task bar so you do not have to go to the Microsoft app each time).\\nYou will need to install pip by running this command sudo apt install python3-pip\\nNB: I had this error message when trying to install pipenv (https://github.com/microsoft/WSL/issues/5663):\\n/sbin/ldconfig.real: Can\\'t link /usr/lib/wsl/lib/libnvoptix_loader.so.1 to libnvoptix.so.1\\n/sbin/ldconfig.real: /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link\\nSo I had to create the following symbolic link:\\nsudo ln -s /usr/lib/wsl/lib/libcuda.so.1 /usr/lib64/libcuda.so\\n(Mélanie Fouesnard)', 'section': '5. Deploying Machine Learning Models', 'question': 'How to install WSL on Windows 10 and 11 ?', 'course': 'machine-learning-zoomcamp', 'id': '185096ad'}]), ('ec88d101', [{'text': \"Do you get errors building the Docker image on the Mac M1 chipset?\\nThe error I was getting was:\\nCould not open '/lib64/ld-linux-x86-64.so.2': No such file or directory\\nThe fix (from here): vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\\nOpen mlbookcamp-code/course-zoomcamp/01-intro/environment/Dockerfile\\nReplace line 1 with\\nFROM --platform=linux/amd64 ubuntu:latest\\nNow build the image as specified. In the end it took over 2 hours to build the image but it did complete in the end.\\nDavid Colton\", 'section': '5. Deploying Machine Learning Models', 'question': 'Error building Docker images on Mac with M1 silicon', 'course': 'machine-learning-zoomcamp', 'id': 'ec88d101'}]), ('7156679d', [{'text': 'Import waitress\\nprint(waitress.__version__)\\nKrishna Anand', 'section': '5. Deploying Machine Learning Models', 'question': 'Method to find the version of any install python libraries in jupyter notebook', 'course': 'machine-learning-zoomcamp', 'id': '7156679d'}]), ('4b2a3181', [{'text': 'Working on getting Docker installed - when I try running hello-world I am getting the error.\\nDocker: Cannot connect to the docker daemon at unix:///var/run/docker.sock. Is the Docker daemon running ?\\nSolution description\\nIf you’re getting this error on WSL, re-install your docker: remove the docker installation from WSL and install Docker Desktop on your host machine (Windows).\\nOn Linux, start the docker daemon with either of these commands:\\nsudo dockerd\\nsudo service docker start\\nAdded by Ugochukwu Onyebuchi', 'section': '5. Deploying Machine Learning Models', 'question': 'Cannot connect to the docker daemon. Is the Docker daemon running?', 'course': 'machine-learning-zoomcamp', 'id': '4b2a3181'}]), ('73bd7fa1', [{'text': 'After using the command “docker build -t churn-prediction .” to build the Docker image, the above error is raised and the image is not created.\\nIn your Dockerfile, change the Python version in the first line the Python version installed in your system:\\nFROM python:3.7.5-slim\\nTo find your python version, use the command python --version. For example:\\npython --version\\n>> Python 3.9.7\\nThen, change it on your Dockerfile:\\nFROM python:3.9.7-slim\\nAdded by Filipe Melo', 'section': '5. Deploying Machine Learning Models', 'question': \"The command '/bin/sh -c pipenv install --deploy --system &&  rm -rf /root/.cache' returned a non-zero code: 1\", 'course': 'machine-learning-zoomcamp', 'id': '73bd7fa1'}]), ('a4d3b1e5', [{'text': 'When the facilitator was adding sklearn to the virtual environment in the lectures, he used sklearn==0.24.1 and it ran smoothly. But while doing the homework and you are asked to use the 1.0.2 version of sklearn, it gives errors.\\nThe solution is to use the full name of sklearn. That is, run it as “pipenv install scikit-learn==1.0.2” and the error will go away, allowing you to install sklearn for the version in your virtual environment.\\nOdimegwu David\\nHomework asks you to install 1.3.1\\nPipenv install scikit-learn==1.3.1\\nUse Pipenv to install Scikit-Learn version 1.3.1\\nGopakumar Gopinathan', 'section': '5. Deploying Machine Learning Models', 'question': 'Running “pipenv install sklearn==1.0.2” gives errors. What should I do?', 'course': 'machine-learning-zoomcamp', 'id': 'a4d3b1e5'}]), ('1d462fe0', [{'text': 'What is the reason we don’t want to keep the docker image in our system and why do we need to run docker containers with `--rm` flag?\\nFor best practice, you don’t want to have a lot of abandoned docker images in your system. You just update it in your folder and trigger the build one more time.\\nThey consume extra space on your disk. Unless you don’t want to re-run the previously existing containers, it is better to use the `--rm` option.\\nThe right way to say: “Why do we remove the docker container in our system?”. Well the docker image is still kept; it is the container that is not kept. Upon execution, images are not modified; only containers are.\\nThe option `--rm` is for removing containers. The images remain until you remove them manually. If you don’t specify a version when building an image, it will always rebuild and replace the latest tag. `docker images` shows you all the image you have pulled or build so far.\\nDuring development and testing you usually specify `--rm` to get the containers auto removed upon exit. Otherwise they get accumulated in a stopped state, taking up space. `docker ps -a` shows you all the containers you have in your host. Each time you change Pipfile (or any file you baked into the container), you rebuild the image under the same tag or a new tag. It’s important to understand the difference between the term “docker image” and “docker container”. Image is what we build with all the resources baked in. You can move it around, maintain it in a repository, share it. Then we use the image to spin up instances of it and they are called containers.\\nAdded by Muhammad Awon', 'section': '5. Deploying Machine Learning Models', 'question': 'Why do we need the --rm flag', 'course': 'machine-learning-zoomcamp', 'id': '1d462fe0'}]), ('366d7563', [{'text': 'When you create the dockerfile the name should be dockerfile and needs to be without extension. One of the problems we can get at this point is to create the dockerfile as a dockerfile extension Dockerfile.dockerfile which creates an error when we build the docker image. Instead we just need to create the file without extension: Dockerfile and will run perfectly.\\nAdded by Pastor Soto', 'section': '5. Deploying Machine Learning Models', 'question': 'Failed to read Dockerfile', 'course': 'machine-learning-zoomcamp', 'id': '366d7563'}]), ('cef156d1', [{'text': 'Refer to the page https://docs.docker.com/desktop/install/mac-install/ remember to check if you have apple chip or intel chip.', 'section': '5. Deploying Machine Learning Models', 'question': 'Install docker on MacOS', 'course': 'machine-learning-zoomcamp', 'id': 'cef156d1'}]), ('b632d2ea', [{'text': 'Problem: When I am trying to pull the image with the docker pull svizor/zoomcamp-model command I am getting an error:\\nUsing default tag: latest\\nError response from daemon: manifest for svizor/zoomcamp-model:latest not found: manifest unknown: manifest unknown\\nSolution: The docker by default uses the latest tag to avoid this use the correct tag from image description. In our case use command:\\ndocker pull svizor/zoomcamp-model:3.10.12-slim\\nAdded by Vladimir Yesipov', 'section': '5. Deploying Machine Learning Models', 'question': 'I cannot pull the image with docker pull command', 'course': 'machine-learning-zoomcamp', 'id': 'b632d2ea'}]), ('514e27bb', [{'text': 'Using the command docker images or docker image ls will dump all information for all local Docker images. It is possible to dump the information only for a specified image by using:\\ndocker image ls <image name>\\nOr alternatively:\\ndocker images <image name>\\nIn action to that it is possible to only dump specific information provided using the option --format which will dump only the size for the specified image name when using the command below:\\ndocker image ls --format \"{{.Size}}\" <image name>\\nOr alternatively:\\ndocker images --format \"{{.Size}}\" <image name>\\nSylvia Schmitt', 'section': '5. Deploying Machine Learning Models', 'question': 'Dumping/Retrieving only the size of for a specific Docker image', 'course': 'machine-learning-zoomcamp', 'id': '514e27bb'}]), ('5c67e086', [{'text': \"It creates them in\\nOSX/Linux: ~/.local/share/virtualenvs/folder-name_cyrptic-hash\\nWindows: C:\\\\Users\\\\<USERNAME>\\\\.virtualenvs\\\\folder-name_cyrptic-hash\\nEg: C:\\\\Users\\\\Ella\\\\.virtualenvs\\\\code-qsdUdabf (for module-05 lesson)\\nThe environment name is the name of the last folder in the folder directory where we used the pipenv install command (or any other pipenv command). E.g. If you run any pipenv command in folder path ~/home/user/Churn-Flask-app, it will create an environment named Churn-Flask-app-some_random_characters, and it's path will be like this: /home/user/.local/share/virtualenvs/churn-flask-app-i_mzGMjX.\\nAll libraries of this environment will be installed inside this folder. To activate this environment, I will need to cd into the project folder again, and type pipenv shell. In short, the location of the project folder acts as an identifier for an environment, in place of any name.\\n(Memoona Tahira)\", 'section': '5. Deploying Machine Learning Models', 'question': 'Where does pipenv create environments and how does it name them?', 'course': 'machine-learning-zoomcamp', 'id': '5c67e086'}]), ('63a81b57', [{'text': 'Launch the container image in interactive mode and overriding the entrypoint, so that it starts a bash command.\\ndocker run -it --entrypoint bash <image>\\nIf the container is already running, execute a command in the specific container:\\ndocker ps (find the container-id)\\ndocker exec -it <container-id> bash\\n(Marcos MJD)', 'section': '5. Deploying Machine Learning Models', 'question': 'How do I debug a docker container?', 'course': 'machine-learning-zoomcamp', 'id': '63a81b57'}]), ('047f57fb', [{'text': \"$ docker exec -it 1e5a1b663052 bash\\nthe input device is not a TTY.  If you are using mintty, try prefixing the command with 'winpty'\\nFix:\\nwinpty docker exec -it 1e5a1b663052 bash\\nA TTY is a terminal interface that supports escape sequences, moving the cursor around, etc.\\nWinpty is a Windows software package providing an interface similar to a Unix pty-master for communicating with Windows console programs.\\nMore info on terminal, shell, console applications hi and so on:\\nhttps://conemu.github.io/en/TerminalVsShell.html\\n(Marcos MJD)\", 'section': '5. Deploying Machine Learning Models', 'question': 'The input device is not a TTY when running docker in interactive mode (Running Docker on Windows in GitBash)', 'course': 'machine-learning-zoomcamp', 'id': '047f57fb'}]), ('11f7371c', [{'text': 'Initially, I did not assume there was a model2. I copied the original model1.bin and dv.bin. Then when I tried to load using\\nCOPY [\"model2.bin\", \"dv.bin\", \"./\"]\\nthen I got the error above in MINGW64 (git bash) on Windows.\\nThe temporary solution I found was to use\\nCOPY [\"*\", \"./\"]\\nwhich I assume combines all the files from the original docker image and the files in your working directory.\\nAdded by Muhammed Tan', 'section': '5. Deploying Machine Learning Models', 'question': 'Error: failed to compute cache key: \"/model2.bin\" not found: not found', 'course': 'machine-learning-zoomcamp', 'id': '11f7371c'}]), ('45f39b76', [{'text': 'Create a virtual environment using the Cmd command (command) and use pip freeze command to write the requirements in the text file\\nKrishna Anand', 'section': '5. Deploying Machine Learning Models', 'question': 'Failed to write the dependencies to pipfile and piplock file', 'course': 'machine-learning-zoomcamp', 'id': '45f39b76'}]), ('94e17563', [{'text': 'f-String not properly keyed in: does anyone knows why i am getting error after import pickle?\\nThe first error showed up because your f-string is using () instead of {} around C. So, should be: f’model_C={C}.bin’\\nThe second error as noticed by Sriniketh, your are missing one parenthesis it should be pickle.dump((dv, model), f_out)\\n(Humberto R.)', 'section': '5. Deploying Machine Learning Models', 'question': 'f-strings', 'course': 'machine-learning-zoomcamp', 'id': '94e17563'}]), ('9dd8efd2', [{'text': \"This error happens because pipenv is already installed but you can't access it from the path.\\nThis error comes out if you run.\\npipenv  --version\\npipenv shell\\nSolution for Windows\\nOpen this option\\nClick here\\nClick in Edit Button\\nMake sure the next two locations are on the PATH, otherwise, add it.\\nC:\\\\Users\\\\AppData\\\\....\\\\Python\\\\PythonXX\\\\\\nC:\\\\Users\\\\AppData\\\\....\\\\Python\\\\PythonXX\\\\Scripts\\\\\\nAdded by Alejandro Aponte\\nNote: this answer assumes you don’t use Anaconda. For Windows, using Anaconda would be a better choice and less prone to errors.\", 'section': '5. Deploying Machine Learning Models', 'question': \"'pipenv' is not recognized as an internal or external command, operable program or batch file.\", 'course': 'machine-learning-zoomcamp', 'id': '9dd8efd2'}]), ('9531dc92', [{'text': 'Following the instruction from video week-5.6, using pipenv to install python libraries throws below error\\nSolution to this error is to make sure that you are working with python==3.9 (as informed in the very first lesson of the zoomcamp) and not python==3.10.\\nAdded by Hareesh Tummala', 'section': '5. Deploying Machine Learning Models', 'question': 'AttributeError: module ‘collections’ has no attribute ‘MutableMapping’', 'course': 'machine-learning-zoomcamp', 'id': '9531dc92'}]), ('14e0e697', [{'text': 'After entering `pipenv shell` don’t forget to use `exit` before `pipenv --rm`, as it may cause errors when trying to install packages, it is unclear whether you are “in the shell”(using Windows) at the moment as there are no clear markers for it.\\nIt can also mess up PATH, if that’s the case, here’s terminal commands for fixing that:\\n# for Windows\\nset VIRTUAL_ENV \"\"\\n# for Unix\\nexport VIRTUAL_ENV=\"\"\\nAlso manually re-creating removed folder at `C:\\\\Users\\\\username\\\\.virtualenvs\\\\removed-envname` can help, removed-envname can be seen at the error message.\\nAdded by Andrii Larkin', 'section': '5. Deploying Machine Learning Models', 'question': \"Q: ValueError: Path not found or generated: WindowsPath('C:/Users/username/.virtualenvs/envname/Scripts')\", 'course': 'machine-learning-zoomcamp', 'id': '14e0e697'}]), ('6189375f', [{'text': 'Set the host to ‘0.0.0.0’ on the flask app and dockerfile then RUN the url using localhost.\\n(Theresa S.)', 'section': '5. Deploying Machine Learning Models', 'question': \"ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\", 'course': 'machine-learning-zoomcamp', 'id': '6189375f'}]), ('3419ee27', [{'text': 'Solution:\\nThis error occurred because I used single quotes around the filenames. Stick to double quotes', 'section': '5. Deploying Machine Learning Models', 'question': 'docker  build ERROR [x/y] COPY …', 'course': 'machine-learning-zoomcamp', 'id': '3419ee27'}]), ('8b8c1603', [{'text': 'I tried the first solution on Stackoverflow which recommended running `pipenv lock` to update the Pipfile.lock. However, this didn’t resolve it. But the following switch to the pipenv installation worked\\nRUN pipenv install --system --deploy --ignore-pipfile', 'section': '5. Deploying Machine Learning Models', 'question': 'Fix error during installation of Pipfile inside Docker container', 'course': 'machine-learning-zoomcamp', 'id': '8b8c1603'}]), ('e54d5411', [{'text': 'Solution\\nThis error was because there was another instance of gunicorn running. So I thought of removing this along with the zoomcamp_test image. However, it didn’t let me remove the orphan container. So I did the following\\nRunning the following commands\\ndocker ps -a <to list all docker containers>\\ndocker images <to list images>\\ndocker stop <container ID>\\ndocker rm <container ID>\\ndocker rmi image\\nI rebuilt the Docker image, and ran it once again; this time it worked correctly and I was able to serve the test script to the endpoint.', 'section': '5. Deploying Machine Learning Models', 'question': 'How to fix error after running the Docker run command', 'course': 'machine-learning-zoomcamp', 'id': 'e54d5411'}]), ('f7b38587', [{'text': 'I was getting the below error when I rebuilt the docker image although the port was not allocated, and it was working fine.\\nError message:\\nError response from daemon: driver failed programming external connectivity on endpoint beautiful_tharp (875be95c7027cebb853a62fc4463d46e23df99e0175be73641269c3d180f7796): Bind for 0.0.0.0:9696 failed: port is already allocated.\\nSolution description\\nIssue has been resolved running the following command:\\ndocker kill $(docker ps -q)\\nhttps://github.com/docker/for-win/issues/2722\\nAsia Saeed', 'section': '5. Deploying Machine Learning Models', 'question': 'Bind for 0.0.0.0:9696 failed: port is already allocated', 'course': 'machine-learning-zoomcamp', 'id': 'f7b38587'}]), ('be86b333', [{'text': 'I was getting the error on client side with this\\nClient Side:\\nFile \"C:\\\\python\\\\lib\\\\site-packages\\\\urllib3\\\\connectionpool.py\", line 703, in urlopen …………………..\\nraise ConnectionError(err, request=request)\\nrequests.exceptions.ConnectionError: (\\'Connection aborted.\\', RemoteDisconnected(\\'Remote end closed connection without response\\'))\\nSevrer Side:\\nIt showed error for gunicorn\\nThe waitress  cmd was running smoothly from server side\\nSolution:\\nUse the ip-address as 0.0.0.0:8000 or 0.0.0.0:9696.They are the ones which do work max times\\nAamir Wani', 'section': '5. Deploying Machine Learning Models', 'question': 'Bind for 127.0.0.1:5000 showing error', 'course': 'machine-learning-zoomcamp', 'id': 'be86b333'}]), ('4ea80460', [{'text': 'Install it by using command\\n% brew install md5sha1sum\\nThen run command to check hash for file to check if they the same with the provided\\n% md5sum model1.bin dv.bin\\nOlga Rudakova', 'section': '5. Deploying Machine Learning Models', 'question': 'Installing md5sum on Macos', 'course': 'machine-learning-zoomcamp', 'id': '4ea80460'}]), ('8006b496', [{'text': 'Problem description:\\nI started a web-server in terminal (command window, powershell, etc.). How can I run another python script, which makes a request to this server?\\nSolution description:\\nJust open another terminal (command window, powershell, etc.) and run a python script.\\nAlena Kniazeva', 'section': '5. Deploying Machine Learning Models', 'question': 'How to run a script while a web-server is working?', 'course': 'machine-learning-zoomcamp', 'id': '8006b496'}]), ('704f95d8', [{'text': \"Problem description:\\nIn video 5.5 when I do pipenv shell and then pipenv run gunicorn --bind 0.0.0.0:9696 predict:app, I get the following warning:\\nUserWarning: Trying to unpickle estimator DictVectorizer from version 1.1.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\\nSolution description:\\nWhen you create a virtual env, you should use the same version of Scikit-Learn that you used for training the model on this case it's 1.1.1. There is version conflicts so we need to make sure our model and dv files are created from the version we are using for the project.\\nBhaskar Sarma\", 'section': '5. Deploying Machine Learning Models', 'question': 'Version-conflict in pipenv', 'course': 'machine-learning-zoomcamp', 'id': '704f95d8'}]), ('a5b3296b', [{'text': \"If you install packages via pipenv install, and get an error that ends like this:\\npipenv.vendor.plette.models.base.ValidationError: {'python_version': '3.9', 'python_full_version': '3.9.13'}\\npython_full_version: 'python_version' must not be present with 'python_full_version'\\npython_version: 'python_full_version' must not be present with 'python_version'\\nDo this:\\nopen Pipfile in nano editor, and remove either the python_version or python_full_version line, press CTRL+X, type Y and click Enter to save changed\\nType pipenv lock to create the Pipfile.lock.\\nDone. Continue what you were doing\", 'section': '5. Deploying Machine Learning Models', 'question': 'Python_version and Python_full_version error after running pipenv install:', 'course': 'machine-learning-zoomcamp', 'id': 'a5b3296b'}]), ('a23b276a', [{'text': 'If during running the  docker build command, you get an error like this:\\nYour Pipfile.lock (221d14) is out of date. Expected: (939fe0).\\nUsage: pipenv install [OPTIONS] [PACKAGES]...\\nERROR:: Aborting deploy\\nOption 1: Delete the pipfile.lock via rm Pipfile, and then rebuild the lock via  pipenv lock from the terminal before retrying the docker build command.\\nOption 2:  If it still doesn’t work, remove the pipenv environment, Pipfile and Pipfile.lock, and create a new one before building docker again. Commands to remove pipenv environment and removing pipfiles:\\npipenv  --rm\\nrm Pipfile*', 'section': '5. Deploying Machine Learning Models', 'question': 'Your Pipfile.lock (221d14) is out of date (during Docker build)', 'course': 'machine-learning-zoomcamp', 'id': 'a23b276a'}]), ('3537eeee', [{'text': 'Ans: Pip uninstall waitress mflow. Then reinstall just mlflow. By this time you should have successfully built your docker image so you dont need to reinstall waitress. All good. Happy learning.\\nAdded by 🅱🅻🅰🆀', 'section': '5. Deploying Machine Learning Models', 'question': 'You are using windows. Conda environment. You then use waitress instead of gunicorn. After a few runs, suddenly mlflow server fails to run.', 'course': 'machine-learning-zoomcamp', 'id': '3537eeee'}]), ('1d6d5b51', [{'text': \"Ans: so you have created the env. You need to make sure you're in eu-west-1 (ireland) when you check the EB environments. Maybe you're in a different region in your console.\\nAdded by Edidiong Esu\", 'section': '5. Deploying Machine Learning Models', 'question': 'Completed creating the environment locally but could not find the environment on AWS.', 'course': 'machine-learning-zoomcamp', 'id': '1d6d5b51'}]), ('3a98b6b7', [{'text': 'Running \\'pip install waitress\\' as a command on GitBash was not downloading the executable file \\'waitress-serve.exe\\'. You need this file to be able to run commands with waitress in Git Bash. To solve this:\\nopen a Jupyter notebook and run the same command \\' pip install waitress\\'. This way the executable file will be downloaded. The notebook may give you this warning : \\'WARNING: The script waitress-serve.exe is installed in \\'c:\\\\Users\\\\....\\\\anaconda3\\\\Scripts\\' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\\'\\nAdd the path where \\'waitress-serve.exe\\' is installed into gitbash\\'s PATH as such:\\nenter the following command in gitbash: nano ~/.bashrc\\nadd the path to \\'waitress-serve.exe\\' to PATH using this command: export PATH=\"/path/to/waitress:$PATH\"\\nclose gitbash and open it again and you should be good to go\\nAdded by Bachar Kabalan', 'section': '5. Deploying Machine Learning Models', 'question': 'Installing waitress on Windows via GitBash: “waitress-serve” command not found', 'course': 'machine-learning-zoomcamp', 'id': '3a98b6b7'}]), ('d42eb923', [{'text': 'Q2.1: Use Pipenv to install Scikit-Learn version 1.3.1\\nThis is an error I got while executing the above step in the ml-zoomcamp conda environment. The error is not fatal and just warns you that explicit language specifications are not set out in our bash profile. A quick-fix is here:\\nhttps://stackoverflow.com/questions/49436922/getting-error-while-trying-to-run-this-command-pipenv-install-requests-in-ma\\nBut one can proceed without addressing it.\\nAdded by Abhirup Ghosh', 'section': '5. Deploying Machine Learning Models', 'question': 'Warning: the environment variable LANG is not set!', 'course': 'machine-learning-zoomcamp', 'id': 'd42eb923'}]), ('42aebe10', [{'text': 'The provided image FROM svizor/zoomcamp-model:3.10.12-slim has a model and dictvectorizer that should be used for question 6. \"model2.bin\", \"dv.bin\"\\nAdded by Quinn Avila', 'section': '5. Deploying Machine Learning Models', 'question': 'Module5 HW Question 6', 'course': 'machine-learning-zoomcamp', 'id': '42aebe10'}]), ('e4f62713', [{'text': 'https://apps.microsoft.com/detail/windows-terminal/9N0DX20HK701?hl=es-419&gl=CO\\nAdded by Dawuta Smit', 'section': '5. Deploying Machine Learning Models', 'question': 'Terminal Used in Week 5 videos:', 'course': 'machine-learning-zoomcamp', 'id': 'e4f62713'}]), ('c13d811f', [{'text': \"Question:\\nWhen running\\npipenv run waitress-serve --listen=localhost:9696 q4-predict:app\\nI get the following:\\nThere was an exception (ValueError) importing your module.\\nIt had these arguments:\\n1. Malformed application 'q4-predict:app'\\nAnswer:\\nWaitress doesn’t accept a dash in the python file name.\\nThe solution is to rename the file replacing a dash with something else for instance with an underscore eg q4_predict.py\\nAdded by Alex Litvinov\", 'section': '5. Deploying Machine Learning Models', 'question': 'waitress-serve shows Malformed application', 'course': 'machine-learning-zoomcamp', 'id': 'c13d811f'}]), ('dfb41f7e', [{'text': 'I wanted to have a fast and simple way to check if the HTTP POST requests are working just running a request from command line. This can be done running ‘curl’. \\n(Used with WSL2 on Windows, should also work on Linux and MacOS)\\ncurl --json \\'<json data>\\' <url>\\n# piping the structure to the command\\ncat <json file path> | curl --json @- <url>\\necho \\'<json data>\\' | curl --json @- <url>\\n# example using piping\\necho \\'{\"job\": \"retired\", \"duration\": 445, \"poutcome\": \"success\"}\\'\\\\\\n| curl --json @- http://localhost:9696/predict\\nAdded by Sylvia Schmitt', 'section': '5. Deploying Machine Learning Models', 'question': 'Testing HTTP POST requests from command line using curl', 'course': 'machine-learning-zoomcamp', 'id': 'dfb41f7e'}]), ('d04e77f8', [{'text': 'Question:\\nWhen executing\\neb local run  --port 9696\\nI get the following error:\\nERROR: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\\nAnswer:\\nThere are two options to fix this:\\nRe-initialize by running eb init -i and choosing the options from a list (the first default option for docker platform should be fine).\\nEdit the ‘.elasticbeanstalk/config.yml’ directly changing the default_platform from Docker to default_platform: Docker running on 64bit Amazon Linux 2023\\nThe disadvantage of the second approach is that the option might not be available the following years\\nAdded by Alex Litvinov', 'section': '5. Deploying Machine Learning Models', 'question': 'NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.', 'course': 'machine-learning-zoomcamp', 'id': 'd04e77f8'}]), ('451c067f', [{'text': \"You need to include the protocol scheme: 'http://localhost:9696/predict'.\\nWithout the http:// part, requests has no idea how to connect to the remote server.\\nNote that the protocol scheme must be all lowercase; if your URL starts with HTTP:// for example, it won’t find the http:// connection adapter either.\\nAdded by George Chizhmak\", 'section': '5. Deploying Machine Learning Models', 'question': \"Requests Error: No connection adapters were found for 'localhost:9696/predict'.\", 'course': 'machine-learning-zoomcamp', 'id': '451c067f'}]), ('9fbfcd61', [{'text': 'While running the docker image if you get the same result check which model you are using.\\nRemember you are using a model downloading model + python version so remember to change the model in your file when running your prediction test.\\nAdded by Ahmed Okka', 'section': '5. Deploying Machine Learning Models', 'question': 'Getting the same result', 'course': 'machine-learning-zoomcamp', 'id': '9fbfcd61'}]), ('1ed8cfde', [{'text': 'Ensure that you used pipenv to install the necessary modules including gunicorn. As pipfiles for virtual environments, you can use pipenv shell and then build+run your docker image. - Akshar Goyal', 'section': '5. Deploying Machine Learning Models', 'question': 'Trying to run a docker image I built but it says it’s unable to start the container process', 'course': 'machine-learning-zoomcamp', 'id': '1ed8cfde'}]), ('3f97f50f', [{'text': \"You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\\nTo copy a file or directory from your local machine into a running Docker container, you can use the `docker cp command`. The basic syntax is as follows:\\ndocker cp /path/to/local/file_or_directory container_id:/path/in/container\\nHrithik Kumar Advani\", 'section': '5. Deploying Machine Learning Models', 'question': 'How do I copy files from my local machine to docker container?', 'course': 'machine-learning-zoomcamp', 'id': '3f97f50f'}]), ('a24a874a', [{'text': 'You can copy files from your local machine into a Docker container using the docker cp command. Here\\'s how to do it:\\nIn the Dockerfile, you can provide the folder containing the files that you want to copy over. The basic syntax is as follows:\\nCOPY [\"src/predict.py\", \"models/xgb_model.bin\", \"./\"]\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tGopakumar Gopinathan', 'section': '5. Deploying Machine Learning Models', 'question': 'How do I copy files from a different folder into docker container’s working directory?', 'course': 'machine-learning-zoomcamp', 'id': 'a24a874a'}]), ('bf563b1f', [{'text': 'I struggled with the command :\\neb init -p docker tumor-diagnosis-serving -r eu-west-1\\nWhich resulted in an error when running : eb local run --port 9696\\nERROR: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\\nI replaced it with :\\neb init -p \"Docker running on 64bit Amazon Linux 2\" tumor-diagnosis-serving -r eu-west-1\\nThis allowed the recognition of the Dockerfile and the build/run of the docker container.\\nAdded by Mélanie Fouesnard', 'section': '5. Deploying Machine Learning Models', 'question': 'I can’t create the environment on AWS Elastic Beanstalk with the command proposed during the video', 'course': 'machine-learning-zoomcamp', 'id': 'bf563b1f'}]), ('21e9facf', [{'text': \"I had this error when creating a AWS ElasticBean environment: eb create tumor-diagnosis-env\\nERROR   Instance deployment: Both 'Dockerfile' and 'Dockerrun.aws.json' are missing in your source bundle. Include at least one of them. The deployment failed.\\nI did not committed the files used to build the container, particularly the Dockerfile. After a git add and git commit of the modified files, the command works.\\nAdded by Mélanie Fouesnard\", 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Dockerfile missing when creating the AWS ElasticBean environment', 'course': 'machine-learning-zoomcamp', 'id': '21e9facf'}]), ('aef786aa', [{'text': 'Week 6 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/06-trees/homework.md\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nHW 4 Solution: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/04-evaluation/homework_4.ipynb\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYouTube Link: 6.X --- https://www.youtube.com/watch?v=GJGmlfZoCoU&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=57\\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n~~~Nukta Bhatia~~~', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'How to get started with Week 6?', 'course': 'machine-learning-zoomcamp', 'id': 'aef786aa'}]), ('68858294', [{'text': 'During the XGBoost lesson, we created a parser to extract the training and validation auc from the standard output. However, we can accomplish that in a more straightforward way.\\nWe can use the evals_result  parameters, which takes an empty dictionary and updates it for each tree. Additionally, you can store the data in a dataframe and plot it in an easier manner.\\nAdded by Daniel Coronel', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'How to get the training and validation metrics from XGBoost?', 'course': 'machine-learning-zoomcamp', 'id': '68858294'}]), ('85ac722e', [{'text': 'You should create sklearn.ensemble.RandomForestRegressor object. It’s rather similar to sklearn.ensemble.RandomForestClassificator for classification problems. Check https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html for more information.\\nAlena Kniazeva', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'How to solve regression problems with random forest in scikit-learn?', 'course': 'machine-learning-zoomcamp', 'id': '85ac722e'}]), ('b61d2e92', [{'text': 'In question 6, I was getting ValueError: feature_names must be string, and may not contain [, ] or < when I was creating DMatrix for train and validation\\nSolution description\\nThe cause of this error is that some of the features names contain special characters like = and <, and I fixed the error by removing them as  follows:\\nfeatures= [i.replace(\"=<\", \"_\").replace(\"=\",\"_\") for i in features]\\nAsia Saeed\\nAlternative Solution:\\nIn my case the equal sign “=” was not a problem, so in my opinion the first part of Asias solution features= [i.replace(\"=<\", \"_\") should work as well.\\nFor me this works:\\nfeatures = []\\nfor f in dv.feature_names_:\\nstring = f.replace(“=<”, “-le”)\\nfeatures.append(string)\\nPeter Ernicke', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'ValueError: feature_names must be string, and may not contain [, ] or <', 'course': 'machine-learning-zoomcamp', 'id': 'b61d2e92'}]), ('8d7392cb', [{'text': 'If you’re getting this error, It is likely because the feature names in dv.get_feature_names_out() are a np.ndarray instead of a list so you have to convert them into a list by using the to_list() method.\\nAli Osman', 'section': '6. Decision Trees and Ensemble Learning', 'question': \"`TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'> ` when training xgboost model.\", 'course': 'machine-learning-zoomcamp', 'id': '8d7392cb'}]), ('c920eef3', [{'text': \"If you’re getting TypeError:\\n“TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'>”,\\nprobably you’ve done this:\\nfeatures = dv.get_feature_names_out()\\nIt gets you np.ndarray instead of list. Converting to list list(features) will not fix this, read below.\\nIf you’re getting ValueError:\\n“ValueError: feature_names must be string, and may not contain [, ] or <”,\\nprobably you’ve either done:\\nfeatures = list(dv.get_feature_names_out())\\nor:\\nfeatures = dv.feature_names_\\nreason is what you get from DictVectorizer here looks like this:\\n['households',\\n'housing_median_age',\\n'latitude',\\n'longitude',\\n'median_income',\\n'ocean_proximity=<1H OCEAN',\\n'ocean_proximity=INLAND',\\n'population',\\n'total_bedrooms',\\n'total_rooms']\\nit has symbols XGBoost doesn’t like ([, ] or <).\\nWhat you can do, is either do not specify “feature_names=” while creating xgb.DMatrix or:\\nimport re\\nfeatures = dv.feature_names_\\npattern = r'[\\\\[\\\\]<>]'\\nfeatures = [re.sub(pattern, '  ', f) for f in features]\\nAdded by Andrii Larkin\", 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Q6: ValueError or TypeError while setting xgb.DMatrix(feature_names=)', 'course': 'machine-learning-zoomcamp', 'id': 'c920eef3'}]), ('5017c9a4', [{'text': 'To install Xgboost, use the code below directly in your jupyter notebook:\\n(Pip 21.3+ is required)\\npip install xgboost\\nYou can update your pip by using the code below:\\npip install --upgrade pip\\nFor more about xgbboost and installation, check here:\\nhttps://xgboost.readthedocs.io/en/stable/install.html\\nAminat Abolade', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'How to Install Xgboost', 'course': 'machine-learning-zoomcamp', 'id': '5017c9a4'}]), ('6ffe101d', [{'text': 'Sometimes someone might wonder what eta means in the tunable hyperparameters of XGBoost and how it helps the model.\\nETA is the learning rate of the model. XGBoost uses gradient descent to calculate and update the model. In gradient descent, we are looking for the minimum weights that help the model to learn the data very well. This minimum weights for the features is updated each time the model passes through the features and learns the features during training. Tuning the learning rate helps you tell the model what speed it would use in deriving the minimum for the weights.', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'What is eta in XGBoost', 'course': 'machine-learning-zoomcamp', 'id': '6ffe101d'}]), ('a55b29ff', [{'text': 'For ensemble algorithms, during the week 6, one bagging algorithm and one boosting algorithm were presented: Random Forest and XGBoost, respectively.\\nRandom Forest trains several models in parallel. The output can be, for example, the average value of all the outputs of each model. This is called bagging.\\nXGBoost trains several models sequentially: the previous model error is used to train the following model. Weights are used to ponderate the models such as the best models have higher weights and are therefore favored for the final output. This method is called boosting.\\nNote that boosting is not necessarily better than bagging.\\nMélanie Fouesnard\\nBagging stands for “Bootstrap Aggregation” - it involves taking multiple samples with replacement to derive multiple training datasets from the original training dataset (bootstrapping), training a classifier (e.g. decision trees or stumps for Random Forests) on each such training dataset, and then combining the the predictions (aggregation) to obtain the final prediction. For classification, predictions are combined via voting; for regression, via averaging. Bagging can be done in parallel, since the various classifiers are independent. Bagging decreases variance (but not bias) and is robust against overfitting.\\nBoosting, on the other hand, is sequential - each model learns from the mistakes of its predecessor. Observations are given different weights - observations/samples misclassified by the previous classifier are given a higher weight, and this process is continued until a stopping condition is reached (e.g. max. No. of models is reached, or error is acceptably small, etc.). Boosting reduces bias & is generally more accurate than bagging, but can be prone to overfitting.\\nRileen', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'What is the difference between bagging and boosting?', 'course': 'machine-learning-zoomcamp', 'id': 'a55b29ff'}]), ('eac70ce3', [{'text': 'I wanted to directly capture the output from the xgboost training for multiple eta values to a dictionary without the need to run the same cell multiple times and manually editing the eta value in between or copy the code for a second eta value.\\nUsing the magic cell command “%%capture output” I was only able to capture the complete output for all iterations for the loop, but. I was able to solve this using the following approach. This is just a code sample to grasp the idea.\\n# This would be the content of the Jupyter Notebook cell\\nfrom IPython.utils.capture import capture_output\\nimport sys\\ndifferent_outputs = {}\\nfor i in range(3):\\nwith capture_output(sys.stdout) as output:\\nprint(i)\\nprint(\"testing capture\")\\ndifferent_outputs[i] = output.stdout\\n# different_outputs\\n# {0: \\'0\\\\ntesting capture\\\\n\\',\\n#  1: \\'1\\\\ntesting capture\\\\n\\',\\n#  2: \\'2\\\\ntesting capture\\\\n\\'}\\nAdded by Sylvia Schmitt', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Capture stdout for each iterations of a loop separately', 'course': 'machine-learning-zoomcamp', 'id': 'eac70ce3'}]), ('5f91f8ca', [{'text': 'Calling roc_auc_score() to get auc is throwing the above error.\\nSolution to this issue is to make sure that you pass y_actuals as 1st argument and y_pred as 2nd argument.\\nroc_auc_score(y_train, y_pred)\\nHareesh Tummala', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'ValueError: continuous format is not supported', 'course': 'machine-learning-zoomcamp', 'id': '5f91f8ca'}]), ('a3be507a', [{'text': 'When rmse stops improving means, when it stops to decrease or remains almost similar.\\nPastor Soto', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Question 3 of homework 6 if i see that rmse goes up at a certain number of n_estimators but then goes back down lower than it was before, should the answer be the number of n_estimators after which rmse initially went up, or the number after which it was its overall lowest value?', 'course': 'machine-learning-zoomcamp', 'id': 'a3be507a'}]), ('9a8faa50', [{'text': 'dot_data = tree.export_graphviz(regr, out_file=None,\\nfeature_names=boston.feature_names,\\nfilled=True)\\ngraphviz.Source(dot_data, format=\"png\")\\nKrishna Anand\\nfrom sklearn import tree\\ntree.plot_tree(dt,feature_names=dv.feature_names_)\\nAdded By Ryan Pramana', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'One of the method to visualize the decision trees', 'course': 'machine-learning-zoomcamp', 'id': '9a8faa50'}]), ('a6e384fe', [{'text': 'Solution: This problem happens because you use DecisionTreeClassifier instead of DecisionTreeRegressor. You should check if you want to use a Decision tree for classification or regression.\\nAlejandro Aponte', 'section': '6. Decision Trees and Ensemble Learning', 'question': \"ValueError: Unknown label type: 'continuous'\", 'course': 'machine-learning-zoomcamp', 'id': 'a6e384fe'}]), ('ddc14ada', [{'text': 'When I run dt = DecisionTreeClassifier() in jupyter in same laptop, each time I re-run it or do (restart kernel + run) I get different values of auc. Some of them are 0.674, 0.652, 0.642, 0.669 and so on.  Anyone knows why it could be? I am referring to 7:40-7:45 of video 6.3.\\nSolution: try setting the random seed e.g\\ndt = DecisionTreeClassifier(random_state=22)\\nBhaskar Sarma', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Different values of auc, each time code is re-run', 'course': 'machine-learning-zoomcamp', 'id': 'ddc14ada'}]), ('593f7569', [{'text': \"They both do the same, it's just less typing from the script.\\nAsked by Andrew Katoch, Added by Edidiong Esu\", 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Does it matter if we let the Python file create the server or if we run gunicorn directly?', 'course': 'machine-learning-zoomcamp', 'id': '593f7569'}, {'text': \"They both do the same, it's just less typing from the script.\", 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Does it matter if we let the Python file create the server or if we run gunicorn directly?', 'course': 'machine-learning-zoomcamp', 'id': '593f7569'}]), ('6cb56405', [{'text': 'When I tried to run example from the video using function ping and can not import it. I use import ping and it was unsuccessful. To fix it I use the statement:\\n\\nfrom [file name] import ping\\nOlga Rudakova', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'No module named ‘ping’?', 'course': 'machine-learning-zoomcamp', 'id': '6cb56405'}]), ('a22a93f1', [{'text': 'The DictVectorizer has a function to get the feature names get_feature_names_out(). This is helpful for example if you need to analyze feature importance but use the dict vectorizer for one hot encoding. Just keep in mind it does return a numpy array so you may need to convert this to a list depending on your usage for example dv.get_feature_names_out() will return a ndarray array of string objects. list(dv.get_feature_names_out()) will convert to a standard list of strings. Also keep in mind that you first need to fit the predictor and response arrays before you have access to the feature names.\\nQuinn Avila', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'DictVectorizer feature names', 'course': 'machine-learning-zoomcamp', 'id': 'a22a93f1'}]), ('b6259dea', [{'text': 'This error occurs because the list of feature names contains some characters like \"<\" that are not supported. To fix this issue, you can replace those problematic characters with supported ones. If you want to create a consistent list of features with no special characters, you can achieve it like this:\\nYou can address this error by replacing problematic characters in the feature names with underscores, like so:\\nfeatures = [f.replace(\\'=<\\', \\'_\\').replace(\\'=\\', \\'_\\') for f in features]\\nThis code will go through the list of features and replace any instances of \"=<\" with \"\", as well as any \"=\" with \"\", ensuring that the feature names only consist of supported characters.', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'ValueError: feature_names must be string, and may not contain [, ] or <', 'course': 'machine-learning-zoomcamp', 'id': 'b6259dea'}]), ('bcfdc6f4', [{'text': \"To make it easier for us to determine which features are important, we can use a horizontal bar chart to illustrate feature importance sorted by value.\\n1. # extract the feature importances from the model\\nfeature_importances = list(zip(features_names, rdr_model.feature_importances_))\\nimportance_df = pd.DataFrame(feature_importances, columns=['feature_names', 'feature_importances'])\\n2. # sort descending the dataframe by using feature_importances value\\nimportance_df = importance_df.sort_values(by='feature_importances', ascending=False)\\n3. # create a horizontal bar chart\\nplt.figure(figsize=(8, 6))\\nsns.barplot(x='feature_importances', y='feature_names', data=importance_df, palette='Blues_r')\\nplt.xlabel('Feature Importance')\\nplt.ylabel('Feature Names')\\nplt.title('Feature Importance Chart')\\nRadikal Lukafiardi\", 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Visualize Feature Importance by using horizontal bar chart', 'course': 'machine-learning-zoomcamp', 'id': 'bcfdc6f4'}]), ('a7e7cdd2', [{'text': 'Instead of using np.sqrt() as the second step. You can extract it using like this way :\\nmean_squared_error(y_val, y_predict_val,squared=False)\\nAhmed Okka', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'RMSE using metrics.root_meas_square()', 'course': 'machine-learning-zoomcamp', 'id': 'a7e7cdd2'}]), ('55477da8', [{'text': 'I like this visual implementation of features importance in scikit-learn library:\\nhttps://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\\nIt actually adds std.errors to features importance -> so that you can trace stability of features (important for a model’s explainability) over the different params of the model.\\nIvan Brigida', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Features Importance graph', 'course': 'machine-learning-zoomcamp', 'id': '55477da8'}]), ('6a245a05', [{'text': 'Expanded error says: xgboost.core.XGBoostError: sklearn needs to be installed in order to use this module. So, sklearn in requirements solved the problem.\\nGeorge Chizhmak', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'xgboost.core.XGBoostError: This app has encountered an error. The original error message is redacted to prevent data leaks.', 'course': 'machine-learning-zoomcamp', 'id': '6a245a05'}]), ('4405bfca', [{'text': 'Information gain  in Y due to X, or the mutual information of Y and X\\nWhere  is the entropy of Y. \\n\\nIf X is completely uninformative about Y:\\nIf X is completely informative about Y: )\\nHrithik Kumar Advani', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Information Gain', 'course': 'machine-learning-zoomcamp', 'id': '4405bfca'}]), ('3e0acc25', [{'text': 'Filling in missing values using an entire dataset before splitting for training/testing/validation causes', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Data Leakage', 'course': 'machine-learning-zoomcamp', 'id': '3e0acc25'}]), ('abaecdf8', [{'text': 'Save model by calling ‘booster.save_model’, see eg\\nLoad model:\\nDawuta Smit\\nThis section is moved to Projects', 'section': '8. Neural Networks and Deep Learning', 'question': 'Serialized Model Xgboost error', 'course': 'machine-learning-zoomcamp', 'id': 'abaecdf8'}]), ('ff40f83b', [{'text': 'TODO', 'section': '8. Neural Networks and Deep Learning', 'question': 'How to get started with Week 8?', 'course': 'machine-learning-zoomcamp', 'id': 'ff40f83b'}]), ('95a16746', [{'text': 'Create or import your notebook into Kaggle.\\nClick on the Three dots at the top right hand side\\nClick on Accelerator\\nChoose T4 GPU\\nKhurram Majeed', 'section': '8. Neural Networks and Deep Learning', 'question': 'How to use Kaggle for Deep Learning?', 'course': 'machine-learning-zoomcamp', 'id': '95a16746'}]), ('46acdd18', [{'text': 'Create or import your notebook into Google Colab.\\nClick on the Drop Down at the top right hand side\\nClick on “Change runtime type”\\nChoose T4 GPU\\nKhurram Majeed', 'section': '8. Neural Networks and Deep Learning', 'question': 'How to use Google Colab for Deep Learning?', 'course': 'machine-learning-zoomcamp', 'id': '46acdd18'}]), ('f721d54b', [{'text': 'Connecting your GPU on Saturn Cloud to Github repository is not compulsory, since you can just download the notebook and copy it to the Github folder. But if you like technology to do things for you, then follow the solution description below:\\nSolution description: Follow the instructions in these github docs to create an SSH private and public key:\\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-ke\\ny-and-adding-it-to-the-ssh-agenthttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account?tool=webui\\nThen the second video on this module about saturn cloud would show you how to add the ssh keys to secrets and authenticate through a terminal.\\nOr alternatively, you could just use the public keys provided by Saturn Cloud by default. To do so, follow these steps:\\nClick on your username and on manage\\nDown below you will see the Git SSH keys section.\\nCopy the default public key provided by Saturn Cloud\\nPaste these key into the SSH keys section of your github repo\\nOpen a terminal on Saturn Cloud and run this command “ssh -T git@github.com”\\nYou will receive a successful authentication notice.\\nOdimegwu David', 'section': '8. Neural Networks and Deep Learning', 'question': 'How do I push from Saturn Cloud to Github?', 'course': 'machine-learning-zoomcamp', 'id': 'f721d54b'}]), ('69cd4897', [{'text': 'This template is referred to in the video 8.1b Setting up the Environment on Saturn Cloud\\nbut the location shown in the video is no longer correct.\\nThis template has been moved to “python deep learning tutorials’ which is shown on the Saturn Cloud home page.\\nSteven Christolis', 'section': '8. Neural Networks and Deep Learning', 'question': 'Where is the Python TensorFlow template on Saturn Cloud?', 'course': 'machine-learning-zoomcamp', 'id': '69cd4897'}]), ('346e799a', [{'text': 'The above error happens since module scipy is not installed in the saturn cloud tensorflow image. While creating the Jupyter server resource, in the “Extra Packages” section under pip in the textbox write scipy. Below the textbox, the pip install scipy command will be displayed. This will ensure when the resource spins up, the scipy package will be automatically installed. This approach can also be followed for additional python packages.\\nSumeet Lalla', 'section': '8. Neural Networks and Deep Learning', 'question': 'Getting error module scipy not found during model training in Saturn Cloud tensorflow image', 'course': 'machine-learning-zoomcamp', 'id': '346e799a'}]), ('551461b2', [{'text': 'Problem description: Uploading the data to saturn cloud from kaggle can be time saving, specially if the dataset is large.\\nYou can just download to your local machine and then upload to a folder on saturn cloud, but there is a better solution that needs to be set once and you have access to all kaggle datasets in saturn cloud.\\nOn your notebook run:\\n!pip install -q kaggle\\nGo to Kaggle website (you need to have an account for this):\\nClick on your profile image -> Account\\nScroll down to the API box\\nClick on Create New API token\\nIt will download a json file with the name kaggle.json store on your local computer. We need to upload this file in the .kaggle folder\\nOn the notebook click on folder icon on the left upper corner\\nThis will take you to the root folder\\nClick on the .kaggle folder\\nOnce inside of the .kaggle folder upload the kaggle.json file that you downloaded\\nRun this command on your notebook:\\n!chmod 600 /home/jovyan/.kaggle/kaggle.json\\nDownload the data using this command:\\n!kaggle datasets download -d agrigorev/dino-or-dragon\\nCreate a folder to unzip your files:\\n!mkdir data\\nUnzip your files inside that folder\\n!unzip dino-or-dragon.zip -d data\\nPastor Soto', 'section': '8. Neural Networks and Deep Learning', 'question': 'How to upload kaggle data to Saturn Cloud?', 'course': 'machine-learning-zoomcamp', 'id': '551461b2'}]), ('c3ba4459', [{'text': 'In order to run tensorflow with gpu on your local machine you’ll need to setup cuda and cudnn.\\nThe process can be overwhelming. Here’s a simplified guide\\nOsman Ali', 'section': '8. Neural Networks and Deep Learning', 'question': 'How to install CUDA & cuDNN on Ubuntu 22.04', 'course': 'machine-learning-zoomcamp', 'id': 'c3ba4459'}]), ('a114ad55', [{'text': 'Problem description:\\nWhen loading saved model getting error: ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.\\nSolution description:\\nBefore loading model need to evaluate the model on input data: model.evaluate(train_ds)\\nAdded by Vladimir Yesipov', 'section': '8. Neural Networks and Deep Learning', 'question': 'Error: (ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.) when loading model.', 'course': 'machine-learning-zoomcamp', 'id': 'a114ad55'}]), ('dd3c8000', [{'text': 'Problem description:\\nWhen follow module 8.1b video to setup git in Saturn Cloud, run `ssh -T git@github.com` lead error `git@github.com: Permission denied (publickey).`\\nSolution description:\\nAlternative way, we can setup git in our Saturn Cloud env with generate SSH key in our Saturn Cloud and add it to our git account host. After it, we can access/manage our git through Saturn’s jupyter server. All steps detailed on this following tutorial: https://saturncloud.io/docs/using-saturn-cloud/gitrepo/\\nAdded by Ryan Pramana', 'section': '8. Neural Networks and Deep Learning', 'question': 'Getting error when connect git on Saturn Cloud: permission denied', 'course': 'machine-learning-zoomcamp', 'id': 'dd3c8000'}]), ('34b0ebfc', [{'text': \"Problem description:\\nGetting an error using <git clone git@github.com:alexeygrigorev/clothing-dataset-small.git>\\nThe error:\\nCloning into 'clothing-dataset'...\\nHost key verification failed.\\nfatal: Could not read from remote repository.\\nPlease make sure you have the correct access rights\\nand the repository exists.\\nSolution description:\\nwhen cloning the repo, you can also chose https - then it should work. This happens when you don't have your ssh key configured.\\n<git clone https://github.com/alexeygrigorev/clothing-dataset-small.git>\\nAdded by Gregory Morris\", 'section': '8. Neural Networks and Deep Learning', 'question': 'Host key verification failed.', 'course': 'machine-learning-zoomcamp', 'id': '34b0ebfc'}]), ('7d11d5ce', [{'text': \"Problem description\\nThe accuracy and the loss are both still the same or nearly the same while training.\\nSolution description\\nIn the homework, you should set class_mode='binary' while reading the data.\\nAlso, problem occurs when you choose the wrong optimizer, batch size, or learning rate\\nAdded by Ekaterina Kutovaia\", 'section': '8. Neural Networks and Deep Learning', 'question': 'The same accuracy on epochs', 'course': 'machine-learning-zoomcamp', 'id': '7d11d5ce'}]), ('e4e45f15', [{'text': 'Problem:\\nWhen resuming training after augmentation, the loss skyrockets (1000+ during first epoch) and accuracy settles around 0.5 – i.e. the model becomes as good as a random coin flip.\\nSolution:\\nCheck that the augmented ImageDataGenerator still includes the option “rescale” as specified in the preceding step.\\nAdded by Konrad Mühlberg', 'section': '8. Neural Networks and Deep Learning', 'question': 'Model breaking after augmentation – high loss + bad accuracy', 'course': 'machine-learning-zoomcamp', 'id': 'e4e45f15'}]), ('b3997e6f', [{'text': \"While doing:\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nmodel = tf.keras.models.load_model('model_saved.h5')\\nIf you get an error message like this:\\nValueError: The channel dimension of the inputs should be defined. The input_shape received is (None, None, None, None), where axis -1 (0-based) is the channel dimension, which found to be `None`.\\nSolution:\\nSaving a model (either yourself via model.save() or via checkpoint when save_weights_only = False) saves two things: The trained model weights (for example the best weights found during training) and the model architecture.  If the number of channels is not explicitly specified in the Input layer of the model, and is instead defined as a variable, the model architecture will not have the value in the variable stored. Therefore when the model is reloaded, it will complain about not knowing the number of channels. See the code below, in the first line, you need to specify number of channels explicitly:\\n# model architecture:\\ninputs = keras.Input(shape=(input_size, input_size, 3))\\nbase = base_model(inputs, training=False)\\nvectors = keras.layers.GlobalAveragePooling2D()(base)\\ninner = keras.layers.Dense(size_inner, activation='relu')(vectors)\\ndrop = keras.layers.Dropout(droprate)(inner)\\noutputs = keras.layers.Dense(10)(drop)\\nmodel = keras.Model(inputs, outputs)\\n(Memoona Tahira)\", 'section': '8. Neural Networks and Deep Learning', 'question': 'Missing channel value error while reloading model:', 'course': 'machine-learning-zoomcamp', 'id': 'b3997e6f'}]), ('e414df91', [{'text': \"Problem:\\nA dataset for homework is in a zipped folder. If you unzip it within a jupyter notebook by means of ! unzip command, you’ll see a huge amount of output messages about unzipping of each image. So you need to suppress this output\\nSolution:\\nExecute the next cell:\\n%%capture\\n! unzip zipped_folder_name.zip -d destination_folder_name\\nAdded by Alena Kniazeva\\nInside a Jupyter Notebook:\\nimport zipfile\\nlocal_zip = 'data.zip'\\nzip_ref = zipfile.ZipFile(local_zip, 'r')\\nzip_ref.extractall('data')\\nzip_ref.close()\", 'section': '8. Neural Networks and Deep Learning', 'question': 'How to unzip a folder with an image dataset and suppress output?', 'course': 'machine-learning-zoomcamp', 'id': 'e414df91'}]), ('f20a3479', [{'text': 'Problem:\\nWhen we run train_gen.flow_from_directory() as in video 8.5, it finds images belonging to 10 classes. Does it understand the names of classes from the names of folders? Or, there is already something going on deep behind?\\nSolution:\\nThe name of class is the folder name\\nIf you just create some random folder with the name \"xyz\", it will also be considered as a class!! The name itself is saying flow_from_directory\\na clear explanation below:\\nhttps://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720\\nAdded by Bhaskar Sarma', 'section': '8. Neural Networks and Deep Learning', 'question': 'How keras flow_from_directory know the names of classes in images?', 'course': 'machine-learning-zoomcamp', 'id': 'f20a3479'}]), ('e7af4968', [{'text': 'Problem:\\nI created a new environment in SaturnCloud and chose the image corresponding to Saturn with Tensorflow, but when I tried to fit the model it showed an error about the missing module: scipy\\nSolution:\\nInstall the module in a new cell: !pip install scipy\\nRestart the kernel and fit the model again\\nAdded by Erick Calderin', 'section': '8. Neural Networks and Deep Learning', 'question': 'Error with scipy missing module in SaturnCloud', 'course': 'machine-learning-zoomcamp', 'id': 'e7af4968'}]), ('9fad096e', [{'text': 'The command to read folders in the dataset in the tensorflow source code is:\\nfor subdir in sorted(os.listdir(directory)):\\n…\\nReference: https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py, line 563\\nThis means folders will be read in alphabetical order. For example, in the case of a folder named dino, and another named dragon, dino will read first and will have class label 0, whereas dragon will be read in next and will have class label 1.\\nWhen a Keras model predicts binary labels, it will only return one value, and this is the probability of class 1 in case of sigmoid activation function in the last dense layer with 2 neurons. The probability of class 0 can be found out by:\\nprob(class(0)) = 1- prob(class(1))\\nIn case of using from_logits to get results, you will get two values for each of the labels.\\nA prediction of 0.8 is saying the probability that the image has class label 1 (in this case dragon), is 0.8, and conversely we can infer the probability that the image has class label 0 is 0.2.\\n(Added by Memoona Tahira)', 'section': '8. Neural Networks and Deep Learning', 'question': 'How are numeric class labels determined in flow_from_directroy using binary class mode and what is meant by the single probability predicted by a binary Keras model:', 'course': 'machine-learning-zoomcamp', 'id': '9fad096e'}]), ('bcdf7407', [{'text': \"It's fine, some small changes are expected\\nAlexey Grigorev\", 'section': '8. Neural Networks and Deep Learning', 'question': 'Does the actual values matter after predicting with a neural network or it should be treated as like hood of falling in a class?', 'course': 'machine-learning-zoomcamp', 'id': 'bcdf7407'}]), ('8d1e7e20', [{'text': 'Problem:\\nI found running the wasp/bee model on my mac laptop had higher reported accuracy and lower std deviation than the HW answers. This may be because of the SGD optimizer. Running this on my mac printed a message about a new and legacy version that could be used.\\nSolution:\\nTry running the same code on google collab or another way. The answers were closer for me on collab. Another tip is to change the runtime to use T4 and the model run’s faster than just CPU\\nAdded by Quinn Avila', 'section': '8. Neural Networks and Deep Learning', 'question': 'What if your accuracy and std training loss don’t match HW?', 'course': 'machine-learning-zoomcamp', 'id': '8d1e7e20'}]), ('2023a9dc', [{'text': 'When running “model.fit(...)” an additional parameter “workers” can be specified for speeding up the data loading/generation. The default value is “1”. Try out which value between 1 and the cpu count on your system performs best.\\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\\nAdded by Sylvia Schmitt', 'section': '8. Neural Networks and Deep Learning', 'question': 'Using multi-threading for data generation in “model.fit()”', 'course': 'machine-learning-zoomcamp', 'id': '2023a9dc'}]), ('468f69ff', [{'text': 'Reproducibility for training runs can be achieved following these instructions: \\nhttps://www.tensorflow.org/versions/r2.8/api_docs/python/tf/config/experimental/enable_op_determinism\\nseed = 1234\\ntf.keras.utils.set_random_seed(seed)\\ntf.config.experimental.enable_op_determinism()\\nThis will work for a script, if this gets executed multiple times.\\nAdded by Sylvia Schmitt', 'section': '8. Neural Networks and Deep Learning', 'question': 'Reproducibility with TensorFlow using a seed point', 'course': 'machine-learning-zoomcamp', 'id': '468f69ff'}]), ('c4ff26e5', [{'text': 'Pytorch is also a deep learning framework that allows to do equivalent tasks as keras. Here is a tutorial to create a CNN from scratch using pytorch :\\nhttps://blog.paperspace.com/writing-cnns-from-scratch-in-pytorch/\\nThe functions have similar goals. The syntax can be slightly different. For the lessons and the homework, we use keras, but one can feel free to make a pull request with the equivalent with pytorch for lessons and homework!\\nMélanie Fouesnard', 'section': '8. Neural Networks and Deep Learning', 'question': 'Can we use pytorch for this lesson/homework ?', 'course': 'machine-learning-zoomcamp', 'id': 'c4ff26e5'}]), ('62722d72', [{'text': \"While training a Keras model you get the error “Failed to find data adapter that can handle input: <class 'keras.src.preprocessing.image.ImageDataGenerator'>, <class 'NoneType'>” you may have unintentionally passed the image generator instead of the dataset to the model\\ntrain_gen = ImageDataGenerator(rescale=1./255)\\ntrain_ds = train_gen.flow_from_directory(…)\\nhistory_after_augmentation = model.fit(\\ntrain_gen, # this should be train_ds!!!\\nepochs=10,\\nvalidation_data=test_gen # this should be test_ds!!!\\n)\\nThe fix is simple and probably obvious once pointed out, use the training and validation dataset (train_ds and val_ds) returned from flow_from_directory\\nAdded by Tzvi Friedman\", 'section': '8. Neural Networks and Deep Learning', 'question': 'Keras model training fails with “Failed to find data adapter”', 'course': 'machine-learning-zoomcamp', 'id': '62722d72'}]), ('d1419be1', [{'text': 'The command ‘nvidia-smi’ has a built-in function which will run it in subsequently updating it every N seconds without the need of using the command ‘watch’.\\nnvidia-smi -l <N seconds>\\nThe following command will run ‘nvidia-smi’ every 2 seconds until interrupted using CTRL+C.\\nnvidia-smi -l 2\\nAdded by Sylvia Schmitt', 'section': '8. Neural Networks and Deep Learning', 'question': 'Running ‘nvidia-smi’ in a loop without using ‘watch’', 'course': 'machine-learning-zoomcamp', 'id': 'd1419be1'}]), ('a5f6f439', [{'text': 'The Python package ‘’ is an interactive GPU process viewer similar to ‘htop’ for CPU.\\nhttps://pypi.org/project//\\nImage source: https://pypi.org/project//\\nAdded by Sylvia Schmitt', 'section': '8. Neural Networks and Deep Learning', 'question': 'Checking GPU and CPU utilization using ‘nvitop’', 'course': 'machine-learning-zoomcamp', 'id': 'a5f6f439'}]), ('879c1ec0', [{'text': \"Let’s say we define our Conv2d layer like this:\\n>> tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3))\\nIt means our input image is RGB (3 channels, 150 by 150 pixels), kernel is 3x3 and number of filters (layer’s width) is 32.\\nIf we check model.summary() we will get this:\\n_________________________________________________________________\\nLayer (type)                Output Shape              Param #\\n=================================================================\\nconv2d (Conv2D)             (None, 148, 148, 32)      896\\nSo where does 896 params come from? It’s computed like this:\\n>>> (3*3*3 +1) * 32\\n896\\n# 3x3 kernel, 3 channels RGB, +1 for bias, 32 filters\\nWhat about the number of “features” we get after the Flatten layer?\\nFor our homework model.summary() for last MaxPooling2d and Flatten layers looked like this:\\n_________________________________________________________________\\nLayer (type)                Output Shape              Param #\\n=================================================================\\nmax_pooling2d_3       (None, 7, 7, 128)         0\\nflatten (Flatten)           (None, 6272)              0\\nSo where do 6272 vectors come from? It’s computed like this:\\n>>> 7*7*128\\n6272\\n# 7x7 “image shape” after several convolutions and poolings, 128 filters\\nAdded by Andrii Larkin\", 'section': '8. Neural Networks and Deep Learning', 'question': 'Q: Where does the number of Conv2d layer’s params come from? Where does the number of “features” we get after the Flatten layer come from?', 'course': 'machine-learning-zoomcamp', 'id': '879c1ec0'}]), ('3ac604c3', [{'text': 'It’s quite useful to understand that all types of models in the course are a plain stack of layers where each layer has exactly one input tensor and one output tensor (Sequential model TF page, Sequential class).\\nYou can simply start from an “empty” model and add more and more layers in a sequential order.\\nThis mode is called “Sequential Model API”  (easier)\\nIn Alexey’s videos it is implemented as chained calls of different entities (“inputs”,“base”, “vectors”,  “outputs”) in a more advanced mode “Functional Model API”.\\nMaybe a more complicated way makes sense when you do Transfer Learning and want to separate “Base” model vs. rest, but in the HW you need to recreate the full model from scratch ⇒ I believe it is easier to work with a sequence of “similar” layers.\\nYou can read more about it in this TF2 tutorial.\\nA really useful Sequential model example is shared in the Kaggle’s “Bee or Wasp” dataset folder with code: notebook\\nAdded by Ivan Brigida\\nFresh Run on Neural Nets\\nWhile correcting an error on neural net architecture, it is advised to do fresh run by restarting kernel, else the model learns on top of previous runs.\\nAdded by Abhijit Chakraborty', 'section': '8. Neural Networks and Deep Learning', 'question': 'Sequential vs. Functional Model Modes in Keras (TF2)', 'course': 'machine-learning-zoomcamp', 'id': '3ac604c3'}]), ('0315aa96', [{'text': \"I found this code snippet fixed my OOM errors, as I have an Nvidia GPU. Can't speak to OOM errors on CPU, though.\\nhttps://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth\\n```\\nphysical_devices = tf.configlist_physical_devices('GPU')\\ntry:\\ntf.config.experimental.set_memory_growth(physical_devices[0],True)\\nexcept:\\n# Invalid device or cannot modify virtual devices once initialized.\\npass\\n```\", 'section': '8. Neural Networks and Deep Learning', 'question': 'Out of memory errors when running tensorflow', 'course': 'machine-learning-zoomcamp', 'id': '0315aa96'}]), ('daf84bc3', [{'text': 'When training the models, in the fit function, you can specify the number of workers/threads.\\nThe number of threads apparently also works for GPUs, and came very handy in google colab for the T4 GPU, since it was very very slow, and workers default value is 1.\\nI changed the workers variable to 2560, following this thread in stackoverflow. I am using the free T4 GPU.  (https://stackoverflow.com/questions/68208398/how-to-find-the-number-of-cores-in-google-colabs-gpu)\\nAdded by Ibai Irastorza', 'section': '8. Neural Networks and Deep Learning', 'question': 'Model training very slow in google colab with T4 GPU', 'course': 'machine-learning-zoomcamp', 'id': 'daf84bc3'}]), ('1e956ca7', [{'text': 'From the keras documentation:\\nDeprecated: tf.keras.preprocessing.image.ImageDataGenerator is not recommended for new code. Prefer loading images with tf.keras.utils.image_dataset_from_directory and transforming the output tf.data.Dataset with preprocessing layers. For more information, see the tutorials for loading images and augmenting images, as well as the preprocessing layer guide.\\nHrithik Kumar Advani', 'section': '9. Serverless Deep Learning', 'question': 'Using image_dataset_from_directory instead of ImageDataGeneratorn for loading images', 'course': 'machine-learning-zoomcamp', 'id': '1e956ca7'}]), ('3ee083ab', [{'text': 'TODO', 'section': '9. Serverless Deep Learning', 'question': 'How to get started with Week 9?', 'course': 'machine-learning-zoomcamp', 'id': '3ee083ab'}]), ('f826cba4', [{'text': 'The week 9 uses a link to github to fetch the models.\\nThe original link was moved to here:\\nhttps://github.com/DataTalksClub/machine-learning-zoomcamp/releases', 'section': '9. Serverless Deep Learning', 'question': 'Where is the model for week 9?', 'course': 'machine-learning-zoomcamp', 'id': 'f826cba4'}]), ('60fa95ed', [{'text': 'Solution description\\nIn the unit 9.6, Alexey ran the command echo ${REMOTE_URI} which turned the URI address in the terminal. There workaround is to set a local variable (REMOTE_URI) and assign your URI address in the terminal and use it to login the registry, for instance, REMOTE_URI=2278222782.dkr.ecr.ap-south-1.amazonaws.com/clothing-tflite-images (fake address). One caveat is that you will lose this variable once the session is terminated.\\nI also had the same problem on Ubuntu terminal. I executed the following two commands:\\n$ export REMOTE_URI=1111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\\n$ echo $REMOTE_URI\\n111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\\nNote: 1. no curly brackets (e.g. echo ${REMOTE_URI}) needed unlike in video 9.6,\\n2. Replace REMOTE_URI with your URI\\n(Bhaskar Sarma)', 'section': '9. Serverless Deep Learning', 'question': 'Executing the command echo ${REMOTE_URI} returns nothing.', 'course': 'machine-learning-zoomcamp', 'id': '60fa95ed'}]), ('53f3ee10', [{'text': 'The command aws ecr get-login --no-include-email returns an invalid choice error:\\nThe solution is to use the following command instead:  aws ecr get-login-password\\nCould simplify the login process with, just replace the <ACCOUNT_NUMBER> and <REGION> with your values:\\nexport PASSWORD=`aws ecr get-login-password`\\ndocker login -u AWS -p $PASSWORD <ACCOUNT_NUMBER>.dkr.ecr.<REGION>.amazonaws.com/clothing-tflite-images\\nAdded by Martin Uribe', 'section': '9. Serverless Deep Learning', 'question': 'Getting a syntax error while trying to get the password from aws-cli', 'course': 'machine-learning-zoomcamp', 'id': '53f3ee10'}]), ('93aa4278', [{'text': 'We can use the keras.models.Sequential() function to pass many parameters of the cnn at once.\\nKrishna Anand', 'section': '9. Serverless Deep Learning', 'question': 'Pass many parameters in the model at once', 'course': 'machine-learning-zoomcamp', 'id': '93aa4278'}]), ('0edeb016', [{'text': 'This error is produced sometimes when building your docker image from the Amazon python base image.\\nSolution description: The following could solve the problem.\\nUpdate your docker desktop if you haven’t done so.\\nOr restart docker desktop and terminal and then build the image all over again.\\nOr if all else fails, first run the following command: DOCKER_BUILDKIT=0  docker build .  then build your image.\\n(optional) Added by Odimegwu David', 'section': '9. Serverless Deep Learning', 'question': 'Getting  ERROR [internal] load metadata for public.ecr.aws/lambda/python:3.8', 'course': 'machine-learning-zoomcamp', 'id': '0edeb016'}]), ('ba186de6', [{'text': \"When trying to run the command  !ls -lh in windows jupyter notebook  , I was getting an error message that says “'ls' is not recognized as an internal or external command,operable program or batch file.\\nSolution description :\\nInstead of !ls -lh , you can use this command !dir , and you will get similar output\\nAsia Saeed\", 'section': '9. Serverless Deep Learning', 'question': \"Problem: 'ls' is not recognized as an internal or external command, operable program or batch file.\", 'course': 'machine-learning-zoomcamp', 'id': 'ba186de6'}]), ('da2f1cf4', [{'text': 'When I run   import tflite_runtime.interpreter as tflite , I get an error message says “ImportError: generic_type: type \"InterpreterWrapper\" is already registered!”\\nSolution description\\nThis error occurs when you import both tensorflow  and tflite_runtime.interpreter  “import tensorflow as tf” and “import tflite_runtime.interpreter as tflite” in the same notebook.  To fix the issue, restart the kernel and import only tflite_runtime.interpreter \" import tflite_runtime.interpreter as tflite\".\\nAsia Saeed', 'section': '9. Serverless Deep Learning', 'question': 'ImportError: generic_type: type \"InterpreterWrapper\" is already registered!', 'course': 'machine-learning-zoomcamp', 'id': 'da2f1cf4'}]), ('7fd648ca', [{'text': 'Problem description:\\nIn command line try to do $ docker build -t dino_dragon\\ngot this Using default tag: latest\\n[2022-11-24T06:48:47.360149000Z][docker-credential-desktop][W] Windows version might not be up-to-date: The system cannot find the file specified.\\nerror during connect: This error may indicate that the docker daemon is not running.: Post\\n.\\nSolution description:\\nYou need to make sure that Docker is not stopped by a third-party program.\\nAndrei Ilin', 'section': '9. Serverless Deep Learning', 'question': 'Windows version might not be up-to-date', 'course': 'machine-learning-zoomcamp', 'id': '7fd648ca'}]), ('42c09143', [{'text': 'When running docker build -t dino-dragon-model it returns the above error\\nThe most common source of this error in this week is because Alex video shows a version of the wheel with python 8, we need to find a wheel with the version that we are working on. In this case python 9. Another common error is to copy the link, this will also produce the same error, we need to download the raw format:\\nhttps://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp39-cp39-linux_x86_64.whl\\nPastor Soto', 'section': '9. Serverless Deep Learning', 'question': 'WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available', 'course': 'machine-learning-zoomcamp', 'id': '42c09143'}]), ('d6d534fc', [{'text': 'Problem description:\\nIn video 9.6, after installing aswcli, we should configure it with aws configure . There it asks for Access Key ID, Secret Access Key, Default Region Name and also Default output format. What we should put for Default output format? Leaving it as  None is okay?\\nSolution description:\\nYes, in my I case I left everything as the provided defaults (except, obviously, the Access key and the secret access key)\\nAdded by Bhaskar Sarma', 'section': '9. Serverless Deep Learning', 'question': 'How to do AWS configure after installing awscli', 'course': 'machine-learning-zoomcamp', 'id': 'd6d534fc'}]), ('b2c0c554', [{'text': 'Problem:\\nWhile passing local testing of the lambda function without issues, trying to test the same input with a running docker instance results in an error message like\\n{‘errorMessage’: ‘Unable to marshal response: Object of type float32 is not JSON serializable’, ‘errorType’: ‘Runtime.MarshalError’, ‘requestId’: ‘f155492c-9af2-4d04-b5a4-639548b7c7ac’, ‘stackTrace’: []}\\nThis happens when a model (in this case the dino vs dragon model) returns individual estimation values as numpy float32 values (arrays). They need to be converted individually to base-Python floats in order to become “serializable”.\\nSolution:\\nIn my particular case, I set up the dino vs dragon model in such a way as to return a label + predicted probability for each class as follows (below is a two-line extract of function predict() in the lambda_function.py):\\npreds = [interpreter.get_tensor(output_index)[0][0], \\\\\\n1-interpreter.get_tensor(output_index)[0][0]]\\nIn which case the above described solution will look like this:\\npreds = [float(interpreter.get_tensor(output_index)[0][0]), \\\\\\nfloat(1-interpreter.get_tensor(output_index)[0][0])]\\nThe rest can be made work by following the chapter 9 (and/or chapter 5!) lecture videos step by step.\\nAdded by Konrad Muehlberg', 'section': '9. Serverless Deep Learning', 'question': 'Object of type float32 is not JSON serializable', 'course': 'machine-learning-zoomcamp', 'id': 'b2c0c554'}]), ('819afebc', [{'text': 'I had this error when running the command line : interpreter.set_tensor(input_index, x) that can be seen in the video 9.3 around 12 minutes.\\nValueError: Cannot set tensor: Got value of type UINT8 but expected type FLOAT32 for input 0, name: serving_default_conv2d_input:0\\nThis is because the X is an int but a float is expected.\\nSolution:\\nI found this solution from this question here https://stackoverflow.com/questions/76102508/valueerror-cannot-set-tensor-got-value-of-type-float64-but-expected-type-float :\\n# Need to convert to float32 before set_tensor\\nX = np.float32(X)\\nThen, it works. I work with tensorflow 2.15.0, maybe the fact that this version is more recent involves this change ?\\nAdded by Mélanie Fouesnard', 'section': '9. Serverless Deep Learning', 'question': 'Error with the line “interpreter.set_tensor(input_index, X”)', 'course': 'machine-learning-zoomcamp', 'id': '819afebc'}]), ('74551c54', [{'text': 'To check your file size using the powershell terminal, you can do the following command lines:\\n$File = Get-Item -Path path_to_file\\n$FileSize = (Get-Item -Path $FilePath).Length\\nNow you can check the size of your file, for example in MB:\\nWrite-host \"MB\":($FileSize/1MB)\\nSource: https://www.sharepointdiary.com/2020/10/powershell-get-file-size.html#:~:text=To%20get%20the%20size%20of,the%20file%2C%20including%20its%20size.\\nAdded by Mélanie Fouesnard', 'section': '9. Serverless Deep Learning', 'question': 'How to easily get file size in powershell terminal ?', 'course': 'machine-learning-zoomcamp', 'id': '74551c54'}]), ('4d98cd09', [{'text': 'I wanted to understand how lambda container images work in depth and how lambda functions are initialized, for this reason, I found the following documentation\\nhttps://docs.aws.amazon.com/lambda/latest/dg/images-create.html\\nhttps://docs.aws.amazon.com/lambda/latest/dg/runtimes-api.html\\nAdded by Alejandro aponte', 'section': '9. Serverless Deep Learning', 'question': 'How do Lambda container images work?', 'course': 'machine-learning-zoomcamp', 'id': '4d98cd09'}]), ('59a81fd5', [{'text': 'The docker image for aws lambda can be created and pushed to aws ecr and the same can be exposed as a REST API through APIGatewayService in a single go using AWS Serverless Framework. Refer the below article for a detailed walkthrough.\\nhttps://medium.com/hoonio/deploy-containerized-serverless-flask-to-aws-lambda-c0eb87c1404d\\nAdded by Sumeet Lalla', 'section': '9. Serverless Deep Learning', 'question': 'How to use AWS Serverless Framework to deploy on AWS Lambda and expose it as REST API through APIGatewayService?', 'course': 'machine-learning-zoomcamp', 'id': '59a81fd5'}]), ('35dbd6e2', [{'text': 'Problem:\\nWhile trying to build docker image in Section 9.5 with the command:\\ndocker build -t clothing-model .\\nIt throws a pip install error for the tflite runtime whl\\nERROR: failed to solve: process \"/bin/sh -c pip install https://github.com/alexeygrigorev/tflite-aws-lambda/blob/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\" did not complete successfully: exit code: 1\\nTry to use this link: https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\\nIf the link above does not work:\\nThe problem is because of the arm architecture of the M1. You will need to run the code on a PC or Ubuntu OS.\\nOr try the code bellow.\\nAdded by Dashel Ruiz Perez\\nSolution:\\nTo build the Docker image, use the command:\\ndocker build --platform linux/amd64 -t clothing-model .\\nTo run the built image, use the command:\\ndocker run -it --rm -p 8080:8080 --platform linux/amd64 clothing-model:latest\\nAdded by Daniel Egbo', 'section': '9. Serverless Deep Learning', 'question': 'Error building docker image on M1 Mac', 'course': 'machine-learning-zoomcamp', 'id': '35dbd6e2'}]), ('e5fe9efe', [{'text': \"Problem: Trying to test API gateway in 9.7 - API Gateway: Exposing the Lambda Function, running: $ python test.py\\nWith error message:\\n{'message': 'Missing Authentication Token'}\\nSolution:\\nNeed to get the deployed API URL for the specific path you are invoking. Example:\\nhttps://<random string>.execute-api.us-east-2.amazonaws.com/test/predict\\nAdded by Andrew Katoch\", 'section': '9. Serverless Deep Learning', 'question': 'Error invoking API Gateway deploy API locally', 'course': 'machine-learning-zoomcamp', 'id': 'e5fe9efe'}]), ('5c043c62', [{'text': 'Problem: When trying to install tflite_runtime with\\n!pip install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime\\none gets an error message above.\\nSolution:\\nfflite_runtime is only available for the os-python version combinations that can be found here: https://google-coral.github.io/py-repo/tflite-runtime/\\nyour combination must be missing here\\nyou can see if any of these work for you https://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite\\nand install the needed one using pip\\neg\\npip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\\nas it is done in the lectures code:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/09-serverless/code/Dockerfile#L4\\nAlternatively, use a virtual machine (with VM VirtualBox, for example) with a Linux system. The other way is to run a code at a virtual machine within cloud service, for example you can use Vertex AI Workbench at GCP (notebooks and terminals are provided there, so all tasks may be performed).\\nAdded by Alena Kniazeva, modified by Alex Litvinov', 'section': '9. Serverless Deep Learning', 'question': 'Error: Could not find a version that satisfies the requirement tflite_runtime (from versions:none)', 'course': 'machine-learning-zoomcamp', 'id': '5c043c62'}]), ('af0739da', [{'text': 'docker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system.\\nYou need to restart the docker services to get rid of the above error\\nKrishna Anand', 'section': '9. Serverless Deep Learning', 'question': 'Docker run error', 'course': 'machine-learning-zoomcamp', 'id': 'af0739da'}]), ('451bc25d', [{'text': 'The docker image can be saved/exported to tar format in local machine using the below command:\\ndocker image save <image-name> -o <name-of-tar-file.tar>\\nThe individual layers of the docker image for the filesystem content can be viewed by extracting the layer.tar present in the <name-of-tar-file.tar> created from above.\\nSumeet Lalla', 'section': '9. Serverless Deep Learning', 'question': 'Save Docker Image to local machine and view contents', 'course': 'machine-learning-zoomcamp', 'id': '451bc25d'}]), ('ea2e7458', [{'text': 'On vscode running jupyter notebook. After I ‘pip install pillow’, my notebook did not recognize using the import for example from PIL import image. After restarting the jupyter notebook the imports worked.\\nQuinn Avila', 'section': '9. Serverless Deep Learning', 'question': 'Jupyter notebook not seeing package', 'course': 'machine-learning-zoomcamp', 'id': 'ea2e7458'}]), ('6ce8e875', [{'text': 'Due to experimenting back and forth so much without care for storage, I just ran out of it on my 30-GB AWS instance. It turns out that deleting docker images does not actually free up any space as you might expect. After removing images, you also need to run docker system prune', 'section': '9. Serverless Deep Learning', 'question': 'Running out of space for AWS instance.', 'course': 'machine-learning-zoomcamp', 'id': '6ce8e875'}]), ('b50e9e2b', [{'text': 'Using the 2.14 version with python 3.11 works fine.\\nIn case it doesn’t work, I tried with tensorflow 2.4.4 whl, however, make sure to run it on top of supported python versions like 3.8, else there will be issues installing tf==2.4.4\\nAdded by Abhijit Chakraborty', 'section': '9. Serverless Deep Learning', 'question': 'Using Tensorflow 2.15 for AWS deployment', 'course': 'machine-learning-zoomcamp', 'id': 'b50e9e2b'}]), ('29311ef5', [{'text': 'see here', 'section': '9. Serverless Deep Learning', 'question': 'Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…”', 'course': 'machine-learning-zoomcamp', 'id': '29311ef5'}]), ('1e0dc11c', [{'text': 'Sign in to the AWS Console: Log in to the AWS Console.\\nNavigate to IAM: Go to the IAM service by clicking on \"Services\" in the top left corner and selecting \"IAM\" under the \"Security, Identity, & Compliance\" section.\\nCreate a new policy: In the left navigation pane, select \"Policies\" and click on \"Create policy.\"\\nSelect the service and actions:\\nClick on \"JSON\" and copy and paste the JSON policy you provided earlier for the specific ECR actions.\\nReview and create the policy:\\nClick on \"Review policy.\"\\nProvide a name and description for the policy.\\nClick on \"Create policy.\"\\nJSON policy:\\n{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"VisualEditor0\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"ecr:CreateRepository\",\\n\"ecr:GetAuthorizationToken\",\\n\"ecr:BatchCheckLayerAvailability\",\\n\"ecr:BatchGetImage\",\\n\"ecr:InitiateLayerUpload\",\\n\"ecr:UploadLayerPart\",\\n\"ecr:CompleteLayerUpload\",\\n\"ecr:PutImage\"\\n],\\n\"Resource\": \"*\"\\n}\\n]\\n}\\nAdded by: Daniel Muñoz-Viveros\\nERROR: failed to solve: public.ecr.aws/lambda/python:3.10: error getting credentials - err: exec: \"docker-credential-desktop.exe\": executable file not found in $PATH, out: ``\\n(WSL2 system)\\nSolved: Delete the file ~/.docker/config.json\\nYishan Zhan', 'section': '9. Serverless Deep Learning', 'question': 'What IAM permission policy is needed to complete Week 9: Serverless?', 'course': 'machine-learning-zoomcamp', 'id': '1e0dc11c'}]), ('1078aeb7', [{'text': 'Add the next lines to vim /etc/docker/daemon.json\\n{\\n\"dns\": [\"8.8.8.8\", \"8.8.4.4\"]\\n}\\nThen, restart docker:  sudo service docker restart\\nIbai Irastorza', 'section': '9. Serverless Deep Learning', 'question': 'Docker Temporary failure in name resolution', 'course': 'machine-learning-zoomcamp', 'id': '1078aeb7'}]), ('7daaca73', [{'text': \"Solution: add compile = False to the load_model function\\nkeras.models.load_model('model_name.h5', compile=False)\\nNadia Paz\", 'section': '9. Serverless Deep Learning', 'question': 'Keras model *.h5 doesn’t load. Error: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`', 'course': 'machine-learning-zoomcamp', 'id': '7daaca73'}]), ('0cfbe2e2', [{'text': 'This deployment setup can be tested locally using AWS RIE (runtime interface emulator).\\nBasically, if your Docker image was built upon base AWS Lambda image (FROM public.ecr.aws/lambda/python:3.10) - just use certain ports for “docker run” and a certain “localhost link” for testing:\\ndocker run -it --rm -p 9000:8080 name\\nThis command runs the image as a container and starts up an endpoint locally at:\\nlocalhost:9000/2015-03-31/functions/function/invocations\\nPost an event to the following endpoint using a curl command:\\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \\'{}\\'\\nExamples of curl testing:\\n* windows testing:\\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \"{\\\\\"url\\\\\": \\\\\"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\\\\\"}\"\\n* unix testing:\\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \\'{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}\\'\\nIf during testing you encounter an error like this:\\n# {\"errorMessage\": \"Unable to marshal response: Object of type float32 is not JSON serializable\", \"errorType\": \"Runtime.MarshalError\", \"requestId\": \"7ea5d17a-e0a2-48d5-b747-a16fc530ed10\", \"stackTrace\": []}\\njust turn your response at lambda_handler() to string - str(result).\\nAdded by Andrii Larkin', 'section': '9. Serverless Deep Learning', 'question': 'How to test AWS Lambda + Docker locally?', 'course': 'machine-learning-zoomcamp', 'id': '0cfbe2e2'}]), ('1460fb65', [{'text': 'Make sure all codes in test.py dont have any dependencies with tensorflow library. One of most common reason that lead the this error is tflite still imported from tensorflow. Change import tensorflow.lite as tflite to import tflite_runtime.interpreter as tflite\\nAdded by Ryan Pramana', 'section': '9. Serverless Deep Learning', 'question': '\"Unable to import module \\'lambda_function\\': No module named \\'tensorflow\\'\" when run python test.py', 'course': 'machine-learning-zoomcamp', 'id': '1460fb65'}]), ('d4f9efdc', [{'text': 'I’ve tried to do everything in Google Colab. Here is a way to work with Docker in Google Colab:\\nhttps://gist.github.com/mwufi/6718b30761cd109f9aff04c5144eb885\\n\\uec03%%shell\\npip install udocker\\nudocker --allow-root install\\n\\uec02!udocker --allow-root run hello-world\\nAdded by Ivan Brigida\\nLambda API Gateway errors:\\n`Authorization header requires \\'Credential\\' parameter. Authorization header requires \\'Signature\\' parameter. Authorization header requires \\'SignedHeaders\\' parameter. Authorization header requires existence of either a \\'X-Amz-Date\\' or a \\'Date\\' header.`\\n`Missing Authentication Token`\\nimport boto3\\nclient = boto3.client(\\'apigateway\\')\\nresponse = client.test_invoke_method(\\nrestApiId=\\'your_rest_api_id\\',\\nresourceId=\\'your_resource_id\\',\\nhttpMethod=\\'POST\\',\\npathWithQueryString=\\'/test/predict\\', #depend how you set up the api\\nbody=\\'{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}\\'\\n)\\nprint(response[\\'body\\'])\\nYishan Zhan\\nUnable to run pip install tflite_runtime from github wheel links?\\nTo overcome this issue, you can download the whl file to your local project folder and in the Docker file add the following lines:\\nCOPY <file-name> .\\nRUN pip install <file-name>\\nAbhijit Chakraborty', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Install Docker (udocker) in Google Colab', 'course': 'machine-learning-zoomcamp', 'id': 'd4f9efdc'}]), ('6a417bfe', [{'text': 'TODO', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'How to get started with Week 10?', 'course': 'machine-learning-zoomcamp', 'id': '6a417bfe'}]), ('ed8b300d', [{'text': 'Running a CNN on your CPU can take a long time and once you’ve run out of free time on some cloud providers, it’s time to pay up. Both can be tackled by installing tensorflow with CUDA support on your local machine if you have the right hardware.\\nI was able to get it working by using the following resources:\\nCUDA on WSL :: CUDA Toolkit Documentation (nvidia.com)\\nInstall TensorFlow with pip\\nStart Locally | PyTorch\\nI included the link to PyTorch so that you can get that one installed and working too while everything is fresh on your mind. Just select your options, and for Computer Platform, I chose CUDA 11.7 and it worked for me.\\nAdded by Martin Uribe', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'How to install Tensorflow in Ubuntu WSL2', 'course': 'machine-learning-zoomcamp', 'id': 'ed8b300d'}]), ('a64aed6b', [{'text': 'If you are running tensorflow on your own machine and you start getting the following errors:\\nAllocator (GPU_0_bfc) ran out of memory trying to allocate 6.88GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\\nTry adding this code in a cell at the beginning of your notebook:\\nconfig = tf.compat.v1.ConfigProto()\\nconfig.gpu_options.allow_growth = True\\nsession = tf.compat.v1.Session(config=config)\\nAfter doing this most of my issues went away. I say most because there was one instance when I still got the error once more, but only during one epoch. I ran the code again, right after it finished, and I never saw the error again.\\nAdded by Martin Uribe', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Getting: Allocator ran out of memory errors?', 'course': 'machine-learning-zoomcamp', 'id': 'a64aed6b'}]), ('727238ee', [{'text': 'In session 10.3, when creating the virtual environment with pipenv and trying to run the script gateway.py, you might get this error:\\nTypeError: Descriptors cannot not be created directly.\\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\\n1. Downgrade the protobuf package to 3.20.x or lower.\\n2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\\nThis will happen if your version of protobuf is one of the newer ones. As a workaround, you can fix the protobuf version to an older one. In my case I got around the issue by creating the environment with:\\npipenv install --python 3.9.13 requests grpcio==1.42.0 flask gunicorn \\\\\\nkeras-image-helper tensorflow-protobuf==2.7.0 protobuf==3.19.6\\nAdded by Ángel de Vicente', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Problem with recent version of protobuf', 'course': 'machine-learning-zoomcamp', 'id': '727238ee'}]), ('85d4901d', [{'text': 'Due to the uncertainties associated with machines, sometimes you can get the error message like this when you try to run a docker command:\\n”Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?”\\nSolution: The solution is simple. The Docker Desktop might no longer be connecting to the WSL Linux distro. What you need to do is go to your Docker Desktop setting and then click on resources. Under resources, click on WSL Integration. You will get a tab like the image below:\\nJust enable additional distros. That’s all. Even if the additional distro is the same as the default WSL distro.\\nOdimegwu David', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'WSL Cannot Connect To Docker Daemon', 'course': 'machine-learning-zoomcamp', 'id': '85d4901d'}]), ('df023a13', [{'text': 'In case the HPA instance does not run correctly even after installing the latest version of Metrics Server from the components.yaml manifest with:\\n>>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\\nAnd the targets still appear as <unknown>\\nRun >>kubectl edit deploy -n kube-system metrics-server\\nAnd search for this line:\\nargs:\\n- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\\nAdd this line in the middle:  - --kubelet-insecure-tls\\nSo that it stays like this:\\nargs:\\n- --kubelet-insecure-tls\\n- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\\nSave and run again >>kubectl get hpa\\nAdded by Marilina Orihuela', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'HPA instance doesn’t run properly', 'course': 'machine-learning-zoomcamp', 'id': 'df023a13'}]), ('48e92d65', [{'text': 'In case the HPA instance does not run correctly even after installing the latest version of Metrics Server from the components.yaml manifest with:\\n>>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\\nAnd the targets still appear as <unknown>\\nRun the following command:\\nkubectl apply -f https://raw.githubusercontent.com/Peco602/ml-zoomcamp/main/10-kubernetes/kube-config/metrics-server-deployment.yaml\\nWhich uses a metrics server deployment file already embedding the - --kubelet-insecure-tls option.\\nAdded by Giovanni Pecoraro', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'HPA instance doesn’t run properly (easier solution)', 'course': 'machine-learning-zoomcamp', 'id': '48e92d65'}]), ('1685cae4', [{'text': \"When I run pip install grpcio==1.42.0 tensorflow-serving-api==2.7.0 to install the libraries in windows machine,  I was getting the below error :\\nERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\\\\\Users\\\\\\\\Asia\\\\\\\\anaconda3\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\google\\\\\\\\protobuf\\\\\\\\internal\\\\\\\\_api_implementation.cp39-win_amd64.pyd'\\nConsider using the `--user` option or check the permissions.\\nSolution description :\\nI was able to install the libraries using below command:\\npip --user install grpcio==1.42.0 tensorflow-serving-api==2.7.0\\nAsia Saeed\", 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Could not install packages due to an OSError: [WinError 5] Access is denied', 'course': 'machine-learning-zoomcamp', 'id': '1685cae4'}]), ('4fb7b21e', [{'text': 'Problem description\\nI was getting the below error message when I run gateway.py after modifying the code & creating virtual environment in  video 10.3 :\\nFile \"C:\\\\Users\\\\Asia\\\\Data_Science_Code\\\\Zoompcamp\\\\Kubernetes\\\\gat.py\", line 9, in <module>\\nfrom tensorflow_serving.apis import predict_pb2\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow_serving\\\\apis\\\\predict_pb2.py\", line 14, in <module>\\nfrom tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow\\\\core\\\\framework\\\\tensor_pb2.py\", line 14, in <module>\\nfrom tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow\\\\core\\\\framework\\\\resource_handle_pb2.py\", line 14, in <module>\\nfrom tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow\\\\core\\\\framework\\\\tensor_shape_pb2.py\", line 36, in <module>\\n_descriptor.FieldDescriptor(\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\google\\\\protobuf\\\\descriptor.py\", line 560, in __new__\\n_message.Message._CheckCalledFromGeneratedFile()\\nTypeError: Descriptors cannot not be created directly.\\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\\n1. Downgrade the protobuf package to 3.20.x or lower.\\n2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\\nSolution description:\\nIssue has been resolved by downgrading protobuf to version 3.20.1.\\npipenv install protobuf==3.20.1\\nAsia Saeed', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'TypeError: Descriptors cannot not be created directly.', 'course': 'machine-learning-zoomcamp', 'id': '4fb7b21e'}]), ('8bd3bfc2', [{'text': 'To install kubectl on windows using the terminal in vscode (powershell), I followed this tutorial: https://medium.com/@ggauravsigra/install-kubectl-on-windows-af77da2e6fff\\nI first downloaded kubectl with curl, with these command lines: https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/#install-kubectl-binary-with-curl-on-windows\\nAt step 3, I followed the tutorial with the copy of the exe file in a specific folder on C drive.\\nThen I added this folder path to PATH in my environment variables.\\nKind can be installed the same way with the curl command on windows, by specifying a folder that will be added to the path environment variable.\\nAdded by Mélanie Fouesnard', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'How to install easily kubectl on windows ?', 'course': 'machine-learning-zoomcamp', 'id': '8bd3bfc2'}]), ('03b5fc59', [{'text': \"First you need to launch a powershell terminal with administrator privilege.\\nFor this we need to install choco library first through the following syntax in powershell:\\nSet-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\\nKrishna Anand\", 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Install kind through choco library', 'course': 'machine-learning-zoomcamp', 'id': '03b5fc59'}]), ('7c31bc9a', [{'text': 'If you are having challenges installing Kind through the Windows Powershell as provided on the website and Choco Library as I did, you can simply install Kind through Go.\\n> Download and Install Go (https://go.dev/doc/install)\\n> Confirm installation by typing the following in Command Prompt -  go version\\n> Proceed by installing Kind by following this command - go install sigs.k8s.io/kind@v0.20.0\\n>Confirm Installation kind --version\\nIt works perfectly.', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Install Kind via Go package', 'course': 'machine-learning-zoomcamp', 'id': '7c31bc9a'}]), ('605efc12', [{'text': \"I ran into an issue where kubectl wasn't working.\\nI kept getting the following error:\\nkubectl get service\\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\\nI searched online for a resolution, but everyone kept talking about creating an environment variable and creating some admin.config file in my home directory.\\nAll hogwash.\\nThe solution to my problem was to just start over.\\nkind delete cluster\\nrm -rf ~/.kube\\nkind create cluster\\nNow when I try the same command again:\\nkubectl get service\\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\\nkubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   53s\\nAdded by Martin Uribe\", 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'The connection to the server localhost:8080 was refused - did you specify the right host or port?', 'course': 'machine-learning-zoomcamp', 'id': '605efc12'}]), ('c5cde96c', [{'text': 'Problem description\\nDue to experimenting back and forth so much without care for storage, I just ran out of it on my 30-GB AWS instance.\\nMy first reflex was to remove some zoomcamp directories, but of course those are mostly code so it didn’t help much.\\nSolution description\\n> docker images\\nrevealed that I had over 20 GBs worth of superseded / duplicate models lying around, so I proceeded to > docker rmi\\na bunch of those — but to no avail!\\nIt turns out that deleting docker images does not actually free up any space as you might expect. After removing images, you also need to run\\n> docker system prune\\nSee also: https://stackoverflow.com/questions/36799718/why-removing-docker-containers-and-images-does-not-free-up-storage-space-on-wind\\nAdded by Konrad Mühlberg', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Running out of storage after building many docker images', 'course': 'machine-learning-zoomcamp', 'id': 'c5cde96c'}]), ('d45d2da6', [{'text': 'Yes, the question does require for you to specify values for CPU and memory in the yaml file, however the question that it is use in the form only refers to the port which do have a define correct value for this specific homework.\\nPastor Soto', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'In HW10 Q6 what does it mean “correct value for CPU and memory”? Aren’t they arbitrary?', 'course': 'machine-learning-zoomcamp', 'id': 'd45d2da6'}]), ('59823c72', [{'text': 'In Kubernetes resource specifications, such as CPU requests and limits, the \"m\" stands for milliCPU, which is a unit of computing power. It represents one thousandth of a CPU core.\\ncpu: \"100m\" means the container is requesting 100 milliCPUs, which is equivalent to 0.1 CPU core.\\ncpu: \"500m\" means the container has a CPU limit of 500 milliCPUs, which is equivalent to 0.5 CPU core.\\nThese values are specified in milliCPUs to allow fine-grained control over CPU resources. It allows you to express CPU requirements and limits in a more granular way, especially in scenarios where your application might not need a full CPU core.\\nAdded by Andrii Larkin', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Why cpu vals for Kubernetes deployment.yaml look like “100m” and “500m”? What does \"m\" mean?', 'course': 'machine-learning-zoomcamp', 'id': '59823c72'}]), ('665f7b27', [{'text': 'Problem: Failing to load docker-image to cluster (when you’ved named a cluster)\\nkind load docker-image zoomcamp-10-model:xception-v4-001\\nERROR: no nodes found for cluster \"kind\"\\nSolution: Specify cluster name with -n\\nkind -n clothing-model load docker-image zoomcamp-10-model:xception-v4-001\\nAndrew Katoch', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Kind cannot load docker image', 'course': 'machine-learning-zoomcamp', 'id': '665f7b27'}]), ('0a406fe0', [{'text': \"Problem: I download kind from the next command:\\ncurl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.17.0/kind-windows-amd64\\nWhen I try\\nkind --version\\nI get: 'kind' is not recognized as an internal or external command, operable program or batch file\\nSolution: The default name of executable is kind-windows-amd64.exe, so that you have to rename this file to  kind.exe. Put this file in specific folder, and add it to PATH\\nAlejandro Aponte\", 'section': '10. Kubernetes and TensorFlow Serving', 'question': \"'kind' is not recognized as an internal or external command, operable program or batch file. (In Windows)\", 'course': 'machine-learning-zoomcamp', 'id': '0a406fe0'}]), ('64b209b0', [{'text': 'Using kind with Rootless Docker or Rootless Podman requires some changes on the system (Linux), see kind – Rootless (k8s.io).\\nSylvia Schmitt', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Running kind on Linux with Rootless Docker or Rootless Podman', 'course': 'machine-learning-zoomcamp', 'id': '64b209b0'}]), ('518c4cb8', [{'text': 'Deploy and Access the Kubernetes Dashboard\\nLuke', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Kubernetes-dashboard', 'course': 'machine-learning-zoomcamp', 'id': '518c4cb8'}]), ('00882c83', [{'text': 'Make sure you are on AWS CLI v2 (check with aws --version)\\nhttps://docs.aws.amazon.com/cli/latest/userguide/cliv2-migration-instructions.html', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Correct AWS CLI version for eksctl', 'course': 'machine-learning-zoomcamp', 'id': '00882c83'}]), ('d6d483ce', [{'text': 'Problem Description:\\nIn video 10.3, when I was testing a flask service, I got the above error. I ran docker run .. in one terminal. When in second terminal I run python gateway.py, I get the above error.\\nSolution: This error has something to do with versions of Flask and Werkzeug. I got the same error, if I just import flask with from flask import Flask.\\nBy running pip freeze > requirements.txt,I found that their versions are Flask==2.2.2 and Werkzeug==2.2.2. This error appears while using an old version of werkzeug (2.2.2) with new version of flask (2.2.2). I solved it by pinning version of Flask into an older version with pipenv install Flask==2.1.3.\\nAdded by Bhaskar Sarma', 'section': '10. Kubernetes and TensorFlow Serving', 'question': \"TypeError: __init__() got an unexpected keyword argument 'unbound_message' while importing Flask\", 'course': 'machine-learning-zoomcamp', 'id': 'd6d483ce'}]), ('f9711723', [{'text': 'As per AWS documentation:\\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html\\nYou need to do: (change the fields in red)\\naws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com\\nAlternatively you can run the following command without changing anything given you have a default region configured\\naws ecr get-login-password --region $(aws configure get region) | docker login --username AWS --password-stdin \"$(aws sts get-caller-identity --query \"Account\" --output text).dkr.ecr.$(aws configure get region).amazonaws.com\"\\nAdded by Humberto Rodriguez', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…”', 'course': 'machine-learning-zoomcamp', 'id': 'f9711723'}]), ('5bda3b94', [{'text': 'While trying to run the docker code on M1:\\ndocker run --platform linux/amd64 -it --rm \\\\\\n-p 8500:8500 \\\\\\n-v $(pwd)/clothing-model:/models/clothing-model/1 \\\\\\n-e MODEL_NAME=\"clothing-model\" \\\\\\ntensorflow/serving:2.7.0\\nIt outputs the error:\\nError:\\nStatus: Downloaded newer image for tensorflow/serving:2.7.0\\n[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/generated_message_reflection.cc:2345] CHECK failed: file != nullptr:\\nterminate called after throwing an instance of \\'google::protobuf::FatalException\\'\\nwhat():  CHECK failed: file != nullptr:\\nqemu: uncaught target signal 6 (Aborted) - core dumped\\n/usr/bin/tf_serving_entrypoint.sh: line 3:     8 Aborted                 tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\\nSolution\\ndocker pull emacski/tensorflow-serving:latest\\ndocker run -it --rm \\\\\\n-p 8500:8500 \\\\\\n-v $(pwd)/clothing-model:/models/clothing-model/1 \\\\\\n-e MODEL_NAME=\"clothing-model\" \\\\\\nemacski/tensorflow-serving:latest-linux_arm64\\nSee more here: https://github.com/emacski/tensorflow-serving-arm\\nAdded by Daniel Egbo', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Error downloading  tensorflow/serving:2.7.0 on Apple M1 Mac', 'course': 'machine-learning-zoomcamp', 'id': '5bda3b94'}]), ('cccd31cf', [{'text': 'Similar to the one above but with a different solution the main reason is that emacski doesn’t seem to maintain the repo any more, the latest image is from 2 years ago at the time of writing (December 2023)\\nProblem:\\nWhile trying to run the docker code on Mac M2 apple silicon:\\ndocker run --platform linux/amd64 -it --rm \\\\\\n-p 8500:8500 \\\\\\n-v $(pwd)/clothing-model:/models/clothing-model/1 \\\\\\n-e MODEL_NAME=\"clothing-model\" \\\\\\ntensorflow/serving\\nYou get an error:\\n/usr/bin/tf_serving_entrypoint.sh: line 3:     7 Illegal instruction     tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\\nSolution:\\nUse bitnami/tensorflow-serving base image\\nLaunch it either using docker run\\ndocker run -d \\\\\\n--name tf_serving \\\\\\n-p 8500:8500 \\\\\\n-p 8501:8501 \\\\\\n-v $(pwd)/clothing-model:/bitnami/model-data/1 \\\\\\n-e TENSORFLOW_SERVING_MODEL_NAME=clothing-model \\\\\\nbitnami/tensorflow-serving:2\\nOr the following docker-compose.yaml\\nversion: \\'3\\'\\nservices:\\ntf_serving:\\nimage: bitnami/tensorflow-serving:2\\nvolumes:\\n- ${PWD}/clothing-model:/bitnami/model-data/1\\nports:\\n- 8500:8500\\n- 8501:8501\\nenvironment:\\n- TENSORFLOW_SERVING_MODEL_NAME=clothing-model\\nAnd run it with\\ndocker compose up\\nAdded by Alex Litvinov', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Illegal instruction error when running tensorflow/serving image on Mac M2 Apple Silicon (potentially on M1 as well)', 'course': 'machine-learning-zoomcamp', 'id': 'cccd31cf'}]), ('57f49999', [{'text': 'Problem: CPU metrics Shows Unknown\\nNAME         REFERENCE           TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\\ncredit-hpa   Deployment/credit   <unknown>/20%   1         3         1          18s\\nFailedGetResourceMetric       2m15s (x169 over 44m)  horizontal-pod-autoscaler  failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API:\\nSolution:\\n-> Delete HPA (kubectl delete hpa credit-hpa)\\n-> kubectl apply -f https://raw.githubusercontent.com/pythianarora/total-practice/master/sample-kubernetes-code/metrics-server.yaml\\n-> Create HPA\\nThis should solve the cpu metrics report issue.\\nAdded by Priya V', 'section': '11. KServe', 'question': 'HPA doesn’t show CPU metrics', 'course': 'machine-learning-zoomcamp', 'id': '57f49999'}]), ('5cb58698', [{'text': 'Problem description:\\nRunning this:\\ncurl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.9/hack/quick_install.sh\" | bash\\nFails with errors because of istio failing to update resources, and you are on kubectl > 1.25.0.\\nCheck kubectl version with kubectl version\\nSolution description\\nEdit the file “quick_install.bash” by downloading it with curl without running bash. Edit the versions of Istio and Knative as per the matrix on the KServe website.\\nRun the bash script now.\\nAdded by Andrew Katoch', 'section': '11. KServe', 'question': 'Errors with istio during installation', 'course': 'machine-learning-zoomcamp', 'id': '5cb58698'}]), ('de650b41', [{'text': 'Problem description\\nSolution description\\n(optional) Added by Name', 'section': 'Projects (Midterm and Capstone)', 'question': 'Problem title', 'course': 'machine-learning-zoomcamp', 'id': 'de650b41'}]), ('9ffacaac', [{'text': 'Answer: You can see them here (it’s taken from the 2022 cohort page). Go to the cohort folder for your own cohort’s deadline.', 'section': 'Projects (Midterm and Capstone)', 'question': 'What are the project deadlines?', 'course': 'machine-learning-zoomcamp', 'id': '9ffacaac'}]), ('4dfb5d4f', [{'text': 'Answer: All midterms and capstones are meant to be solo projects. [source @Alexey]', 'section': 'Projects (Midterm and Capstone)', 'question': 'Are projects solo or collaborative/group work?', 'course': 'machine-learning-zoomcamp', 'id': '4dfb5d4f'}]), ('0b8739b7', [{'text': 'Answer: Ideally midterms up to module-06, capstones include all modules in that cohort’s syllabus. But you can include anything extra that you want to feature. Just be sure to document anything not covered in class.\\nAlso watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nMore discussions:\\n[source1] [source2] [source3]', 'section': 'Projects (Midterm and Capstone)', 'question': 'What modules, topics, problem-sets should a midterm/capstone project cover? Can I do xyz?', 'course': 'machine-learning-zoomcamp', 'id': '0b8739b7'}]), ('9eb52679', [{'text': \"These links apply to all projects, actually. Again, for some cohorts, the modules/syllabus might be different, so always check in your cohort’s folder as well for additional or different instructions, if any.\\nMidterm Project Sample: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/cohorts/2021/07-midterm-project\\nMidTerm Project Deliverables: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/projects\\nSubmit MidTerm Project: https://docs.google.com/forms/d/e/1FAIpQLSfgmOk0QrmHu5t0H6Ri1Wy_FDVS8I_nr5lY3sufkgk18I6S5A/viewform\\nDatasets:\\nhttps://www.kaggle.com/datasets and https://www.kaggle.com/competitions\\nhttps://archive.ics.uci.edu/ml/index.php\\nhttps://data.europa.eu/en\\nhttps://www.openml.org/search?type=data\\nhttps://newzealand.ai/public-data-sets\\nhttps://datasetsearch.research.google.com\\nWhat to do and Deliverables\\nThink of a problem that's interesting for you and find a dataset for that\\nDescribe this problem and explain how a model could be used\\nPrepare the data and doing EDA, analyze important features\\nTrain multiple models, tune their performance and select the best model\\nExport the notebook into a script\\nPut your model into a web service and deploy it locally with Docker\\nBonus points for deploying the service to the cloud\", 'section': 'Projects (Midterm and Capstone)', 'question': 'Crucial Links', 'course': 'machine-learning-zoomcamp', 'id': '9eb52679'}]), ('7a1fcfd9', [{'text': 'Answer: Previous cohorts projects page has instructions (youtube).\\nhttps://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2022/projects.md#midterm-project\\nAlexey and his team will compile a g-sheet with links to submitted projects with our hashed emails (just like when we check leaderboard for homework) that are ours to review within the evaluation deadline.\\n~~~ Added by Nukta Bhatia ~~~', 'section': 'Projects (Midterm and Capstone)', 'question': 'How to conduct peer reviews for projects?', 'course': 'machine-learning-zoomcamp', 'id': '7a1fcfd9'}]), ('1cfa62c5', [{'text': 'See the answer here.', 'section': 'Projects (Midterm and Capstone)', 'question': 'Computing the hash for project review', 'course': 'machine-learning-zoomcamp', 'id': '1cfa62c5'}]), ('2a78f52e', [{'text': 'For the learning in public for this midterm project it seems that has a total value of 14!, Does this mean that we need make 14 posts?, Or the regular seven posts for each module and each one with a value of 2?, Or just one with a total value of 14?\\n14 posts, one for each day', 'section': 'Projects (Midterm and Capstone)', 'question': 'Learning in public links for the projects', 'course': 'machine-learning-zoomcamp', 'id': '2a78f52e'}]), ('68aeab64', [{'text': 'You can use git-lfs (https://git-lfs.com/) for upload large file to github repository.\\nRyan Pramana', 'section': 'Projects (Midterm and Capstone)', 'question': \"My dataset is too large and I can't loaded in GitHub , do anyone knows about a solution?\", 'course': 'machine-learning-zoomcamp', 'id': '68aeab64'}]), ('9a7c26e0', [{'text': 'If you have submitted two projects (and peer-reviewed at least 3 course-mates’ projects for each submission), you will get the certificate for the course. According to the course coordinator, Alexey Grigorev, only two projects are needed to get the course certificate.\\n(optional) David Odimegwu', 'section': 'Projects (Midterm and Capstone)', 'question': 'What If I submitted only two projects and failed to submit the third?', 'course': 'machine-learning-zoomcamp', 'id': '9a7c26e0'}]), ('1fd83eb9', [{'text': 'Yes. You only need to review peers when you submit your project.\\nConfirmed on Slack by Alexey Grigorev (added by Rileen Sinha)', 'section': 'Projects (Midterm and Capstone)', 'question': \"I did the first two projects and skipped the last one so I wouldn't have two peer review in second capstone right?\", 'course': 'machine-learning-zoomcamp', 'id': '1fd83eb9'}]), ('fbaa5b20', [{'text': 'Regarding Point 4 in the midterm deliverables, which states, \"Train multiple models, tune their performance, and select the best model,\" you might wonder, how many models should you train? The answer is simple: train as many as you can. The term \"multiple\" implies having more than one model, so as long as you have more than one, you\\'re on the right track.', 'section': 'Projects (Midterm and Capstone)', 'question': 'How many models should I train?', 'course': 'machine-learning-zoomcamp', 'id': 'fbaa5b20'}]), ('37eab341', [{'text': 'I am not sure how the project evaluate assignment works? Where do I find this? I have access to all the capstone 2 project, perhaps, I can randomly pick any to review.\\nAnswer:\\nThe link provided for example (2023/Capstone link ): https://docs.google.com/forms/d/e/1FAIpQLSdgoepohpgbM4MWTAHWuXa6r3NXKnxKcg4NDOm0bElAdXdnnA/viewform contains a list of all submitted projects to be evaluated. More specific, you are to review 3 assigned peer projects. In the spreadsheet are 3 hash values of your assigned peer projects. However, you need to derive the your hash value of your email address and find the value on the spreadsheet under the (reviewer_hash) heading.\\nTo calculate your hash value run the python code below:\\nfrom hashlib import sha1\\ndef compute_hash(email):\\nreturn sha1(email.lower().encode(\\'utf-8\\')).hexdigest()\\n# Example usage **** enter your email below (Example1@gmail.com)****\\nemail = \"Example1@gmail.com\"\\nhashed_email = compute_hash(email)\\nprint(\"Original Email:\", email)\\nprint(\"Hashed Email (SHA-1):\", hashed_email)\\nEdit the above code to replace Example1@gmail.com as your email address\\nStore and run the above python code from your terminal. See below as the Hashed Email (SHA-1) value\\nYou then go to the link: https://docs.google.com/spreadsheets/d/e/2PACX-1vR-7RRtq7AMx5OzI-tDbkzsbxNLm-NvFOP5OfJmhCek9oYcDx5jzxtZW2ZqWvBqc395UZpHBv1of9R1/pubhtml?gid=876309294&single=true\\nLastly, copy the “Hashed Email (SHA-1): bd9770be022dede87419068aa1acd7a2ab441675” value and search for 3 identical entries. There you should see your peer project to be reviewed.\\nBy Emmanuel Ayeni', 'section': 'Projects (Midterm and Capstone)', 'question': 'How does the project evaluation work for you as a peer reviewer?', 'course': 'machine-learning-zoomcamp', 'id': '37eab341'}]), ('57754faf', [{'text': 'Alexey Grigorev: “It’s based on all the scores to make sure most of you pass.”                                                   By Annaliese Bronz\\nOther course-related questions that don’t fall into any of the categories above or can apply to more than one category/module', 'section': 'Miscellaneous', 'question': 'Do you pass a project based on the average of everyone else’s scores or based on the total score you earn?', 'course': 'machine-learning-zoomcamp', 'id': '57754faf'}]), ('6979c5d1', [{'text': 'Answer: The train.py file will be used by your peers to review your midterm project. It is for them to cross-check that your training process works on someone else’s system. It should also be included in the environment in conda or with pipenv.\\nOdimegwu David', 'section': 'Miscellaneous', 'question': 'Why do I need to provide a train.py file when I already have the notebook.ipynb file?', 'course': 'machine-learning-zoomcamp', 'id': '6979c5d1'}]), ('a1bd8c34', [{'text': \"Pip install pillow - install pillow library\\nfrom PIL import Image\\nimg = Image.open('aeroplane.png')\\nFrom numpy import asarray\\nnumdata=asarray(img)\\nKrishna Anand\", 'section': 'Miscellaneous', 'question': 'Loading the Image with PILLOW library and converting to numpy array', 'course': 'machine-learning-zoomcamp', 'id': 'a1bd8c34'}]), ('b2ab0fc1', [{'text': \"Ans: train.py has to be a python file. This is because running a python script for training a model is much simpler than running a notebook and that's how training jobs usually look like in real life.\", 'section': 'Miscellaneous', 'question': 'Is a train.py file necessary when you have a train.ipynb file in your midterm project directory?', 'course': 'machine-learning-zoomcamp', 'id': 'b2ab0fc1'}]), ('80c439a9', [{'text': 'Yes, you can create a mobile app or interface that manages these forms and validations. But you should also perform validations on backend.\\nYou can also check Streamlit: https://github.com/DataTalksClub/project-of-the-week/blob/main/2022-08-14-frontend.md\\nAlejandro Aponte', 'section': 'Miscellaneous', 'question': 'Is there a way to serve up a form for users to enter data for the model to crunch on?', 'course': 'machine-learning-zoomcamp', 'id': '80c439a9'}]), ('ff93b86e', [{'text': \"Using model.feature_importances_ can gives you an error:\\nAttributeError: 'Booster' object has no attribute 'feature_importances_'\\nAnswer: if you train the model like this: model = xgb.train you should use get_score() instead\\nEkaterina Kutovaia\", 'section': 'Miscellaneous', 'question': 'How to get feature importance for XGboost model', 'course': 'machine-learning-zoomcamp', 'id': 'ff93b86e'}]), ('fcd86c8f', [{'text': 'In the Elastic Container Service task log, error “[Errno 12] Cannot allocate memory” showed up.\\nJust increase the RAM and CPU in your task definition.\\nHumberto Rodriguez', 'section': 'Miscellaneous', 'question': '[Errno 12] Cannot allocate memory in AWS Elastic Container Service', 'course': 'machine-learning-zoomcamp', 'id': 'fcd86c8f'}]), ('236864c2', [{'text': \"When running a docker container with waitress serving the app.py for making predictions, pickle will throw an error that can't get attribute <name_of_class> on module __main__.\\nThis does not happen when Flask is used directly, i.e. not through waitress.\\nThe problem is that the model uses a custom column transformer class, and when the model was saved, it was saved from the __main__ module (e.g. python train.py). Pickle will reference the class in the global namespace (top-level code): __main__.<custom_class>.\\nWhen using waitress, waitress will load the predict_app module and this will call pickle.load, that will try to find __main__.<custom_class> that does not exist.\\nSolution:\\nPut the class into a separate module and import it in both the script that saves the model (e.g. train.py) and the script that loads the model (e.g. predict.py)\\nNote: If Flask is used (no waitress) in predict.py, and predict.py has the definition of the class, When  it is run: python predict.py, it will work because the class is in the same namespace as the one used when the model was saved (__main__).\\nDetailed info: https://stackoverflow.com/questions/27732354/unable-to-load-files-using-pickle-and-multiple-modules\\nMarcos MJD\", 'section': 'Miscellaneous', 'question': 'Pickle error: can’t get attribute XXX on module __main__', 'course': 'machine-learning-zoomcamp', 'id': '236864c2'}]), ('efc4a04f', [{'text': 'There are different techniques, but the most common used are the next:\\nDataset transformation (for example, log transformation)\\nClipping high values\\nDropping these observations\\nAlena Kniazeva', 'section': 'Miscellaneous', 'question': 'How to handle outliers in a dataset?', 'course': 'machine-learning-zoomcamp', 'id': 'efc4a04f'}]), ('15f361b7', [{'text': 'I was getting the below error message when I was trying to create docker image using bentoml\\n[bentoml-cli] `serve` failed: Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"service\": No module named \\'sklearn\\'\\nSolution description\\nThe cause was because , in bentofile.yaml, I wrote sklearn instead of scikit-learn. Issue was fixed after I modified the packages list as below.\\npackages: # Additional pip packages required by the service\\n- xgboost\\n- scikit-learn\\n- pydantic\\nAsia Saeed', 'section': 'Miscellaneous', 'question': 'Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"service\": No module named \\'sklearn\\'', 'course': 'machine-learning-zoomcamp', 'id': '15f361b7'}]), ('dbbce78b', [{'text': \"You might see a long error message with something about sparse matrices, and in the swagger UI, you get a code 500 error with “” (empty string) as output.\\nPotential reason: Setting DictVectorizer or OHE to sparse while training, and then storing this in a pipeline or custom object in the benotml model saving stage in train.py. This means that when the custom object is called in service.py, it will convert each input to a different sized sparse matrix, and this can't be batched due to inconsistent length. In this case, bentoml model signatures should have batchable set to False for production during saving the bentoml mode in train.py.\\n(Memoona Tahira)\", 'section': 'Miscellaneous', 'question': 'BentoML not working with –production flag at any stage: e.g. with bentoml serve and while running the bentoml container', 'course': 'machine-learning-zoomcamp', 'id': 'dbbce78b'}]), ('f3a00e15', [{'text': 'Problem description:\\nDo we have to run everything?\\nYou are encouraged, if you can, to run them. As this provides another opportunity to learn from others.\\nNot everyone will be able to run all the files, in particular the neural networks.\\nSolution description:\\nAlternatively, can you see that everything you need to reproduce is there: the dataset is there, the instructions are there, are there any obvious errors and so on.\\nRelated slack conversation here.\\n(Gregory Morris)', 'section': 'Miscellaneous', 'question': 'Reproducibility', 'course': 'machine-learning-zoomcamp', 'id': 'f3a00e15'}]), ('9102b3c0', [{'text': \"If your model is too big for github one option is to try and compress the model using joblib. For example joblib.dump(model, model_filename, compress=('zlib', 6) will use zlib to compress the model. Just note this could take a few moments as the model is being compressed.\\nQuinn Avila\", 'section': 'Miscellaneous', 'question': 'Model too big', 'course': 'machine-learning-zoomcamp', 'id': '9102b3c0'}]), ('70d89fdf', [{'text': \"When you try to push the docker image to Google Container Registry and get this message “unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials.”, type this below on console, but first install https://cloud.google.com/sdk/docs/install, this is to be able to use gcloud in console:\\ngcloud auth configure-docker\\n(Jesus Acuña)\", 'section': 'Miscellaneous', 'question': 'Permissions to push docker to Google Container Registry', 'course': 'machine-learning-zoomcamp', 'id': '70d89fdf'}]), ('c5d6a804', [{'text': 'I am getting this error message when I tried to install tflite in a pipenv environment\\nError:  An error occurred while installing tflite_runtime!\\nError text:\\nERROR: Could not find a version that satisfies the requirement tflite_runtime (from versions: none)\\nERROR: No matching distribution found for tflite_runtime\\nThis version of tflite do not run on python 3.10, the way we can make it work is by install python 3.9, after that it would install the tflite_runtime without problem.\\nPastor Soto\\nCheck all available versions here:\\nhttps://google-coral.github.io/py-repo/tflite-runtime/\\nIf you don’t find a combination matching your setup, try out the options at\\nhttps://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite\\nwhich you can install as shown in the lecture, e.g.\\npip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\\nFinally, if nothing works, use the TFLite included in TensorFlow for local development, and use Docker for testing Lambda.\\nRileen Sinha (based on discussions on Slack)', 'section': 'Miscellaneous', 'question': 'Tflite_runtime unable to install', 'course': 'machine-learning-zoomcamp', 'id': 'c5d6a804'}]), ('8c7f089f', [{'text': \"Error: ImageDataGenerator name 'scipy' is not defined.\\nCheck that scipy is installed in your environment.\\nRestart jupyter kernel and try again.\\nMarcos MJD\", 'section': 'Miscellaneous', 'question': 'Error when running ImageDataGenerator.flow_from_dataframe', 'course': 'machine-learning-zoomcamp', 'id': '8c7f089f'}]), ('739bcccf', [{'text': 'Tim from BentoML has prepared a dedicated video tutorial wrt this use case here:\\nhttps://www.youtube.com/watch?v=7gI1UH31xb4&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=97\\nKonrad Muehlberg', 'section': 'Miscellaneous', 'question': 'How to pass BentoML content / docker container to Amazon Lambda', 'course': 'machine-learning-zoomcamp', 'id': '739bcccf'}]), ('4603e4e5', [{'text': \"In deploying model part, I wanted to test my model locally on a test-image data and I had this silly error after the following command:\\nurl = 'https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg'\\nX = preprocessor.from_url(url)\\nI got the error:\\nUnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7f797010a590>\\nSolution:\\nAdd ?raw=true after .jpg in url. E.g. as below\\nurl = ‘https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg?raw=true’\\nBhaskar Sarma\", 'section': 'Miscellaneous', 'question': 'Error UnidentifiedImageError: cannot identify image file', 'course': 'machine-learning-zoomcamp', 'id': '4603e4e5'}]), ('0a7c328e', [{'text': 'Problem: If you run pipenv install and get this message. Maybe manually change Pipfile and Pipfile.lock.\\nSolution: Run: ` pipenv lock` for fix this problem and dependency files\\nAlejandro Aponte', 'section': 'Miscellaneous', 'question': '[pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies', 'course': 'machine-learning-zoomcamp', 'id': '0a7c328e'}]), ('77efd069', [{'text': 'Problem: In the course this function worked to get the features from the dictVectorizer instance: dv.get_feature_names(). But in my computer did not work. I think it has to do with library versions and but apparently that function will be deprecated soon:\\nOld: https://scikit-learn.org/0.22/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\\nNew: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\\nSolution: change the line dv.get_feature_names() to list(dv.get_feature_names_out))\\nIbai Irastorza', 'section': 'Miscellaneous', 'question': 'Get_feature_names() not found', 'course': 'machine-learning-zoomcamp', 'id': '77efd069'}]), ('cc60f7bc', [{'text': 'Problem happens when contacting the server waiting to send your predict-test and your data here in the correct shape.\\nThe problem was the format input to the model wasn’t in the right shape. Server receives the data in json format (dict) which is not suitable for the model. U should convert it to like numpy arrays.\\nAhmed Okka', 'section': 'Miscellaneous', 'question': 'Error decoding JSON response: Expecting value: line 1 column 1 (char 0)', 'course': 'machine-learning-zoomcamp', 'id': 'cc60f7bc'}]), ('aa13dd66', [{'text': \"Q: Hii folks, I tried deploying my docker image on Render, but it won't I get SIGTERM everytime.\\nI think .5GB RAM is not enough, is there any other free alternative available ?\\nA: aws (amazon), gcp (google), saturn.\\nBoth aws and gcp give microinstance for free for a VERY long time, and a bunch more free stuff.\\nSaturn even provides free GPU instances. Recent promo link from mlzoomcamp for Saturn:\\n“You can sign up here: https://bit.ly/saturn-mlzoomcamp\\nWhen you sign up, write in the chat box that you're an ML Zoomcamp student and you should get extra GPU hours (something like 150)”\\nAdded by Andrii Larkin\", 'section': 'Miscellaneous', 'question': 'Free cloud alternatives', 'course': 'machine-learning-zoomcamp', 'id': 'aa13dd66'}]), ('c41e479c', [{'text': \"Problem description: I have one column day_of_the_month . It has values 1, 2, 20, 25 etc. and int . I have a second column month_of_the_year. It has values jan, feb, ..dec. and are string. I want to convert these two columns into one column day_of_the_year and I want them to be int. 2 and jan should give me 2, i.e. 2nd day of the year, 1 and feb should give me 32, i.e. 32 nd day of the year. What is the simplest pandas-way to do it?\\nSolution description:\\nconvert dtype in day_of_the_month column from int to str with df['day_of_the_month'] = df['day_of_the_month'].map(str)\\nconvert month_of_the_year column in jan, feb ...,dec into 1,2, ..,12 string using map()\\nconvert day and month into a datetime object with:\\ndf['date_formatted'] = pd.to_datetime(\\ndict(\\nyear='2055',\\nmonth=df['month'],\\nday=df['day']\\n)\\n)\\nget day of year with: df['day_of_year']=df['date_formatted'].dt.dayofyear\\n(Bhaskar Sarma)\", 'section': 'Miscellaneous', 'question': 'Getting day of the year from day and month column', 'course': 'machine-learning-zoomcamp', 'id': 'c41e479c'}]), ('2f28dcf1', [{'text': 'How to visualize the predictions per classes after training a neural net\\nSolution description\\nclasses, predictions = zip(*dict(zip(classes, predictions)).items())\\nplt.figure(figsize=(12, 3))\\nplt.bar(classes, predictions)\\nLuke', 'section': 'Miscellaneous', 'question': 'Chart for classes and predictions', 'course': 'machine-learning-zoomcamp', 'id': '2f28dcf1'}]), ('7a69cccf', [{'text': 'You can convert the prediction output values to a datafarme using \\ndf = pd.DataFrame.from_dict(dict, orient=\\'index\\' , columns=[\"Prediction\"])\\nEdidiong Esu', 'section': 'Miscellaneous', 'question': 'Convert dictionary values to Dataframe table', 'course': 'machine-learning-zoomcamp', 'id': '7a69cccf'}]), ('20174c95', [{'text': 'The image dataset for the competition was in a different layout from what we used in the dino vs dragon lesson. Since that’s what was covered, some folks were more comfortable with that setup, so I wrote a script that would generate it for them\\nIt can be found here: kitchenware-dataset-generator | Kaggle\\nMartin Uribe', 'section': 'Miscellaneous', 'question': 'Kitchenware Classification Competition Dataset Generator', 'course': 'machine-learning-zoomcamp', 'id': '20174c95'}]), ('f2cd48b6', [{'text': 'Install Nvidia drivers: https://www.nvidia.com/download/index.aspx.\\nWindows:\\nInstall Anaconda prompt https://www.anaconda.com/\\nTwo options:\\nInstall package ‘tensorflow-gpu’ in Anaconda\\nInstall the Tensorflow way https://www.tensorflow.org/install/pip#windows-native\\nWSL/Linux:\\nWSL: Use the Windows Nvida drivers, do not touch that.\\nTwo options:\\nInstall the Tensorflow way https://www.tensorflow.org/install/pip#linux_1\\nMake sure to follow step 4 to install CUDA by environment\\nAlso run:\\necho ‘export XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib/> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\\nInstall CUDA toolkit 11.x.x https://developer.nvidia.com/cuda-toolkit-archive\\nInstall https://developer.nvidia.com/rdp/cudnn-download\\nNow you should be able to do training/inference with GPU in Tensorflow\\n(Learning in public links Links to social media posts where you share your progress with others (LinkedIn, Twitter, etc). Use #mlzoomcamp tag. The scores for this part will be capped at 7 points. Please make sure the posts are valid URLs starting with \"https://\" Does it mean that I should provide my linkedin link? or it means that I should write a post that I have completed my first assignement? (\\nANS (by ezehcp7482@gmail.com): Yes, provide the linkedIN link to where you posted.\\nezehcp7482@gmail.com:\\nPROBLEM: Since I had to put up a link to a public repository, I had to use Kaggle and uploading the dataset therein was a bit difficult; but I had to ‘google’ my way out.\\nANS: See this link for a guide (https://www.kaggle.com/code/dansbecker/finding-your-files-in-kaggle-kernels/notebook)', 'section': 'Miscellaneous', 'question': 'CUDA toolkit and cuDNN Install for Tensorflow', 'course': 'machine-learning-zoomcamp', 'id': 'f2cd48b6'}]), ('59b4324f', [{'text': 'When multiplying matrices, the order of multiplication is important.\\nFor example:\\nA (m x n) * B (n x p) = C (m x p)\\nB (n x p) * A (m x n) = D (n x n)\\nC and D are matrices of different sizes and usually have different values. Therefore the order is important in matrix multiplication and changing the order changes the result.\\nBaran Akın', 'section': 'Miscellaneous', 'question': 'About getting the wrong result when multiplying matrices', 'course': 'machine-learning-zoomcamp', 'id': '59b4324f'}]), ('e1dc1ed9', [{'text': 'Refer to https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/01-intro/06-environment.md\\n(added by Rileen Sinha)', 'section': 'Miscellaneous', 'question': 'None of the videos have how to install the environment in Mac, does someone have instructions for Mac with M1 chip?', 'course': 'machine-learning-zoomcamp', 'id': 'e1dc1ed9'}]), ('fc60bf3b', [{'text': \"Depends on whether the form will still be open. If you're lucky and it's open, you can submit your homework and it will be evaluated. if closed - it's too late.\\n(Added by Rileen Sinha, based on answer by Alexey on Slack)\", 'section': 'Miscellaneous', 'question': 'I may end up submitting the assignment late. Would it be evaluated?', 'course': 'machine-learning-zoomcamp', 'id': 'fc60bf3b'}]), ('1e60e888', [{'text': 'Yes. Whoever corrects the homework will only be able to access the link if the repository is public.\\n(added by Tano Bugelli)\\nHow to install Conda environment in my local machine?\\nWhich ide is recommended for machine learning?', 'section': 'Miscellaneous', 'question': 'Does the github repository need to be public?', 'course': 'machine-learning-zoomcamp', 'id': '1e60e888'}]), ('44552c2e', [{'text': 'Install w get:\\n!which wget\\nDownload data:\\n!wget -P /content/drive/My\\\\ Drive/Downloads/ URL\\n(added by Paulina Hernandez)', 'section': 'Miscellaneous', 'question': 'How to use wget with Google Colab?', 'course': 'machine-learning-zoomcamp', 'id': '44552c2e'}]), ('7116b3be', [{'text': \"Features (X) must always be formatted as a 2-D array to be accepted by scikit-learn.\\nUse reshape to reshape a 1D array to a 2D.\\n\\t\\t\\t\\t\\t\\t\\t(-Aileah) :>\\n(added by Tano\\nfiltered_df = df[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]\\n# Select only the desired columns\\nselected_columns = [\\n'latitude',\\n'longitude',\\n'housing_median_age',\\n'total_rooms',\\n'total_bedrooms',\\n'population',\\n'households',\\n'median_income',\\n'median_house_value'\\n]\\nfiltered_df = filtered_df[selected_columns]\\n# Display the first few rows of the filtered DataFrame\\nprint(filtered_df.head())\", 'section': 'Miscellaneous', 'question': 'Features in scikit-learn?', 'course': 'machine-learning-zoomcamp', 'id': '7116b3be'}]), ('5d4d206e', [{'text': 'FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead', 'section': 'Miscellaneous', 'question': 'When I plotted using Matplot lib to check if median has a tail, I got the error below how can one bypass?', 'course': 'machine-learning-zoomcamp', 'id': '5d4d206e'}]), ('387093cc', [{'text': 'When trying to rerun the docker file in Windows, as opposed to developing in WSL/Linux, I got the error of:\\n```\\nWarning: Python 3.11 was not found on your system…\\nNeither ‘pipenv’ nor ‘asdf’ could be found to install Python.\\nYou can specify specific versions of Python with:\\n$ pipenv –python path\\\\to\\\\python\\n```\\nThe solution was to add Python311 installation folder to the PATH and restart the system and run the docker file again. That solved the error.\\n(Added by Abhijit Chakraborty)', 'section': 'Miscellaneous', 'question': 'Reproducibility in different OS', 'course': 'machine-learning-zoomcamp', 'id': '387093cc'}]), ('d12a2657', [{'text': 'You may quickly deploy your project to DigitalOcean App Cloud. The process is relatively straightforward. The deployment costs about 5 USD/month. The container needs to be up until the end of the project evaluation.\\nSteps:\\nRegister in DigitalOcean\\nGo to Apps -> Create App.\\nYou will need to choose GitHub as a service provider.\\nEdit Source Directory (if your project is not in the repo root)\\nIMPORTANT: Go to settings -> App Spec and edit the Dockerfile path so it looks like ./project/Dockerfile path relative to your repo root\\nRemember to add model files if they are not built automatically during the container build process.\\nBy Dmytro Durach', 'section': 'Miscellaneous', 'question': 'Deploying to Digital Ocean', 'course': 'machine-learning-zoomcamp', 'id': 'd12a2657'}]), ('eb7a57a6', [{'text': \"I’m just looking back at the lessons in week 3 (churn prediction project), and lesson 3.6 talks about Feature Importance for categorical values. At 8.12, the mutual info scores show that the some features are more important than others, but then in lesson 3.10 the Logistic Regression model is trained on all of the categorical variables (see 1:35). Once we have done feature importance, is it best to train your model only on the most important features?\\nNot necessarily - rather, any feature that can offer additional predictive value should be included (so, e.g. predict with & without including that feature; if excluding it drops performance, keep it, else drop it). A few individually important features might in fact be highly correlated with others, & dropping some might be fine. There are many feature selection algorithms, it might be interesting to read up on them (among the methods we've learned so far in this course, L1 regularization (Lasso) implicitly does feature selection by shrinking some weights all the way to zero).\\nBy Rileen Sinha\", 'section': 'Miscellaneous', 'question': 'Is it best to train your model only on the most important features?', 'course': 'machine-learning-zoomcamp', 'id': 'eb7a57a6'}]), ('d6f0c6ea', [{'text': 'You can consider several different approaches:\\nSampling: In the exploratory phase, you can use random samples of the data.\\nChunking: When you do need all the data, you can read and process it in chunks that do fit in the memory.\\nOptimizing data types: Pandas’ automatic data type inference (when reading data in) might result in e.g. float64 precision being used to represent integers, which wastes space. You might achieve substantial memory reduction by optimizing the data types.\\nUsing Dask, an open-source python project which parallelizes Numpy and Pandas.\\n(see, e.g. https://www.vantage-ai.com/en/blog/4-strategies-how-to-deal-with-large-datasets-in-pandas)\\nBy Rileen Sinha', 'section': 'Miscellaneous', 'question': 'How can I work with very large datasets, e.g. the New York Yellow Taxi dataset, with over a million rows?', 'course': 'machine-learning-zoomcamp', 'id': 'd6f0c6ea'}]), ('9f261648', [{'text': 'Technically, yes. Advisable? Not really. Reasons:\\nSome homework(s) asks for specific python library versions.\\nAnswers may not match in MCQ options if using different languages other than Python 3.10 (the recommended version for 2023 cohort)\\nAnd as for midterms/capstones, your peer-reviewers may not know these other languages. Do you want to be penalized for others not knowing these other languages?\\nYou can create a separate repo using course’s lessons but written in other languages for your own learnings, but not advisable for submissions.\\ntx[source]', 'section': 'Miscellaneous', 'question': 'Can I do the course in other languages, like R or Scala?', 'course': 'machine-learning-zoomcamp', 'id': '9f261648'}]), ('aa7ff0f7', [{'text': 'Yes, it’s allowed (as per Alexey).\\nAdded By Rileen Sinha', 'section': 'Miscellaneous', 'question': 'Is use of libraries like fast.ai or huggingface allowed in the capstone and competition, or are they considered to be \"too much help\"?', 'course': 'machine-learning-zoomcamp', 'id': 'aa7ff0f7'}]), ('387bdc5f', [{'text': 'The TF and TF Serving versions have to match (as per solution from the slack channel)\\nAdded by Chiedu Elue', 'section': 'Miscellaneous', 'question': 'Flask image was built and tested successfully, but tensorflow serving image was built and unable to test successfully. What could be the problem?', 'course': 'machine-learning-zoomcamp', 'id': '387bdc5f'}]), ('c6a22665', [{'text': 'I’ve seen LinkedIn users list DataTalksClub as Experience with titles as:\\nMachine Learning Fellow\\nMachine Learning Student\\nMachine Learning Participant\\nMachine Learning Trainee\\nPlease note it is best advised that you do not list the experience as an official “job” or “internship” experience since DataTalksClub did not hire you, nor financially compensate you.\\nOther ways you can incorporate the experience in the following sections:\\nOrganizations\\nProjects\\nSkills\\nFeatured\\nOriginal posts\\nCertifications\\nCourses\\nBy Annaliese Bronz\\nInteresting question, I put the link of my project into my CV as showcase and make posts to show my progress.\\nBy Ani Mkrtumyan', 'section': 'Miscellaneous', 'question': 'Any advice for adding the Machine Learning Zoomcamp experience to your LinkedIn profile?', 'course': 'machine-learning-zoomcamp', 'id': 'c6a22665'}]), ('0560e827', [{'text': 'MLOps Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course, and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\n[Problem description]\\n[Solution description]\\n(optional) Added by Name', 'section': '+-General course questions', 'question': 'Format for questions: [Problem title]', 'course': 'mlops-zoomcamp', 'id': '0560e827'}]), ('59812e77', [{'text': 'Approximately 3 months. For each module, about 1 week with possible deadline extensions (in total 6~9 weeks), 2 weeks for working on the capstone project and 1 week for peer review.', 'section': '+-General course questions', 'question': 'What is the expected duration of this course or that for each module?', 'course': 'mlops-zoomcamp', 'id': '59812e77'}]), ('dce0bb09', [{'text': 'The difference is the Orchestration and Monitoring modules. Those videos will be re-recorded. The rest should mostly be the same.\\nAlso all of the homeworks will be changed for the 2023 cohort.', 'section': '+-General course questions', 'question': 'What’s the difference between the 2023 and 2022 course?', 'course': 'mlops-zoomcamp', 'id': 'dce0bb09'}]), ('4920d4e9', [{'text': 'Yes, it will start in May 2024', 'section': '+-General course questions', 'question': 'Will there be a 2024 Cohort? When will the 2024 cohort start?', 'course': 'mlops-zoomcamp', 'id': '4920d4e9'}]), ('0f1d2765', [{'text': 'Please choose the closest one to your answer. Also do not post your answer in the course slack channel.', 'section': '+-General course questions', 'question': 'What if my answer is not exactly the same as the choices presented?', 'course': 'mlops-zoomcamp', 'id': '0f1d2765'}]), ('4eef2f81', [{'text': 'Please pick up a problem you want to solve yourself. Potential datasets can be found on either Kaggle, Hugging Face, Google, AWS, or the UCI Machine Learning Datasets Repository.', 'section': '+-General course questions', 'question': 'Are we free to choose our own topics for the final project?', 'course': 'mlops-zoomcamp', 'id': '4eef2f81'}]), ('7f93c032', [{'text': 'In order to obtain the certificate, completion of the final capstone project is mandatory. The completion of weekly homework assignments is optional, but they can contribute to your overall progress and ranking on the top 100 leaderboard.', 'section': '+-General course questions', 'question': 'Can I still graduate when I didn’t complete homework for week x?', 'course': 'mlops-zoomcamp', 'id': '7f93c032'}]), ('ee6f7c89', [{'text': 'You can get a few cloud points by using kubernetes even if you deploy it only locally. Or you can use local stack too to mimic AWS\\nAdded by Ming Jun, Asked by Ben Pacheco, Answered by Alexey Grigorev', 'section': 'Module 1: Introduction', 'question': 'For the final project, is it required to be put on the cloud?', 'course': 'mlops-zoomcamp', 'id': 'ee6f7c89'}]), ('b63b12e0', [{'text': 'For those who are not using VSCode (or other similar IDE), you can automate port-forwarding for Jupyter Notebook by adding the following line of code to your\\n~/.ssh/config file (under the mlops-zoomcamp host):\\nLocalForward 127.0.0.1:8899 127.0.0.1:8899\\nThen you can launch Jupyter Notebook using the following command: jupyter notebook --port=8899 --no-browser and copy paste the notebook URL into your browser.\\nAdded by Vishal', 'section': 'Module 1: Introduction', 'question': 'Port-forwarding without Visual Studio', 'course': 'mlops-zoomcamp', 'id': 'b63b12e0'}]), ('892c22c1', [{'text': 'You can install the Jupyter extension to open notebooks in VSCode.\\nAdded by Khubaib', 'section': 'Module 1: Introduction', 'question': 'Opening Jupyter in VSCode', 'course': 'mlops-zoomcamp', 'id': '892c22c1'}]), ('13d38e8d', [{'text': 'In case one would like to set a github repository (e.g. for Homeworks), one can follow 2 great tutorials that helped a lot\\nSetting up github on AWS instance - this\\nSetting up keys on AWS instance - this\\nThen, one should be able to push to its repo\\nAdded by Daniel Hen (daniel8hen@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Configuring Github to work from the remote VM', 'course': 'mlops-zoomcamp', 'id': '13d38e8d'}]), ('7d64e9e0', [{'text': \"Faced issue while setting up JUPYTER NOTEBOOK on AWS. I was unable to access it from my desktop. (I am not using visual studio and hence faced problem)\\nRun\\njupyter notebook --generate-config\\nEdit file /home/ubuntu/.jupyter/jupyter_notebook_config.py to add following line:\\nNotebookApp.ip = '*'\\nAdded by Atul Gupta (samatul@gmail.com)\", 'section': 'Module 1: Introduction', 'question': 'Opening Jupyter in AWS', 'course': 'mlops-zoomcamp', 'id': '7d64e9e0'}]), ('645f0a55', [{'text': 'If you wish to use WSL on your windows machine, here are the setup instructions:\\nCommand: Sudo apt install wget\\nGet Anaconda download address here. wget <download address>\\nTurn on Docker Desktop WFree Download | AnacondaSL2\\nCommand: git clone <github repository address>\\nVSCODE on WSL\\nJupyter: pip3 install jupyter\\nAdded by Gregory Morris (gwm1980@gmail.com)\\nAll in all softwares at one shop:\\nYou can use anaconda which has all built in services like pycharm, jupyter\\nAdded by Khaja Zaffer (khajazaffer@aln.iseg.ulisboa.pt)\\nFor windows “wsl --install” in Powershell\\nAdded by Vadim Surin (vdmsurin@gmai.com)', 'section': 'Module 1: Introduction', 'question': 'WSL instructions', 'course': 'mlops-zoomcamp', 'id': '645f0a55'}]), ('7297b7fc', [{'text': 'If you create a folder data and download datasets or raw files in your local repository. Then to push all your code to remote repository without this files or folder please use gitignore file. The simple way to create it do the following steps\\n1. Create empty .txt file (using text editor or command line)\\n2. Safe as .gitignore (. must use the dot symbol)\\n3. Add rules\\n *.parquet - to ignore all parquet files\\ndata/ - to ignore all files in folder data\\n\\nFor more pattern read GIT documentation\\nhttps://git-scm.com/docs/gitignore\\nAdded by Olga Rudakova (olgakurgan@gmail.com)', 'section': 'Module 1: Introduction', 'question': '.gitignore how-to', 'course': 'mlops-zoomcamp', 'id': '7297b7fc'}]), ('68154f64', [{'text': \"Make sure when you stop an EC2 instance that it actually stops (there's a meme about it somewhere). There are green circles (running), orange (stopping), and red (stopped). Always refresh the page to make sure you see the red circle and status of stopped.\\nEven when an EC2 instance is stopped, there WILL be other charges that are incurred (e.g. if you uploaded data to the EC2 instance, this data has to be stored somewhere, usually an EBS volume and this storage incurs a cost).\\nYou can set up billing alerts. (I've never done this, so no advice on how to do this).\\n(Question by: Akshit Miglani (akshit.miglani09@gmail.com) and Answer by Anna Vasylytsya)\", 'section': 'Module 1: Introduction', 'question': 'AWS suggestions', 'course': 'mlops-zoomcamp', 'id': '68154f64'}]), ('dc7b6f51', [{'text': 'You can get invitation code by coursera and use it in account to verify it it has different characteristics.\\nI really love it\\nhttps://www.youtube.com/watch?v=h_GdX6KtXjo', 'section': 'Module 1: Introduction', 'question': 'IBM Cloud an alternative for AWS', 'course': 'mlops-zoomcamp', 'id': 'dc7b6f51'}]), ('b25c6ca3', [{'text': \"I am worried about the cost of keeping an AWS instance running during the course.\\nWith the instance specified during working environment setup, if you remember to Stop Instance once you finished your work for the day.  Using that strategy, in a day with about 5 hours of work you will pay around $0.40 USD which will account for $12 USD per month, which seems to be an affordable amount.\\nYou must remember that you would have a different IP public address every time you Restart your instance, and you would need to edit your ssh Config file.  It's worth the time though.\\nAdditionally, AWS enables you to set up an automatic email alert if a predefined budget is exceeded.\\nHere is a tutorial to set this up.\\nAlso, you can estimate the cost yourself, using AWS pricing calculator (to use it you don’t even need to be logged in).\\nAt the time of writing (20.05.2023) t3a.xlarge instance with 2 hr/day usage (which translates to 10 hr/week that should be enough to complete the course) and 30GB EBS monthly cost is 10.14 USD\\nHere’s a link to the estimate\\nAdded by Alex Litvinov (aaalex.lit@gmail.com)\", 'section': 'Module 1: Introduction', 'question': 'AWS costs', 'course': 'mlops-zoomcamp', 'id': 'b25c6ca3'}]), ('9f69ca26', [{'text': 'For many parts - yes. Some things like kinesis are not in AWS free tier, but you can do it locally with localstack.', 'section': 'Module 1: Introduction', 'question': 'Is the AWS free tier enough for doing this course?', 'course': 'mlops-zoomcamp', 'id': '9f69ca26'}]), ('0f1ddc9e', [{'text': 'When I click an open IP-address in an AWS EC2 instance I get an error: “This site can’t be reached”. What should I do?\\nThis ip-address is not required to be open in a browser. It is needed to connect to the running EC2 instance via terminal from your local machine or via terminal from a remote server with such command, for example if:\\nip-address is 11.111.11.111\\ndownloaded key name is razer.pem (the key should be moved to a hidden folder .ssh)\\nyour user name is user_name\\nssh -i /Users/user_name/.ssh/razer.pem ubuntu@11.111.11.111', 'section': 'Module 1: Introduction', 'question': 'AWS EC2: this site can’t be reached', 'course': 'mlops-zoomcamp', 'id': '0f1ddc9e'}]), ('01f61154', [{'text': 'After this command `ssh -i ~/.ssh/razer.pem ubuntu@XX.XX.XX.XX` I got this error: \"unprotected private key file\". This page (https://99robots.com/how-to-fix-permission-error-ssh-amazon-ec2-instance/) explains how to fix this error. Basically you need to change the file permissions of the key file with this command: chmod 400 ~/.ssh/razer.pem', 'section': 'Module 1: Introduction', 'question': 'Unprotected private key file!', 'course': 'mlops-zoomcamp', 'id': '01f61154'}]), ('d43c32ba', [{'text': 'My SSH connection to AWS cannot last more than a few minutes, whether via terminal or VS code.\\nMy config:\\n# Copy Configuration in local nano editor, then Save it!\\nHost mlops-zoomcamp                                         # ssh connection calling name\\nUser ubuntu                                             # username AWS EC2\\nHostName <instance-public-IPv4-addr>                    # Public IP, it changes when Source EC2 is turned off.\\nIdentityFile ~/.ssh/name-of-your-private-key-file.pem   # Private SSH key file path\\nLocalForward 8888 localhost:8888                        # Connecting to a service on an internal network from the outside, static forward or set port user forward via on vscode\\nStrictHostKeyChecking no\\nAdded by Muhammed Çelik\\nThe disconnection will occur whether I SSH via WSL2 or via VS Code, and usually occurs after I run some code, i.e. “import mlflow”, so not particularly intense computation.\\nI cannot reconnect to the instance without stopping and restarting with a new IPv4 address.\\nI’ve gone through steps listed on this page: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-linux-resolve-ssh-connection-errors/\\nInbound rule should allow all incoming IPs for SSH.\\nWhat I expect to happen:\\nSSH connection should remain while I’m actively using the instance, and if it does disconnect, I should be able to reconnect back.\\nSolution: sometimes the hang ups are caused by the instance running out of memory. In one instance, using EC2 feature to view screenshot of the instance as a means to troubleshoot, it was the OS out-of-memory feature which killed off some critical processes. In this case, if we can’t use a higher compute VM with more RAM, try adding a swap file, which uses the disk as RAM substitute and prevents the OOM error. Follow Ubuntu’s documentation here: https://help.ubuntu.com/community/SwapFaq.\\nAlternatively follow AWS’s own doc, which mirrors Ubuntu’s: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-memory-swap-file/', 'section': 'Module 1: Introduction', 'question': 'AWS EC2 instance constantly drops SSH connection', 'course': 'mlops-zoomcamp', 'id': 'd43c32ba'}]), ('a044d267', [{'text': 'Everytime I restart my EC2 instance I keep getting different IP and need to update the config file manually.\\n\\nSolution: You can create a script like this to automatically update the IP address of your EC2 instance.https://github.com/dimzachar/mlops-zoomcamp/blob/master/notes/Week_1/update_ssh_config.md', 'section': 'Module 1: Introduction', 'question': 'AWS EC2 IP Update', 'course': 'mlops-zoomcamp', 'id': 'a044d267'}]), ('abf8ccdc', [{'text': 'Make sure to use an instance with enough compute capabilities such as a t2.xlarge. You can check the monitoring tab in the EC2 dashboard to monitor your instance.', 'section': 'Module 1: Introduction', 'question': 'VS Code crashes when connecting to Jupyter', 'course': 'mlops-zoomcamp', 'id': 'abf8ccdc'}]), ('26918af3', [{'text': 'Error “ValueError: X has 526 features, but LinearRegression is expecting 525 features as input.” when running your Linear Regression Model on the validation data set:\\nSolution: The DictVectorizer creates an initial mapping for the features (columns). When calling the DictVecorizer again for the validation dataset transform should be used as it will ignore features that it did not see when fit_transform was last called. E.g.\\nX_train = dv.fit_transform(train_dict)\\nX_test = dv.transform(test_dict)', 'section': 'Module 1: Introduction', 'question': 'X has 526 features, but expecting 525 features', 'course': 'mlops-zoomcamp', 'id': '26918af3'}]), ('a5234ac0', [{'text': 'If some dependencies are missing\\nInstall following packages\\npandas\\nmatplotlib\\nscikit-learn\\nfastparquet\\npyarrow\\nseaborn\\npip install -r requirements.txt\\nI have seen this error when using pandas.read_parquet(), the solution is to install pyarrow or fastparquet by doing !pip install pyarrow in the notebook\\nNOTE: if you’re using Conda instead of pip, install fastparquet rather than pyarrow, as it is much easier to install and it’s functionally identical to pyarrow for our needs.', 'section': 'Module 1: Introduction', 'question': 'Missing dependencies', 'course': 'mlops-zoomcamp', 'id': 'a5234ac0'}]), ('af22c52a', [{'text': 'The evaluation RMSE I get doesn’t figure within the options!\\nIf you’re evaluating the model on the entire February data, try to filter outliers using the same technique you used on the train data (0≤duration≤60) and you’ll get a RMSE which is (approximately) in the options. Also don’t forget to convert the columns data types to str before using the DictVectorizer.\\nAnother option: Along with filtering outliers, additionally filter on null values by replacing them with -1.  You will get a RMSE which is (almost same as) in the options. Use ‘.round(2)’ method to round it to 2 decimal points.\\nWarning deprecation\\nThe python interpreter warning of modules that have been deprecated  and will be removed in future releases as well as making suggestion how to go about your code.\\nFor example\\nC:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\seaborn\\\\distributions.py:2619:\\nFutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\\nwarnings.warn(msg, FutureWarning)\\nTo suppress the warnings, you can include this code at the beginning of your notebook\\nimport warnings\\nwarnings.filterwarnings(\"ignore\")', 'section': 'Module 1: Introduction', 'question': 'No RMSE value in the options', 'course': 'mlops-zoomcamp', 'id': 'af22c52a'}]), ('2aaac94c', [{'text': 'sns.distplot(df_train[\"duration\"])\\nCan be replaced with\\nsns.histplot(\\ndf_train[\"duration\"] , kde=True,\\nstat=\"density\", kde_kws=dict(cut=3), bins=50,\\nalpha=.4, edgecolor=(1, 1, 1, 0.4),\\n)\\nTo get almost identical result', 'section': 'Module 1: Introduction', 'question': 'How to replace distplot with histplot', 'course': 'mlops-zoomcamp', 'id': '2aaac94c'}]), ('9d15c9e9', [{'text': 'You need to replace the capital letter “L” with a small one “l”', 'section': 'Module 1: Introduction', 'question': \"KeyError: 'PULocationID'  or  'DOLocationID'\", 'course': 'mlops-zoomcamp', 'id': '9d15c9e9'}]), ('79b88d0b', [{'text': 'I have faced a problem while reading the large parquet file. I tried some workarounds but they were NOT successful with Jupyter.\\nThe error message is:\\nIndexError: index 311297 is out of bounds for axis 0 with size 131743\\nI solved it by performing the homework directly as a python script.\\nAdded by Ibraheem Taha (ibraheemtaha91@gmail.com)\\nYou can try using the Pyspark library\\nAnswered by kamaldeen (kamaldeen32@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Reading large parquet files', 'course': 'mlops-zoomcamp', 'id': '79b88d0b'}]), ('45485322', [{'text': 'First remove the outliers (trips with unusual duration) before plotting\\nAdded by Ibraheem Taha (ibraheemtaha91@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Distplot takes too long', 'course': 'mlops-zoomcamp', 'id': '45485322'}]), ('d5eab395', [{'text': 'Problem: RMSE on test set was too high when hot encoding the validation set with a previously fitted OneHotEncoder(handle_unknown=’ignore’) on the training set, while DictVectorizer would yield the correct RMSE.\\nIn principle both transformers should behave identically when treating categorical features (at least in this week’s homework where we don’t have sequences of strings in each row):\\nFeatures are put into binary columns encoding their presence (1) or absence (0)\\nUnknown categories are imputed as zeroes in the hot-encoded matrix', 'section': 'Module 1: Introduction', 'question': 'RMSE on test set too high', 'course': 'mlops-zoomcamp', 'id': 'd5eab395'}]), ('282957fb', [{'text': 'A: Alexey’s answer https://www.youtube.com/watch?v=8uJ36ZZr_Is&t=13s\\nIn summary,\\npd.get_dummies or OHE can come up with result in different orders and handle missing data differently, so train and val set would have different columns during train and validation\\nDictVectorizer would ignore missing (in train) and new (in val) datasets\\nOther sources:\\nhttps://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor\\nhttps://scikit-learn.org/stable/modules/feature_extraction.html\\nhttps://innovation.alteryx.com/encode-smarter/\\n~ ellacharmed', 'section': 'Module 1: Introduction', 'question': 'Q: Using of OneHotEncoder instead of DictVectorizer', 'course': 'mlops-zoomcamp', 'id': '282957fb'}]), ('39ad14fd', [{'text': \"Why didn't get_dummies in pandas library or OneHotEncoder in scikit-learn library be used for one-hot encoding? I know OneHotEncoder is the most common and useful. One-hot coding can also be done using the eye or identity components of the NumPy library.\\nM.Sari\\nOneHotEncoder has the option to output a row column tuple matrix. DictVectorizer is a one step method to encode and support row column tuple matrix output.\\nHarinder(sudwalh@gmail.com)\", 'section': 'Module 1: Introduction', 'question': 'Q: Why did we not use OneHotEncoder(sklearn) instead of DictVectorizer ?', 'course': 'mlops-zoomcamp', 'id': '39ad14fd'}]), ('e34df2a5', [{'text': 'How to check that we removed the outliers?\\nUse the pandas function describe() which can provide a report of the data distribution along with the statistics to describe the data. For example, after clipping the outliers using boolean expression, the min and max can be verified using\\ndf[‘duration’].describe()', 'section': 'Module 1: Introduction', 'question': 'Clipping outliers', 'course': 'mlops-zoomcamp', 'id': 'e34df2a5'}]), ('c91b6b57', [{'text': 'pd.get_dummies and DictVectorizer both create a one-hot encoding on string values. Therefore you need to convert the values in PUlocationID and DOlocationID to string.\\nIf you convert the values in PUlocationID and DOlocationID from numeric to string, the NaN values get converted to the string \"nan\".  With DictVectorizer the RMSE is the same whether you use \"nan\" or \"-1\" as string representation for the NaN values. Therefore the representation doesn\\'t have to be \"-1\" specifically, it could also be some other string.', 'section': 'Module 1: Introduction', 'question': 'Replacing NaNs for pickup location and drop off location with -1 for One-Hot Encoding', 'course': 'mlops-zoomcamp', 'id': 'c91b6b57'}]), ('4aa8eafc', [{'text': 'Problem: My LinearRegression RSME is very close to the answer but not exactly the same. Is this normal?\\nAnswer: No, LinearRegression is an deterministic model, it should always output the same results when given the same inputs.\\nAnswer:\\nCheck if you have treated the outlier properly for both train and validation sets\\nCheck if the one hot encoding has been done properly by looking at the shape of one hot encoded feature matrix. If it shows 2 features, there is wrong with one hot encoding. Hint: the drop off and pick up codes need to be converted to proper data format and then DictVectorizer is fitted.\\nHarshit Lamba (hlamba19@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Slightly different RSME', 'course': 'mlops-zoomcamp', 'id': '4aa8eafc'}]), ('a9daaab0', [{'text': 'Problem: I’m facing an extremely low RMSE score (eg: 4.3451e-6) - what shall I do?\\nAnswer: Recheck your code to see if your model is learning the target prior to making the prediction. If the target variable is passed in as a parameter while fitting the model, chances are the model would score extremely low. However, that’s not what you would want and would much like to have your model predict that. A good way to check that is to make sure your X_train doesn’t contain any part of your y_train. The same stands for validation too.\\nSnehangsu De (desnehangsu@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Extremely low RSME', 'course': 'mlops-zoomcamp', 'id': 'a9daaab0'}]), ('931f9626', [{'text': 'Problem: how to enable auto completion in jupyter notebook? Tab doesn’t work for me\\nSolution: !pip install --upgrade jedi==0.17.2\\nChristopher R.J.(romanjaimesc@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Enabling Auto-completion in jupyter notebook', 'course': 'mlops-zoomcamp', 'id': '931f9626'}]), ('782e1723', [{'text': \"Problem: While following the steps in the videos you may have problems trying to download with wget the files. Usually it is a 403 error type (Forbidden access).\\nSolution: The links point to files on cloudfront.net, something like this:\\nhttps://d37ci6vzurychx.cloudfront.net/tOSError: Could not open parquet input source '<Buffer>': Invalid: Parquet OSError: Could not open parquet input source '<Buffer>': Invalid: Parquet rip+data/green_tripdata_2021-01.parquet\\nI’m not download the dataset directly, i use dataset URL and run this in the file.\\nUpdate(27-May-2023): Vikram\\nI am able to download the data from the below link. This is from the official  NYC trip record page (https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). Copy link from page directly as the below url might get changed if the NYC decides to move away from this. Go to the page , right click and use copy link.\\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-01.parquet\\n(Asif)\\nCopy the link address and replace the cloudfront.net part with s3.amazonaws.com/nyc-tlc/, so it looks like this:\\nhttps://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-01.parquet\\nMario Tormo (mario@tormo-romero.eu)\\nOSError: Could not open parquet input source '<Buffer>': Invalid: Parquet\", 'section': 'Module 1: Introduction', 'question': 'Downloading the data from the NY Taxis datasets gives error : 403 Forbidden', 'course': 'mlops-zoomcamp', 'id': '782e1723'}]), ('4e08c86a', [{'text': 'Problem: PyCharm (remote) doesn’t see conda execution path. So, I cannot use conda env (which is located on a remote server).\\nSolution: In remote server in command line write “conda activate envname”, after write “which python” - it gives you python execution path. After you can use this path when you will add new interpreter in PyCharm: add local interpreter -> system interpreter -> and put the path with python.\\nSalimov Ilnaz (salimovilnaz777@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Using PyCharm & Conda env in remote development', 'course': 'mlops-zoomcamp', 'id': '4e08c86a'}]), ('34bcad27', [{'text': 'Problem: The output of DictVectorizer was taking up too much memory. So much so, that I couldn’t even fit the linear regression model before running out of memory on my 16 GB machine.\\nSolution: In the example for DictVectorizer in the scikit-learn website, they set the parameter “sparse” as False. Although this helps with viewing the results, this results in a lot of memory usage. The solution is to either use “sparse=True” instead, or leave it at the default which is also True.\\nAhmed Fahim (afahim03@yahoo.com)', 'section': 'Module 1: Introduction', 'question': 'Running out of memory', 'course': 'mlops-zoomcamp', 'id': '34bcad27'}]), ('96144e66', [{'text': 'Problem: For me, Installing anaconda didn’t modify the .bashrc profile. That means Anaconda env was not activated even after exiting and relaunching the unix shell.\\nSolution:\\nFor bash : Initiate conda again, which will add entries for anaconda in .bashrc file.\\n$ cd YOUR_PATH_ANACONDA/bin $ ./conda init bash\\nThat will automatically edit your .bashrc.\\nReload:\\n$ source ~/.bashrc\\nAhamed Irshad (daisyfuentesahamed@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Activating Anaconda env in .bashrc', 'course': 'mlops-zoomcamp', 'id': '96144e66'}]), ('840f739d', [{'text': 'While working through the HW1, you will realize that the training and the validation data set feature sizes are different. I was trying to figure out why and went down the entire rabbit hole only to see that I wasn’t doing ```transform``` on the premade dictionary vectorizer instead of ```fit_transform```. You already have the dictionary vectorizer made so no need to execute the fit pipeline on the model.\\nSam Lim(changhyeonlim@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'The feature size is different for training set and validation set', 'course': 'mlops-zoomcamp', 'id': '840f739d'}]), ('bf006ff9', [{'text': 'I found a good guide how to get acces to your machine again when you removed your public key.\\nUsing the following link you can go to Session Manager and log in to your instance and create public key again. https://repost.aws/knowledge-center/ec2-linux-fix-permission-denied-errors\\nThe main problem for me here was to get my old public key, so for doing this you should run the following command: ssh-keygen -y -f /path_to_key_pair/my-key-pair.pem\\nFor more information: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/describe-keys.html#retrieving-the-public-key\\nHanna Zhukavets (a.zhukovec1901@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Permission denied (publickey) Error (when you remove your public key on the AWS machine)', 'course': 'mlops-zoomcamp', 'id': 'bf006ff9'}]), ('f178d4a0', [{'text': 'Problem: The February dataset has been used as a validation/test dataset and been stripped of the outliers in a similar manner to the train dataset (taking only the rows for the duration between 1 and 60, inclusive). The RMSE obtained afterward is in the thousands.\\nAnswer: The sparsematrix result from DictVectorizer shouldn’t be turned into an ndarray. After removing that part of the code, I ended up receiving a correct result .\\nTahina Mahatoky (tahinadanny@gmail.com)', 'section': 'Module 1: Introduction', 'question': 'Overfitting: Absurdly high RMSE on the validation dataset', 'course': 'mlops-zoomcamp', 'id': 'f178d4a0'}]), ('b80401a2', [{'text': 'more specific error line:\\nfrom sklearn.feature_extraction import DictVectorizer\\nI had this issue and to solve it I did\\n!pip install scikit-learn\\nJoel Auccapuclla (auccapuclla 2013@gmail.com)', 'section': 'Module 2: Experiment tracking', 'question': 'Can’t import sklearn', 'course': 'mlops-zoomcamp', 'id': 'b80401a2'}]), ('88002d35', [{'text': 'Problem: Localhost:5000 Unavailable // Access to Localhost Denied // You don’t have authorization to view this page (127.0.0.1:5000)\\n\\nSolution: If you are on an chrome browser you need to head to `chrome://net-internals/#sockets` and press “Flush Socket Pools”', 'section': 'Module 2: Experiment tracking', 'question': 'Access Denied at Localhost:5000 - Authorization Issue', 'course': 'mlops-zoomcamp', 'id': '88002d35'}]), ('fe61aa5b', [{'text': \"You have something running on the 5000 port. You need to stop it.\\nAnswer: On terminal in mac .\\nRun ps -A | grep gunicorn\\nLook for the number process id which is the 1st number after running the command\\nkill 13580\\nwhere 13580  represents the process number.\\nSource\\nwarrie.warrieus@gmail.com\\nOr by executing the following command it will kill all the processes using port 5000:\\n>> sudo fuser -k 5000/tcp\\nAnswered by Vaibhav Khandelwal\\nJust execute in the command below in he command line to kill the running port\\n->> kill -9 $(ps -A | grep python | awk '{print $1}')\\nAnswered by kamaldeen (kamaldeen32@gmail.com)\\nChange to different port (5001 in this case)\\n>> mlflow ui --backend-store-uri sqlite:///mlflow.db --port 5001\\nAnswered by krishna (nellaikrishna@gmail.com)\", 'section': 'Module 2: Experiment tracking', 'question': \"Connection in use: ('127.0.0.1', 5000)\", 'course': 'mlops-zoomcamp', 'id': 'fe61aa5b'}]), ('b9adeb39', [{'text': 'Running python register_model.py results in the following error:\\nValueError: could not convert string to float: \\'0 int\\\\n1   float\\\\n2     hyperopt_param\\\\n3       Literal{n_estimators}\\\\n4       quniform\\\\n5         Literal{10}\\\\n6         Literal{50}\\\\n7         Literal{1}\\'\\nFull Traceback:\\nTraceback (most recent call last):\\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 101, in <module>\\nrun(args.data_path, args.top_n)\\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 67, in run\\ntrain_and_log_model(data_path=data_path, params=run.data.params)\\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 41, in train_and_log_model\\nparams = space_eval(SPACE, params)\\nFile \"/Users/name/miniconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/hyperopt/fmin.py\", line 618, in space_eval\\nrval = pyll.rec_eval(space, memo=memo)\\nFile \"/Users/name/miniconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/hyperopt/pyll/base.py\", line 902, in rec_eval\\nrval = scope._impls[node.name](*args, **kwargs)\\nValueError: could not convert string to float: \\'0 int\\\\n1   float\\\\n2     hyperopt_param\\\\n3       Literal{n_estimators}\\\\n4       quniform\\\\n5         Literal{10}\\\\n6         Literal{50}\\\\n7         Literal{1}\\'\\nSolution: There are two plausible errors to this. Both are in the hpo.py file where the hyper-parameter tuning is run. The objective function should look like this.\\n\\n   def objective(params):\\n# It\\'s important to set the \"with\" statement and the \"log_params\" function here\\n# in order to properly log all the runs and parameters.\\nwith mlflow.start_run():\\n# Log the parameters\\nmlflow.log_params(params)\\nrf = RandomForestRegressor(**params)\\nrf.fit(X_train, y_train)\\ny_pred = rf.predict(X_valid)\\n# Calculate and log rmse\\nrmse = mean_squared_error(y_valid, y_pred, squared=False)\\nmlflow.log_metric(\\'rmse\\', rmse)\\nIf you add the with statement before this function, and just after the following line\\nX_valid, y_valid = load_pickle(os.path.join(data_path, \"valid.pkl\"))\\nand you log the parameters just after the search_space dictionary is defined, like this\\nsearch_space = {....}\\n# Log the parameters\\nmlflow.log_params(search_space)\\nThen there is a risk that the parameters will be logged in group. As a result, the\\nparams = space_eval(SPACE, params)\\nregister_model.py file will receive the parameters in group, while in fact it expects to receive them one by one. Thus, make sure that the objective function looks as above.\\nAdded by Jakob Salomonsson', 'section': 'Module 2: Experiment tracking', 'question': 'Could not convert string to float - ValueError', 'course': 'mlops-zoomcamp', 'id': 'b9adeb39'}]), ('ebc13686', [{'text': 'Make sure you launch the mlflow UI from the same directory as thec that is running the experiments (same directory that has the mlflow directory and the database that stores the experiments).\\nOr navigate to the correct directory when specifying the tracking_uri.\\nFor example:\\nIf the mlflow.db is in a subdirectory called database, the tracking uri would be ‘sqllite:///database/mlflow.db’\\nIf the mlflow.db is a directory above your current directory: the tracking uri would be ‘sqlite:///../mlflow.db’\\nAnswered by Anna Vasylytsya\\nAnother alternative is to use an absolute path to mlflow.db rather than relative path\\nAnd yet another alternative is to launch the UI from the same notebook by executing the following code cell\\nimport subprocess\\nMLFLOW_TRACKING_URI = \"sqlite:///data/mlflow.db\"\\nsubprocess.Popen([\"mlflow\", \"ui\", \"--backend-store-uri\", MLFLOW_TRACKING_URI])\\nAnd then using the same MLFLOW_TRACKING_URI when initializing mlflow or the client\\nclient = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)', 'section': 'Module 2: Experiment tracking', 'question': 'Experiment not visible in MLflow UI', 'course': 'mlops-zoomcamp', 'id': 'ebc13686'}]), ('939f9c33', [{'text': \"Problem:\\nGetting\\nERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE\\nduring MLFlow's installation process, particularly while installing the Numpy package using pip\\nWhen I installed mlflow using ‘pip install mlflow’ on 27th May 2022, I got the following error while numpy was getting installed through mlflow:\\n\\nCollecting numpy\\nDownloading numpy-1.22.4-cp310-cp310-win_amd64.whl (14.7 MB)\\n|██████████████              \\t| 6.3 MB 107 kB/s eta 0:01:19\\nERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE.\\nIf you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\\nnumpy from https://files.pythonhosted.org/packages/b5/50/d7978137464251c393df28fe0592fbb968110f752d66f60c7a53f7158076/numpy-1.22.4-cp310-cp310-win_amd64.whl#sha256=3e1ffa4748168e1cc8d3cde93f006fe92b5421396221a02f2274aab6ac83b077 (from mlflow):\\nExpected sha256 3e1ffa4748168e1cc8d3cde93f006fe92b5421396221a02f2274aab6ac83b077\\nGot    \\t15e691797dba353af05cf51233aefc4c654ea7ff194b3e7435e6eec321807e90\\nSolution:\\nThen when I install numpy separately (and not as part of mlflow), numpy gets installed (same version), and then when I do 'pip install mlflow', it also goes through.\\nPlease note that the above may not be consistently simulatable, but please be aware of this issue that could occur during pip install of mlflow.\\nAdded by Venkat Ramakrishnan\", 'section': 'Module 2: Experiment tracking', 'question': 'Hash Mismatch Error with Package Installation', 'course': 'mlops-zoomcamp', 'id': '939f9c33'}]), ('b5c3e6af', [{'text': 'After deleting an experiment from UI, the deleted experiment still persists in the database.\\nSolution: To delete this experiment permanently, follow these steps.\\nAssuming you are using sqlite database;\\nInstall ipython sql using the following command: pip install ipython-sql\\nIn your jupyter notebook, load the SQL magic scripts with this: %load_ext sql\\nLoad the database with this: %sql sqlite:///nameofdatabase.db\\nRun the following SQL script to delete the experiment permanently: check link', 'section': 'Module 2: Experiment tracking', 'question': 'How to Delete an Experiment Permanently from MLFlow UI', 'course': 'mlops-zoomcamp', 'id': 'b5c3e6af'}]), ('80554fc2', [{'text': 'Problem: I cloned the public repo, made edits, committed and pushed them to my own repo. Now I want to get the recent commits from the public repo without overwriting my own changes to my own repo. Which command(s) should I use?\\nThis is what my config looks like (in case this might be useful):\\n[core]\\nrepositoryformatversion = 0\\nfilemode = true\\nbare = false\\nlogallrefupdates = true\\nignorecase = true\\nprecomposeunicode = true\\n[remote \"origin\"]\\nurl = git@github.com:my_username/mlops-zoomcamp.git\\nfetch = +refs/heads/*:refs/remotes/origin/*\\n[branch \"main\"]\\nremote = origin\\nmerge = refs/heads/main\\nSolution: You should fork DataClubsTak’s repo instead of cloning it. On GitHub, click “Fetch and Merge” under the menu “Fetch upstream” at the main page of your own', 'section': 'Module 2: Experiment tracking', 'question': 'How to Update Git Public Repo Without Overwriting Changes', 'course': 'mlops-zoomcamp', 'id': '80554fc2'}]), ('943df153', [{'text': 'This is caused by ```mlflow.xgboost.autolog()``` when version 1.6.1 of xgboost\\nDowngrade to 1.6.0\\n```pip install xgboost==1.6.0``` or update requirements file with xgboost==1.6.0 instead of xgboost\\nAdded by Nakul Bajaj', 'section': 'Module 2: Experiment tracking', 'question': 'Image size of 460x93139 pixels is too large. It must be less than 2^16 in each direction.', 'course': 'mlops-zoomcamp', 'id': '943df153'}]), ('b8d3c55e', [{'text': 'Since the version 1.29 the list_experiments method was deprecated and then removed in the later version\\nYou should use search_experiments instead\\nAdded by Alex Litvinov', 'section': 'Module 2: Experiment tracking', 'question': \"MlflowClient object has no attribute 'list_experiments'\", 'course': 'mlops-zoomcamp', 'id': 'b8d3c55e'}]), ('67bf60c6', [{'text': 'Make sure `mlflow.autolog()` ( or framework-specific autolog ) written BEFORE `with mlflow.start_run()` not after.\\nAlso make sure that all dependencies for the autologger are installed, including matplotlib. A warning about uninstalled dependencies will be raised.\\nMohammed Ayoub Chettouh', 'section': 'Module 2: Experiment tracking', 'question': 'MLflow Autolog not working', 'course': 'mlops-zoomcamp', 'id': '67bf60c6'}]), ('336f5e36', [{'text': 'If you’re running MLflow on a remote VM, you need to forward the port too like we did in Module 1 for Jupyter notebook port 8888. Simply connect your server to VS Code, as we did, and add 5000 to the PORT like in the screenshot:\\nAdded by Sharon Ibejih\\nIf you are running MLflow locally and 127.0.0.1:5000 shows a blank page navigate to localhost:5000 instead.', 'section': 'Module 2: Experiment tracking', 'question': 'MLflow URL (http://127.0.0.1:5000), doesn’t open.', 'course': 'mlops-zoomcamp', 'id': '336f5e36'}]), ('fd2b9972', [{'text': 'Got the same warning message as Warrie Warrie when using “mlflow.xgboost.autolog()”\\nIt turned out that this was just a warning message and upon checking MLflow UI (making sure that no “tag” filters were included), the model was actually automatically tracked in the MLflow.\\nAdded by Bengsoon Chuah, Asked by Warrie Warrie, Answered by Anna Vasylytsya & Ivan Starovit', 'section': 'Module 2: Experiment tracking', 'question': 'MLflow.xgboost Autolog Model Signature Failure', 'course': 'mlops-zoomcamp', 'id': 'fd2b9972'}]), ('75cd9b7a', [{'text': \"mlflow.exceptions.MlflowException: Cannot set a deleted experiment 'cross-sell' as the active experiment. You can restore the experiment, or permanently delete the  experiment to create a new one.\\nThere are many options to solve in this link: https://stackoverflow.com/questions/60088889/how-do-you-permanently-delete-an-experiment-in-mlflow\", 'section': 'Module 2: Experiment tracking', 'question': 'MlflowException: Unable to Set a Deleted Experiment', 'course': 'mlops-zoomcamp', 'id': '75cd9b7a'}]), ('51c99586', [{'text': 'You do not have enough disk space to install the requirements. You can either increase the base EBS volume by following this link or add an external disk to your instance and configure conda installation to happen on the external disk.\\nAbinaya Mahendiran\\nOn GCP: I added another disk to my vm and followed this guide to mount the disk. Confirm the mount by running df -H (disk free) command in bash shell. I also deleted Anaconda and instead used miniconda. I downloaded miniconda in the additional disk that I mounted and when installing miniconda, enter the path to the extra disk instead of the default disk, this way conda is installed on the extra disk.\\nYang Cao', 'section': 'Module 2: Experiment tracking', 'question': 'No Space Left on Device - OSError[Errno 28]', 'course': 'mlops-zoomcamp', 'id': '51c99586'}]), ('089c8c18', [{'text': 'I was using an old version of sklearn due to which I got the wrong number of parameters because in the latest version min_impurity_split for randomforrestRegressor was deprecated. Had to upgrade to the latest version to get the correct number of params.', 'section': 'Module 2: Experiment tracking', 'question': 'Parameters Mismatch in Homework Q3', 'course': 'mlops-zoomcamp', 'id': '089c8c18'}]), ('f4b82056', [{'text': \"Error: I installed all the libraries from the requirements.txt document in a new environment as follows:\\npip install -r requirementes.txt\\nThen when I run mlflow from my terminal like this:\\nmlflow\\nI get this error:\\nSOLUTION: You need to downgrade the version of 'protobuf' module to 3.20.x or lower. Initially, it was version=4.21, I installed protobuf==3.20\\npip install protobuf==3.20\\nAfter which I was able to run mlflow from my terminal.\\n-Submitted by Aashnna Soni\", 'section': 'Module 2: Experiment tracking', 'question': 'Protobuf error when installing MLflow', 'course': 'mlops-zoomcamp', 'id': 'f4b82056'}]), ('dd2e7dc9', [{'text': 'Please check your current directory while running the mlflow ui command. You need to run mlflow ui or mlflow server command in the right directory.', 'section': 'Module 2: Experiment tracking', 'question': 'Setting up Artifacts folders', 'course': 'mlops-zoomcamp', 'id': 'dd2e7dc9'}]), ('3fcbd80e', [{'text': 'If you have problem with setting up MLflow for experiment tracking on GCP, you can check these two links:\\nhttps://kargarisaac.github.io/blog/mlops/data%20engineering/2022/06/15/MLFlow-on-GCP.html\\nhttps://kargarisaac.github.io/blog/mlops/2022/08/26/machine-learning-workflow-orchestration-zenml.html', 'section': 'Module 2: Experiment tracking', 'question': 'Setting up MLflow experiment tracker on GCP', 'course': 'mlops-zoomcamp', 'id': '3fcbd80e'}]), ('924fcf47', [{'text': 'Solution: Downgrade setuptools (I downgraded 62.3.2 -> 49.1.0)', 'section': 'Module 2: Experiment tracking', 'question': 'Setuptools Replacing Distutils - MLflow Autolog Warning', 'course': 'mlops-zoomcamp', 'id': '924fcf47'}]), ('58240887', [{'text': 'I can’t sort runs in MLFlow\\nMake sure you are in table view (not list view) in the MLflow UI.\\nAdded and Answered by Anna Vasylytsya', 'section': 'Module 2: Experiment tracking', 'question': 'Sorting runs in MLflow UI', 'course': 'mlops-zoomcamp', 'id': '58240887'}]), ('67d343f2', [{'text': 'Problem: When I ran `$ mlflow ui` on a remote server and try to open it in my local browser I got an exception  and the page with mlflow ui wasn’t loaded.\\nSolution: You should `pip uninstall flask` on your remote server on conda env and after it install Flask `pip install Flask`. It is because the base conda env has ~flask<1.2, and when you clone it to your new work env, you are stuck with this old version.\\nAdded by Salimov Ilnaz', 'section': 'Module 2: Experiment tracking', 'question': \"TypeError: send_file() unexpected keyword 'max_age' during MLflow UI Launch\", 'course': 'mlops-zoomcamp', 'id': '67d343f2'}]), ('6de95c2a', [{'text': 'Problem: After successfully installing mlflow using pip install mlflow on my Windows system, I am trying to run the mlflow ui command but it throws the following error:\\nFileNotFoundError: [WinError 2] The system cannot find the file specified\\nSolution: Add C:\\\\Users\\\\{User_Name}\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts to the PATH\\nAdded by Alex Litvinov', 'section': 'Module 2: Experiment tracking', 'question': 'mlflow ui on Windows FileNotFoundError: [WinError 2] The system cannot find the file specified', 'course': 'mlops-zoomcamp', 'id': '6de95c2a'}]), ('2ff28e5b', [{'text': 'Running “python hpo.py --data_path=./your-path --max_evals=50” for the homework leads to the following error: TypeError: unsupported operand type(s) for -: \\'str\\' and \\'int\\'\\nFull Traceback:\\nFile \"~/repos/mlops/02-experiment-tracking/homework/hpo.py\", line 73, in <module>\\nrun(args.data_path, args.max_evals)\\nFile \"~/repos/mlops/02-experiment-tracking/homework/hpo.py\", line 47, in run\\nfmin(\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 540, in fmin\\nreturn trials.fmin(\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/base.py\", line 671, in fmin\\nreturn fmin(\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 586, in fmin\\nrval.exhaust()\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 364, in exhaust\\nself.run(self.max_evals - n_done, block_until_done=self.asynchronous)\\nTypeError: unsupported operand type(s) for -: \\'str\\' and \\'int\\'\\nSolution:\\nThe --max_evals argument in hpo.py has no defined datatype and will therefore implicitly be treated as string. It should be an integer, so that the script can work correctly. Add type=int to the argument definition:\\nparser.add_argument(\\n\"--max_evals\",\\ntype=int,\\ndefault=50,\\nhelp=\"the number of parameter evaluations for the optimizer to explore.\"\\n)', 'section': 'Module 2: Experiment tracking', 'question': 'Unsupported Operand Type Error in hpo.py', 'course': 'mlops-zoomcamp', 'id': '2ff28e5b'}]), ('29c6bbf1', [{'text': 'Getting the following warning when running mlflow.sklearn:\\n\\n2022/05/28 04:36:36 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow. […]\\nSolution: use 0.22.1 <= scikit-learn <= 1.1.0\\nReference: https://www.mlflow.org/docs/latest/python_api/mlflow.sklearn.html', 'section': 'Module 2: Experiment tracking', 'question': 'Unsupported Scikit-Learn version', 'course': 'mlops-zoomcamp', 'id': '29c6bbf1'}]), ('bd09df94', [{'text': 'Problem: CLI commands (mlflow experiments list) do not return experiments\\nSolution description: need to set environment variable for the Tracking URI:\\n$ export MLFLOW_TRACKING_URI=http://127.0.0.1:5000\\nAdded and Answered by Dino Vitale', 'section': 'Module 2: Experiment tracking', 'question': 'Mlflow CLI does not return experiments', 'course': 'mlops-zoomcamp', 'id': 'bd09df94'}]), ('af887c59', [{'text': 'Problem: After starting the tracking server, when we try to use the mlflow cli commands as listed here, most of them can’t seem to find the experiments that have been run with the tracking server\\nSolution: We need to set the environment variable MLFLOW_TRACKING_URI to the URI of the sqlite database. This is something like “export MLFLOW_TRACKING_URI=sqlite:///{path to sqlite database}” . After this, we can view the experiments from the command line using commands like “mlflow experiments search”\\nEven after this commands like “mlflow gc” doesn’t seem to get the tracking uri, and they have to be passed explicitly as an argument every time the command is run.\\nAhmed Fahim (afahim03@yahoo.com)', 'section': 'Module 2: Experiment tracking', 'question': 'Viewing MLflow Experiments using MLflow CLI', 'course': 'mlops-zoomcamp', 'id': 'af887c59'}]), ('ee7c59ea', [{'text': 'All the experiment and other tracking information in mlflow are stored in sqllite database provided while initiating the mlflow ui command. This database can be inspected using Pycharm’s Database tab by using the SQLLite database type. Once the connection is created as below, the tables can be queried and inspected using regular SQL. The same applies for any SQL backed database such as postgres as well.\\nThis is very useful to understand the entity structure of the data being stored within mlflow and useful for any kind of systematic archiving of model tracking for longer periods.\\nAdded by Senthilkumar Gopal', 'section': 'Module 2: Experiment tracking', 'question': 'Viewing SQLlite Data Raw & Deleting Experiments Manually', 'course': 'mlops-zoomcamp', 'id': 'ee7c59ea'}]), ('a2531c75', [{'text': 'Solution : It is another way to start it for remote hosting a mlflow server. For example, if you are multiple colleagues working together on something you most likely would not run mlflow on one laptop but rather everyone would connect to the same server running mlflow\\nAnswer by Christoffer Added by Akshit Miglani (akshit.miglani09@gmail.com)', 'section': 'Module 2: Experiment tracking', 'question': 'What does launching the tracking server locally mean?', 'course': 'mlops-zoomcamp', 'id': 'a2531c75'}]), ('bc4b2320', [{'text': 'Problem: parameter was not recognized during the model registry\\nSolution: parameters should be added in previous to the model registry. The parameters can be added by mlflow.log_params(params) so that the dictionary can be directly appended to the data.run.params.\\nAdded and Answered by Sam Lim', 'section': 'Module 2: Experiment tracking', 'question': 'Parameter adding in case of max_depth not recognized', 'course': 'mlops-zoomcamp', 'id': 'bc4b2320'}]), ('f69fb077', [{'text': 'Problem: Max_depth is not recognize even when I add the mlflow.log_params\\nSolution: the mlflow.log_params(params) should be added to the hpo.py script, but if you run it it will append the new model to the previous run that doesn’t contain the parameters, you should either remove the previous experiment or change it\\nPastor Soto', 'section': 'Module 2: Experiment tracking', 'question': 'Max_depth is not recognize even when I add the mlflow.log_params', 'course': 'mlops-zoomcamp', 'id': 'f69fb077'}]), ('e223524c', [{'text': \"Problem: About week_2 homework: The register_model.py  script, when I copy it into a jupyter notebook fails and spits out the following error. AttributeError: 'tuple' object has no attribute 'tb_frame'\\nSolution: remove click decorators\", 'section': 'Module 2: Experiment tracking', 'question': \"AttributeError: 'tuple' object has no attribute 'tb_frame'\", 'course': 'mlops-zoomcamp', 'id': 'e223524c'}]), ('0f08bec7', [{'text': 'Problem: when running the preprocess_data.py file you get the following error:\\n\\nwandb: ERROR api_key not configured (no-tty). call wandb.login(key=[your_api_key])\\nSolution: Go to your WandB profile (top RHS) → user settings → scroll down to “Danger Zone” and copy your API key. \\n\\nThen before running preprocess_data.py, add and run the following cell in your notebook:\\n\\n%%bash\\n\\nWandb login <YOUR_API_KEY_HERE>.\\nAdded and Answered by James Gammerman (jgammerman@gmail.com)', 'section': 'Module 2: Experiment tracking', 'question': 'WandB API error', 'course': 'mlops-zoomcamp', 'id': '0f08bec7'}]), ('8b4b1685', [{'text': 'Please make sure you following the order below nd enabling the autologging before constructing the dataset. If you still have this issue check that your data is in format compatible with XGBoost.\\n# Enable MLflow autologging for XGBoost\\nmlflow.xgboost.autolog()\\n# Construct your dataset\\nX_train, y_train = ...\\n# Train your XGBoost model\\nmodel = xgb.XGBRegressor(...)\\nmodel.fit(X_train, y_train)\\nAdded by Olga Rudakova', 'section': 'Module 2: Experiment tracking', 'question': 'WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.', 'course': 'mlops-zoomcamp', 'id': '8b4b1685'}]), ('ecfc5c07', [{'text': 'Problem\\nUsing wget command to download either data or python scripts on Windows, I am using the notebook provided by Visual Studio and despite having a python virtual env, it did not recognize the pip command.\\nSolution: Use python -m pip, this same for any other command. Ie. python -m wget\\nAdded by Erick Calderin', 'section': 'Module 2: Experiment tracking', 'question': 'wget not working', 'course': 'mlops-zoomcamp', 'id': 'ecfc5c07'}]), ('a1b68c52', [{'text': \"Problem: Open/run github notebook(.ipynb) directly in Google Colab\\nSolution: Change the domain from 'github.com' to 'githubtocolab.com'. The notebook will open in Google Colab.\\nOnly works with Public repo.\\nAdded by Ming Jun\\nNavigating in Wandb UI became difficult to me, I had to intuit some options until I found the correct one.\\nSolution: Go to the official doc.\\nAdded by Erick Calderin\", 'section': 'Module 2: Experiment tracking', 'question': 'Open/run github notebook(.ipynb) directly in Google Colab', 'course': 'mlops-zoomcamp', 'id': 'a1b68c52'}]), ('483e7d61', [{'text': 'Problem: Someone asked why we are using this type of split approach instead of just a random split.\\nSolution: For example, I have some models at work that train on Jan 1 2020 — Aug 1 2021 time period, and then test on Aug 1 - Dec 31 2021, and finally validate on Jan - March or something\\nWe do these “out of time”  validations to do a few things:\\nCheck for seasonality of our data\\nWe know if the RMSE for Test is 5 say, and then RMSE for validation is 20, then there’s serious seasonality to the data we are looking at, and now we might change to Time Series approaches\\nIf I’m predicting on Mar 30 2023 the outcomes for the next 3 months, the “random sample” in our train/test would have caused data leakage, overfitting, and poor model performance in production. We mustn’t take information about the future and apply it to the present when we are predicting in a model context.\\nThese are two of, I think, the biggest points for why we are doing jan/feb/march. I wouldn’t do it any other way.\\nTrain: Jan\\nTest: Feb\\nValidate: March\\nThe point of validation is to report out model metrics to leadership, regulators, auditors, and record the models performance to then later analyze target drift\\nAdded by Sam LaFell\\nProblem: If you get an error while trying to run the mlflow server on AWS CLI with S3 bucket and POSTGRES database:\\nReproducible Command:\\nmlflow server -h 0.0.0.0 -p 5000 --backend-store-uri postgresql://<DB_USERNAME>:<DB_PASSWORD>@<DB_ENDPOINT>:<DB_PORT>/<DB_NAME> --default-artifact-root s3://<BUCKET_NAME>\\nError:\\n\"urllib3 v2.0 only supports OpenSSL 1.1.1+, currently \"\\nImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the \\'ssl\\' module is compiled with \\'OpenSSL 1.0.2k-fips  26 Jan 2017\\'. See: https://github.com/urllib3/urllib3/issues/2168\\nSolution: Upgrade mlflow using\\nCode: pip3 install --upgrade mlflow\\nResolution: It downgrades urllib3 2.0.3 to 1.26.16 which is compatible with mlflow and ssl 1.0.2\\nInstalling collected packages: urllib3\\nAttempting uninstall: urllib3\\nFound existing installation: urllib3 2.0.3\\nUninstalling urllib3-2.0.3:\\nSuccessfully uninstalled urllib3-2.0.3\\nSuccessfully installed urllib3-1.26.16\\nAdded by Sarvesh Thakur', 'section': 'Module 3: Orchestration', 'question': 'Why do we use Jan/Feb/March for Train/Test/Validation Purposes?', 'course': 'mlops-zoomcamp', 'id': '483e7d61'}]), ('e5c33f50', [{'text': 'Problem description\\nSolution description\\n(optional) Added by Name', 'section': 'Module 3: Orchestration', 'question': 'Problem title', 'course': 'mlops-zoomcamp', 'id': 'e5c33f50'}]), ('cbf13b19', [{'text': 'Here', 'section': 'Module 4: Deployment', 'question': 'Where is the FAQ for Prefect questions?', 'course': 'mlops-zoomcamp', 'id': 'cbf13b19'}]), ('39861d6e', [{'text': 'Windows with AWS CLI already installed\\nAWS CLI version:\\naws-cli/2.4.24 Python/3.8.8 Windows/10 exe/AMD64 prompt/off\\nExecuting\\n$(aws ecr get-login --no-include-email)\\nshows error\\naws.exe: error: argument operation: Invalid choice, valid choices are…\\nUse this command instead. More info here:\\nhttps://docs.aws.amazon.com/cli/latest/reference/ecr/get-login-password.html\\naws ecr get-login-password \\\\\\n--region <region> \\\\\\n| docker login \\\\\\n--username AWS \\\\\\n--password-stdin <aws_account_id>.dkr.ecr.<region>.amazonaws.com\\nAdded by MarcosMJD', 'section': 'Module 4: Deployment', 'question': 'aws.exe: error: argument operation: Invalid choice — Docker can not login to ECR.', 'course': 'mlops-zoomcamp', 'id': '39861d6e'}]), ('3dac15ff', [{'text': 'Use ` at the end of each line except the last. Note that multiline string does not need `.\\nEscape “ to “\\\\ .\\nUse $env: to create env vars (non-persistent). E.g.:\\n$env:KINESIS_STREAM_INPUT=\"ride_events\"\\naws kinesis put-record --cli-binary-format raw-in-base64-out `\\n--stream-name $env:KINESIS_STREAM_INPUT `\\n--partition-key 1 `\\n--data \\'{\\n\\\\\"ride\\\\\": {\\n\\\\\"PULocationID\\\\\": 130,\\n\\\\\"DOLocationID\\\\\": 205,\\n\\\\\"trip_distance\\\\\": 3.66\\n},\\n\\\\\"ride_id\\\\\": 156\\n}\\'\\nAdded by MarcosMJD', 'section': 'Module 4: Deployment', 'question': 'Multiline commands in Windows Powershell', 'course': 'mlops-zoomcamp', 'id': '3dac15ff'}]), ('32686722', [{'text': \"If one gets pipenv failures for pipenv install command -\\nAttributeError: module 'collections' has no attribute 'MutableMapping'\\nIt happens because you use the system Python (3.10) for pipenv.\\nIf you previously installed pipenv with apt-get, remove it - sudo-apt remove pipenv\\nMake sure you have a non-system Python installed in your environment. The easiest way to do it is to install anaconda or miniconda\\nNext, install pipenv to your non-system Python. If you use the setup from the lectures, it’s just this: pip install pipenv\\nNow re-run pipenv install XXXX (relevant dependencies) - should work\\nTested and worked on AWS instance, similar to the config Alexey presented in class.\\nAdded by Daniel HenSSL\", 'section': 'Module 4: Deployment', 'question': \"Pipenv installation not working (AttributeError: module 'collections' has no attribute 'MutableMapping')\", 'course': 'mlops-zoomcamp', 'id': '32686722'}]), ('22521751', [{'text': 'First check if SSL module configured with following command:\\nPython -m ssl\\n\\nIf the output of this is empty there is no problem with SSL configuration.\\n\\nThen you should upgrade your pipenv package in your current environment to resolve the problem.\\nAdded by Kenan Arslanbay', 'section': 'Module 4: Deployment', 'question': \"module is not available (Can't connect to HTTPS URL)\", 'course': 'mlops-zoomcamp', 'id': '22521751'}]), ('81ad4784', [{'text': \"During scikit-learn installation via the command:\\npipenv install scikit-learn==1.0.2\\nThe following error is raised:\\nModuleNotFoundError: No module named 'pip._vendor.six'\\nThen, one should:\\nsudo apt install python-six\\npipenv --rm\\npipenv install scikit-learn==1.0.2\\nAdded by Giovanni Pecoraro\", 'section': 'Module 4: Deployment', 'question': \"No module named 'pip._vendor.six'\", 'course': 'mlops-zoomcamp', 'id': '81ad4784'}]), ('29b5651e', [{'text': 'Problem description. How can we use Jupyter notebooks with the Pipenv environment?\\nSolution: Refer to this stackoverflow question. Basically install jupyter and ipykernel using pipenv. And then register the kernel with `python -m ipykernel install --user --name=my-virtualenv-name` inside the Pipenv shell. If you are using Jupyter notebooks in VS Code, doing this will also add the virtual environment in the list of kernels.\\nAdded by Ron Medina', 'section': 'Module 4: Deployment', 'question': 'Pipenv with Jupyter', 'course': 'mlops-zoomcamp', 'id': '29b5651e'}]), ('ca79bbe8', [{'text': \"Problem: I tried to run starter notebook on pipenv environment but had issues with no output on prints. \\nI used scikit-learn==1.2.2 and python==3.10\\nTornado version was 6.3.2\\n\\nSolution: The error you're encountering seems to be a bug related to Tornado, which is a Python web server and networking library. It's used by Jupyter under the hood to handle networking tasks.\\nDowngrading to tornado==6.1 fixed the issue\\nhttps://stackoverflow.com/questions/54971836/no-output-jupyter-notebook\", 'section': 'Module 4: Deployment', 'question': 'Pipenv with Jupyter no output', 'course': 'mlops-zoomcamp', 'id': 'ca79bbe8'}]), ('668f1ad9', [{'text': 'Problem description:  You might get an error ‘Invalid base64’ after running the ‘aws kinesis put-record’ command on your local machine. This might be the case if you are using the AWS CLI version 2 (note that in the video 4.4, around 57:42, you can see a warning since the instructor is using v1 of the CLI.\\nSolution description: To get around this, pass the argument ‘--cli-binary-format raw-in-base64-out’. This will encode your data string into base64 before passing it to kinesis\\nAdded by M', 'section': 'Module 4: Deployment', 'question': '‘Invalid base64’ error after running `aws kinesis put-record`', 'course': 'mlops-zoomcamp', 'id': '668f1ad9'}]), ('7a6f23eb', [{'text': 'Problem description:   Running starter.ipynb in homework’s Q1 will show up this error.\\nSolution description: Update pandas (actually pandas version was the latest, but several dependencies are updated).\\nAdded by Marcos Jimenez', 'section': 'Module 4: Deployment', 'question': 'Error index 311297 is out of bounds for axis 0 with size 131483 when loading parquet file.', 'course': 'mlops-zoomcamp', 'id': '7a6f23eb'}]), ('232e5557', [{'text': 'Use command $pipenv lock to force the creation of Pipfile.lock\\nAdded by Bijay P.', 'section': 'Module 4: Deployment', 'question': 'Pipfile.lock was not created along with Pipfile', 'course': 'mlops-zoomcamp', 'id': '232e5557'}]), ('e44ec04a', [{'text': 'This issue is usually due to the pythonfinder module in pipenv.\\nThe solution to this involves manually changing the scripts as describe here python_finder_fix\\nAdded by Ridwan Amure', 'section': 'Module 4: Deployment', 'question': 'Permission Denied using Pipenv', 'course': 'mlops-zoomcamp', 'id': 'e44ec04a'}]), ('55fdb8b9', [{'text': 'When passing arguments to a script via command line and converting it to a 4 digit number using f’{year:04d}’, this error showed up.\\nThis happens because all inputs from the command line are read as string by the script. They need to be converted to numeric/integer before transformation in fstring.\\nyear = int(sys.argv[1])\\nf’{year:04d}’\\nIf you use click library just edit a decorator\\n@click.command()\\n@click.option( \"--year\",  help=\"Year for evaluation\",   type=int)\\ndef  your_function(year):\\n<<Your code>>\\nAdded by Taras Sh', 'section': 'Module 4: Deployment', 'question': \"Error while parsing arguments via CLI  [ValueError: Unknown format code 'd' for object of type 'str']\", 'course': 'mlops-zoomcamp', 'id': '55fdb8b9'}]), ('bf9082a2', [{'text': 'Ensure the correct image is being used to derive from.\\nCopy the data from local to the docker image using the COPY command to a relative path. Using absolute paths within the image might be troublesome.\\nUse paths starting from /app and don’t forget to do WORKDIR /app before actually performing the code execution.\\nMost common commands\\nBuild container using docker build -t mlops-learn .\\nExecute the script using docker run -it --rm mlops-learn\\n<mlops-learn> is just a name used for the image and does not have any significance.', 'section': 'Module 4: Deployment', 'question': 'Dockerizing tips', 'course': 'mlops-zoomcamp', 'id': 'bf9082a2'}]), ('e7906e44', [{'text': 'If you are trying to run Flask gunicorn & MLFlow server from the same container, defining both in Dockerfile with CMD will only run MLFlow & not Flask.\\nSolution: Create separate shell script with server run commands, for eg:\\n> \\tscript1.sh\\n#!/bin/bash\\ngunicorn --bind=0.0.0.0:9696 predict:app\\nAnother script with e.g. MLFlow server:\\n>\\tscript2.sh\\n#!/bin/bash\\nmlflow server -h 0.0.0.0 -p 5000 --backend-store-uri=sqlite:///mlflow.db --default-artifact-root=g3://zc-bucket/mlruns/\\nCreate a wrapper script to run above 2 scripts:\\n>\\twrapper_script.sh\\n#!/bin/bash\\n# Start the first process\\n./script1.sh &\\n# Start the second process\\n./script2.sh &\\n# Wait for any process to exit\\nwait -n\\n# Exit with status of process that exited first\\nexit $?\\nGive executable permissions to all scripts:\\nchmod +x *.sh\\nNow we can define last line of Dockerfile as:\\n> \\tDockerfile\\nCMD ./wrapper_script.sh\\nDont forget to expose all ports defined by services!', 'section': 'Module 4: Deployment', 'question': 'Running multiple services in a Docker container', 'course': 'mlops-zoomcamp', 'id': 'e7906e44'}]), ('76d8892e', [{'text': 'Problem description cannot generate pipfile.lock raise InstallationError( pip9.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with error code 1\\nSolution: you need to force and upgrade wheel and pipenv\\nJust run the command line :\\npip install --user --upgrade --upgrade-strategy eager pipenv wheel', 'section': 'Module 4: Deployment', 'question': 'Cannot generate pipfile.lock raise InstallationError( pip9.exceptions.InstallationError)', 'course': 'mlops-zoomcamp', 'id': '76d8892e'}]), ('c5c2c82a', [{'text': \"Problem description. How can we connect s3 bucket to MLFLOW?\\nSolution: Use boto3 and AWS CLI to store access keys. The access keys are what will be used by boto3 (AWS' Python API tool) to connect with the AWS servers. If there are no Access Keys how can they make sure that they have the right to access this Bucket? Maybe you're a malicious actor (Hacker for ex). The keys must be present for boto3 to talk to the AWS servers and they will provide access to the Bucket if you possess the right permissions. You can always set the Bucket as public so anyone can access it, now you don't need access keys because AWS won't care.\\nRead more here: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\\nAdded by Akshit Miglani\", 'section': 'Module 4: Deployment', 'question': 'Connecting s3 bucket to MLFLOW', 'course': 'mlops-zoomcamp', 'id': 'c5c2c82a'}]), ('82b6c143', [{'text': 'Even though the upload works using aws cli and boto3 in Jupyter notebook.\\nSolution set the AWS_PROFILE environment variable (the default profile is called default)', 'section': 'Module 4: Deployment', 'question': 'Uploading to s3 fails with An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The AWS Access Key Id you provided does not exist in our records.\"', 'course': 'mlops-zoomcamp', 'id': '82b6c143'}]), ('77d9a742', [{'text': 'Problem description: lib_lightgbm.so Reason: image not found\\nSolution description: Add “RUN apt-get install libgomp1” to your docker. (change installer command based on OS)\\nAdded by Kazeem Hakeem', 'section': 'Module 4: Deployment', 'question': 'Dockerizing lightgbm', 'course': 'mlops-zoomcamp', 'id': '77d9a742'}]), ('1667e95d', [{'text': 'When the request is processed in lambda function, mlflow library raises:\\n2022/09/19 21:18:47 WARNING mlflow.pyfunc: Encountered an unexpected error (AttributeError(\"module \\'dataclasses\\' has no attribute \\'__version__\\'\")) while detecting model dependency mismatches. Set logging level to DEBUG to see the full traceback.\\nSolution: Increase the memory of the lambda function.\\nAdded by MarcosMJD', 'section': 'Module 4: Deployment', 'question': 'Error raised when executing mlflow’s pyfunc.load_model in lambda function.', 'course': 'mlops-zoomcamp', 'id': '1667e95d'}]), ('624a3525', [{'text': 'Just a note if you are following the video but also using the repo’s notebook The notebook is the end state of the video which eventually uses mlflow pipelines.\\nJust watch the video and be patient. Everything will work :)\\nAdded by Quinn Avila', 'section': 'Module 4: Deployment', 'question': '4.3 FYI Notebook is end state of Video -', 'course': 'mlops-zoomcamp', 'id': '624a3525'}]), ('1db86601', [{'text': 'Problem description: I was having issues because my python script was not reading AWS credentials from env vars, after building the image I was running it like this:\\ndocker run -it homework-04 -e AWS_ACCESS_KEY_ID=xxxxxxxx -e AWS_SECRET_ACCESS_KEY=xxxxxx\\nSolution 1:\\n\\nEnvironment Variables: \\nYou can set the AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN (if you are using AWS STS) environment variables. You can set these in your shell, or you can include them in your Docker run command like this:\\nI found out by myself that those variables must be passed before specifying the name of the image, as follow:\\ndocker run -e AWS_ACCESS_KEY_ID=xxxxxxxx -e AWS_SECRET_ACCESS_KEY=xxxxxx -it homework-04\\nAdded by Erick Cal\\nSolution 2 (if AWS credentials were not found):\\nAWS Configuration Files: \\nThe AWS SDKs and CLI will check the ~/.aws/credentials and ~/.aws/config files for credentials if they exist. You can map these files into your Docker container using volumes:\\n\\ndocker run -it --rm -v ~/.aws:/root/.aws homework:v1', 'section': 'Module 4: Deployment', 'question': 'Passing envs to my docker image', 'course': 'mlops-zoomcamp', 'id': '1db86601'}]), ('047baefe', [{'text': 'If anyone is troubleshooting or just interested in seeing the model listed on the image svizor/zoomcamp-model:mlops-3.10.0-slim.\\nCreate a dockerfile. (yep thats all) and build “docker build -t zoomcamp_test .”\\nFROM svizor/zoomcamp-model:mlops-3.10.0-slim\\nRun “docker run -it zoomcamp_test ls /app” output -> model.bin\\nThis will list the contents of the app directory and “model.bin” should output. With this you could just copy your files, for example “copy myfile .” maybe a requirements file and this can be run for example “docker run -it myimage myscript arg1 arg2 ”. Of course keep in mind a build is needed everytime you change the Dockerfile.\\nAnother variation is to have it run when you run the docker file.\\n“””\\nFROM svizor/zoomcamp-model:mlops-3.10.0-slim\\nWORKDIR /app\\nCMD ls\\n“””\\nJust keep in mind CMD is needed because the RUN commands are used for building the image and the CMD is used at container runtime. And in your example you probably want to run a script or should we say CMD a script.\\nQuinn Avila', 'section': 'Module 4: Deployment', 'question': 'How to see the model in the docker container in app/?', 'course': 'mlops-zoomcamp', 'id': '047baefe'}]), ('4f240372', [{'text': 'To resolve this make sure to build the docker image with the platform tag, like this:\\n“docker build -t homework:v1 --platform=linux/arm64 .”', 'section': 'Module 4: Deployment', 'question': \"WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\", 'course': 'mlops-zoomcamp', 'id': '4f240372'}]), ('7aef625b', [{'text': \"Solution: instead of input_file = f'https://s3.amazonaws.com/nyc-tlc/trip+data/{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet'  use input_file = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet'\\nIlnaz Salimov\\nsalimovilnaz777@gmail.com\", 'section': 'Module 4: Deployment', 'question': 'HTTPError: HTTP Error 403: Forbidden when call apply_model() in score.ipynb', 'course': 'mlops-zoomcamp', 'id': '7aef625b'}]), ('a3aa3a7d', [{'text': 'i\\'m getting this error ModuleNotFoundError: No module named \\'pipenv.patched.pip._vendor.urllib3.response\\'\\nand Resolved from this command pip install pipenv --force-reinstall\\ngetting this errror site-packages\\\\pipenv\\\\patched\\\\pip\\\\_vendor\\\\urllib3\\\\connectionpool.py\"\\nResolved from this command pip install -U pip and pip install requests\\nAsif', 'section': 'Module 5: Monitoring', 'question': \"ModuleNotFoundError: No module named 'pipenv.patched.pip._vendor.urllib3.response'\", 'course': 'mlops-zoomcamp', 'id': 'a3aa3a7d'}]), ('d2719204', [{'text': 'Problem description: When running docker-compose up as shown in the video 5.2 if you go to http://localhost:3000/ you get asked for a username and a password.\\nSolution: for both of them the default is “admin”. Then you can enter your new password. \\nSee also here\\nAdded by JaimeRV', 'section': 'Module 5: Monitoring', 'question': 'Login window in Grafana', 'course': 'mlops-zoomcamp', 'id': 'd2719204'}]), ('30b8e8e6', [{'text': 'Problem Description : In Linux, when starting services using docker compose up --build  as shown in video 5.2, the services won’t start and instead we get message unknown flag: --build in command prompt.\\nSolution : Since we install docker-compose separately in Linux, we have to run docker-compose up --build instead of docker compose up --build\\nAdded by Ashish Lalchandani', 'section': 'Module 5: Monitoring', 'question': 'Error in starting monitoring services in Linux', 'course': 'mlops-zoomcamp', 'id': '30b8e8e6'}]), ('f33fc6e9', [{'text': 'Problem: When running prepare.py getting KeyError: ‘content-length’\\nSolution: From Emeli Dral:\\nIt seems to me that the link we used in prepare.py to download taxi data does not work anymore. I substituted the instruction:\\nurl = f\"https://nyc-tlc.s3.amazonaws.com/trip+data/{file}\\nby the\\nurl = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{file}\"\\nin the prepare.py and it worked for me. Hopefully, if you do the same you will be able to get those data.', 'section': 'Module 5: Monitoring', 'question': 'KeyError ‘content-length’ when running prepare.py', 'course': 'mlops-zoomcamp', 'id': 'f33fc6e9'}]), ('d828de2a', [{'text': 'Problem description\\nWhen I run the command “docker-compose up –build” and send the data to the real-time prediction service. The service will return “Max retries exceeded with url: /api”.\\nIn my case it because of my evidently service exit with code 2 due to the “app.py” in evidently service cannot import “from pyarrow import parquet as pq”.\\nSolution description\\nThe first solution is just install the pyarrow module “pip install pyarrow”\\nThe second solution is restart your machine.\\nThe third solution is if the first and second one didn’t work with your machine. I found that “app.py” of evidently service didn’t use that module. So comment the pyarrow module out and the problem was solved for me.\\nAdded by Surawut Jirasaktavee', 'section': 'Module 5: Monitoring', 'question': 'Evidently service exit with code 2', 'course': 'mlops-zoomcamp', 'id': 'd828de2a'}]), ('03f20ec1', [{'text': 'When using evidently if you get this error.\\nYou probably forgot to and parentheses () just and opening and closing and you are good to go.\\nQuinn Avila', 'section': 'Module 5: Monitoring', 'question': 'ValueError: Incorrect item instead of a metric or metric preset was passed to Report', 'course': 'mlops-zoomcamp', 'id': '03f20ec1'}]), ('249726fe', [{'text': 'You will get an error if you didn’t add a target=’duration_min’\\nIf you want to use RegressionQualityMetric() you need a target=’duration_min and you need this added to you current_data[‘duration_min’]\\nQuinn Avila', 'section': 'Module 5: Monitoring', 'question': 'For the report RegressionQualityMetric()', 'course': 'mlops-zoomcamp', 'id': '249726fe'}]), ('4e492af0', [{'text': 'Problem description\\nValueError: Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by LinearRegression.\\nSolution description\\nThis happens because the generated data is based on an early date therefore the training dataset would be empty.\\nAdjust the following\\nbegin = datetime.datetime(202X, X, X, 0, 0)\\nAdded by Luke', 'section': 'Module 5: Monitoring', 'question': 'Found array with 0 sample(s)', 'course': 'mlops-zoomcamp', 'id': '4e492af0'}]), ('10011dc1', [{'text': 'Problem description\\nGetting “target columns” “prediction columns” not present errors after adding a metric\\nSolution description\\nMake sure to read through the documentation on what is required or optional when adding the metric. I added DatasetCorrelationsMetric which doesn’t require any parameters because the metric evaluates for correlations among the features.\\nSam Lim', 'section': 'Module 5: Monitoring', 'question': 'Adding additional metric', 'course': 'mlops-zoomcamp', 'id': '10011dc1'}]), ('92fb909a', [{'text': 'When you try to login in Grafana with standard requisites (admin/admin) it throw up an error.\\nAfter run grafana-cli admin reset-admin-password admin in Grafana container the problem will be fixed\\nAdded by Artem Glazkov', 'section': 'Module 5: Monitoring', 'question': 'Standard login in Grafana does not work', 'course': 'mlops-zoomcamp', 'id': '92fb909a'}]), ('2b8cb640', [{'text': 'Problem description. While my metric generation script was still running, I noticed that the charts in Grafana don’t get updated.\\nSolution description. There are two things to pay attention to:\\nRefresh interval: set it to a small value: 5-10-30 seconds\\nUse your local timezone in a call to `pytz.timezone` – I couldn’t get updates before changing this from the original value “Europe/London” to my own zone', 'section': 'Module 5: Monitoring', 'question': 'The chart in Grafana doesn’t get updates', 'course': 'mlops-zoomcamp', 'id': '2b8cb640'}]), ('d4ceab0b', [{'text': 'Problem description. Prefect server was not running locally, I ran `prefect server start` command but it stopped immediately..\\nSolution description. I used Prefect cloud to run the script, however I created an issue on the Prefect github.\\nBy Erick Calderin', 'section': 'Module 5: Monitoring', 'question': 'Prefect server was not running locally', 'course': 'mlops-zoomcamp', 'id': 'd4ceab0b'}]), ('482e575f', [{'text': 'Solution. Using docker CLI run docker system prune to remove unused things (build cache, containers, images etc)\\nAlso, to see what’s taking space before pruning you can run docker system df\\nBy Alex Litvinov', 'section': 'Module 5: Monitoring', 'question': 'no disk space left error when doing docker compose up', 'course': 'mlops-zoomcamp', 'id': '482e575f'}]), ('33e775eb', [{'text': 'Problem: when run docker-compose up –build, you may see this error. To solve, add `command: php -S 0.0.0.0:8080 -t /var/www/html` in adminer block in yml file like:\\nadminer:\\ncommand: php -S 0.0.0.0:8080 -t /var/www/html\\nimage: adminer\\n…\\nIlnaz Salimov\\nsalimovilnaz777@gmail.com', 'section': 'Module 5: Monitoring', 'question': 'Failed to listen on :::8080 (reason: php_network_getaddresses: getaddrinfo failed: Address family for hostname not supported)', 'course': 'mlops-zoomcamp', 'id': '33e775eb'}]), ('19a3d34a', [{'text': 'Problem: Can we generate charts like Evidently inside Grafana?\\nSolution: In Grafana that would be a stat panel (just a number) and scatter plot panel (I believe it requires a plug-in). However, there is no native way to quickly recreate this exact Evidently dashboard. You\\'d need to make sure you have all the relevant information logged to your Grafana data source, and then design your own plots in Grafana.\\nIf you want to recreate the Evidently visualizations externally, you can export the Evidently output in JSON with include_render=True\\n(more details here https://docs.evidentlyai.com/user-guide/customization/json-dict-output) and then parse information from it for your external visualization layer. To include everything you need for non-aggregated visuals, you should also add \"raw_data\": True  option (more details here https://docs.evidentlyai.com/user-guide/customization/report-data-aggregation).\\nOverall, this specific plot with under- and over-performance segments is more useful during debugging, so might be easier to access it ad hoc using Evidently.\\nAdded by Ming Jun, Asked by Luke, Answered by Elena Samuylova', 'section': 'Module 6: Best practices', 'question': 'Generate Evidently Chart in Grafana', 'course': 'mlops-zoomcamp', 'id': '19a3d34a'}]), ('55c68f23', [{'text': \"You may get an error ‘{'errorMessage': 'Unable to locate credentials', …’ from the print statement in test_docker.py after running localstack with kinesis.\\nTo fix this, in the docker-compose.yaml file, in addition to the environment variables like AWS_DEFAULT_REGION, add two other variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. Their value is not important; anything like abc will suffice\\nAdded by M\\nOther possibility is just to run\\naws --endpoint-url http://localhost:4566 configure\\nAnd providing random values for AWS Access Key ID , AWS Secret Access Key, Default region name, and Default output format.\\nAdded by M.A. Monjas\", 'section': 'Module 6: Best practices', 'question': 'Get an error ‘Unable to locate credentials’ after running localstack with kinesis', 'course': 'mlops-zoomcamp', 'id': '55c68f23'}]), ('54020f0a', [{'text': \"You may get an error while creating a bucket with localstack and the boto3 client:\\nbotocore.exceptions.ClientError: An error occurred (IllegalLocationConstraintException) when calling the CreateBucket operation: The unspecified location constraint is incompatible for the region specific endpoint this request was sent to.\\nTo fix this, instead of creating a bucket via\\ns3_client.create_bucket(Bucket='nyc-duration')\\nCreate it with\\ns3_client.create_bucket(Bucket='nyc-duration', CreateBucketConfiguration={\\n'LocationConstraint': AWS_DEFAULT_REGION})\\nyam\\nAdded by M\", 'section': 'Module 6: Best practices', 'question': 'Get an error ‘ unspecified location constraint is incompatible ’', 'course': 'mlops-zoomcamp', 'id': '54020f0a'}]), ('b6249d2c', [{'text': 'When executing an AWS CLI command (e.g., aws s3 ls), you can get the error <botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>.\\nTo fix it, simply set the AWS CLI environment variables:\\nexport AWS_DEFAULT_REGION=eu-west-1\\nexport AWS_ACCESS_KEY_ID=foobar\\nexport AWS_SECRET_ACCESS_KEY=foobar\\nTheir value is not important; anything would be ok.\\nAdded by Giovanni Pecoraro', 'section': 'Module 6: Best practices', 'question': 'Get an error “<botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>” after running an AWS CLI command', 'course': 'mlops-zoomcamp', 'id': 'b6249d2c'}]), ('31543d95', [{'text': 'At every commit the above error is thrown and no pre-commit hooks are ran.\\nMake sure the indentation in .pre-commit-config.yaml is correct. Especially the 4 spaces ahead of every `repo` statement\\nAdded by M. Ayoub C.', 'section': 'Module 6: Best practices', 'question': 'Pre-commit triggers an error at every commit: “mapping values are not allowed in this context”', 'course': 'mlops-zoomcamp', 'id': '31543d95'}]), ('e147bbb6', [{'text': 'No option to remove pytest test\\nRemove .vscode folder located on the folder you previously used for testing, e.g. folder code (from week6-best-practices) was chosen to test, so you may remove .vscode inside the folder.\\nAdded by Rizdi Aprilian', 'section': 'Module 6: Best practices', 'question': 'Could not reconfigure pytest from zero after getting done with previous folder', 'course': 'mlops-zoomcamp', 'id': 'e147bbb6'}]), ('dc55657f', [{'text': 'Problem description\\nFollowing video 6.3, at minute 11:23, get records command returns empty Records.\\nSolution description\\nAdd --no-sign-request to Kinesis get records call:\\n aws --endpoint-url=http://localhost:4566 kinesis get-records --shard-iterator […] --no-sign-request', 'section': 'Module 6: Best practices', 'question': 'Empty Records in Kinesis Get Records with LocalStack', 'course': 'mlops-zoomcamp', 'id': 'dc55657f'}]), ('f6979915', [{'text': \"Problem description\\ngit commit -m 'Updated xxxxxx'\\nAn error has occurred: InvalidConfigError:\\n==> File .pre-commit-config.yaml\\n=====> 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\\nSolution description\\nSet uft-8 encoding when creating the pre-commit yaml file:\\npre-commit sample-config | out-file .pre-commit-config.yaml -encoding utf8\\nAdded by MarcosMJD\", 'section': 'Module 6: Best practices', 'question': 'In Powershell, Git commit raises utf-8 encoding error after creating pre-commit yaml file', 'course': 'mlops-zoomcamp', 'id': 'f6979915'}]), ('1076a121', [{'text': \"Problem description\\ngit commit -m 'Updated xxxxxx'\\n[INFO] Initializing environment for https://github.com/pre-commit/pre-commit-hooks.\\n[INFO] Installing environment for https://github.com/pre-commit/pre-commit-hooks.\\n[INFO] Once installed this environment will be reused.\\nAn unexpected error has occurred: CalledProcessError: command:\\n…\\nreturn code: 1\\nexpected return code: 0\\nstdout:\\nAttributeError: 'PythonInfo' object has no attribute 'version_nodot'\\nSolution description\\nClear app-data of the virtualenv\\npython -m virtualenv api -vvv --reset-app-data\\nAdded by MarcosMJD\", 'section': 'Module 6: Best practices', 'question': \"Git commit with pre-commit hook raises error ‘'PythonInfo' object has no attribute 'version_nodot'\", 'course': 'mlops-zoomcamp', 'id': '1076a121'}]), ('aa203ca7', [{'text': 'Problem description\\nProject structure:\\n/sources/production/model_service.py\\n/sources/tests/unit_tests/test_model_service.py (“from production.model_service import ModelService)\\nWhen running python test_model_service.py from the sources directory, it works.\\nWhen running pytest ./test/unit_tests fails. ‘No module named ‘production’’\\nSolution description\\nUse python -m pytest ./test/unit_tests\\nExplanation: pytest does not add to the sys.path the path where pytest is run.\\nYou can run python -m pytest, or alternatively export PYTHONPATH=. Before executing pytest\\nAdded by MarcosMJD', 'section': 'Module 6: Best practices', 'question': 'Pytest error ‘module not found’ when if using custom packages in the source code', 'course': 'mlops-zoomcamp', 'id': 'aa203ca7'}]), ('8b04605d', [{'text': 'Problem description\\nProject structure:\\n/sources/production/model_service.py\\n/sources/tests/unit_tests/test_model_service.py (“from production.model_service import ModelService)\\ngit commit -t ‘test’ raises ‘No module named ‘production’’ when calling pytest hook\\n- repo: local\\nhooks:\\n- id: pytest-check\\nname: pytest-check\\nentry: pytest\\nlanguage: system\\npass_filenames: false\\nalways_run: true\\nargs: [\\n\"tests/\"\\n]\\nSolution description\\nUse this hook instead:\\n- repo: local\\nhooks:\\n- id: pytest-check\\nname: pytest-check\\nentry: \"./sources/tests/unit_tests/run.sh\"\\nlanguage: system\\ntypes: [python]\\npass_filenames: false\\nalways_run: true\\nAnd make sure that run.sh sets the right directory and run pytest:\\ncd \"$(dirname \"$0\")\"\\ncd ../..\\nexport PYTHONPATH=.\\npipenv run pytest ./tests/unit_tests\\nAdded by MarcosMJD', 'section': 'Module 6: Best practices', 'question': 'Pytest error ‘module not found’ when using pre-commit hooks if using custom packages in the source code', 'course': 'mlops-zoomcamp', 'id': '8b04605d'}]), ('a3b9af04', [{'text': 'Problem description\\nThis is the step in the ci yml file definition:\\n- name: Run Unit Tests\\nworking-directory: \"sources\"\\nrun: ./tests/unit_tests/run.sh\\nWhen executing github ci action, error raises:\\n…/tests/unit_test/run.sh Permission error\\nError: Process completed with error code 126\\nSolution description\\nAdd execution  permission to the script and commit+push:\\ngit update-index --chmod=+x .\\\\sources\\\\tests\\\\unit_tests\\\\run.sh\\nAdded by MarcosMJD', 'section': 'Module 6: Best practices', 'question': 'Github actions: Permission denied error when executing script file', 'course': 'mlops-zoomcamp', 'id': 'a3b9af04'}]), ('b16aae74', [{'text': 'Problem description\\nWhen a docker-compose file contains a lot of containers, running the containers may take too much resource. There is a need to easily select only a group of containers while ignoring irrelevant containers during testing.\\nSolution description\\nAdd profiles: [“profile_name”] in the service definition.\\nWhen starting up the service, add `--profile profile_name` in the command.\\nAdded by Ammar Chalifah', 'section': 'Module 6: Best practices', 'question': 'Managing Multiple Docker Containers with docker-compose profile', 'course': 'mlops-zoomcamp', 'id': 'b16aae74'}]), ('66326a87', [{'text': 'Problem description\\nIf you are having problems with the integration tests and kinesis double check that your aws regions match on the docker-compose and local config. Otherwise you will be creating a stream in the wrong region\\nSolution description\\nFor example set ~/.aws/config region = us-east-1 and the docker-compose.yaml - AWS_DEFAULT_REGION=us-east-1\\nAdded by Quinn Avila', 'section': 'Module 6: Best practices', 'question': 'AWS regions need to match docker-compose', 'course': 'mlops-zoomcamp', 'id': '66326a87'}]), ('fb3c4150', [{'text': 'Problem description\\nPre-commit command was failing with isort repo.\\nSolution description\\nSet version to 5.12.0\\nAdded by Erick Calderin', 'section': 'Module 6: Best practices', 'question': 'Isort Pre-commit', 'course': 'mlops-zoomcamp', 'id': 'fb3c4150'}]), ('886d1617', [{'text': 'Problem description\\nInfrastructure created in AWS with CD-Deploy Action needs to be destroyed\\nSolution description\\nFrom local:\\nterraform init -backend-config=\"key=mlops-zoomcamp-prod.tfstate\" --reconfigure\\nterraform destroy --var-file vars/prod.tfvars\\nAdded by Erick Calderin', 'section': 'Module 6: Best practices', 'question': 'How to destroy infrastructure created via GitHub Actions', 'course': 'mlops-zoomcamp', 'id': '886d1617'}])])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashes.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3e495f9-fdac-436f-88e9-44ae68844ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593f7569 2\n"
     ]
    }
   ],
   "source": [
    "for k, values in hashes.items():\n",
    "    if len(values) > 1:\n",
    "        print(k, len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56bed2ec-9b14-4e7c-9db4-87900b30a674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"They both do the same, it's just less typing from the script.\\nAsked by Andrew Katoch, Added by Edidiong Esu\",\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'Does it matter if we let the Python file create the server or if we run gunicorn directly?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '593f7569'},\n",
       " {'text': \"They both do the same, it's just less typing from the script.\",\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'Does it matter if we let the Python file create the server or if we run gunicorn directly?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '593f7569'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashes['593f7569']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d25e980-8a04-4186-94ef-c357bdc1b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "686a488a-67af-4f21-8538-2180dc085fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents-with-ids.json', 'wt') as f_out:\n",
    "    json.dump(documents, f_out, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c567515e-a923-487f-9b4d-9f4ce370e2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"text\": \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  \\u201cOffice Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon\\u2019t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
      "    \"section\": \"General course-related questions\",\n",
      "    \"question\": \"Course - When will the course start?\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"id\": \"c02e79ef\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites\",\n"
     ]
    }
   ],
   "source": [
    "!head documents-with-ids.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c062325-5608-4da6-80bf-b9ac371bc17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You emulate a student who's taking our course.\n",
    "Formulate 5 questions this student might ask based on a FAQ record. The record\n",
    "should contain the answer to the questions, and the questions should be complete and not too short.\n",
    "If possible, use as fewer words as possible from the record. \n",
    "\n",
    "The record:\n",
    "\n",
    "section: {section}\n",
    "question: {question}\n",
    "answer: {text}\n",
    "\n",
    "Provide the output in parsable JSON without using code blocks. Do not add any text other than the list of questions:\n",
    "\n",
    "[\"question1\", \"question2\", ..., \"question5\"]\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1554357-38cd-4965-84e6-bf2bbbfab880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - When will the course start?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c02e79ef'},\n",
       " {'text': 'GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - What are the prerequisites for this course?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '1f6520ca'},\n",
       " {'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I still join the course after the start date?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '7842b56a'},\n",
       " {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '0bbf41ec'},\n",
       " {'text': 'You can start by installing and setting up all the dependencies and requirements:\\nGoogle cloud account\\nGoogle Cloud SDK\\nPython 3 (installed with Anaconda)\\nTerraform\\nGit\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - What can I do before the course starts?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '63394d91'},\n",
       " {'text': \"There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\\nData-Engineering (Jan - Apr)\\nMLOps (May - Aug)\\nMachine Learning (Sep - Jan)\\nThere's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - how many Zoomcamps in a year?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '2ed9b986'},\n",
       " {'text': 'Yes. For the 2024 edition we are using Mage AI instead of Prefect and re-recorded the terraform videos, For 2023, we used Prefect instead of Airflow..',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Is the current cohort going to be different from the previous cohort?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '93e2c8ed'},\n",
       " {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I follow the course after it finishes?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a482086d'},\n",
       " {'text': 'Yes, the slack channel remains open and you can ask questions there. But always sDocker containers exit code w search the channel first and second, check the FAQ (this document), most likely all your questions are already answered here.\\nYou can also tag the bot @ZoomcampQABot to help you conduct the search, but don’t rely on its answers 100%, it is pretty good though.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I get support if I take the course in the self-paced mode?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'eb56ae98'},\n",
       " {'text': 'All the main videos are stored in the Main “DATA ENGINEERING” playlist (no year specified). The Github repository has also been updated to show each video with a thumbnail, that would bring you directly to the same playlist below.\\nBelow is the MAIN PLAYLIST’. And then you refer to the year specific playlist for additional videos for that year like for office hours videos etc. Also find this playlist pinned to the slack channel.\\nh\\nttps://youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&si=NspQhtZhZQs1B9F-',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Which playlist on YouTube should I refer to?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '4292531b'},\n",
       " {'text': 'It depends on your background and previous experience with modules. It is expected to require about 5 - 15 hours per week. [source1] [source2]\\nYou can also calculate it yourself using this data and then update this answer.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - \\u200b\\u200bHow many hours per week am I expected to spend on this  course?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'ea739c65'},\n",
       " {'text': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'cb257ee5'},\n",
       " {'text': 'The zoom link is only published to instructors/presenters/TAs.\\nStudents participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\\nDon’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Office Hours - What is the video/zoom link to the stream for the “Office Hour” or workshop sessions?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '04aa4897'},\n",
       " {'text': 'Yes! Every “Office Hours” will be recorded and available a few minutes after the live session is over; so you can view (or rewatch) whenever you want.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Office Hours - I can’t attend the “Office hours” / workshop, will it be recorded?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '9681be3b'},\n",
       " {'text': 'You can find the latest and up-to-date deadlines here: https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml\\nAlso, take note of Announcements from @Au-Tomator for any extensions or other news. Or, the form may also show the updated deadline, if Instructor(s) has updated it.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Homework - What are homework and project deadlines?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a1daf537'},\n",
       " {'text': 'No, late submissions are not allowed. But if the form is still not closed and it’s after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page.y\\nOlder news:[source1] [source2]',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Homework - Are late submissions of homework allowed?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'be5bfee4'},\n",
       " {'text': 'Answer: In short, it’s your repository on github, gitlab, bitbucket, etc\\nIn long, your repository or any other location you have your code where a reasonable person would look at it and think yes, you went through the week and exercises.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Homework - What is the homework URL in the homework link?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '0e424a44'},\n",
       " {'text': 'After you submit your homework it will be graded based on the amount of questions in a particular homework. You can see how many points you have right on the page of the homework up top. Additionally in the leaderboard you will find the sum of all points you’ve earned - points for Homeworks, FAQs and Learning in Public. If homework is clear, others work as follows: if you submit something to FAQ, you get one point, for each learning in a public link you get one point.\\n(https://datatalks-club.slack.com/archives/C01FABYF2RG/p1706846846359379?thread_ts=1706825019.546229&cid=C01FABYF2RG)',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Homework and Leaderboard - what is the system for points in the course management platform?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '29865466'},\n",
       " {'text': 'When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. If you want to see what your Display name is.\\nGo to the Homework submission link →  https://courses.datatalks.club/de-zoomcamp-2024/homework/hw2 - Log in > Click on ‘Data Engineering Zoom Camp 2024’ > click on ‘Edit Course Profile’ - your display name is here, you can also change it should you wish:',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '016d46a1'},\n",
       " {'text': 'Yes, for simplicity (of troubleshooting against the recorded videos) and stability. [source]\\nBut Python 3.10 and 3.11 should work fine.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Environment - Is Python 3.9 still the recommended version to use in 2024?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '47972cb1'},\n",
       " {'text': 'You can set it up on your laptop or PC if you prefer to work locally from your laptop or PC.\\nYou might face some challenges, especially for Windows users. If you face cnd2\\nIf you prefer to work on the local machine, you may start with the week 1 Introduction to Docker and follow through.\\nHowever, if you prefer to set up a virtual machine, you may start with these first:\\nUsing GitHub Codespaces\\nSetting up the environment on a cloudV Mcodespace\\nI decided to work on a virtual machine because I have different laptops & PCs for my home & office, so I can work on this boot camp virtually anywhere.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Environment - Should I use my local machine, GCP, or GitHub Codespaces for my environment?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'ddf6c1b3'},\n",
       " {'text': 'GitHub Codespaces offers you computing Linux resources with many pre-installed tools (Docker, Docker Compose, Python).\\nYou can also open any GitHub repository in a GitHub Codespace.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Environment - Is GitHub codespaces an alternative to using cli/git bash to ingest the data and create a docker file?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'ac25d3af'},\n",
       " {'text': \"It's up to you which platform and environment you use for the course.\\nGithub codespaces or GCP VM are just possible options, but you can do the entire course from your laptop.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Environment - Do we really have to use GitHub codespaces? I already have PostgreSQL & Docker installed.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '251218fc'},\n",
       " {'text': 'Choose the approach that aligns the most with your idea for the end project\\nOne of those should suffice. However, BigQuery, which is part of GCP, will be used, so learning that is probably a better option. Or you can set up a local environment for most of this course.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Environment - Do I need both GitHub Codespaces and GCP?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '3c0114ce'},\n",
       " {'text': '1. To open Run command window, you can either:\\n(1-1) Use the shortcut keys: \\'Windows + R\\', or\\n(1-2) Right Click \"Start\", and click \"Run\" to open.\\n2. Registry Values Located in Registry Editor, to open it: Type \\'regedit\\' in the Run command window, and then press Enter.\\' 3. Now you can change the registry values \"Autorun\" in \"HKEY_CURRENT_USER\\\\Software\\\\Microsoft\\\\Command Processor\" from \"if exists\" to a blank.\\nAlternatively, You can simplify the solution by deleting the fingerprint saved within the known_hosts file. In Windows, this file is placed at  C:\\\\Users\\\\<your_user_name>\\\\.ssh\\\\known_host',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'This happens when attempting to connect to a GCP VM using VSCode on a Windows machine. Changing registry value in registry editor',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f43f5fe7'},\n",
       " {'text': 'For uniformity at least, but you’re not restricted to GCP, you can use other cloud platforms like AWS if you’re comfortable with other cloud platforms, since you get every service that’s been provided by GCP in Azure and AWS or others..\\nBecause everyone has a google account, GCP has a free trial period and gives $300 in credits  to new users. Also, we are working with BigQuery, which is a part of GCP.\\nNote that to sign up for a free GCP account, you must have a valid credit card.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Environment - Why are we using GCP and not other cloud providers?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'd061525d'},\n",
       " {'text': 'No, if you use GCP and take advantage of their free trial.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Should I pay for cloud services?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '1cd01b2c'},\n",
       " {'text': 'You can do most of the course without a cloud. Almost everything we use (excluding BigQuery) can be run locally. We won’t be able to provide guidelines for some things, but most of the materials are runnable without GCP.\\nFor everything in the course, there’s a local alternative. You could even do the whole course locally.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Environment - The GCP and other cloud providers are unavailable in some countries. Is it possible to provide a guide to installing a home lab?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'e4a7c3b0'},\n",
       " {'text': 'Yes, you can. Just remember to adapt all the information on the videos to AWS. Besides, the final capstone will be evaluated based on the task: Create a data pipeline! Develop a visualisation!\\nThe problem would be when you need help. You’d need to rely on  fellow coursemates who also use AWS (or have experience using it before), which might be in smaller numbers than those learning the course with GCP.\\nAlso see Is it possible to use x tool instead of the one tool you use?',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Environment - I want to use AWS. May I do that?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '7cd1912e'},\n",
       " {'text': 'We will probably have some calls during the Capstone period to clear some questions but it will be announced in advance if that happens.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Besides the “Office Hour” which are the live zoom calls?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '52393fb3'},\n",
       " {'text': 'We will use the same data, as the project will essentially remain the same as last year’s. The data is available here',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Are we still using the NYC Trip data for January 2021? Or are we using the 2022 data?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '10515af5'},\n",
       " {'text': 'No, but we moved the 2022 stuff here',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Is the 2022 repo deleted?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'cdb86a97'},\n",
       " {'text': 'Yes, you can use any tool you want for your project.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Can I use Airflow instead for my final project?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '3e0114ad'},\n",
       " {'text': 'Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\\nThe course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\\nShould you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Is it possible to use tool “X” instead of the one tool you use in the course?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'b2799574'},\n",
       " {'text': 'Star the repo! Share it with friends if you find it useful ❣️\\nCreate a PR if you see you can improve the text or the structure of the repository.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'How can we contribute to the course?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '2f19301f'},\n",
       " {'text': 'Yes! Linux is ideal but technically it should not matter. Students last year used all 3 OSes successfully',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Environment - Is the course [Windows/mac/Linux/...] friendly?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '7c700adb'},\n",
       " {'text': \"Have no idea how past cohorts got past this as I haven't read old slack messages, and no FAQ entries that I can find.\\nLater modules (module-05 & RisingWave workshop) use shell scripts in *.sh files and most Windows users not using WSL would hit a wall and cannot continue, even in git bash or MINGW64. This is why WSL environment setup is recommended from the start.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Environment - Roadblock for Windows users in modules with *.sh (shell scripts).',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '44b14808'},\n",
       " {'text': 'Yes to both! check out this document: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Any books or additional resources you recommend?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '76e4baf6'},\n",
       " {'text': 'You will have two attempts for a project. If the first project deadline is over and you’re late or you submit the project and fail the first attempt, you have another chance to submit the project with the second attempt.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Project - What is Project Attemp #1 and Project Attempt #2 exactly?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '48b533a8'},\n",
       " {'text': \"The first step is to try to solve the issue on your own. Get used to solving problems and reading documentation. This will be a real life skill you need when employed. [ctrl+f] is your friend, use it! It is a universal shortcut and works in all apps/browsers.\\nWhat does the error say? There will often be a description of the error or instructions on what is needed or even how to fix it. I have even seen a link to the solution. Does it reference a specific line of your code?\\nRestart app or server/pc.\\nGoogle it, use ChatGPT, Bing AI etc.\\nIt is going to be rare that you are the first to have the problem, someone out there has posted the fly issue and likely the solution.\\nSearch using: <technology> <problem statement>. Example: pgcli error column c.relhasoids does not exist.\\nThere are often different solutions for the same problem due to variation in environments.\\nCheck the tech’s documentation. Use its search if available or use the browsers search function.\\nTry uninstall (this may remove the bad actor) and reinstall of application or reimplementation of action. Remember to restart the server/pc for reinstalls.\\nSometimes reinstalling fails to resolve the issue but works if you uninstall first.\\nPost your question to Stackoverflow. Read the Stackoverflow guide on posting good questions.\\nhttps://stackoverflow.com/help/how-to-ask\\nThis will be your real life. Ask an expert in the future (in addition to coworkers).\\nAsk in Slack\\nBefore asking a question,\\nCheck Pins (where the shortcut to the repo and this FAQ is located)\\nUse the slack app’s search function\\nUse the bot @ZoomcampQABot to do the search for you\\ncheck the FAQ (this document), use search [ctrl+f]\\nWhen asking a question, include as much information as possible:\\nWhat are you coding on? What OS?\\nWhat command did you run, which video did you follow? Etc etc\\nWhat error did you get? Does it have a line number to the “offending” code and have you check it for typos?\\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.\\nDO NOT use screenshots, especially don’t take pictures from a phone.\\nDO NOT tag instructors, it may discourage others from helping you. Copy and paste errors; if it’s long, just post it in a reply to your thread.\\nUse ``` for formatting your code.\\nUse the same thread for the conversation (that means reply to your own thread).\\nDO NOT create multiple posts to discuss the issue.\\nlearYou may create a new post if the issue reemerges down the road. Describe what has changed in the environment.\\nProvide additional information in the same thread of the steps you have taken for resolution.\\nTake a break and come back later. You will be amazed at how often you figure out the solution after letting your brain rest. Get some fresh air, workout, play a video game, watch a tv show, whatever allows your brain to not think about it for a little while or even until the next day.\\nRemember technology issues in real life sometimes take days or even weeks to resolve.\\nIf somebody helped you with your problem and it's not in the FAQ, please add it there. It will help other students.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'How to troubleshoot issues',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '954044d1'},\n",
       " {'text': 'When the troubleshooting guide above does not help resolve it and you need another pair of eyeballs to spot mistakes. When asking a question, include as much information as possible:\\nWhat are you coding on? What OS?\\nWhat command did you run, which video did you follow? Etc etc\\nWhat error did you get? Does it have a line number to the “offending” code and have you check it for typos?\\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'How to ask questions',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a820b9b3'},\n",
       " {'text': 'After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\\nHaving this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\\nThis is also a great resource: https://dangitgit.com/',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'How do I use Git / GitHub for this course?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f2945cd2'},\n",
       " {'text': 'Error: Makefile:2: *** missing separator.  Stop.\\nSolution: Tabs in document should be converted to Tab instead of spaces. Follow this stack.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'VS Code: Tab using spaces',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'eb9d376f'},\n",
       " {'text': \"If you’re running Linux on Windows Subsystem for Linux (WSL) 2, you can open HTML files from the guest (Linux) with whatever Internet Browser you have installed on the host (Windows). Just install wslu and open the page with wslview <file>, for example:\\nwslview index.html\\nYou can customise which browser to use by setting the BROWSER environment variable first. For example:\\nexport BROWSER='/mnt/c/Program Files/Firefox/firefox.exe'\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Opening an HTML file with a Windows browser from Linux running on WSL',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '72f25f6d'},\n",
       " {'text': 'This tutorial shows you how to set up the Chrome Remote Desktop service on a Debian Linux virtual machine (VM) instance on Compute Engine. Chrome Remote Desktop allows you to remotely access applications with a graphical user interface.\\nTaxi Data - Yellow Taxi Trip Records downloading error, Error no or XML error webpage\\nWhen you try to download the 2021 data from TLC website, you get this error:\\nIf you click on the link, and ERROR 403: Forbidden on the terminal.\\nWe have a backup, so use it instead: https://github.com/DataTalksClub/nyc-tlc-data\\nSo the link should be https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\\nNote: Make sure to unzip the “gz” file (no, the “unzip” command won’t work for this.)\\n“gzip -d file.gz”g',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Set up Chrome Remote Desktop for Linux on Compute Engine',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a1e59afc'},\n",
       " {'text': 'In this video, we store the data file as “output.csv”. The data file won’t store correctly if the file extension is csv.gz instead of csv. One alternative is to replace csv_name = “output.cs -v” with the file name given at the end of the URL. Notice that the URL for the yellow taxi data is: https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz where the highlighted part is the name of the file. We can parse this file name from the URL and use it as csv_name. That is, we can replace csv_name = “output.csv” with\\ncsv_name = url.split(“/”)[-1] . Then when we use csv_name to using pd.read_csv, there won’t be an issue even though the file name really has the extension csv.gz instead of csv since the pandas read_csv function can read csv.gz files directly.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Taxi Data - How to handle taxi data files, now that the files are available as *.csv.gz?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '71c10610'},\n",
       " {'text': 'Yellow Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\\nGreen Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Taxi Data - Data Dictionary for NY Taxi data?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '17a5aea1'},\n",
       " {'text': 'You can unzip this downloaded parquet file, in the command line. The result is a csv file which can be imported with pandas using the pd.read_csv() shown in the videos.\\n‘’’gunzip green_tripdata_2019-09.csv.gz’’’\\nSOLUTION TO USING PARQUET FILES DIRECTLY IN PYTHON SCRIPT ingest_data.py\\nIn the def main(params) add this line\\nparquet_name= \\'output.parquet\\'\\nThen edit the code which downloads the files\\nos.system(f\"wget {url} -O {parquet_name}\")\\nConvert the download .parquet file to csv and rename as csv_name to keep it relevant to the rest of the code\\ndf = pd.read_parquet(parquet_name)\\ndf.to_csv(csv_name, index=False)',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Taxi Data - Unzip Parquet file',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '5a275db7'},\n",
       " {'text': '“wget is not recognized as an internal or external command”, you need to install it.\\nOn Ubuntu, run:\\n$ sudo apt-get install wget\\nOn MacOS, the easiest way to install wget is to use Brew:\\n$ brew install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\n$ choco install wget\\nOr you can download a binary (https://gnuwin32.sourceforge.net/packages/wget.htm) and put it to any location in your PATH (e.g. C:/tools/)\\nAlso, you can following this step to install Wget on MS Windows\\n* Download the latest wget binary for windows from [eternallybored] (https://eternallybored.org/misc/wget/) (they are available as a zip with documentation, or just an exe)\\n* If you downloaded the zip, extract all (if windows built in zip utility gives an error, use [7-zip] (https://7-zip.org/)).\\n* Rename the file `wget64.exe` to `wget.exe` if necessary.\\n* Move wget.exe to your `Git\\\\mingw64\\\\bin\\\\`.\\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need to use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAlternatively, you can just paste the file URL into your web browser and download the file normally that way. You’ll want to move the resulting file into your working directory.\\nAlso recommended a look at the python library requests for the loading gz file  https://pypi.org/project/requests',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'lwget is not recognized as an internal or external command',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '7ec0f9b0'},\n",
       " {'text': 'Firstly, make sure that you add “!” before wget if you’re running your command in a Jupyter Notebook or CLI. Then, you can check one of this 2 things (from CLI):\\nUsing the Python library wget you installed with pip, try python -m wget <url>\\nWrite the usual command and add --no-check-certificate at the end. So it should be:\\n!wget <website_url> --no-check-certificate',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'wget - ERROR: cannot verify <website> certificate  (MacOS)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'bb1ba786'},\n",
       " {'text': 'For those who wish to use the backslash as an escape character in Git Bash for Windows (as Alexey normally does), type in the terminal: bash.escapeChar=\\\\ (no need to include in .bashrc)',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Git Bash - Backslash as an escape character in Git Bash for Windows',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '2f83dbe7'},\n",
       " {'text': 'Instruction on how to store secrets that will be avialable in GitHub  Codespaces.\\nManaging your account-specific secrets for GitHub Codespaces - GitHub Docs',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'GitHub Codespaces - How to store secrets',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '543ff080'},\n",
       " {'text': \"Make sure you're able to start the Docker daemon, and check the issue immediately down below:\\nAnd don’t forget to update the wsl in powershell the  command is wsl –update\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker - Cannot connect to Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'd407d65b'},\n",
       " {'text': \"As the official Docker for Windows documentation says, the Docker engine can either use the\\nHyper-V or WSL2 as its backend. However, a few constraints might apply\\nWindows 10 Pro / 11 Pro Users: \\nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\\nWindows 10 Home / 11 Home Users: \\nOn the other hand, Users of the 'Home' version do NOT have the option Hyper-V option enabled, which means, you can only get Docker up and running using the WSL2 credentials(Windows Subsystem for Linux). Url\\nYou can find the detailed instructions to do so here: rt ghttps://pureinfotech.com/install-wsl-windows-11/\\nIn case, you run into another issue while trying to install WSL2 (WslRegisterDistribution failed with error: 0x800701bc), Make sure you update the WSL2 Linux Kernel, following the guidelines here: \\n\\nhttps://github.com/microsoft/WSL/issues/5393\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker - Error during connect: In the default daemon configuration on Windows, the docker client must be run with elevated privileges to connect.: Post: \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/containers/create\" : open //./pipe/docker_engine: The system cannot find the file specified',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c9375c56'},\n",
       " {'text': 'Whenever a `docker pull is performed (either manually or by `docker-compose up`), it attempts to fetch the given image name (pgadmin4, for the example above) from a repository (dbpage).\\nIF the repository is public, the fetch and download happens without any issue whatsoever.\\nFor instance:\\ndocker pull postgres:13\\ndocker pull dpage/pgadmin4\\nBE ADVISED:\\n\\nThe Docker Images we\\'ll be using throughout the Data Engineering Zoomcamp are all public (except when or if explicitly said otherwise by the instructors or co-instructors).\\n\\nMeaning: you are NOT required to perform a docker login to fetch them. \\n\\nSo if you get the message above saying \"docker login\\': denied: requested access to the resource is denied. That is most likely due to a typo in your image name:\\n\\nFor instance:\\n$ docker pull dbpage/pgadmin4\\nWill throw that exception telling you \"repository does not exist or may require \\'docker login\\'\\nError response from daemon: pull access denied for dbpage/pgadmin4, repository does not exist or \\nmay require \\'docker login\\': denied: requested access to the resource is denied\\nBut that actually happened because the actual image is dpage/pgadmin4 and NOT dbpage/pgadmin4\\nHow to fix it:\\n$ docker pull dpage/pgadmin4\\nEXTRA NOTES:\\nIn the real world, occasionally, when you\\'re working for a company or closed organisation, the Docker image you\\'re trying to fetch might be under a private repo that your DockerHub Username was granted access to.\\nFor which cases, you must first execute:\\n$ docker login\\nFill in the details of your username and password.\\nAnd only then perform the `docker pull` against that private repository\\nWhy am I encountering a \"permission denied\" error when creating a PostgreSQL Docker container for the New York Taxi Database with a mounted volume on macOS M1?\\nIssue Description:\\nWhen attempting to run a Docker command similar to the one below:\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\mount\\npostgres:13\\nYou encounter the error message:\\ndocker: Error response from daemon: error while creating mount source path \\'/path/to/ny_taxi_postgres_data\\': chown /path/to/ny_taxi_postgres_data: permission denied.\\nSolution:\\n1- Stop Rancher Desktop:\\nIf you are using Rancher Desktop and face this issue, stop Rancher Desktop to resolve compatibility problems.\\n2- Install Docker Desktop:\\nInstall Docker Desktop, ensuring that it is properly configured and has the required permissions.\\n2-Retry Docker Command:\\nRun the Docker command again after switching to Docker Desktop. This step resolves compatibility issues on some systems.\\nNote: The issue occurred because Rancher Desktop was in use. Switching to Docker Desktop resolves compatibility problems and allows for the successful creation of PostgreSQL containers with mounted volumes for the New York Taxi Database on macOS M1.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker - docker pull dbpage',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'e866156b'},\n",
       " {'text': 'When I runned command to create postgre in docker container it created folder on my local machine to mount it to volume inside container. It has write and read protection and owned by user 999, so I could not delete it by simply drag to trash.  My obsidian could not started due to access error, so I had to change placement of this folder and delete old folder by this command:\\nsudo rm -r -f docker_test/\\n- where `rm` - remove, `-r` - recursively, `-f` - force, `docker_test/` - folder.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker - can’t delete local folder that mounted to docker volume',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '16370470'},\n",
       " {'text': 'First off, make sure you\\'re running the latest version of Docker for Windows, which you can download from here. Sometimes using the menu to \"Upgrade\" doesn\\'t work (which is another clear indicator for you to uninstall, and reinstall with the latest version)\\nIf docker is stuck on starting, first try to switch containers by right clicking the docker symbol from the running programs and switch the containers from windows to linux or vice versa\\n[Windows 10 / 11 Pro Edition] The Pro Edition of Windows can run Docker either by using Hyper-V or WSL2 as its backend (Docker Engine)\\nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\\nIf you opt-in for WSL2, you can follow the same steps as detailed in the tutorial here',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': \"Docker - Docker won't start or is stuck in settings (Windows 10 / 11)\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '316df755'},\n",
       " {'text': \"It is recommended by the Docker do\\n[Windows 10 / 11 Home Edition] If you're running a Home Edition, you can still make it work with WSL2 (Windows Subsystem for Linux) by following the tutorial here\\nIf even after making sure your WSL2 (or Hyper-V) is set up accordingly, Docker remains stuck, you can try the option to Reset to Factory Defaults or do a fresh install.\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Should I run docker commands from the windows file system or a file system of a Linux distribution in WSL?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f3aa9252'},\n",
       " {'text': 'More info in the Docker Docs on Best Practises',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker - cs to store all code in your default Linux distro to get the best out of file system performance (since Docker runs on WSL2 backend by default for Windows 10 Home / Windows 11 Home users).',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a4abe7a5'},\n",
       " {'text': 'You may have this error:\\n$ docker run -it ubuntu bash\\nthe input device is not a TTY. If you are using mintty, try prefixing the command with \\'winpty\\'\\nerror:\\nSolution:\\nUse winpty before docker command (source)\\n$ winpty docker run -it ubuntu bash\\nYou also can make an alias:\\necho \"alias docker=\\'winpty docker\\'\" >> ~/.bashrc\\nOR\\necho \"alias docker=\\'winpty docker\\'\" >> ~/.bash_profile',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker - The input device is not a TTY (Docker run for Windows)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'fb930700'},\n",
       " {'text': \"You may have this error:\\nRetrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.u\\nrllib3.connection.HTTPSConnection object at 0x7efe331cf790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')':\\n/simple/pandas/\\nPossible solution might be:\\n$ winpty docker run -it --dns=8.8.8.8 --entrypoint=bash python:3.9\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker - Cannot pip install on Docker container (Windows)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'aa187680'},\n",
       " {'text': 'Even after properly running the docker script the folder is empty in the vs code  then try this (For Windows)\\nwinpty docker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"C:\\\\Users\\\\abhin\\\\dataengg\\\\DE_Project_git_connected\\\\DE_OLD\\\\week1_set_up\\\\docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\" \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nHere quoting the absolute path in  the -v parameter is solving the issue and all the files are visible in the Vs-code ny_taxi folder as shown in the video',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker - ny_taxi_postgres_data is empty',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'b000e899'},\n",
       " {'text': 'Check this article for details - Setting up docker in macOS\\nFrom researching it seems this method might be out of date, it seems that since docker changed their licensing model, the above is a bit hit and miss. What worked for me was to just go to the docker website and download their dmg. Haven’t had an issue with that method.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'dasDocker - Setting up Docker on Mac',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '9c66759f'},\n",
       " {'text': '$ docker run -it\\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"admin\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"/mnt/path/to/ny_taxi_postgres_data\":\"/var/lib/postgresql/data\" \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nCCW\\nThe files belonging to this database system will be owned by user \"postgres\".\\nThis use The database cluster will be initialized with locale \"en_US.utf8\".\\nThe default databerrorase encoding has accordingly been set to \"UTF8\".\\nxt search configuration will be set to \"english\".\\nData page checksums are disabled.\\nfixing permissions on existing directory /var/lib/postgresql/data ... initdb: f\\nerror: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted  volume\\nOne way to solve this issue is to create a local docker volume and map it to postgres data directory /var/lib/postgresql/data\\nThe input dtc_postgres_volume_local must match in both commands below\\n$ docker volume create --name dtc_postgres_volume_local -d local\\n$ docker run -it\\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v dtc_postgres_volume_local:/var/lib/postgresql/data \\\\\\n-p 5432:5432\\\\\\npostgres:13\\nTo verify the above command works in (WSL2 Ubuntu 22.04, verified 2024-Jan), go to the Docker Desktop app and look under Volumes - dtc_postgres_volume_local would be listed there. The folder ny_taxi_postgres_data would however be empty, since we used an alternative config.\\nAn alternate error could be:\\ninitdb: error: directory \"/var/lib/postgresql/data\" exists but is not empty\\nIf you want to create a new database system, either remove or empthe directory \"/var/lib/postgresql/data\" or run initdb\\nwitls',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': '1Docker - Could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'e3106e07'},\n",
       " {'text': 'Mapping volumes on Windows could be tricky. The way it was done in the course video doesn’t work for everyone.\\nFirst, if yo\\nmove your data to some folder without spaces. E.g. if your code is in “C:/Users/Alexey Grigorev/git/…”, move it to “C:/git/…”\\nTry replacing the “-v” part with one of the following options:\\n-v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v //c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v /c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v //c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n--volume //driveletter/path/ny_taxi_postgres_data/:/var/lib/postgresql/data\\nwinpty docker run -it\\n-e POSTGRES_USER=\"root\"\\n-e POSTGRES_PASSWORD=\"root\"\\n-e POSTGRES_DB=\"ny_taxi\"\\n-v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-p 5432:5432\\npostgres:1\\nTry adding winpty before the whole command\\n3\\nwin\\nTry adding quotes:\\n-v \"/c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v \"//c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v “/c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v \"//c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v \"c:\\\\some\\\\path\\\\ny_taxi_postgres_data\":/var/lib/postgresql/data\\nNote:  (Window) if it automatically creates a folder called “ny_taxi_postgres_data;C” suggests you have problems with volume mapping, try deleting both folders and replacing “-v” part with other options. For me “//c/” works instead of “/c/”. And it will work by automatically creating a correct folder called “ny_taxi_postgres_data”.\\nA possible solution to this error would be to use /”$(pwd)”/ny_taxi_postgres_data:/var/lib/postgresql/data (with quotes’ position varying as in the above list).\\nYes for windows use the command it works perfectly fine\\n-v /”$(pwd)”/ny_taxi_postgres_data:/var/lib/postgresql/data\\nImportant: note how the quotes are placed.\\nIf none of these options work, you can use a volume name instead of the path:\\n-v ny_taxi_postgres_data:/var/lib/postgresql/data\\nFor Mac: You can wrap $(pwd) with quotes like the highlighted.\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"$(pwd)\"/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\nPostgres:13\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"$(pwd)\"/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nSource:https://stackoverflow.com/questions/48522615/docker-error-invalid-reference-format-repository-name-must-be-lowercase',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker - invalid reference format: repository name must be lowercase (Mounting volumes with Docker on Windows)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '72229da5'},\n",
       " {'text': 'Change the mounting path. Replace it with one of following:\\n-v /e/zoomcamp/...:/var/lib/postgresql/data\\n-v /c:/.../ny_taxi_postgres_data:/var/lib/postgresql/data\\\\ (leading slash in front of c:)',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker - Error response from daemon: invalid mode: \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '58c9f99f'},\n",
       " {'text': 'When you run this command second time\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v <your path>:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nThe error message above could happen. That means you should not mount on the second run. This command helped me:\\nWhen you run this command second time\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-p 5432:5432 \\\\\\npostgres:13',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': \"Docker - Error response from daemon: error while creating buildmount source path '/run/desktop/mnt/host/c/<your path>': mkdir /run/desktop/mnt/host/c: file exists\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'bc42139a'},\n",
       " {'text': 'This error appeared when running the command: docker build -t taxi_ingest:v001 .\\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn’t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\\nsudo chown -R $USER dir_path\\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAdded by\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tKenan Arslanbay',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': \"Docker - build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a146e3ee'},\n",
       " {'text': 'You might have installed docker via snap. Run “sudo snap status docker” to verify.\\nIf you have “error: unknown command \"status\", see \\'snap help\\'.” as a response than deinstall docker and install via the official website\\nBind for 0.0.0.0:5432 failed: port is a',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker - ERRO[0000] error waiting for container: context canceled',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '593a85ba'},\n",
       " {'text': 'Found the issue in the PopOS linux. It happened because our user didn’t have authorization rights to the host folder ( which also caused folder seems empty, but it didn’t!).\\n✅Solution:\\nJust add permission for everyone to the corresponding folder\\nsudo chmod -R 777 <path_to_folder>\\nExample:\\nsudo chmod -R 777 ny_taxi_postgres_data/',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker - build error checking context: can’t stat ‘/home/fhrzn/Projects/…./ny_taxi_postgres_data’',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '50bd1a71'},\n",
       " {'text': 'This happens on Ubuntu/Linux systems when trying to run the command to build the Docker container again.\\n$ docker build -t taxi_ingest:v001 .\\nA folder is created to host the Docker files. When the build command is executed again to rebuild the pipeline or create a new one the error is raised as there are no permissions on this new folder. Grant permissions by running this comtionmand;\\n$ sudo chmod -R 755 ny_taxi_postgres_data\\nOr use 777 if you still see problems. 755 grants write access to only the owner.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker - failed to solve with frontend dockerfile.v0: failed to read dockerfile: error from sender: open ny_taxi_postgres_data: permission denied.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f409f751'},\n",
       " {'text': 'Get the network name via: $ docker network ls.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker - Docker network name',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '7d217da3'},\n",
       " {'text': 'Sometimes, when you try to restart a docker image configured with a network name, the above message appears. In this case, use the following command with the appropriate container name:\\n>>> If the container is running state, use docker stop <container_name>\\n>>> then, docker rm pg-database\\nOr use docker start instead of docker run in order to restart the docker image without removing it.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker - Error response from daemon: Conflict. The container name \"pg-database\" is already in use by container “xxx”.  You have to remove (or rename) that container to be able to reuse that name.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '09081824'},\n",
       " {'text': 'Typical error: sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"pgdatabase\" to address: Name or service not known\\nWhen running docker-compose up -d see which network is created and use this for the ingestions script instead of pg-network and see the name of the database to use instead of pgdatabase\\nE.g.:\\npg-network becomes 2docker_default\\nPgdatabase becomes 2docker-pgdatabase-1',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker - ingestion when using docker-compose could not translate host name',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '4df80c55'},\n",
       " {'text': 'terraformRun this command before starting your VM:\\nOn Intel CPU:\\nmodprobe -r kvm_intel\\nmodprobe kvm_intel nested=1\\nOn AMD CPU:\\nmodprobe -r kvm_amd\\nmodprobe kvm_amd nested=1',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker - Cannot install docker on MacOS/Windows 11 VM running on top of Linux (due to Nested virtualization).',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '3aee7261'},\n",
       " {'text': 'It’s very easy to manage your docker container, images, network and compose projects from VS Code.\\nJust install the official extension and launch it from the left side icon.\\nIt will work even if your Docker runs on WSL2, as VS Code can easily connect with your Linux.\\nDocker - How to stop a container?\\nUse the following command:\\n$ docker stop <container_id>',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker - Connecting from VS Code',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '6497b659'},\n",
       " {'text': \"When you see this in logs, your container with postgres is not accepting any requests, so if you attempt to connect, you'll get this error:\\nconnection failed: server closed the connection unexpectedly\\nThis probably means the server terminated abnormally before or while processing the request.\\nIn this case, you need to delete the directory with data (the one you map to the container with the -v flag) and restart the container.\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker - PostgreSQL Database directory appears to contain a database. Database system is shut down',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a02f2039'},\n",
       " {'text': 'On few versions of Ubuntu, snap command can be used to install Docker.\\nsudo snap install docker',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker not installable on Ubuntu',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c6db65aa'},\n",
       " {'text': 'error: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted  volume\\nif you have used the prev answer (just before this) and have created a local docker volume, then you need to tell the compose file about the named volume:\\nvolumes:\\ndtc_postgres_volume_local:  # Define the named volume here\\n# services mentioned in the compose file auto become part of the same network!\\nservices:\\nyour remaining code here . . .\\nnow use docker volume inspect dtc_postgres_volume_local to see the location by checking the value of Mountpoint\\nIn my case, after i ran docker compose up the mounting dir created was named ‘docker_sql_dtc_postgres_volume_local’ whereas it should have used the already existing ‘dtc_postgres_volume_local’\\nAll i did to fix this is that I renamed the existing ‘dtc_postgres_volume_local’ to ‘docker_sql_dtc_postgres_volume_local’ and removed the newly created one (just be careful when doing this)\\nrun docker compose up again and check if the table is there or not!',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker-Compose - mounting error',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f476a606'},\n",
       " {'text': 'Couldn’t translate host name to address\\nMake sure postgres database is running.\\n\\n\\u200b\\u200bUse the command to start containers in detached mode: docker-compose up -d\\n(data-engineering-zoomcamp) hw % docker compose up -d\\n[+] Running 2/2\\n⠿ Container pg-admin     Started                                                                                                                                                                      0.6s\\n⠿ Container pg-database  Started\\nTo view the containers use: docker ps.\\n(data-engineering-zoomcamp) hw % docker ps\\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                           NAMES\\nfaf05090972e   postgres:13      \"docker-entrypoint.s…\"   39 seconds ago   Up 37 seconds   0.0.0.0:5432->5432/tcp          pg-database\\n6344dcecd58f   dpage/pgadmin4   \"/entrypoint.sh\"         39 seconds ago   Up 37 seconds   443/tcp, 0.0.0.0:8080->80/tcp   pg-admin\\nhw\\nTo view logs for a container: docker logs <containerid>\\n(data-engineering-zoomcamp) hw % docker logs faf05090972e\\nPostgreSQL Database directory appears to contain a database; Skipping initialization\\n2022-01-25 05:58:45.948 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\\n2022-01-25 05:58:45.954 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\\n2022-01-25 05:58:45.984 UTC [28] LOG:  database system was interrupted; last known up at 2022-01-24 17:48:35 UTC\\n2022-01-25 05:58:48.581 UTC [28] LOG:  database system was not properly shut down; automatic recovery in\\nprogress\\n2022-01-25 05:58:48.602 UTC [28] LOG:  redo starts at 0/872A5910\\n2022-01-25 05:59:33.726 UTC [28] LOG:  invalid record length at 0/98A3C160: wanted 24, got 0\\n2022-01-25 05:59:33.726 UTC [28\\n] LOG:  redo done at 0/98A3C128\\n2022-01-25 05:59:48.051 UTC [1] LOG:  database system is ready to accept connections\\nIf docker ps doesn’t show pgdatabase running, run: docker ps -a\\nThis should show all containers, either running or stopped.\\nGet the container id for pgdatabase-1, and run',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker-Compose - Error translating host name to address',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'e41b100c'},\n",
       " {'text': 'After executing `docker-compose up` - if you lose database data and are unable to successfully execute your Ingestion script (to re-populate your database) but receive the following error:\\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name /data_pgadmin:/var/lib/pgadmin\"pg-database\" to address: Name or service not known\\nDocker compose is creating its own default network since it is no longer specified in a docker execution command or file. Docker Compose will emit to logs the new network name. See the logs after executing `docker compose up` to find the network name and change the network name argument in your Ingestion script.\\nIf problems persist with pgcli, we can use HeidiSQL,usql\\nKrishna Anand',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker-Compose -  Data retention (could not translate host name \"pg-database\" to address: Name or service not known)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'cd0f9300'},\n",
       " {'text': 'It returns --> Error response from daemon: network 66ae65944d643fdebbc89bd0329f1409dec2c9e12248052f5f4c4be7d1bdc6a3 not found\\nTry:\\ndocker ps -a to see all the stopped & running containers\\nd to nuke all the containers\\nTry: docker-compose up -d again ports\\nOn localhost:8080 server → Unable to connect to server: could not translate host name \\'pg-database\\' to address: Name does not resolve\\nTry: new host name, best without “ - ” e.g. pgdatabase\\nAnd on docker-compose.yml, should specify docker network & specify the same network in both  containers\\nservices:\\npgdatabase:\\nimage: postgres:13\\nenvironment:\\n- POSTGRES_USER=root\\n- POSTGRES_PASSWORD=root\\n- POSTGRES_DB=ny_taxi\\nvolumes:\\n- \"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\\nports:\\n- \"5431:5432\"\\nnetworks:\\n- pg-network\\npgadmin:\\nimage: dpage/pgadmin4\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n- PGADMIN_DEFAULT_PASSWORD=root\\nports:\\n- \"8080:80\"\\nnetworks:\\n- pg-network\\nnetworks:\\npg-network:\\nname: pg-network',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker-Compose - Hostname does not resolve',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '7f845a1c'},\n",
       " {'text': 'So one common issue is when you run docker-compose on GCP, postgres won’t persist it’s data to mentioned path for example:\\nservices:\\n…\\n…\\npgadmin:\\n…\\n…\\nVolumes:\\n“./pgadmin”:/var/lib/pgadmin:wr”\\nMight not work so in this use you can use Docker Volume to make it persist, by simply changing\\nservices:\\n…\\n….\\npgadmin:\\n…\\n…\\nVolumes:\\npgadmin:/var/lib/pgadmin\\nvolumes:\\nPgadmin:',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker-Compose - Persist PGAdmin docker contents on GCP',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '36e54439'},\n",
       " {'text': 'The docker will keep on crashing continuously\\nNot working after restart\\ndocker engine stopped\\nAnd failed to fetch extensions pop ups will on screen non-stop\\nSolution :\\nTry checking if latest version of docker is installed / Try updating the docker\\nIf Problem still persist then final solution is to reinstall docker\\n(Just have to fetch images again else no issues)',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker engine stopped_failed to fetch extensions',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '32e8450c'},\n",
       " {'text': 'As per the lessons,\\nPersisting pgAdmin configuration (i.e. server name) is done by adding a “volumes” section:\\nservices:\\npgdatabase:\\n[...]\\npgadmin:\\nimage: dpage/pgadmin4\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n- PGADMIN_DEFAULT_PASSWORD=root\\nvolumes:\\n- \"./pgAdmin_data:/var/lib/pgadmin/sessions:rw\"\\nports:\\n- \"8080:80\"\\nIn the example above, ”pgAdmin_data” is a folder on the host machine, and “/var/lib/pgadmin/sessions” is the session settings folder in the pgAdmin container.\\nBefore running docker-compose up on the YAML file, we also need to give the pgAdmin container access to write to the “pgAdmin_data” folder. The container runs with a username called “5050” and user group “5050”. The bash command to give access over the mounted volume is:\\nsudo chown -R 5050:5050 pgAdmin_data',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker-Compose - Persist PGAdmin configuration',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '96606db2'},\n",
       " {'text': 'This happens if you did not create the docker group and added your user. Follow these steps from the link:\\nguides/docker-without-sudo.md at main · sindresorhus/guides · GitHub\\nAnd then press ctrl+D to log-out and log-in again. pgAdmin: Maintain state so that it remembers your previous connection\\nIf you are tired of having to setup your database connection each time that you fire up the containers, all you have to do is create a volume for pgAdmin:\\nIn your docker-compose.yaml file, enter the following into your pgAdmin declaration:\\nvolumes:\\n- type: volume\\nsource: pgadmin_data\\ntarget: /var/lib/pgadmin\\nAlso add the following to the end of the file:ls\\nvolumes:\\nPgadmin_data:',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker-Compose - dial unix /var/run/docker.sock: connect: permission denied',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '0882bfac'},\n",
       " {'text': 'This is happen to me after following 1.4.1 video where we are installing docker compose in our Google Cloud VM. In my case, the docker-compose file downloaded from github named docker-compose-linux-x86_64 while it is more convenient to use docker-compose command instead. So just change the docker-compose-linux-x86_64 into docker-compose.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker-Compose - docker-compose still not available after changing .bashrc',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '7d067f5c'},\n",
       " {'text': 'Installing pass via ‘sudo apt install pass’ helped to solve the issue. More about this can be found here: https://github.com/moby/buildkit/issues/1078',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker-Compose - Error getting credentials after running docker-compose up -d',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'ff352621'},\n",
       " {'text': \"For everyone who's having problem with Docker compose, getting the data in postgres and similar issues, please take care of the following:\\ncreate a new volume on docker (either using the command line or docker desktop app)\\nmake the following changes to your docker-compose.yml file (see attachment)\\nset low_memory=false when importing the csv file (df = pd.read_csv('yellow_tripdata_2021-01.csv', nrows=1000, low_memory=False))\\nuse the below function (in the upload-data.ipynb) for better tracking of your ingestion process (see attachment)\\nOrder of execution:\\n(1) open terminal in 2_docker_sql folder and run docker compose up\\n(2) ensure no other containers are running except the one you just executed (pgadmin and pgdatabase)\\n(3) open jupyter notebook and begin the data ingestion\\n(4) open pgadmin and set up a server (make sure you use the same configurations as your docker-compose.yml file like the same name (pgdatabase), port, databasename (ny_taxi) etc.\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker-Compose - Errors pertaining to docker-compose.yml and pgadmin setup',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '2d653208'},\n",
       " {'text': 'Locate config.json file for docker (check your home directory; Users/username/.docker).\\nModify credsStore to credStore\\nSave and re-run',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker Compose up -d error getting credentials - err: exec: \"docker-credential-desktop\": executable file not found in %PATH%, out: ``',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f09ea61e'},\n",
       " {'text': 'To figure out which docker-compose you need to download from https://github.com/docker/compose/releases you can check your system with these commands:\\nuname -s  -> return Linux most likely\\nuname -m -> return \"flavor\"\\nOr try this command -\\nsudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker-Compose - Which docker-compose binary to use for WSL?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'fbd3d2bb'},\n",
       " {'text': 'If you wrote the docker-compose.yaml file exactly like the video, you might run into an error like this:dev\\nservice \"pgdatabase\" refers to undefined volume dtc_postgres_volume_local: invalid compose project\\nIn order to make it work, you need to include the volume in your docker-compose file. Just add the following:\\nvolumes:\\ndtc_postgres_volume_local:\\n(Make sure volumes are at the same level as services.)',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker-Compose - Error undefined volume in Windows/WSL',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '0b014d0c'},\n",
       " {'text': 'Error:  initdb: error: could not change permissions of directory\\nIssue: WSL and Windows do not manage permissions in the same way causing conflict if using the Windows file system rather than the WSL file system.\\nSolution: Use Docker volumes.\\nWhy: Volume is used for storage of persistent data and not for use of transferring files. A local volume is unnecessary.\\nBenefit: This resolves permission issues and allows for better management of volumes.\\nNOTE: the ‘user:’ is not necessary if using docker volumes, but is if using local drive.\\n</>  docker-compose.yaml\\nservices:\\npostgres:\\nimage: postgres:15-alpine\\ncontainer_name: postgres\\nuser: \"0:0\"\\nenvironment:\\n- POSTGRES_USER=postgres\\n- POSTGRES_PASSWORD=postgres\\n- POSTGRES_DB=ny_taxi\\nvolumes:\\n- \"pg-data:/var/lib/postgresql/data\"\\nports:\\n- \"5432:5432\"\\nnetworks:\\n- pg-network\\npgadmin:\\nimage: dpage/pgadmin4\\ncontainer_name: pgadmin\\nuser: \"${UID}:${GID}\"\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=email@some-site.com\\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\\nvolumes:\\n- \"pg-admin:/var/lib/pgadmin\"\\nports:\\n- \"8080:80\"\\nnetworks:\\n- pg-network\\nnetworks:\\npg-network:\\nname: pg-network\\nvolumes:\\npg-data:\\nname: ingest_pgdata\\npg-admin:\\nname: ingest_pgadmin',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'WSL Docker directory permissions error',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'd21bff1d'},\n",
       " {'text': 'Cause : If Running on git bash or vm in windows pgadmin doesnt work easily LIbraries like psycopg2 and libpq ar required still the error persists.\\nSolution- I use psql instead of pgadmin totally same\\nPip install psycopg2\\ndock',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker - If pgadmin is not working for Querying in Postgres Use PSQL',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '6afb7b55'},\n",
       " {'text': 'Cause:\\nIt happens because the apps are not updated. To be specific, search for any pending updates for Windows Terminal, WSL and Windows Security updates.\\nSolution\\nfor updating Windows terminal which worked for me:\\nGo to Microsoft Store.\\nGo to the library of apps installed in your system.\\nSearch for Windows terminal.\\nUpdate the app and restart your system to  see the changes.\\nFor updating the Windows security updates:\\nGo to Windows updates and check if there are any pending updates from Windows, especially security updates.\\nDo restart your system once the updates are downloaded and installed successfully.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'WSL - Insufficient system resources exist to complete the requested service.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'b51c3b82'},\n",
       " {'text': 'Up restardoting the same issue appears. Happens out of the blue on windows.\\nSolution 1: Fixing DNS Issue (credit: reddit) this worked for me personally\\nreg add \"HKLM\\\\System\\\\CurrentControlSet\\\\Services\\\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"4\" /f\\nRestart your computer and then enable it with the following\\nreg add \"HKLM\\\\System\\\\CurrentControlSet\\\\Services\\\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"2\" /f\\nRestart your OS again. It should work.\\nSolution 2: right click on running Docker icon (next to clock) and chose \"Switch to Linux containers\"\\nbash: conda: command not found\\nDatabase is uninitialized and superuser password is not specified.\\nDatabase is uninitialized and superuser password is not specified.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'WSL - WSL integration with distro Ubuntu unexpectedly stopped with exit code 1.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '326af690'},\n",
       " {'text': 'Issue when trying to run the GPC VM through SSH through WSL2,  probably because WSL2 isn’t looking for .ssh keys in the correct folder. My case I was trying to run this command in the terminal and getting an error\\nPC:/mnt/c/Users/User/.ssh$ ssh -i gpc [username]@[my external IP]\\nYou can try to use sudo before the command\\nSudo .ssh$ ssh -i gpc [username]@[my external IP]\\nYou can also try to cd to your folder and change the permissions for the private key SSH file.\\nchmod 600 gpc\\nIf that doesn’t work, create a .ssh folder in the home diretory of WSL2 and copy the content of windows .ssh folder to that new folder.\\ncd ~\\nmkdir .ssh\\ncp -r /mnt/c/Users/YourUsername/.ssh/* ~/.ssh/\\nYou might need to adjust the permissions of the files and folders in the .ssh directory.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'WSL - Permissions too open at Windows',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c2ec9047'},\n",
       " {'text': 'Such as the issue above, WSL2 may not be referencing the correct .ssh/config path from Windows. You can create a config file at the home directory of WSL2.\\ncd ~\\nmkdir .ssh\\nCreate a config file in this new .ssh/ folder referencing this folder:\\nHostName [GPC VM external IP]\\nUser [username]\\nIdentityFile ~/.ssh/[private key]',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'WSL - Could not resolve host name',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '3b711e73'},\n",
       " {'text': 'Change TO Socket\\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi\\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'PGCLI - connection failed: :1), port 5432 failed: could not receive data from server: Connection refused could not send SSL negotiation packet: Connection refused',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'cfe07c9d'},\n",
       " {'text': 'probably some installation error, check out sy',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'PGCLI --help error',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'acf42bb8'},\n",
       " {'text': 'In this section of the course, the 5432 port of pgsql is mapped to your computer’s 5432 port. Which means you can access the postgres database via pgcli directly from your computer.\\nSo No, you don’t need to run it inside another container. Your local system will do.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'PGCLI - INKhould we run pgcli inside another docker container?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '176ce516'},\n",
       " {'text': 'FATAL:  password authentication failed for user \"root\"\\nobservations: Below in bold do not forget the folder that was created ny_taxi_postgres_data\\nThis happens if you have a local Postgres installation in your computer. To mitigate this, use a different port, like 5431, when creating the docker container, as in: -p 5431: 5432\\nThen, we need to use this port when connecting to pgcli, as shown below:\\npgcli -h localhost -p 5431 -u root -d ny_taxi\\nThis will connect you to your postgres docker container, which is mapped to your host’s 5431 port (though you might choose any port of your liking as long as it is not occupied).\\nFor a more visual and detailed explanation, feel free to check the video 1.4.2 - Port Mapping and Networks in Docker\\nIf you want to debug: the following can help (on a MacOS)\\nTo find out if something is blocking your port (on a MacOS):\\nYou can use the lsof command to find out which application is using a specific port on your local machine. `lsof -i :5432`wi\\nOr list the running postgres services on your local machine with launchctl\\nTo unload the running service on your local machine (on a MacOS):\\nunload the launch agent for the PostgreSQL service, which will stop the service and free up the port  \\n`launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\\nthis one to start it again\\n`launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\\nChanging port from 5432:5432 to 5431:5432 helped me to avoid this error.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'PGCLI - FATAL: password authentication failed for user \"root\" (You already have Postgres)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '3e5d1e9b'},\n",
       " {'text': 'I get this error\\npgcli -h localhost -p 5432 -U root -d ny_taxi\\nTraceback (most recent call last):\\nFile \"/opt/anaconda3/bin/pgcli\", line 8, in <module>\\nsys.exit(cli())\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1128, in __call__\\nreturn self.main(*args, **kwargs)\\nFile \"/opt/anaconda3/lib/python3.9/sitYe-packages/click/core.py\", line\\n1053, in main\\nrv = self.invoke(ctx)\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1395, in invoke\\nreturn ctx.invoke(self.callback, **ctx.params)\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 754, in invoke\\nreturn __callback(*args, **kwargs)\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/pgcli/main.py\", line 880, in cli\\nos.makedirs(config_dir)\\nFile \"/opt/anaconda3/lib/python3.9/os.py\", line 225, in makedirspython\\nmkdir(name, mode)PermissionError: [Errno 13] Permission denied: \\'/Users/vray/.config/pgcli\\'\\nMake sure you install pgcli without sudo.\\nThe recommended approach is to use conda/anaconda to make sure your system python is not affected.\\nIf conda install gets stuck at \"Solving environment\" try these alternatives: https://stackoverflow.com/questions/63734508/stuck-at-solving-environment-on-anaconda',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': \"PGCLI - PermissionError: [Errno 13] Permission denied: '/some/path/.config/pgcli'\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '78833f32'},\n",
       " {'text': 'ImportError: no pq wrapper available.\\nAttempts made:\\n- couldn\\'t import \\\\dt\\nopg \\'c\\' implementation: No module named \\'psycopg_c\\'\\n- couldn\\'t import psycopg \\'binary\\' implementation: No module named \\'psycopg_binary\\'\\n- couldn\\'t import psycopg \\'python\\' implementation: libpq library not found\\nSolution:\\nFirst, make sure your Python is set to 3.9, at least.\\nAnd the reason for that is we have had cases of \\'psycopg2-binary\\' failing to install because of an old version of Python (3.7.3). \\n\\n0. You can check your current python version with: \\n$ python -V(the V must be capital)\\n1. Based on the previous output, if you\\'ve got a 3.9, skip to Step #2\\n   Otherwispye better off with a new environment with 3.9\\n$ conda create –name de-zoomcamp python=3.9\\n$ conda activate de-zoomcamp\\n2. Next, you should be able to install the lib for postgres like this:\\n```\\n$ e\\n$ pip install psycopg2_binary\\n```\\n3. Finally, make sure you\\'re also installing pgcli, but use conda for that:\\n```\\n$ pgcli -h localhost -U root -d ny_taxisudo\\n```\\nThere, you should be good to go now!\\nAnother solution:\\nRun this\\npip install \"psycopg[binary,pool]\"',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'PGCLI - no pq wrapper available.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '63823f21'},\n",
       " {'text': 'If your Bash prompt is stuck on the password command for postgres\\nUse winpty:\\nwinpty pgcli -h localhost -p 5432 -u root -d ny_taxi\\nAlternatively, try using Windows terminal or terminal in VS code.\\nEditPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\\nThe error above was faced continually despite inputting the correct password\\nSolution\\nOption 1: Stop the PostgreSQL service on Windows\\nOption 2 (using WSL): Completely uninstall Protgres 12 from Windows and install postgresql-client on WSL (sudo apt install postgresql-client-common postgresql-client libpq-dev)\\nOption 3: Change the port of the docker container\\nNEW SOLUTION: 27/01/2024\\nPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\\nIf you’ve got the error above, it’s probably because you were just like me, closed the connection to the Postgres:13 image in the previous step of the tutorial, which is\\n\\ndocker run -it \\\\\\n-e POSTGRES_USER=root \\\\\\n-e POSTGRES_PASSWORD=root \\\\\\n-e POSTGRES_DB=ny_taxi \\\\\\n-v d:/git/data-engineering-zoomcamp/week_1/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nSo keep the database connected and you will be able to implement all the next steps of the tutorial.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'PGCLI -  stuck on password prompt',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'b36ea564'},\n",
       " {'text': 'Problem: If you have already installed pgcli but bash doesn\\'t recognize pgcli\\nOn Git bash: bash: pgcli: command not found\\nOn Windows Terminal: pgcli: The term \\'pgcli\\' is not recognized…\\nSolution: Try adding a Python path C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts to Windows PATH\\nFor details:\\nGet the location: pip list -v\\nCopy C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\n3. Replace site-packages with Scripts: C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts\\nIt can also be that you have Python installed elsewhere.\\nFor me it was under c:\\\\python310\\\\lib\\\\site-packages\\nSo I had to add c:\\\\python310\\\\lib\\\\Scripts to PATH, as shown below.\\nPut the above path in \"Path\" (or \"PATH\") in System Variables\\nReference: https://stackoverflow.com/a/68233660',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'PGCLI - pgcli: command not found',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'e2a46ce5'},\n",
       " {'text': 'In case running pgcli  locally causes issues or you do not want to install it locally you can use it running in a Docker container instead.\\nBelow the usage with values used in the videos of the course for:\\nnetwork name (docker network)\\npostgres related variables for pgcli\\nHostname\\nUsername\\nPort\\nDatabase name\\n$ docker run -it --rm --network pg-network ai2ys/dockerized-pgcli:4.0.1\\n175dd47cda07:/# pgcli -h pg-database -U root -p 5432 -d ny_taxi\\nPassword for root:\\nServer: PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1)\\nVersion: 4.0.1\\nHome: http://pgcli.com\\nroot@pg-database:ny_taxi> \\\\dt\\n+--------+------------------+-------+-------+\\n| Schema | Name             | Type  | Owner |\\n|--------+------------------+-------+-------|\\n| public | yellow_taxi_data | table | root  |\\n+--------+------------------+-------+-------+\\nSELECT 1\\nTime: 0.009s\\nroot@pg-database:ny_taxi>',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'PGCLI - running in a Docker container',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '27bdbc3f'},\n",
       " {'text': 'PULocationID will not be recognized but “PULocationID” will be. This is because unquoted \"Localidentifiers are case insensitive. See docs.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'PGCLI - case sensitive use “Quotations” around columns with capital letters',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f7c5d8da'},\n",
       " {'text': 'When using the command `\\\\d <database name>` you get the error column `c.relhasoids does not exist`.\\nResolution:\\nUninstall pgcli\\nReinstall pgclidatabase \"ny_taxi\" does not exist\\nRestart pc',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'PGCLI - error column c.relhasoids does not exist',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c91ad8f2'},\n",
       " {'text': \"This happens while uploading data via the connection in jupyter notebook\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\\nThe port 5432 was taken by another postgres. We are not connecting to the port in docker, but to the port on our machine. Substitute 5431 or whatever port you mapped to for port 5432.\\nAlso if this error is still persistent , kindly check if you have a service in windows running postgres , Stopping that service will resolve the issue\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"root\"',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '88bf31a0'},\n",
       " {'text': 'Can happen when connecting via pgcli\\npgcli -h localhost -p 5432 -U root -d ny_taxi\\nOr while uploading data via the connection in jupyter notebook\\nengine = create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')\\nThis can happen when Postgres is already installed on your computer. Changing the port can resolve that (e.g. from 5432 to 5431).\\nTo check whether there even is a root user with the ability to login:\\nTry: docker exec -it <your_container_name> /bin/bash\\nAnd then run\\n???\\nAlso, you could change port from 5432:5432 to 5431:5432\\nOther solution that worked:\\nChanging `POSTGRES_USER=juroot` to `PGUSER=postgres`\\nBased on this: postgres with docker compose gives FATAL: role \"root\" does not exist error - Stack Overflow\\nAlso `docker compose down`, removing folder that had postgres volume, running `docker compose up` again.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  role \"root\" does not exist',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '23524e6d'},\n",
       " {'text': '~\\\\anaconda3\\\\lib\\\\site-packages\\\\psycopg2\\\\__init__.py in connect(dsn, connection_factory, cursor_factory, **kwargs)\\n120\\n121     dsn = _ext.make_dsn(dsn, **kwargs)\\n--> 122     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\\n123     if cursor_factory is not None:\\n124         conn.cursor_factory = cursor_factory\\nOperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  database \"ny_taxi\" does not exist\\nMake sure postgres is running. You can check that by running `docker ps`\\n✅Solution: If you have postgres software installed on your computer before now, build your instance on a different port like 8080 instead of 5432',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  dodatabase \"ny_taxi\" does not exist',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '9211bbd6'},\n",
       " {'text': \"Issue:\\ne…\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '5db86809'},\n",
       " {'text': 'In the join queries, if we mention the column name directly or enclosed in single quotes it’ll throw an error says “column does not exist”.\\n✅Solution: But if we enclose the column names in double quotes then it will work',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Postgres - \"Column does not exist\" but it actually does (Pyscopg2 error in MacBook Pro M2)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '20c604dd'},\n",
       " {'text': 'pgAdmin has a new version. Create server dialog may not appear. Try using register-> server instead.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'pgAdmin - Create server dialog does not appear',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'b11b8c15'},\n",
       " {'text': 'Using GitHub Codespaces in the browser resulted in a blank screen after the login to pgAdmin (running in a Docker container). The terminal of the pgAdmin container was showing the following error message:\\nCSRFError: 400 Bad Request: The referrer does not match the host.\\nSolution #1:\\nAs recommended in the following issue  https://github.com/pgadmin-org/pgadmin4/issues/5432 setting the following environment variable solved it.\\nPGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\"\\nModified “docker run” command\\ndocker run --rm -it \\\\\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\\\\n-e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\\\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\\\\n-p \"8080:80\" \\\\\\n--name pgadmin \\\\\\n--network=pg-network \\\\\\ndpage/pgadmin4:8.2\\nSolution #2:\\nUsing the local installed VSCode to display GitHub Codespaces.\\nWhen using GitHub Codespaces in the locally installed VSCode (opening a Codespace or creating/starting one) this issue did not occur.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'pgAdmin - Blank/white screen after login (browser)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a6475348'},\n",
       " {'text': 'I am using a Mac Pro device and connect to the GCP Compute Engine via Remote SSH - VSCode. But when I trying to run the PgAdmin container via docker run or docker compose command, I am failed to access the pgAdmin address via my browser. I have switched to another browser, but still can not access the pgAdmin address. So I modified a little bit the configuration from the previous DE Zoomcamp repository like below and can access the pgAdmin address:\\nSolution #1:\\nModified “docker run” command\\ndocker run --rm -it \\\\\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\\\\n-e PGADMIN_DEFAULT_PASSWORD=\"pgadmin\" \\\\\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\\\\n-e PGADMIN_LISTEN_ADDRESS=0.0.0.0 \\\\\\n-e PGADMIN_LISTEN_PORT=5050 \\\\\\n-p 5050:5050 \\\\\\n--network=de-zoomcamp-network \\\\\\n--name pgadmin-container \\\\\\n--link postgres-container \\\\\\n-t dpage/pgadmin4\\nSolution #2:\\nModified docker-compose.yaml configuration (via “docker compose up” command)\\npgadmin:\\nimage: dpage/pgadmin4\\ncontainer_name: pgadmin-conntainer\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\\n- PGADMIN_CONFIG_WTF_CSRF_ENABLED=False\\n- PGADMIN_LISTEN_ADDRESS=0.0.0.0\\n- PGADMIN_LISTEN_PORT=5050\\nvolumes:\\n- \"./pgadmin_data:/var/lib/pgadmin/data\"\\nports:\\n- \"5050:5050\"\\nnetworks:\\n- de-zoomcamp-network\\ndepends_on:\\n- postgres-conntainer\\nPython - ModuleNotFoundError: No module named \\'pysqlite2\\'\\nImportError: DLL load failed while importing _sqlite3: The specified module could not be found. ModuleNotFoundError: No module named \\'pysqlite2\\'\\nThe issue seems to arise from the missing of sqlite3.dll in path \".\\\\Anaconda\\\\Dlls\\\\\".\\n✅I solved it by simply copying that .dll file from \\\\Anaconda3\\\\Library\\\\bin and put it under the path mentioned above. (if you are using anaconda)',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'pgAdmin - Can not access/open the PgAdmin address via browser',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '1ea7680e'},\n",
       " {'text': 'If you follow the video 1.2.2 - Ingesting NY Taxi Data to Postgres and you execute all the same\\nsteps as Alexey does, you will ingest all the data (~1.3 million rows) into the table yellow_taxi_data as expected.\\nHowever, if you try to run the whole script in the Jupyter notebook for a second time from top to bottom, you will be missing the first chunk of 100000 records. This is because there is a call to the iterator before the while loop that puts the data in the table. The while loop therefore starts by ingesting the second chunk, not the first.\\n✅Solution: remove the cell “df=next(df_iter)” that appears higher up in the notebook than the while loop. The first time w(df_iter) is called should be within the while loop.\\n📔Note: As this notebook is just used as a way to test the code, it was not intended to be run top to bottom, and the logic is tidied up in a later step when it is instead inserted into a .py file for the pipeline',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Python - Ingestion with Jupyter notebook - missing 100000 records',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '10acd478'},\n",
       " {'text': '{t_end - t_start} seconds\")\\nimport pandas as pd\\ndf = pd.read_csv(\\'path/to/file.csv.gz\\', /app/ingest_data.py:1: DeprecationWarning:)\\nIf you prefer to keep the uncompressed csv (easier preview in vscode and similar), gzip files can be unzipped using gunzip (but not unzip). On a Ubuntu local or virtual machine, you may need to apt-get install gunzip first.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Python - Iteration csv without error',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '752e8452'},\n",
       " {'text': \"Pandas can interpret “string” column values as “datetime” directly when reading the CSV file using “pd.read_csv” using the parameter “parse_dates”, which for example can contain a list of column names or column indices. Then the conversion afterwards is not required anymore.\\npandas.read_csv — pandas 2.1.4 documentation (pydata.org)\\nExample from week 1\\nimport pandas as pd\\ndf = pd.read_csv(\\n'yellow_tripdata_2021-01.csv',\\nnrows=100,\\nparse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\\ndf.info()\\nwhich will output\\n<class 'pandas.core.frame.DataFrame'>\\nRangeIndex: 100 entries, 0 to 99\\nData columns (total 18 columns):\\n#   Column                 Non-Null Count  Dtype\\n---  ------                 --------------  -----\\n0   VendorID               100 non-null    int64\\n1   tpep_pickup_datetime   100 non-null    datetime64[ns]\\n2   tpep_dropoff_datetime  100 non-null    datetime64[ns]\\n3   passenger_count        100 non-null    int64\\n4   trip_distance          100 non-null    float64\\n5   RatecodeID             100 non-null    int64\\n6   store_and_fwd_flag     100 non-null    object\\n7   PULocationID           100 non-null    int64\\n8   DOLocationID           100 non-null    int64\\n9   payment_type           100 non-null    int64\\n10  fare_amount            100 non-null    float64\\n11  extra                  100 non-null    float64\\n12  mta_tax                100 non-null    float64\\n13  tip_amount             100 non-null    float64\\n14  tolls_amount           100 non-null    float64\\n15  improvement_surcharge  100 non-null    float64\\n16  total_amount           100 non-null    float64\\n17  congestion_surcharge   100 non-null    float64\\ndtypes: datetime64[ns](2), float64(9), int64(6), object(1)\\nmemory usage: 14.2+ KB\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'iPython - Pandas parsing dates with ‘read_csv’',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'aa6f52b8'},\n",
       " {'text': 'os.system(f\"curl -LO {url} -o {csv_name}\")',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Python - Python cant ingest data from the github link provided using curl',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '3dacbb98'},\n",
       " {'text': 'When a CSV file is compressed using Gzip, it is saved with a \".csv.gz\" file extension. This file type is also known as a Gzip compressed CSV file. When you want to read a Gzip compressed CSV file using Pandas, you can use the read_csv() function, which is specifically designed to read CSV files. The read_csv() function accepts several parameters, including a file path or a file-like object. To read a Gzip compressed CSV file, you can pass the file path of the \".csv.gz\" file as an argument to the read_csv() function.\\nHere is an example of how to read a Gzip compressed CSV file using Pandas:\\ndf = pd.read_csv(\\'file.csv.gz\\'\\n, compression=\\'gzip\\'\\n, low_memory=False\\n)',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Python - Pandas can read *.csv.gzip',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '8b71a398'},\n",
       " {'text': \"Contrary to panda’s read_csv method there’s no such easy way to iterate through and set chunksize for parquet files. We can use PyArrow (Apache Arrow Python bindings) to resolve that.\\nimport pyarrow.parquet as pq\\noutput_name = “https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet”\\nparquet_file = pq.ParquetFile(output_name)\\nparquet_size = parquet_file.metadata.num_rows\\nengine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\\ntable_name=”yellow_taxi_schema”\\n# Clear table if exists\\npq.read_table(output_name).to_pandas().head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\\n# default (and max) batch size\\nindex = 65536\\nfor i in parquet_file.iter_batches(use_threads=True):\\nt_start = time()\\nprint(f'Ingesting {index} out of {parquet_size} rows ({index / parquet_size:.0%})')\\ni.to_pandas().to_sql(name=table_name, con=engine, if_exists='append')\\nindex += 65536\\nt_end = time()\\nprint(f'\\\\t- it took %.1f seconds' % (t_end - t_start))\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Python - How to iterate through and ingest parquet file',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'aa244fa0'},\n",
       " {'text': 'Error raised during the jupyter notebook’s cell execution:\\nfrom sqlalchemy import create_engine.\\nSolution: Version of Python module “typing_extensions” >= 4.6.0. Can be updated by Conda or pip.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': \"Python - SQLAlchemy - ImportError: cannot import name 'TypeAliasType' from 'typing_extensions'.\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'eac816d7'},\n",
       " {'text': 'create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \"TypeError: \\'module\\' object is not callable\"\\nSolution:\\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\\nengine = create_engine(conn_string)',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': \"Python - SQLALchemy - TypeError 'module' object is not callable\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'd44d1c77'},\n",
       " {'text': \"Error raised during the jupyter notebook’s cell execution:\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\\nSolution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': \"Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'ed34766a'},\n",
       " {'text': 'Unable to add Google Cloud SDK PATH to Windows\\nWindows error: The installer is unable to automatically update your system PATH. Please add  C:\\\\tools\\\\google-cloud-sdk\\\\bin\\nif you are constantly getting this feedback. Might be that you needed to add Gitbash to your Windows path:\\nOne way of doing that is to use conda: ‘If you are not already using it\\nDownload the Anaconda Navigator\\nMake sure to check the box (add conda to the path when installing navigator: although not recommended do it anyway)\\nYou might also need to install git bash if you are not already using it(or you might need to uninstall it to reinstall it properly)\\nMake sure to check the following boxes while you install Gitbash\\nAdd a GitBash to Windows Terminal\\nUse Git and optional Unix tools from the command prompt\\nNow open up git bash and type conda init bash This should modify your bash profile\\nAdditionally, you might want to use Gitbash as your default terminal.\\nOpen your Windows terminal and go to settings, on the default profile change Windows power shell to git bash',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'GCP - Unable to add Google Cloud SDK PATH to Windows',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'fd714677'},\n",
       " {'text': 'It asked me to create a project. This should be done from the cloud console. So maybe we don’t need this FAQ.\\nWARNING: Project creation failed: HttpError accessing <https://cloudresourcemanager.googleapis.com/v1/projects?alt=json>: response: <{\\'vtpep_pickup_datetimeary\\': \\'Origin, X-Origin, Referer\\', \\'content-type\\': \\'application/json; charset=UTF-8\\', \\'content-encoding\\': \\'gzip\\', \\'date\\': \\'Mon, 24 Jan 2022 19:29:12 GMT\\', \\'server\\': \\'ESF\\', \\'cache-control\\': \\'private\\', \\'x-xss-protection\\': \\'0\\', \\'x-frame-options\\': \\'SAMEORIGIN\\', \\'x-content-type-options\\': \\'nosniff\\', \\'server-timing\\': \\'gfet4t7; dur=189\\', \\'alt-svc\\': \\'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"\\', \\'transfer-encoding\\': \\'chunked\\', \\'status\\': 409}>, content <{\\n\"error\": {\\n\"code\": 409,\\n\"message\": \"Requested entity alreadytpep_pickup_datetime exists\",\\n\"status\": \"ALREADY_EXISTS\"\\n}\\n}\\nFrom Stackoverflow: https://stackoverflow.com/questions/52561383/gcloud-cli-cannot-create-project-the-project-id-you-specified-is-already-in-us?rq=1\\nProject IDs are unique across all projects. That means if any user ever had a project with that ID, you cannot use it. testproject is pretty common, so it\\'s not surprising it\\'s already taken.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'GCP - Project creation failed: HttpError accessing … Requested entity alreadytpep_pickup_datetime exists',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '9de2c3e9'},\n",
       " {'text': 'If you receive the error: “Error 403: The project to be billed is associated with an absent billing account., accountDisabled” It is most likely because you did not enter YOUR project ID. The snip below is from video 1.3.2\\nThe value you enter here will be unique to each student. You can find this value on your GCP Dashboard when you login.\\nAshish Agrawal\\nAnother possibility is that you have not linked your billing account to your current project',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'GCP - The project to be billed is associated with an absent billing account',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '827dd4af'},\n",
       " {'text': 'GCP Account Suspension Inquiry\\nIf Google refuses your credit/debit card, try another - I’ve got an issue with Kaspi (Kazakhstan) but it worked with TBC (Georgia).\\nUnfortunately, there’s small hope that support will help.\\nIt seems that Pyypl web-card should work too.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'GCP - OR-CBAT-15 ERROR Google cloud free trial account',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a42a7e8c'},\n",
       " {'text': 'The ny-rides.json is your private file in Google Cloud Platform (GCP). \\n\\nAnd here’s the way to find it:\\nGCP -> Select project with your  instance -> IAM & Admin -> Service Accounts Keys tab -> add key, JSON as key type, then click create\\nNote: Once you go into Service Accounts Keys tab, click the email, then you can see the “KEYS” tab where you can add key as a JSON as its key type',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'GCP - Where can I find the “ny-rides.json” file?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '4eefdd01'},\n",
       " {'text': 'In this lecture, Alexey deleted his instance in Google Cloud. Do I have to do it?\\nNope. Do not delete your instance in Google Cloud platform. Otherwise, you have to do this twice for the week 1 readings.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'GCP - Do I need to delete my instance in Google Cloud?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '0282578d'},\n",
       " {'text': 'System Resource Usage:\\ntop or htop: Shows real-time information about system resource usage, including CPU, memory, and processes.\\nfree -h: Displays information about system memory usage and availability.\\ndf -h: Shows disk space usage of file systems.\\ndu -h <directory>: Displays disk usage of a specific directory.\\nRunning Processes:\\nps aux: Lists all running processes along with detailed information.\\nNetwork:\\nifconfig or ip addr show: Shows network interface configuration.\\nnetstat -tuln: Displays active network connections and listening ports.\\nHardware Information:\\nlscpu: Displays CPU information.\\nlsblk: Lists block devices (disks and partitions).\\nlshw: Lists hardware configuration.\\nUser and Permissions:\\nwho: Shows who is logged on and their activities.\\nw: Displays information about currently logged-in users and their processes.\\nPackage Management:\\napt list --installed: Lists installed packages (for Ubuntu and Debian-based systems)',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Commands to inspect the health of your VM:',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'bd3e60fd'},\n",
       " {'text': 'if you’ve got the error\\n│ Error: Error updating Dataset \"projects/<your-project-id>/datasets/demo_dataset\": googleapi: Error 403: Billing has not been enabled for this project. Enable billing at https://console.cloud.google.com/billing. The default table expiration time must be less than 60 days, billingNotEnabled\\nbut you’ve set your billing account indeed, then try to disable billing for the project and enable it again. It worked for ME!',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Billing account has not been enabled for this project. But you’ve done it indeed!',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c4e9bc60'},\n",
       " {'text': 'for windows if you having trouble install SDK try follow these steps on the link, if you getting this error:\\nThese credentials will be used by any library that requests Application Default Credentials (ADC).\\nWARNING:\\nCannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\\nFor me:\\nI reinstalled the sdk using unzip file “install.bat”,\\nafter successfully checking gcloud version,\\nrun gcloud init to set up project before\\nyou run gcloud auth application-default login\\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md\\nGCP VM - I cannot get my Virtual Machine to start because GCP has no resources.\\nClick on your VM\\nCreate an image of your VM\\nOn the page of the image, tell GCP to create a new VM instance via the image\\nOn the settings page, change the location',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'GCP - Windows Google Cloud SDK install issue:gcp',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f10b49be'},\n",
       " {'text': 'The reason this video about the GCP VM exists is that many students had problems configuring their env. You can use your own env if it works for you.\\nAnd the advantage of using your own environment is that if you are working in a Github repo where you can commit, you will be able to commit the changes that you do. In the VM the repo is cloned via HTTPS so it is not possible to directly commit, even if you are the owner of the repo.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'GCP VM - Is it necessary to use a GCP VM? When is it useful?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '3184bd8b'},\n",
       " {'text': \"I am trying to create a directory but it won't let me do it\\nUser1@DESKTOP-PD6UM8A MINGW64 /\\n$ mkdir .ssh\\nmkdir: cannot create directory ‘.ssh’: Permission denied\\nYou should do it in your home directory. Should be your home (~)\\nLocal. But it seems you're trying to do it in the root folder (/). Should be your home (~)\\nLink to Video 1.4.1\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'GCP VM - mkdir: cannot create directory ‘.ssh’: Permission denied',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '8bea4d53'},\n",
       " {'text': \"Failed to save '<file>': Unable to write file 'vscode-remote://ssh-remote+de-zoomcamp/home/<user>/data_engineering_course/week_2/airflow/dags/<file>' (NoPermissions (FileSystemError): Error: EACCES: permission denied, open '/home/<user>/data_engineering_course/week_2/airflow/dags/<file>')\\nYou need to change the owner of the files you are trying to edit via VS Code. You can run the following command to change the ownership.\\nssh\\nsudo chown -R <user> <path to your directory>\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'GCP VM - Error while saving the file in VM via VS Code',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '86d11cc0'},\n",
       " {'text': 'Question: I connected to my VM perfectly fine last week (ssh) but when I tried again this week, the connection request keeps timing out.\\n✅Answer: Start your VM. Once the VM is running, copy its External IP and paste that into your config file within the ~/.ssh folder.\\ncd ~/.ssh\\ncode config ← this opens the config file in VSCode',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': '. GCP VM - VM connection request timeout',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '2cb48591'},\n",
       " {'text': '(reference: https://serverfault.com/questions/953290/google-compute-engine-ssh-connect-to-host-ip-port-22-operation-timed-out)Go to edit your VM.\\nGo to section Automation\\nAdd Startup script\\n```\\n#!/bin/bash\\nsudo ufw allow ssh\\n```\\nStop and Start VM.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'GCP VM -  connect to host port 22 no route to host',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '9523c813'},\n",
       " {'text': 'You can easily forward the ports of pgAdmin, postgres and Jupyter Notebook using the built-in tools in Ubuntu and without any additional client:\\nFirst, in the VM machine, launch docker-compose up -d and jupyter notebook in the correct folder.\\nFrom the local machine, execute: ssh -i ~/.ssh/gcp -L 5432:localhost:5432 username@external_ip_of_vm\\nExecute the same command but with ports 8080 and 8888.\\nNow you can access pgAdmin on local machine in browser typing localhost:8080\\nFor Jupyter Notebook, type localhost:8888 in the browser of your local machine. If you have problems with the credentials, it is possible that you have to copy the link with the access token provided in the logs of the terminal of the VM machine when you launched the jupyter notebook command.\\nTo forward both pgAdmin and postgres use, ssh -i ~/.ssh/gcp -L 5432:localhost:5432 -L 8080:localhost:8080 modito@35.197.218.128',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'GCP VM - Port forwarding from GCP without using VS Code',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '4f8d9174'},\n",
       " {'text': 'If you are using MS VS Code and running gcloud in WSL2, when you first try to login to gcp via the gcloud cli gcloud auth application-default login, you will see a message like this, and nothing will happen\\nAnd there might be a prompt to ask if you want to open it via browser, if you click on it, it will open up a page with error message\\nSolution : you should instead hover on the long link, and ctrl + click the long link\\n\\nClick configure Trusted Domains here\\n\\nPopup will appear, pick first or second entry\\nNext time you gcloud auth, the login page should popup via default browser without issues',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'GCP gcloud + MS VS Code - gcloud auth hangs',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '29f84a82'},\n",
       " {'text': 'It is an internet connectivity error, terraform is somehow not able to access the online registry. Check your VPN/Firewall settings (or just clear cookies or restart your network). Try terraform init again after this, it should work.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Terraform - Error: Failed to query available provider packages │ Could not retrieve the list of available versions for provider hashicorp/google: could not query │ provider registry for registry.terrafogorm.io/hashicorp/google: the request failed after 2 attempts, │ please try again later',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '20a01fd0'},\n",
       " {'text': \"The issue was with the network. Google is not accessible in my country, I am using a VPN. And The terminal program does not automatically follow the system proxy and requires separate proxy configuration settings.I opened a Enhanced Mode in Clash, which is a VPN app, and 'terraform apply' works! So if you encounter the same issue, you can ask help for your vpn provider.\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Terraform - Error:Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=coherent-ascent-379901\": oauth2: cannot fetch token: Post \"https://oauth2.googleapis.com/token\": dial tcp 172.217.163.42:443: i/o timeout',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '5a712a20'},\n",
       " {'text': 'https://techcommunity.microsoft.com/t5/azure-developer-community-blog/configuring-terraform-on-windows-10-linux-sub-system/ba-p/393845',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Terraform - Install for WSL',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '06021091'},\n",
       " {'text': 'https://github.com/hashicorp/terraform/issues/14513',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Terraform - Error acquiring the state lock',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'df8ea7e8'},\n",
       " {'text': 'When running\\nterraform apply\\non wsl2 I\\'ve got this error:\\n│ Error: Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=<your-project-id>\": oauth2: cannot fetch token: 400 Bad Request\\n│ Response: {\"error\":\"invalid_grant\",\"error_description\":\"Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.\"}\\nIT happens because there may be time desync on your machine which affects computing JWT\\nTo fix this, run the command\\nsudo hwclock -s\\nwhich fixes your system time.\\nReference',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Terraform - Error 400 Bad Request.  Invalid JWT Token  on WSL.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '1093daf5'},\n",
       " {'text': '│ Error: googleapi: Error 403: Access denied., forbidden\\nYour $GOOGLE_APPLICATION_CREDENTIALS might not be pointing to the correct file \\nrun = export GOOGLE_APPLICATION_CREDENTIALS=~/.gc/YOUR_JSON.json\\nAnd then = gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Terraform - Error 403 : Access denied',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '947213b1'},\n",
       " {'text': \"One service account is enough for all the services/resources you'll use in this course. After you get the file with your credentials and set your environment variable, you should be good to go.\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Terraform - Do I need to make another service account for terraform before I get the keys (.json file)?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '002d4943'},\n",
       " {'text': 'Here: https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Terraform - Where can I find the Terraform 1.1.3 Linux (AMD 64)?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '8dc77677'},\n",
       " {'text': 'You get this error because I run the command terraform init outside the working directory, and this is wrong.You need first to navigate to the working directory that contains terraform configuration files, and and then run the command.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Terraform - Terraform initialized in an empty directory! The directory has no Terraform configuration files. You may begin working with Terraform immediately by creating Terraform configuration files.g',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '29d3d343'},\n",
       " {'text': 'The error:\\nError: googleapi: Error 403: Access denied., forbidden\\n│\\nand\\n│ Error: Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes.\\nFor this solution make sure to run:\\necho $GOOGLE_APPLICATION_CREDENTIALS\\necho $?\\nSolution:\\nYou have to set again the GOOGLE_APPLICATION_CREDENTIALS as Alexey did in the environment set-up video in week1:\\nexport GOOGLE_APPLICATION_CREDENTIALS=\"<path/to/your/service-account-authkeys>.json',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Terraform - Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'e2095203'},\n",
       " {'text': \"The error:\\nError: googleapi: Error 403: terraform-trans-campus@trans-campus-410115.iam.gserviceaccount.com does not have storage.buckets.create access to the Google Cloud project. Permission 'storage.buckets.create' denied on resource (or it may not exist)., forbidden\\nThe solution:\\nYou have to declare the project name as your Project ID, and not your Project name, available on GCP console Dashboard.\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Terraform - Error creating Bucket: googleapi: Error 403: Permission denied to access ‘storage.buckets.create’',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '22a2b9f2'},\n",
       " {'text': 'provider \"google\" {\\nproject     = var.projectId\\ncredentials = file(\"${var.gcpkey}\")\\n#region      = var.region\\nzone = var.zone\\n}',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'To ensure the sensitivity of the credentials file, I had to spend lot of time to input that as a file.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '5d7588f0'},\n",
       " {'text': 'For the HW1 I encountered this issue. The solution is\\nSELECT * FROM zones AS z WHERE z.\"Zone\" = \\'Astoria Zone\\';\\nI think columns which start with uppercase need to go between “Column”. I ran into a lot of issues like this and “ ” made it work out.\\nAddition to the above point, for me, there is no ‘Astoria Zone’, only ‘Astoria’ is existing in the dataset.\\nSELECT * FROM zones AS z WHERE z.\"Zone\" = \\'Astoria’;',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': \"SQL - SELECT * FROM zones_taxi WHERE Zone='Astoria Zone'; Error Column Zone doesn't exist\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '5276a695'},\n",
       " {'text': 'It is inconvenient to use quotation marks all the time, so it is better to put the data to the database all in lowercase, so in Pandas after\\ndf = pd.read_csv(‘taxi+_zone_lookup.csv’)\\nAdd the row:\\ndf.columns = df.columns.str.lower()',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': \"SQL - SELECT Zone FROM taxi_zones Error Column Zone doesn't exist\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '70c159df'},\n",
       " {'text': 'Solution (for mac users): os.system(f\"curl {url} --output {csv_name}\")',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'CURL - curl: (6) Could not resolve host: output.csv',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f55efcf0'},\n",
       " {'text': 'To resolve this, ensure that your config file is in C/User/Username/.ssh/config',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'SSH Error: ssh: Could not resolve hostname linux: Name or service not known',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '2b7a8512'},\n",
       " {'text': 'If you use Anaconda (recommended for the course), it comes with pip, so the issues is probably that the anaconda’s Python is not on the PATH.\\nAdding it to the PATH is different for each operation system.\\nFor Linux and MacOS:\\nOpen a terminal.\\nFind the path to your Anaconda installation. This is typically `~/anaconda3` or `~/opt/anaconda3`.\\nAdd Anaconda to your PATH with the command: `export PATH=\"/path/to/anaconda3/bin:$PATH\"`.\\nTo make this change permanent, add the command to your `.bashrc` (Linux) or `.bash_profile` (MacOS) file.\\nOn Windows, python and pip are in different locations (python is in the anaconda root, and pip is in Scripts). With GitBash:\\nLocate your Anaconda installation. The default path is usually `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3`.\\nDetermine the correct path format for Git Bash. Paths in Git Bash follow the Unix-style, so convert the Windows path to a Unix-style path. For example, `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3` becomes `/c/Users/[YourUsername]/Anaconda3`.\\nAdd Anaconda to your PATH with the command: `export PATH=\"/c/Users/[YourUsername]/Anaconda3/:/c/Users/[YourUsername]/Anaconda3/Scripts/$PATH\"`.\\nTo make this change permanent, add the command to your `.bashrc` file in your home directory.\\nRefresh your environment with the command: `source ~/.bashrc`.\\nFor Windows (without Git Bash):\\nRight-click on \\'This PC\\' or \\'My Computer\\' and select \\'Properties\\'.\\nClick on \\'Advanced system settings\\'.\\nIn the System Properties window, click on \\'Environment Variables\\'.\\nIn the Environment Variables window, select the \\'Path\\' variable in the \\'System variables\\' section and click \\'Edit\\'.\\nIn the Edit Environment Variable window, click \\'New\\' and add the path to your Anaconda installation (typically `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3` and C:\\\\Users\\\\[YourUsername]\\\\Anaconda3\\\\Scripts`).\\nClick \\'OK\\' in all windows to apply the changes.\\nAfter adding Anaconda to the PATH, you should be able to use `pip` from the command line. Remember to restart your terminal (or command prompt in Windows) to apply these changes.',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': \"'pip' is not recognized as an internal or external command, operable program or batch file.\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '1cd746c4'},\n",
       " {'text': \"Resolution: You need to stop the services which is using the port.\\nRun the following:\\n```\\nsudo kill -9 `sudo lsof -t -i:<port>`\\n```\\n<port> being 8080 in this case. This will free up the port for use.\\n~ Abhijit Chakraborty\\nError: error response from daemon: cannot stop container: 1afaf8f7d52277318b71eef8f7a7f238c777045e769dd832426219d6c4b8dfb4: permission denied\\nResolution: In my case, I had to stop docker and restart the service to get it running properly\\nUse the following command:\\n```\\nsudo systemctl restart docker.socket docker.service\\n```\\n~ Abhijit Chakraborty\\nError: cannot import module psycopg2\\nResolution: Run the following command in linux:\\n```\\nsudo apt-get install libpq-dev\\npip install psycopg2\\n```\\n~ Abhijit Chakraborty\\nError: docker build Error checking context: 'can't stat '<path-to-file>'\\nResolution: This happens due to insufficient permission for docker to access a certain file within the directory which hosts the Dockerfile.\\n1. You can create a .dockerignore file and add the directory/file which you want Dockerfile to ignore while build.\\n2. If the above does not work, then put the dockerfile and corresponding script, `\\t1.py` in our case to a subfolder. and run `docker build ...`\\nfrom inside the new folder.\\n~ Abhijit Chakraborty\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Error: error starting userland proxy: listen tcp4 0.0.0.0:8080: bind: address already in use',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '6d367222'},\n",
       " {'text': 'To get a pip-friendly requirements.txt file file from Anaconda use\\nconda install pip then `pip list –format=freeze > requirements.txt`.\\n`conda list -d > requirements.txt` will not work and `pip freeze > requirements.txt` may give odd pathing.',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Anaconda to PIP',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '84e601e1'},\n",
       " {'text': 'Prefect: https://docs.google.com/document/d/1K_LJ9RhAORQk3z4Qf_tfGQCDbu8wUWzru62IUscgiGU/edit?usp=sharing\\nAirflow: https://docs.google.com/document/d/1-BwPAsyDH_mAsn8HH5z_eNYVyBMAtawJRjHHsjEKHyY/edit?usp=sharing',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Where are the FAQ questions from the previous cohorts for the orchestration module?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '4cf83cc2'},\n",
       " {'text': 'Issue : Docker containers exit instantly with code 132, upon docker compose up\\nMage documentation has it listing the cause as \"older architecture\" .\\nThis might be a hardware issue, so unless you have another computer, you can\\'t solve it without purchasing a new one, so the next best solution is a VM.\\nThis is from a student running on a VirtualBox VM, Ubuntu 22.04.3 LTS, Docker version 25.0.2. So not having the context on how the vbox was spin up with (CPU, RAM, network, etc), it’s really inconclusive at this time.',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Docker - 2.2.2 Configure Mage',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '5adc5188'},\n",
       " {'text': 'This issue was occurring with Windows WSL 2\\nFor me this was because WSL 2 was not dedicating enough cpu cores to Docker.The load seems to take up at least one cpu core so I recommend dedicating at least two.\\nOpen Bash and run the following code:\\n$ cd ~\\n$ ls -la\\nLook for the .wsl config file:\\n-rw-r--r-- 1 ~1049089       31 Jan 25 12:54  .wslconfig\\nUsing a text editing tool of your choice edit or create your .wslconfig file:\\n$ nano .wslconfig\\nPaste the following into the new file/ edit the existing file in this format and save:\\n*** Note - for memory– this is the RAM on your machine you can dedicate to Docker, your situation may be different than mine ***\\n[wsl2]\\nprocessors=<Number of Processors - at least 2!> example: 4\\nmemory=<memory> example:4GB\\nExample:\\nOnce you do that run:\\n$ wsl --shutdown\\nThis shuts down WSL\\nThen Restart Docker Desktop - You should now be able to load the .csv.gz file without the error into a pandas dataframe',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'WSL - 2.2.3 Mage - Unexpected Kernel Restarts; Kernel Running out of memory:',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '3ef0bb96'},\n",
       " {'text': 'The issue and solution on the link:\\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1706817366764269?thread_ts=1706815324.993529&cid=C01FABYF2RG',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': '2.2.3 Configuring Postgres',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a41ce360'},\n",
       " {'text': 'Check that the POSTGRES_PORT variable in the io_config.yml  file is set to port 5432, which is the default postgres port. The POSTGRES_PORT variable is the mage container port, not the host port. Hence, there’s no need to set the POSTGRES_PORT to 5431 just because you already have a conflicting postgres installation in your host machine.',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'MAGE - 2.2.3 OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5431 failed: Connection refused',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'b1cf59e5'},\n",
       " {'text': 'You forgot to select ‘dev’ profile in the dropdown menu next to where you select ‘PostgreSQL’ in the connection drop down.',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'MAGE - 2.2.4 executing SELECT 1; results in KeyError',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f9d6f8bd'},\n",
       " {'text': 'If you are getting this error. Update your mage io_config.yaml file, and specify a timeout value set to 600 like this.\\nMake sure to save your changes.\\nMAGE - 2.2.4 Testing BigQuery connection using SQL 404 error:\\nNotFound: 404 Not found: Dataset ny-rides-diegogutierrez:None was not found in location northamerica-northeast1\\nIf you get this error even with all roles/permissions given to the service account check if you have ticked the box where it says “Use raw SQL”, just like the image below.',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': \"MAGE -2.2.4 ConnectionError: ('Connection aborted.', TimeoutError('The write operation timed out'))\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f3adb937'},\n",
       " {'text': 'Solution: https://stackoverflow.com/questions/48056381/google-client-invalid-jwt-token-must-be-a-short-lived-token',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': \"Problem: RefreshError: ('invalid_grant: Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.'})\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'eb3d6d36'},\n",
       " {'text': \"Origin of Solution (Mage Slack-Channel): https://mageai.slack.com/archives/C03HTTWFEKE/p1706543947795599\\nProblem: This error can often be seen after solving the error mentioned in 2.2.4. The error can be found in Mage version 0.9.61 and is a side-effect of the update of the code for data-loader blocks.\\nNote: Mage 0.9.62 has been released, as of Feb 5 2024. Please recheck. Solution below may be obsolete\\nSolution: Using a “fixed” version of the docker container\\nPull updated docker image from docker-hub\\nmageai/mageaidocker pull:alpha\\nUpdate docker-compose.yaml\\nversion: '3'\\nservices:\\nmagic:\\nimage: mageai/mageai:alpha  <--- instead of “latest”-tag\\ndocker-compose up\\nThe original Error is still present, but the SQL-query will return the desired result:\\n--------------------------------------------------------------------------------------\",\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Mage - 2.2.4 IndexError: list index out of range',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a76e1f4d'},\n",
       " {'text': 'Add\\nif not path.parent.is_dir():\\npath.parent.mkdir(parents=True)\\npath = Path(path).as_posix()\\nsee:\\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1675774214591809?thread_ts=1675768839.028879&cid=C01FABYF2RG',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': '2.2.6 OSError: Cannot save file into a non-existent directory: \\'..\\\\\\\\..\\\\\\\\data\\\\\\\\yellow\\'\\\\n\")',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '934facf8'},\n",
       " {'text': 'The video DE Zoomcamp 2.2.7 is missing  the actual deployment of Mage using Terraform to GCP. The steps for the deployment were not covered in the video.\\nI successfully deployed it and wanted to share some key points:\\nIn variables.tf, set the project_id default value to your GCP project ID.\\nEnable the Cloud Filestore API:\\nVisit the Google Cloud Console.to\\nNavigate to \"APIs & Services\" > \"Library.\"\\nSearch for \"Cloud Filestore API.\"\\nClick on the API and enable it.\\nTo perform the deployment:\\nterraform init\\nterraform apply\\nPlease note that during the terraform apply step, Terraform will prompt you to enter the PostgreSQL password. After that, it will ask for confirmation to proceed with the deployment. Review the changes, type \\'yes\\' when prompted, and press Enter.',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'GCP - 2.2.7d Deploying Mage to GCP',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a2c7b59f'},\n",
       " {'text': 'If you want to rune multiple docker containers from different directories. Then make sure to change the port mappings in the docker-compose.yml file.\\nports:\\n- 8088:6789\\nThe 8088 port in above case is hostport, where mage will run on your local machine. You can customize this as long as the port is available. If you are running on VM, make sure to forward the port too. You need to keep the container port to 6789 as this is the port where mage is running.\\nGCP - 2.2.7d Deploying Mage to Google Cloud\\nWhile terraforming all the resources inside a VM created in GCS the following error is shown.\\nError log:\\nmodule.lb-http.google_compute_backend_service.default[\"default\"]: Creating...\\n╷\\n│ Error: Error creating GlobalAddress: googleapi: Error 403: Request had insufficient authentication scopes.\\n│ Details:\\n│ [\\n│   {\\n│     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\\n│     \"domain\": \"googleapis.com\",\\n│     \"metadatas\": {\\n│       \"method\": \"compute.beta.GlobalAddressesService.Insert\",\\n│       \"service\": \"compute.googleapis.com\"\\n│     },\\n│     \"reason\": \"ACCESS_TOKEN_SCOPE_INSUFFICIENT\"\\n│   }\\n│ ]\\n│\\n│ More details:\\n│ Reason: insufficientPermissions, Message: Insufficient Permission\\nThis error might happen when you are using a VM inside GCS. To use the Google APIs from a GCP virtual machine you need to add the cloud platform scope (\"https://www.googleapis.com/auth/cloud-platform\") to your VM when it is created.\\nSince ours is already created you can just stop it and change the permissions. You can do it in the console, just go to \"EDIT\", g99o all the way down until you find \"Cloud API access scopes\". There you can \"Allow full access to all Cloud APIs\". I did this and all went smoothly generating all the resources needed. Hope it helps if you encounter this same error.\\nResources: https://stackoverflow.com/questions/35928534/403-request-had-insufficient-authentication-scopes-during-gcloud-container-clu',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Ruuning Multiple Mage instances in Docker from different directories',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '997d4aaa'},\n",
       " {'text': 'If you are on the free trial account on GCP you will face this issue when trying to deploy the infrastructures with terraform. This service is not available for this kind of account.\\nThe solution I found was to delete the load_balancer.tf file and to comment or delete the rows that differentiate it on the main.tf file. After this just do terraform destroy to delete any infrastructure created on the fail attempts and re-run the terraform apply.\\nCode on main.tf to comment/delete:\\nLine 166, 167, 168',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'GCP - 2.2.7d Load Balancer Problem (Security Policies quota)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'bc269b95'},\n",
       " {'text': \"If you get the following error\\nYou have to edit variables.tf on the gcp folder, set your project-id and region and zones properly. Then, run terraform apply again.\\nYou can find correct regions/zones here: https://cloud.google.com/compute/docs/regions-zones\\nDeploying MAGE to GCP  with Terraform via the VM (2.2.7)\\nFYI - It can take up to 20 minutes to deploy the MAGE Terraform files if you are using a GCP Virtual Machine. It is normal, so don’t interrupt the process or think it’s taking too long. If you have, make sure you run a terraform destroy before trying again as you will have likely partially created resources which will cause errors next time you run `terraform apply`.\\n`terraform destroy` may not completely delete partial resources - go to Google Cloud Console and use the search bar at the top to search for the ‘app.name’ you declared in your variables.tf file; this will list all resources with that name - make sure you delete them all before running `terraform apply` again.\\nWhy are my GCP free credits going so fast? MAGE .tf files - Terraform Destroy not destroying all Resources\\nI checked my GCP billing last night & the MAGE Terraform IaC didn't destroy a GCP Resource called Filestore as ‘mage-data-prep- it has been costing £5.01 of my free credits each day  I now have £151 left - Alexey has assured me that This amount WILL BE SUFFICIENT funds to finish the course. Note to anyone who had issues deploying the MAGE terraform code: check your billing account to see what you're being charged for (main menu - billing) (even if it's your free credits) and run a search for 'mage-data-prep' in the top bar just to be sure that your resources have been destroyed - if any come up delete them.\",\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'GCP - 2.2.7d Part 2 - Getting error when you run terraform apply',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '10ea342e'},\n",
       " {'text': '```\\n│ Error: Error creating Connector: googleapi: Error 403: Permission \\'vpcaccess.connectors.create\\' denied on resource \\'//vpcaccess.googleapis.com/projects/<ommit>/locations/us-west1\\' (or it may not exist).\\n│ Details:\\n│ [\\n│   {\\n│     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\\n│     \"domain\": \"vpcaccess.googleapis.com\",\\n│     \"metadata\": {\\n│       \"permission\": \"vpcaccess.connectors.create\",\\n│       \"resource\": \"projects/<ommit>/locations/us-west1\"\\n│     },\\n│     \"reason\": \"IAM_PERMISSION_DENIED\"\\n│   }\\n│ ]\\n│\\n│   with google_vpc_access_connector.connector,\\n│   on fs.tf line 19, in resource \"google_vpc_access_connector\" \"connector\":\\n│   19: resource \"google_vpc_access_connector\" \"connector\" {\\n│\\n```\\nSolution: Add Serverless VPC Access Admin to Service Account.\\nLine 148',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': \"Question: Permission 'vpcaccess.connectors.create'\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '4bd23594'},\n",
       " {'text': 'Git won’t push an empty folder to GitHub, so if you put a file in that folder and then push, then you should be good to go.\\nOr - in your code- make the folder if it doesn’t exist using Pathlib as shown here: https://stackoverflow.com/a/273227/4590385.\\nFor some reason, when using github storage, the relative path for writing locally no longer works. Try using two separate paths, one full path for the local write, and the original relative path for GCS bucket upload.',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': \"File Path: Cannot save file into a non-existent directory: 'data/green'\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'b0d48cd7'},\n",
       " {'text': 'The green dataset contains lpep_pickup_datetime while the yellow contains tpep_pickup_datetime. Modify the script(s) depending on  the dataset as required.',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'No column name lpep_pickup_datetime / tpep_pickup_datetime',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '70a37f2c'},\n",
       " {'text': 'pd.read_csv\\ndf_iter = pd.read_csv(dataset_url, iterator=True, chunksize=100000)\\nThe data needs to be appended to the parquet file using the fastparquet engine\\ndf.to_parquet(path, compression=\"gzip\", engine=\\'fastparquet\\', append=True)',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Process to download the VSC using Pandas is killed right away',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '8ab78bee'},\n",
       " {'text': 'denied: requested access to the resource is denied\\nThis can happen when you\\nHaven\\'t logged in properly to Docker Desktop (use docker login -u \"myusername\")\\nHave used the wrong username when pushing to docker images. Use the same one as your username and as the one you build on\\ndocker image build -t <myusername>/<imagename>:<tag>\\ndocker image push <myusername>/<imagename>:<tag>',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Push to docker image failure',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '54c6db2f'},\n",
       " {'text': \"16:21:35.607 | INFO    | Flow run 'singing-malkoha' - Executing 'write_bq-b366772c-0' immediately...\\nKilled\\nSolution:  You probably are running out of memory on your VM and need to add more.  For example, if you have 8 gigs of RAM on your VM, you may want to expand that to 16 gigs.\",\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Flow script fails with “killed” message:',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c5b998f3'},\n",
       " {'text': 'After playing around with prefect for a while this can happen.\\nSsh to your VM and run sudo du -h --block-size=G | sort -n -r | head -n 30 to see which directory needs the most space.\\nMost likely it will be …/.prefect/storage, where your cached flows are stored. You can delete older flows from there. You also have to delete the corresponding flow in the UI, otherwise it will throw you an error, when you try to run your next flow.\\nSSL Certificate Verify: (I got it when trying to run flows on MAC): urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]\\npip install certifi\\n/Applications/Python\\\\ {ver}/Install\\\\ Certificates.command\\nor\\nrunning the “Install Certificate.command” inside of the python{ver} folder',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'GCP VM: Disk Space is full',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'eec29536'},\n",
       " {'text': 'It means your container consumed all available RAM allocated to it. It can happen in particular when working on Question#3 in the homework as the dataset is relatively large and containers eat a lot of memory in general.\\nI would recommend restarting your computer and only starting the necessary processes to run the container. If that doesn’t work, allocate more resources to docker. If also that doesn’t work because your workstation is a potato, you can use an online compute environment service like GitPod, which is free under under 50 hours / month of use.',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Docker: container crashed with status code 137.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '727e5a69'},\n",
       " {'text': 'In Q3 there was a task to run the etl script from web to GCS. The problem was, it wasn’t really an ETL straight from web to GCS, but it was actually a web to local storage to local memory to GCS over network ETL. Yellow data is about 100 MB each per month compressed and ~700 MB after uncompressed on memory\\nThis leads to a problem where i either got a network type error because my not so good 3rd world internet or i got my WSL2 crashed/hanged because out of memory error and/or 100% resource usage hang.\\nSolution:\\nif you have a lot of time at hand, try compressing it to parquet and writing it to GCS with the timeout argument set to a really high number (the default os 60 seconds)\\nthe yellow taxi data for feb 2019 is about 100MB as parquet file\\ngcp_cloud_storage_bucket_block.upload_from_path(\\nfrom_path=f\"{path}\",\\nto_path=path,\\ntimeout=600\\n)',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Timeout due to slow upload internet',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'da899638'},\n",
       " {'text': 'This error occurs when you try to re-run the export block, of the transformed green_taxi data to PostgreSQL.\\nWhat you’ll need to do is to drop the table using SQL in Mage (screenshot below).\\nYou should be able to re-run the block successfully after dropping the table.',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'UndefinedColumn: column \"ratecode_id\", \"rate_code_id\" “vendor_id”, “pu_location_id”, “do_location_id” of relation \"green_taxi\" does not exist - Export transformed green_taxi data to PostgreSQL',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'dde58c8f'},\n",
       " {'text': 'SettingWithCopyWarning:\\nA value is trying to be set on a copy of a slice from a DataFrame.\\nUse the data.loc[] = value syntax instead of df[] = value to ensure that the new column is being assigned to the original dataframe instead of a copy of a dataframe or a series.',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Homework - Q3 SettingWithCopyWarning Error:',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '207be93b'},\n",
       " {'text': 'CSV Files are very big in nyc data, so we instead of using Pandas/Python kernel , we can try Pyspark Kernel\\nDocumentation of Mage for using pyspark kernel: https://docs.mage.ai/integrations/spark-pyspark\\n?',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Since I was using slow laptop, and we have so big csv files, I used pyspark kernel in mage instead of python, How to do it?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f0617e65'},\n",
       " {'text': 'So we will first delete the connection between blocks then we can remove the connection.',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'I got an error when I was deleting  BLOCK IN A PIPELINE',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '6290a1a6'},\n",
       " {'text': 'While Editing the Pipeline Name It throws permission denied error.\\n(Work around)In that case proceed with the work and save later on revisit it will let you edit.',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Mage UI won’t let you edit the Pipeline name?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '5a06248c'},\n",
       " {'text': 'Solution n°1 if you want to download everything :\\n```\\nimport pyarrow as pa\\nimport pyarrow.parquet as pq\\nfrom pyarrow.fs import GcsFileSystem\\n…\\n@data_loader\\ndef load_data(*args, **kwargs):\\n    bucket_name = YOUR_BUCKET_NAME_HERE\\'\\n    blob_prefix = \\'PATH / TO / WHERE / THE / PARTITIONS / ARE\\'\\n    root_path = f\"{bucket_name}/{blob_prefix}\"\\npa_table = pq.read_table(\\n        source=root_path,\\n        filesystem=GcsFileSystem(),        \\n    )\\n\\n    return pa_table.to_pandas()\\nSolution n°2 if you want to download only some dates :\\n@data_loader\\ndef load_data(*args, **kwargs):\\ngcs = pa.fs.GcsFileSystem()\\nbucket_name = \\'YOUR_BUCKET_NAME_HERE\\'\\nblob_prefix = \\'\\'PATH / TO / WHERE / THE / PARTITIONS / ARE\\'\\'\\nroot_path = f\"{bucket_name}/{blob_prefix}\"\\npa_dataset = pq.ParquetDataset(\\npath_or_paths=root_path,\\nfilesystem=gcs,\\nfilters=[(\\'lpep_pickup_date\\', \\'>=\\', \\'2020-10-01\\'), (\\'lpep_pickup_date\\', \\'<=\\', \\'2020-10-31\\')]\\n)\\nreturn pa_dataset.read().to_pandas()\\n# More information about the pq.Parquet.Dataset : Encapsulates details of reading a complete Parquet dataset possibly consisting of multiple files and partitions in subdirectories. Documentation here :\\nhttps://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html#pyarrow.parquet.ParquetDataset\\nERROR: UndefinedColumn: column \"vendor_id\" of relation \"green_taxi\" does not exist\\nTwo possible solutions both of them work in the same way.\\nOpen up a Data Loader connect using SQL - RUN the command \\n`DROP TABLE mage.green_taxi`\\nElse, Open up a Data Extractor of SQL  - increase the rows to above the number of rows in the dataframe (you can find that in the bottom of the transformer block) change the Write Policy to `Replace` and run the SELECT statement',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'How do I make Mage load the partitioned files that we created on 2.2.4, to load them into BigQuery ?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c46a2e9e'},\n",
       " {'text': \"All mage files are in your /home/src/folder where you saved your credentials.json so you should be able to access them locally. You will see a folder for ‘Pipelines’,  'data loaders', 'data transformers' & 'data exporters' - inside these will be the .py or .sql files for the blocks you created in your pipeline.\\nRight click & ‘download’ the pipeline itself to your local machine (which gives you metadata, pycache and other files)\\nAs above, download each .py/.sql file that corresponds to each block you created for the pipeline. You'll find these under 'data loaders', 'data transformers' 'data exporters'\\nMove the downloaded files to your GitHub repo folder & commit your changes.\",\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Git - What Files Should I Submit for Homework 2 & How do I get them out of MAGE:',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '0513ab8a'},\n",
       " {'text': 'Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\\nMove the contents of the .gitignore file in your main .gitignore.\\nUse the terminal to cd into the Mage folder and:\\nrun “git remote remove origin” to de-couple the Mage repo,\\nrun “rm -rf .git” to delete local git files,\\nrun “git add .” to add the current folder as changes to stage, commit and push.',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Git - How do I include the files in the Mage repo (including exercise files and homework) in a personal copy of the Data Engineering Zoomcamp repo?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a9385356'},\n",
       " {'text': \"When try to add three assertions:\\nvendor_id is one of the existing values in the column (currently)\\npassenger_count is greater than 0\\ntrip_distance is greater than 0\\nto test_output, I got ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). Below is my code:\\ndata_filter = (data['passenger_count'] > 0) and (data['trip_distance'] > 0)\\nAfter looking for solutions at Stackoverflow, I found great discussion about it. So I changed my code into:\\ndata_filter = (data['passenger_count'] > 0) & (data['trip_distance'] > 0)\",\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Got ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c30468c0'},\n",
       " {'text': 'This happened when I just booted up my PC, continuing from the progress I was doing from yesterday.\\nAfter cd-ing into your directory, and running docker compose up , the web interface for the Mage shows, but the files that I had yesterday was gone.\\nIf your files are gone, go ahead and close the web interface, and properly shutting down the mage docker compose by doing Ctrl + C once. Try running it again. This worked for me more than once (yes the issue persisted with my PC twice)\\nAlso, you should check if you’re in the correct repository before doing docker compose up . This was discussed in the Slack #course-data-engineering channel',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Mage AI Files are Gone/disappearing',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '305aead7'},\n",
       " {'text': 'The above errors due to “ at the trailing side and it need to be modified with ‘ quotes at both ends\\nKrishna Anand',\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Mage - Errors in io.config.yaml file',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '77410975'},\n",
       " {'text': \"Problem: The following error occurs when attempting to export data from Mage to a GCS bucket using pyarrow suggesting Mage doesn’t have the necessary permissions to access the specified GCP credentials .json file.\\nArrowException: Unknown error: google::cloud::Status(UNKNOWN: Permanent error GetBucketMetadata: Could not create a OAuth2 access token to authenticate the request. The request was not sent, as such an access token is required to complete the request successfully. Learn more about Google Cloud authentication at https://cloud.google.com/docs/authentication. The underlying error message was: Cannot open credentials file /home/src/...\\nSolution: Inside the Mage app:\\nCreate a credentials folder (e.g. gcp-creds) within the magic-zoomcamp folder\\nIn the credentials folder create a .json key file (e.g. mage-gcp-creds.json)\\nCopy/paste GCP service account credentials into the .json key file and save\\nUpdate code to point to this file. E.g.\\nenviron['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/src/magic-zoomcamp/gcp-creds/mage-gcp-creds.json'\",\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Mage - ArrowException Cannot open credentials file',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '0952abde'},\n",
       " {'text': \"Oserror: google::cloud::status(unavailable: retry policy exhausted getbucketmetadata: could not create a OAuth2 access token to authenticate the request. the request was not sent, as such an access token is required to complete the request successfully. learn more about google cloud authentication at https://cloud.google.com/docs/authentication. the underlying error message was: performwork() - curl error [6]=couldn't resolve host name)\",\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Mage - OSError',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '7c4326eb'},\n",
       " {'text': \"Problem: The following error occurs when attempting to export data from Mage to a GCS bucket. Assigned service account doesn’t have the necessary permissions access Google Cloud Storage Bucket\\nPermissionError: [Errno 13] google::cloud::Status(PERMISSION_DENIED: Permanent error GetBucketMetadata:... .iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist). error_info={reason=forbidden, domain=global, metadata={http_status_code=403}}). Detail: [errno 13] Permission denied\\nSolution: Add Cloud Storage Admin role to the service account:\\nGo to project in Google Cloud Console>IAM & Admin>IAM\\nClick Edit principal (pencil symbol) to the right of the service account you are using\\nClick + ADD ANOTHER ROLE\\nSelect Cloud Storage>Storage Admin\\nClick Save\",\n",
       "  'section': 'Module 2: Workflow Orchestration',\n",
       "  'question': 'Mage - PermissionError service account does not have storage.buckets.get access to the Google Cloud Storage bucket',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a1fc1a14'},\n",
       " {'text': '1. Make sure your pyspark script is ready to be send to Dataproc cluster\\n2. Create a Dataproc Cluster in GCP Console\\n3. Make sure to edit the service account and add new role - Dataproc Editor\\n4. Copy the python script ./notebooks/pyspark_script.py and place it under GCS bucket path\\n5. Make sure gcloud cli is installed either in Mage manually or  via your Dockerfile and docker-compose files. This is needed to let Mage access google Dataproc and the script it needs to execute. Refer - Installing the latest gcloud CLI\\n6. Use the Bigquery/Dataproc script mentioned here - https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/code/cloud.md . Use Mage to trigger the query',\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'Trigger Dataproc from Mage',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '6d67fba9'},\n",
       " {'text': 'A:\\n1 solution) Add -Y flag, so that apt-get automatically agrees to install additional packages\\n2) Use python ZipFile package, which is included in all modern python distributions',\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'Docker-compose takes infinitely long to install zip unzip packages for linux, which are required to unpack datasets',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '06876291'},\n",
       " {'text': 'Make sure to use Nullable dataTypes, such as Int64 when appliable.',\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'GCS Bucket - error when writing data from web to GCS:',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '690ba010'},\n",
       " {'text': 'Ultimately, when trying to ingest data into a BigQuery table, all files within a given directory must have the same schema.\\nWhen dealing for example with the FHV Datasets from 2019, however (see image below), one can see that the files for \\'2019-05\\', and 2019-06, have the columns \"PUlocationID\" and \"DOlocationID\" as Integers, while for the period of \\'2019-01\\' through \\'2019-04\\', the same column is defined as FLOAT.\\nSo while importing these files as parquet to BigQuery, the first one will be used to define the schema of the table, while all files following that will be used to append data on the existing table. Which means, they must all follow the very same schema of the file that created the table.\\nSo, in order to prevent errors like that, make sure to enforce the data types for the columns on the DataFrame before you serialize/upload them to BigQuery. Like this:\\npd.read_csv(\"path_or_url\").astype({\\n\\t\"col1_name\": \"datatype\",\\t\\n\\t\"col2_name\": \"datatype\",\\t\\n\\t...\\t\\t\\t\\t\\t\\n\\t\"colN_name\": \"datatype\" \\t\\n})',\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': \"GCS Bucket - Failed to create table: Error while reading data, error message: Parquet column 'XYZ' has type INT which does not match the target cpp_type DOUBLE. File: gs://path/to/some/blob.parquet\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'b6fdd91d'},\n",
       " {'text': \"If you receive the error gzip.BadGzipFile: Not a gzipped file (b'\\\\n\\\\n'), this is because you have specified the wrong URL to the FHV dataset. Make sure to use https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/{dataset_file}.csv.gz\\nEmphasising the ‘/releases/download’ part of the URL.\",\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'GCS Bucket - Fix Error when importing FHV data to GCS',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '155aa868'},\n",
       " {'text': 'Krishna Anand',\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'GCS Bucket - Load Data From URL list in to GCP Bucket',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'e78cf960'},\n",
       " {'text': 'Check the Schema\\nYou might have a wrong formatting\\nTry to upload the CSV.GZ files without formatting or going through pandas via wget\\nSee this Slack conversation for helpful tips',\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'GCS Bucket - I query my dataset and get a Bad character (ASCII 0) error?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '9afa1f74'},\n",
       " {'text': 'Run the following command to check if “BigQuery Command Line Tool” is installed or not: gcloud components list\\nYou can also use bq.cmd instead of bq to make it work.',\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'GCP BQ - “bq: command not found”',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'fac138a7'},\n",
       " {'text': 'Use big queries carefully,\\nI created by bigquery dataset on an account where my free trial was exhausted, and got a bill of $80.\\nUse big query in free credits and destroy all the datasets after creation.\\nCheck your Billing daily! Especially if you’ve spinned up a VM.',\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'GCP BQ - Caution in using bigquery:no',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '0174dde5'},\n",
       " {'text': 'Be careful when you create your resources on GCP, all of them have to share the same Region in order to allow load data from GCS Bucket to BigQuery. If you forgot it when you created them, you can create a new dataset on BigQuery using the same Region which you used on your GCS Bucket.\\nThis means that your GCS Bucket and the BigQuery dataset are placed in different regions. You have to create a new dataset inside BigQuery in the same region with your GCS bucket and store the data in the newly created dataset.',\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'GCP BQ - Cannot read and write in different locations: source: EU, destination: US - Loading data from GCS into BigQuery (different Region):',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '1023ee65'},\n",
       " {'text': \"Make sure to create the BigQuery dataset in the very same location that you've created the GCS Bucket. For instance, if your GCS Bucket was created in `us-central1`, then BigQuery dataset must be created in the same region (us-central1, in this example)\",\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'GCP BQ - Cannot read and write in different locations: source: <REGION_HERE>, destination: <ANOTHER_REGION_HERE>',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'effd2bfa'},\n",
       " {'text': 'By the way, this isn’t a problem/solution, but a useful hint:\\nPlease, remember to save your progress in BigQuery SQL Editor.\\nI was almost finishing the homework, when my Chrome Tab froze and I had to reload it. Then I lost my entire SQL script.\\nSave your script from time to time. Just click on the button at the top bar. Your saved file will be available on the left panel.\\nAlternatively, you can copy paste your queries into an .sql file in your preferred editor (Notepad++, VS Code, etc.). Using the .sql extension will provide convenient color formatting.',\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'GCP BQ - Remember to save your queries',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '5b55273c'},\n",
       " {'text': 'Ans :  While real-time analytics might not be explicitly mentioned, BigQuery has real-time data streaming capabilities, allowing for potential integration in future project iterations.',\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'GCP BQ - Can I use BigQuery for real-time analytics in this project?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '1835bfe0'},\n",
       " {'text': \"could not parse 'pickup_datetime' as timestamp for field pickup_datetime (position 2)\\nThis error is caused by invalid data in the timestamp column. A way to identify the problem is to define the schema from the external table using string datatype. This enables the queries to work at which point we can filter out the invalid rows from the import to the materialised table and insert the fields with the timestamp data type.\",\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'GCP BQ - Unable to load data from external tables into a materialized table in BigQuery due to an invalid timestamp error that are added while appending data to the file in Google Cloud Storage',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '04656af5'},\n",
       " {'text': 'Background:\\n`pd.read_parquet`\\n`pd.to_datetime`\\n`pq.write_to_dataset`\\nReference:\\nhttps://stackoverflow.com/questions/48314880/are-parquet-file-created-with-pyarrow-vs-pyspark-compatible\\nhttps://stackoverflow.com/questions/57798479/editing-parquet-files-with-python-causes-errors-to-datetime-format\\nhttps://www.reddit.com/r/bigquery/comments/16aoq0u/parquet_timestamp_to_bq_coming_across_as_int/?share_id=YXqCs5Jl6hQcw-kg6-VgF&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1\\nSolution:\\nAdd `use_deprecated_int96_timestamps=True` to `pq.write_to_dataset` function, like below\\npq.write_to_dataset(\\ntable,\\nroot_path=root_path,\\nfilesystem=gcs,\\nuse_deprecated_int96_timestamps=True\\n# Write timestamps to INT96 Parquet format\\n)',\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'GCP BQ - Error Message in BigQuery: annotated as a valid Timestamp, please annotate it as TimestampType(MICROS) or TimestampType(MILLIS)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '2d6536d3'},\n",
       " {'text': 'Solution:\\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage use PyArrow to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won\\'t be converted to timestamp when loaded by BigQuery later on.\\nimport pyarrow as pa\\nimport pyarrow.parquet as pq\\nimport os\\nif \\'data_exporter\\' not in globals():\\nfrom mage_ai.data_preparation.decorators import data_exporter\\n# Replace with the location of your service account key JSON file.\\nos.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'/home/src/personal-gcp.json\\'\\nbucket_name = \"<YOUR_BUCKET_NAME>\"\\nobject_key = \\'nyc_taxi_data_2022.parquet\\'\\nwhere = f\\'{bucket_name}/{object_key}\\'\\n@data_exporter\\ndef export_data(data, *args, **kwargs):\\ntable = pa.Table.from_pandas(data, preserve_index=False)\\ngcs = pa.fs.GcsFileSystem()\\npq.write_table(\\ntable,\\nwhere,\\n# Convert integer columns in Epoch milliseconds\\n# to Timestamp columns in microseconds (\\'us\\') so\\n# they can be loaded into BigQuery with the right\\n# data type\\ncoerce_timestamps=\\'us\\',\\nfilesystem=gcs\\n)\\nSolution 2:\\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage, provide PyArrow with explicit schema to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won\\'t be converted to timestamp when loaded by BigQuery later on.\\nschema = pa.schema([\\n(\\'vendor_id\\', pa.int64()),\\n(\\'lpep_pickup_datetime\\', pa.timestamp(\\'ns\\')),\\n(\\'lpep_dropoff_datetime\\', pa.timestamp(\\'ns\\')),\\n(\\'store_and_fwd_flag\\', pa.string()),\\n(\\'ratecode_id\\', pa.int64()),\\n(\\'pu_location_id\\', pa.int64()),\\n(\\'do_location_id\\', pa.int64()),\\n(\\'passenger_count\\', pa.int64()),\\n(\\'trip_distance\\', pa.float64()),\\n(\\'fare_amount\\', pa.float64()),\\n(\\'extra\\', pa.float64()),\\n(\\'mta_tax\\', pa.float64()),\\n(\\'tip_amount\\', pa.float64()),\\n(\\'tolls_amount\\', pa.float64()),\\n(\\'improvement_surcharge\\', pa.float64()),\\n(\\'total_amount\\', pa.float64()),\\n(\\'payment_type\\', pa.int64()),\\n(\\'trip_type\\', pa.int64()),\\n(\\'congestion_surcharge\\', pa.float64()),\\n(\\'lpep_pickup_month\\', pa.int64())\\n])\\ntable = pa.Table.from_pandas(data, schema=schema)',\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'GCP BQ - Datetime columns in Parquet files created from Pandas show up as integer columns in BigQuery',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '0516ccbe'},\n",
       " {'text': 'Reference:\\nhttps://cloud.google.com/bigquery/docs/external-data-cloud-storage\\nSolution:\\nfrom google.cloud import bigquery\\n# Set table_id to the ID of the table to create\\ntable_id = f\"{project_id}.{dataset_name}.{table_name}\"\\n# Construct a BigQuery client object\\nclient = bigquery.Client()\\n# Set the external source format of your table\\nexternal_source_format = \"PARQUET\"\\n# Set the source_uris to point to your data in Google Cloud\\nsource_uris = [ f\\'gs://{bucket_name}/{object_key}/*\\']\\n# Create ExternalConfig object with external source format\\nexternal_config = bigquery.ExternalConfig(external_source_format)\\n# Set source_uris that point to your data in Google Cloud\\nexternal_config.source_uris = source_uris\\nexternal_config.autodetect = True\\ntable = bigquery.Table(table_id)\\n# Set the external data configuration of the table\\ntable.external_data_configuration = external_config\\ntable = client.create_table(table)  # Make an API request.\\nprint(f\\'Created table with external source: {table_id}\\')\\nprint(f\\'Format: {table.external_data_configuration.source_format}\\')',\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'GCP BQ - Create External Table using Python',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '6052513d'},\n",
       " {'text': 'Reference:\\nhttps://stackoverflow.com/questions/60941726/can-bigquery-api-overwrite-existing-table-view-with-create-table-tables-inser\\nSolution:\\nCombine with “Create External Table using Python”, use it before “client.create_table” function.\\ndef tableExists(tableID, client):\\n\"\"\"\\nCheck if a table already exists using the tableID.\\nreturn : (Boolean)\\n\"\"\"\\ntry:\\ntable = client.get_table(tableID)\\nreturn True\\nexcept Exception as e: # NotFound:\\nreturn False',\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'GCP BQ - Check BigQuery Table Exist And Delete',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '7a71fa2c'},\n",
       " {'text': 'To avoid this error you can upload data from Google Cloud Storage to BigQuery through BigQuery Cloud Shell using the command:\\n$ bq load  --autodetect --allow_quoted_newlines --source_format=CSV dataset_name.table_name \"gs://dtc-data-lake-bucketname/fhv/fhv_tripdata_2019-*.csv.gz\"',\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'GCP BQ - Error: Missing close double quote (\") character',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f83d9435'},\n",
       " {'text': 'Solution: This problem arises if your gcs and bigquery storage is in different regions.\\nOne potential way to solve it:\\nGo to your google cloud bucket and check the region in field named “Location”\\nNow in bigquery, click on three dot icon near your project name and select create dataset.\\nIn region filed choose the same regions as you saw in your google cloud bucket',\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'GCP BQ - Cannot read and write in different locations: source: asia-south2, destination: US',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'dbf65e11'},\n",
       " {'text': 'There are multiple benefits of using Cloud Functions to automate tasks in Google Cloud.\\nUse below Cloud Function python script to load files directly to BigQuery. Use your project id, dataset id & table id as defined by you.\\nimport tempfile\\nimport requests\\nimport logging\\nfrom google.cloud import bigquery\\ndef hello_world(request):\\n# table_id = <project_id.dataset_id.table_id>\\ntable_id = \\'de-zoomcap-project.dezoomcamp.fhv-2019\\'\\n# Create a new BigQuery client\\nclient = bigquery.Client()\\nfor month in range(4, 13):\\n# Define the schema for the data in the CSV.gz files\\nurl = \\'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-{:02d}.csv.gz\\'.format(month)\\n# Download the CSV.gz file from Github\\nresponse = requests.get(url)\\n# Create new table if loading first month data else append\\nwrite_disposition_string = \"WRITE_APPEND\" if month > 1 else \"WRITE_TRUNCATE\"\\n# Defining LoadJobConfig with schema of table to prevent it from changing with every table\\njob_config = bigquery.LoadJobConfig(\\nschema=[\\nbigquery.SchemaField(\"dispatching_base_num\", \"STRING\"),\\nbigquery.SchemaField(\"pickup_datetime\", \"TIMESTAMP\"),\\nbigquery.SchemaField(\"dropOff_datetime\", \"TIMESTAMP\"),\\nbigquery.SchemaField(\"PUlocationID\", \"STRING\"),\\nbigquery.SchemaField(\"DOlocationID\", \"STRING\"),\\nbigquery.SchemaField(\"SR_Flag\", \"STRING\"),\\nbigquery.SchemaField(\"Affiliated_base_number\", \"STRING\"),\\n],\\nskip_leading_rows=1,\\nwrite_disposition=write_disposition_string,\\nautodetect=True,\\nsource_format=\"CSV\",\\n)\\n# Load the data into BigQuery\\n# Create a temporary file to prevent the exception- AttributeError: \\'bytes\\' object has no attribute \\'tell\\'\"\\nwith tempfile.NamedTemporaryFile() as f:\\nf.write(response.content)\\nf.seek(0)\\njob = client.load_table_from_file(\\nf,\\ntable_id,\\nlocation=\"US\",\\njob_config=job_config,\\n)\\njob.result()\\nlogging.info(\"Data for month %d successfully loaded into table %s.\", month, table_id)\\nreturn \\'Data loaded into table {}.\\'.format(table_id)',\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'GCP BQ - Tip: Using Cloud Function to read csv.gz files from github directly to BigQuery in Google Cloud:',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c489266b'},\n",
       " {'text': 'You need to uncheck cache preferences in query settings',\n",
       "  'section': 'Module 3: Data Warehousing',\n",
       "  'question': 'GCP BQ - When querying two different tables external and materialized you get the same result when count(distinct(*))',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'ebd63566'},\n",
       " {'text': 'Problem: When you inject data into GCS using Pandas, there is a chance that some dataset has missing values on  DOlocationID and PUlocationID. Pandas by default will cast these columns as float data type, causing inconsistent data type between parquet in GCS and schema defined in big query. You will see something like this:\\nSolution:\\nFix the data type issue in data pipeline\\nBefore injecting data into GCS, use astype and Int64 (which is different from int64 and accept both missing value and integer exist in the column) to cast the columns.\\nSomething like:\\ndf[\"PUlocationID\"] = df.PUlocationID.astype(\"Int64\")\\ndf[\"DOlocationID\"] = df.DOlocationID.astype(\"Int64\")\\nNOTE: It is best to define the data type of all the columns in the Transformation section of the ETL pipeline before loading to BigQuery',\n",
       "  'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "  'question': 'GCP BQ - How to handle type error from big query and parquet data?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f7252f17'},\n",
       " {'text': 'Problem occurs when misplacing content after fro``m clause in BigQuery SQLs.\\nCheck to remove any extra apaces or any other symbols, keep in lowercases, digits and dashes only',\n",
       "  'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "  'question': 'GCP BQ - Invalid project ID . Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '47a43bb0'},\n",
       " {'text': 'No. Based on the documentation for Bigquery, it does not support more than 1 column to be partitioned.\\n[source]',\n",
       "  'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "  'question': 'GCP BQ - Does BigQuery support multiple columns partition?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f3f13def'},\n",
       " {'text': 'Error Message:\\nPARTITION BY expression must be DATE(<timestamp_column>), DATE(<datetime_column>), DATETIME_TRUNC(<datetime_column>, DAY/HOUR/MONTH/YEAR), a DATE column, TIMESTAMP_TRUNC(<timestamp_column>, DAY/HOUR/MONTH/YEAR), DATE_TRUNC(<date_column>, MONTH/YEAR), or RANGE_BUCKET(<int64_column>, GENERATE_ARRAY(<int64_value>, <int64_value>[, <int64_value>]))\\nSolution:\\nConvert the column to datetime first.\\ndf[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\\ndf[\"dropOff_datetime\"] = pd.to_datetime(df[\"dropOff_datetime\"])',\n",
       "  'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "  'question': 'GCP BQ - DATE() Error in BigQuery',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '4fd37712'},\n",
       " {'text': 'Native tables are tables where the data is stored in BigQuery.  External tables store the data outside BigQuery, with BigQuery storing metadata about that external table.\\nResources:\\nhttps://cloud.google.com/bigquery/docs/external-tables\\nhttps://cloud.google.com/bigquery/docs/tables-intro',\n",
       "  'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "  'question': 'GCP BQ - Native tables vs External tables in BigQuery?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '8abeca36'},\n",
       " {'text': 'Issue: Tried running command to export ML model from BQ to GCS from Week 3\\nbq --project_id taxi-rides-ny extract -m nytaxi.tip_model gs://taxi_ml_model/tip_model\\nIt is failing on following error:\\nBigQuery error in extract operation: Error processing job Not found: Dataset was not found in location US\\nI verified the BQ data set and gcs bucket are in the same region- us-west1. Not sure how it gets location US. I couldn’t find the solution yet.\\nSolution:  Please enter correct project_id and gcs_bucket folder address. My gcs_bucket folder address is\\ngs://dtc_data_lake_optimum-airfoil-376815/tip_model',\n",
       "  'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "  'question': 'GCP BQ ML - Unable to run command (shown in video) to export ML model from BQ to GCS',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '16c16ff9'},\n",
       " {'text': \"To solve this error mention the location = US when creating the dim_zones table\\n{{ config(\\nmaterialized='table',\\nlocation='US'\\n) }}\\nJust Update this part to solve the issue and run the dim_zones again and then run the fact_trips\",\n",
       "  'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "  'question': 'Dim_zones.sql Dataset was not found in location US When Running fact_trips.sql',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c65d8fd9'},\n",
       " {'text': 'Solution: proceed with setting up serving_dir on your computer as in the extract_model.md file. Then instead of\\ndocker pull tensorflow/serving\\nuse\\ndocker pull emacski/tensorflow-serving\\nThen\\ndocker run -p 8500:8500 -p 8501:8501 --mount type=bind,source=`pwd`/serving_dir/tip_model,target=/models/tip_model -e MODEL_NAME=tip_model -t emacski/tensorflow-serving\\nThen run the curl command as written, and you should get a prediction.',\n",
       "  'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "  'question': 'GCP BQ ML - Export ML model to make predictions does not work for MacBook with Apple M1 chip (arm architecture).',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c1a95536'},\n",
       " {'text': 'Try deleting data you’ve saved to your VM locally during ETLs\\nKill processes related to deleted files\\nDownload ncdu and look for large files (pay particular attention to files related to Prefect)\\nIf you delete any files related to Prefect, eliminate caching from your flow code',\n",
       "  'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "  'question': 'VMs - What do I do if my VM runs out of space?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'bba0da04'},\n",
       " {'text': \"Ans: What they mean is that they don't want you to do anything more than that. You should load the files into the bucket and create an external table based on those files (but nothing like cleaning the data and putting it in parquet format)\",\n",
       "  'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "  'question': \"Homework - What does it mean “Stop with loading the files into a bucket.' Stop with loading the files into a bucket?”\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a2120335'},\n",
       " {'text': 'If for whatever reason you try to read parquets directly from nyc.gov’s cloudfront into pandas, you might run into this error:\\npyarrow.lib.ArrowInvalid: Casting from timestamp[us] to timestamp[ns] would result in out of bounds\\nCause:\\nthere is one errant data record where the dropOff_datetime was set to year 3019 instead of 2019.\\npandas uses “timestamp[ns]” (as noted above), and int64 only allows a ~580 year range, centered on 2000. See `pd.Timestamp.max` and `pd.Timestamp.min`\\nThis becomes out of bounds when pandas tries to read it because 3019 > 2300 (approx value of pd.Timestamp.Max\\nFix:\\nUse pyarrow to read it:\\nimport pyarrow.parquet as pq df = pq.read_table(\\'fhv_tripdata_2019-02.parquet\\').to_pandas(safe=False)\\nHowever this results in weird timestamps for the offending record\\nRead the datetime columns separately using pq.read_table\\n\\ntable = pq.read_table(‘taxi.parquet’)\\ndatetimes = [‘list of datetime column names’]\\ndf_dts = pd.DataFrame()\\nfor col in datetimes:\\ndf_dts[col] = pd.to_datetime(table .column(col), errors=\\'coerce\\')\\n\\nThe `errors=’coerce’` parameter will convert the out of bounds timestamps into either the max or the min\\nUse parquet.compute.filter to remove the offending rows\\n\\nimport pyarrow.compute as pc\\ntable = pq.read_table(\"‘taxi.parquet\")\\ndf = table.filter(\\npc.less_equal(table[\"dropOff_datetime\"], pa.scalar(pd.Timestamp.max))\\n).to_pandas()',\n",
       "  'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "  'question': 'Homework - Reading parquets from nyc.gov directly into pandas returns Out of bounds error',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a4ba2478'},\n",
       " {'text': 'Answer: The 2022 NYC taxi data parquet files are available for each month separately. Therefore, you need to add all 12 files to your GCS bucket and then refer to them using the URIs option when creating an external table in BigQuery. You can use the wildcard \"*\" to refer to all 12 files using a single string.',\n",
       "  'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "  'question': 'Question: for homework 3 , we need all 12 parquet files for green taxi 2022 right ?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '74c361fe'},\n",
       " {'text': 'This can help avoid schema issues in the homework. \\nDownload files locally and use the ‘upload files’ button in GCS at the desired path. You can upload many files at once. You can also choose to upload a folder.',\n",
       "  'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "  'question': 'Homework - Uploading files to GCS via GUI',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'b9b3ef9f'},\n",
       " {'text': 'Ans: Take a careful look at the format of the dates in the question.',\n",
       "  'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "  'question': 'Homework - Qn 5: The partitioned/clustered table isn’t giving me the prediction I expected',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '009ac612'},\n",
       " {'text': 'Many people aren’t getting an exact match, but are very close to one of the options. As per Alexey said to choose the closest option.',\n",
       "  'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "  'question': 'Homework - Qn 6: Did anyone get an exact match for one of the options given in Module 3 homework Q6?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '68815ec2'},\n",
       " {'text': 'UnicodeDecodeError: \\'utf-8\\' codec can\\'t decode byte 0xa0 in position 41721: invalid start byte\\nSolution:\\nStep 1: When reading the data from the web into the pandas dataframe mention the encoding as follows:\\npd.read_csv(dataset_url, low_memory=False, encoding=\\'latin1\\')\\nStep 2: When writing the dataframe from the local system to GCS as a csv mention the encoding as follows:\\ndf.to_csv(path_on_gsc, compression=\"gzip\", encoding=\\'utf-8\\')\\nAlternative: use pd.read_parquet(url)',\n",
       "  'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "  'question': 'Python - invalid start byte Error Message',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c8ad08b3'},\n",
       " {'text': 'A generator is a function in python that returns an iterator using the yield keyword.\\nA generator is a special type of iterable, similar to a list or a tuple, but with a crucial difference. Instead of creating and storing all the values in memory at once, a generator generates values on-the-fly as you iterate over it. This makes generators memory-efficient, particularly when dealing with large datasets.',\n",
       "  'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "  'question': 'Python - Generators in python',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'd68b433f'},\n",
       " {'text': 'The read_parquet function supports a list of files as an argument. The list of files will be merged into a single result table.',\n",
       "  'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "  'question': 'Python - Easiest way to read multiple files at the same time?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'e265ee5a'},\n",
       " {'text': \"Incorrect:\\ndf['DOlocationID'] = pd.to_numeric(df['DOlocationID'], downcast=integer) or\\ndf['DOlocationID'] = df['DOlocationID'].astype(int)\\nCorrect:\\ndf['DOlocationID'] = df['DOlocationID'].astype('Int64')\",\n",
       "  'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "  'question': \"Python - These won't work. You need to make sure you use Int64:\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '0e7dfddc'},\n",
       " {'text': \"ValueError: Path /Users/kt/.prefect/storage/44ccce0813ed4f24ab2d3783de7a9c3a does not exist.\\nRemove ```cache_key_fn=task_input_hash ``` as it’s in argument in your function & run your flow again.\\nNote: catche key is beneficial if you happen to run the code multiple times, it won't repeat the process which you have finished running in the previous run.  That means, if you have this ```cache_key``` in your initial run, this might cause the error.\",\n",
       "  'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "  'question': 'Prefect - Error on Running Prefect Flow to Load data to GCS',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '0a059700'},\n",
       " {'text': '@task\\ndef download_file(url: str, file_path: str):\\nresponse = requests.get(url)\\nopen(file_path, \"wb\").write(response.content)\\nreturn file_path\\n@flow\\ndef extract_from_web() -> None:\\nfile_path = download_file(url=f\\'{url-filename}.csv.gz\\',file_path=f\\'{filename}.csv.gz\\')',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Prefect - Tip: Downloading csv.gz from a url in a prefect environment (sample snippet).',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'feca7402'},\n",
       " {'text': 'Update the seed column types in the dbt_project.yaml file\\nfor using double : float\\nfor using int : numeric\\nDBT Cloud production error: prod dataset not available in location EU\\nProblem: I am trying to deploy my DBT  models to production, using DBT Cloud. The data should live in BigQuery.  The dataset location is EU.  However, when I am running the model in production, a prod dataset is being create in BigQuery with a location US and the dbt invoke build is failing giving me \"ERROR 404: porject.dataset:prod not available in location EU\". I tried different ways to fix this. I am not sure if there is a more simple solution then creating my project or buckets in location US. Hope anyone can help here.\\nNote: Everything is working fine in development mode, the issue is just happening when scheduling and running job in production\\nSolution: I created the prod dataset manually in BQ and specified EU, then I ran the job.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'If you are getting not found in location us error.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '1f519b1a'},\n",
       " {'text': 'Error: This project does not have a development environment configured. Please create a development environment and configure your development credentials to use the dbt IDE.\\nThe error itself tells us how to solve this issue, the guide is here. And from videos @1:42 and also slack chat',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Setup - No development environment',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '43c454c7'},\n",
       " {'text': \"Runtime Error\\ndbt was unable to connect to the specified database.\\nThe database returned the following error:\\n>Database Error\\nAccess Denied: Project <project_name>: User does not have bigquery.jobs.create permission in project <project_name>.\\nCheck your database credentials and try again. For more information, visit:\\nhttps://docs.getdbt.com/docs/configure-your-profile\\nSteps to resolve error in Google Cloud:\\n1. Navigate to IAM & Admin and select IAM\\n2. Click Grant Access if your newly created dbt service account isn't listed\\n3. In New principals field, add your service account\\n4. Select a Role and search for BigQuery Job User to add\\n5. Go back to dbt cloud project setup and Test your connection\\n6. Note: Also add BigQuery Data Owner, Storage Object Admin, & Storage Admin to prevent permission issues later in the course\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Setup - Connecting dbt Cloud with BigQuery Error',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'd7ad69da'},\n",
       " {'text': 'error: This dbt Cloud run was cancelled because a valid dbt project was not found. Please check that the repository contains a proper dbt_project.yml config file. If your dbt project is located in a subdirectory of the connected repository, be sure to specify its location on the Project settings page in dbt Cloud',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Dbt build error',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '03fdb780'},\n",
       " {'text': \"Error: Failed to clone repository.\\ngit clone git@github.com:DataTalksClub/data-engineering-zoomcamp.git /usr/src/develop/…\\nCloning into '/usr/src/develop/...\\nWarning: Permanently added 'github.com,140.82.114.4' (ECDSA) to the list of known hosts.\\ngit@github.com: Permission denied (publickey).\\nfatal: Could not read from remote repository.\\nIssue: You don’t have permissions to write to DataTalksClub/data-engineering-zoomcamp.git\\nSolution 1: Clone the repository and use this forked repo, which contains your github username. Then, proceed to specify the path, as in:\\n[your github username]/data-engineering-zoomcamp.git\\nSolution 2: create a fresh repo for dbt-lessons. We’d need to do branching and PRs in this lesson, so it might be a good idea to also not mess up your whole other repo. Then you don’t have to create a subfolder for the dbt project files\\nSolution 3: Use https link\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Setup - Failed to clone repository.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '9c85f3aa'},\n",
       " {'text': \"Solution:\\nCheck if you’re on the Developer Plan. As per the prerequisites, you'll need to be enrolled in the Team Plan or Enterprise Plan to set up a CI Job in dbt Cloud.\\nSo If you're on the Developer Plan, you'll need to upgrade to utilise CI Jobs.\\nNote from another user: I’m in the Team Plan (trial period) but the option is still disabled. What worked for me instead was this. It works for the Developer (free) plan.\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'dbt job - Triggered by pull requests is disabled when I try to create a new Continuous Integration job in dbt cloud.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '63026349'},\n",
       " {'text': 'Issue: If the DBT cloud IDE loading indefinitely then giving you this error\\nSolution: check the dbt_cloud_setup.md  file and make a SSH Key and use gitclone to import repo into dbt project, copy and paste deploy key back in your repo setting.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Setup - Your IDE session was unable to start. Please contact support.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '6ba02f77'},\n",
       " {'text': 'Issue: If you don’t define the column format while converting from csv to parquet Python will “choose” based on the first rows.\\n✅Solution: Defined the schema while running web_to_gcp.py pipeline.\\nSebastian adapted the script:\\nhttps://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py\\nNeed a quick change to make the file work with gz files, added the following lines (and don’t forget to delete the file at the end of each iteration of the loop to avoid any problem of disk space)\\nfile_name_gz = f\"{service}_tripdata_{year}-{month}.csv.gz\"\\nopen(file_name_gz, \\'wb\\').write(r.content)\\nos.system(f\"gzip -d {file_name_gz}\")\\nos.system(f\"rm {file_name_init}.*\")\\nSame ERROR - When running dbt run for fact_trips.sql, the task failed with error:\\n“Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64”\\n开启屏幕阅读器支持\\n要启用屏幕阅读器支持，请按Ctrl+Alt+Z。要了解键盘快捷键，请按Ctrl+斜杠。\\n查找和替换\\nReason: Parquet files have their own schema. Some parquet files for green data have records with decimals in ehail_fee column.\\nThere are some possible fixes:\\nDrop ehail_feel column since it is not really used. For instance when creating a partitioned table from the external table in BigQuery\\nSELECT * EXCEPT (ehail_fee) FROM…\\nModify stg_green_tripdata.sql model using this line cast(0 as numeric) as ehail_fee.\\nModify Airflow dag to make the conversion and avoid the error.\\npv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types = {\\'ehail_fee\\': \\'float64\\'}))\\nSame type of ERROR - parquet files with different data types - Fix it with pandas\\nHere is another possibility that could be interesting:\\nYou can specify the dtypes when importing the file from csv to a dataframe with pandas\\npd.from_csv(..., dtype=type_dict)\\nOne obstacle is that the regular int64 pandas use (I think this is from the numpy library) does not accept null values (NaN, not a number). But you can use the pandas Int64 instead, notice capital ‘I’. The type_dict is a python dictionary mapping the column names to the dtypes.\\nSources:\\nhttps://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\\nNullable integer data type — pandas 1.5.3 documentation',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'DBT - I am having problems with columns datatype while running DBT/BigQuery',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '8b14286c'},\n",
       " {'text': 'If the provided URL isn’t working for you (https://nyc-tlc.s3.amazonaws.com/trip+data/):\\nWe can use the GitHub CLI to easily download the needed trip data from https://github.com/DataTalksClub/nyc-tlc-data, and manually upload to a GCS bucket.\\nInstructions on how to download the CLI here: https://github.com/cli/cli\\nCommands to use:\\ngh auth login\\ngh release list -R DataTalksClub/nyc-tlc-data\\ngh release download yellow -R DataTalksClub/nyc-tlc-data\\ngh release download green -R DataTalksClub/nyc-tlc-data\\netc.\\nNow you can upload the files to a GCS bucket using the GUI.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Ingestion: When attempting to use the provided quick script to load trip data into GCS, you receive error Access Denied from the S3 bucket',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '14a876ea'},\n",
       " {'text': \"R: This conversion is needed for the question 3 of homework, in order to process files for fhv data. The error is:\\npyarrow.lib.ArrowInvalid: CSV parse error: Expected 7 columns, got 1: B02765\\nCause: Some random line breaks in this particular file.\\nFixed by opening a bash in the container executing the dag and manually running the following command that deletes all \\\\n not preceded by \\\\r.\\nperl -i -pe 's/(?<!\\\\r)\\\\n/\\\\1/g' fhv_tripdata_2020-01.csv\\nAfter that, clear the failed task in Airflow to force re-execution.\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Ingestion - Error thrown by format_to_parquet_task when converting fhv_tripdata_2020-01.csv using Airflow',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '1cf5be74'},\n",
       " {'text': 'I initially followed data-engineering-zoomcamp/03-data-warehouse/extras/web_to_gcs.py at main · DataTalksClub/data-engineering-bootcamp (github.com)\\nBut it was taking forever for the yellow trip data and when I tried to download and upload the parquet files directly to GCS, that works fine but when creating the Bigquery table, there was a schema inconsistency issue\\nThen I found another hack shared in the slack which was suggested by Victoria.\\n[Optional] Hack for loading data to BigQuery for Week 4 - YouTube\\nPlease watch until the end as there is few schema changes required to be done',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Hack to load yellow and green trip data for 2019 and 2020',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '315ac3cc'},\n",
       " {'text': '“gs\\\\storage_link\\\\*.parquet” need to be added in destination folder',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Move many files (more than one) from Google cloud storage bucket to Big query',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c5c3beba'},\n",
       " {'text': 'One common cause experienced is lack of space after running prefect several times. When running prefect, check the folder ‘.prefect/storage’ and delete the logs now and then to avoid the problem.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'GCP VM - All of sudden ssh stopped working for my VM after my last restart',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f19be91b'},\n",
       " {'text': 'You can try to do this steps:',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'GCP VM - If you have lost SSH access to your machine due to lack of space. Permission denied (publickey)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '33db7dc7'},\n",
       " {'text': 'R: Go to BigQuery, and check the location of BOTH\\nThe source dataset (trips_data_all), and\\nThe schema you’re trying to write to (name should be \\tdbt_<first initial><last name> (if you didn’t change the default settings at the end when setting up your project))\\nLikely, your source data will be in your region, but the write location will be a multi-regional location (US in this example). Delete these datasets, and recreate them with your specified region and the correct naming format.\\nAlternatively, instead of removing datasets, you can specify the single-region location you are using. E.g. instead of ‘location: US’, specify the region, so ‘location: US-east1’. See this Github comment for more detail. Additionally please see this post of Sandy\\nIn DBT cloud you can actually specify the location using the following steps:\\nGPo to your profile page (top right drop-down --> profile)\\nThen go to under Credentials --> Analytics (you may have customised this name)\\nClick on Bigquery >\\nHit Edit\\nUpdate your location, you may need to re-upload your service account JSON to re-fetch your private key, and save. (NOTE: be sure to exactly copy the region BigQuery specifies your dataset is in.)',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': '404 Not found: Dataset eighth-zenith-372015:trip_data_all was not found in location us-west1',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '67ef8f87'},\n",
       " {'text': 'Error: `dbt_utils.surrogate_key` has been replaced by `dbt_utils.generate_surrogate_key`\\nFix:\\nReplace dbt_utils.surrogate_key  with dbt_utils.generate_surrogate_key in stg_green_tripdata.sql\\nWhen executing dbt run after fact_trips.sql has been created, the task failed with error:\\nR: “Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.”\\n1. Fixed by adding the Storage Object Viewer role to the service account in use in BigQuery.\\n2. Add the related roles to the service account in use in GCS.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'When executing dbt run after installing dbt-utils latest version i.e., 1.0.0 warning has generated',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '6acf2e77'},\n",
       " {'text': 'You need to create packages.yml file in main project directory and add packages’ meta data:\\npackages:\\n- package: dbt-labs/dbt_utils\\nversion: 0.8.0\\nAfter creating file run:\\nAnd hit enter.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'When You are getting error dbt_utils not found',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '18430f10'},\n",
       " {'text': \"Ensure you properly format your yml file. Check the build logs if the run was completed successfully. You can expand the command history console (where you type the --vars '{'is_test_run': 'false'}')  and click on any stage’s logs to expand and read errors messages or warnings.\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Lineage is currently unavailable. Check that your project does not contain compilation errors or contact support if this error persists.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'afb7a40a'},\n",
       " {'text': \"Make sure you use:\\ndbt run --var ‘is_test_run: false’ or\\ndbt build --var ‘is_test_run: false’\\n(watch out for formatted text from this document: re-type the single quotes). If that does not work, use --vars '{'is_test_run': 'false'}' with each phrase separately quoted.\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Build - Why do my Fact_trips only contain a few days of data?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'd6a5b80e'},\n",
       " {'text': 'Check if you specified if_exists argument correctly when writing data from GCS to BigQuery. When I wrote my automated flow for each month of the years 2019 and 2020 for green and yellow data I had specified if_exists=\"replace\" while I was experimenting with the flow setup. Once you want to run the flow for all months in 2019 and 2020 make sure to set if_exists=\"append\"\\nif_exists=\"replace\" will replace the whole table with only the month data that you are writing into BigQuery in that one iteration -> you end up with only one month in BigQuery (the last one you inserted)\\nif_exists=\"append\" will append the new monthly data -> you end up with data from all months',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Build - Why do my fact_trips only contain one month of data?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'de426d2f'},\n",
       " {'text': \"R: After the second SELECT, change this line:\\ndate_trunc('month', pickup_datetime) as revenue_month,\\nTo this line:\\ndate_trunc(pickup_datetime, month) as revenue_month,\\nMake sure that “month” isn’t surrounded by quotes!\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'BigQuery returns an error when I try to run the dm_monthly_zone_revenue.sql model.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '354f0e10'},\n",
       " {'text': 'For this instead:\\n{{ dbt_utils.generate_surrogate_key([ \\n     field_a, \\n     field_b, \\n     field_c,\\n     …,\\n     field_z\\n]) }}',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Replace: \\n{{ dbt_utils.surrogate_key([ \\n     field_a, \\n     field_b, \\n     field_c,\\n     …,\\n     field_z     \\n]) }}',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '98fae8d0'},\n",
       " {'text': 'Remove the dataset from BigQuery which was created by dbt and run dbt run again so that it will recreate the dataset in BigQuery with the correct location',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'I changed location in dbt, but dbt run still gives me an error',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'cb678fde'},\n",
       " {'text': 'Remove the dataset from BigQuery created by dbt and run again (with test disabled) to ensure the dataset created has all the rows.\\nDBT - Why am I getting a new dataset after running my CI/CD Job? / What is this new dbt dataset in BigQuery?\\nAnswer: when you create the CI/CD job, under ‘Compare Changes against an environment (Deferral) make sure that you select ‘ No; do not defer to another environment’ - otherwise dbt won’t merge your dev models into production models; it will create a new environment called ‘dbt_cloud_pr_number of pull request’',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'I ran dbt run without specifying variable which gave me a table of 100 rows. I ran again with the variable value specified but my table still has 100 rows in BQ.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '39bfb043'},\n",
       " {'text': \"Vic created three different datasets in the videos.. dbt_<name> was used for development and you used a production dataset for the production environment. What was the use for the staging dataset?\\nR: Staging, as the name suggests, is like an intermediate between the raw datasets and the fact and dim tables, which are the finished product, so to speak. You'll notice that the datasets in staging are materialised as views and not tables.\\nVic didn't use it for the project, you just need to create production and dbt_name + trips_data_all that you had already.\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Why do we need the Staging dataset?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '351a078a'},\n",
       " {'text': 'Try removing the “network: host” line in docker-compose.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'DBT Docs Served but Not Accessible via Browser',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '61da1919'},\n",
       " {'text': 'Go to Account settings >> Project >> Analytics >> Click on your connection >> go all the way down to Location and type in the GCP location just as displayed in GCP (e.g. europe-west6). You might need to reupload your GCP key.\\nDelete your dataset in GBQ\\nRebuild project: dbt build\\nNewly built dataset should be in the correct location',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'BigQuery adapter: 404 Not found: Dataset was not found in location europe-west6',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '6528c6ae'},\n",
       " {'text': 'Create a new branch to edit. More on this can be found here in the dbt docs.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Dbt+git - Main branch is “read-only”',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c0d3a2e8'},\n",
       " {'text': 'Create a new branch for development, then you can merge it to the main branch\\nCreate a new branch and switch to this branch. It allows you to make changes. Then you can commit and push the changes to the “main” branch.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': \"Dbt+git - It appears that I can't edit the files because I'm in read-only mode. Does anyone know how I can change that?\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '859a97c5'},\n",
       " {'text': \"Error:\\nTriggered by pull requests\\nThis feature is only available for dbt repositories connected through dbt Cloud's native integration with Github, Gitlab, or Azure DevOps\\nSolution: Contrary to the guide on DTC repo, don’t use the Git Clone option. Use the Github one instead. Step-by-step guide to UN-LINK Git Clone and RE-LINK with Github in the next entry below\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Dbt deploy + Git CI - cannot create CI checks job for deployment to Production. See more discussion in slack chat',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '32469a2d'},\n",
       " {'text': 'If you’re trying to configure CI with Github and on the job’s options you can’t see Run on Pull Requests? on triggers, you have to reconnect with Github using native connection instead clone by SSH. Follow these steps:\\nOn Profile Settings > Linked Accounts connect your Github account with dbt project allowing the permissions asked. More info at https://docs.getdbt.com/docs/collaborate/git/connect-gith\\nDisconnect your current Github’s configuration from Account Settings > Projects (analytics) > Github connection. At the bottom left appears the button Disconnect, press it.\\nOnce we have confirmed the change, we can configure it again. This time, choose Github and it will appear in all repositories which you have allowed to work with dbt. Select your repository and it’s ready.\\nGo to the Deploy > job configuration’s page and go down until Triggers and now you can see the option Run on Pull Requests:',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Dbt deploy + Git CI - Unable to configure Continuous Integration (CI) with Github',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c599b3a0'},\n",
       " {'text': \"If you're following video DE Zoomcamp 4.3.1 - Building the First DBT Models, you may have encountered an issue at 14:25 where the Lineage graph isn't displayed and a Compilation Error occurs, as shown in the attached image. Don't worry - a quick fix for this is to simply save your schema.yml file. Once you've done this, you should be able to view your Lineage graph without any further issues.\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': \"Compilation Error (Model 'model.my_new_project.stg_green_tripdata' (models/staging/stg_green_tripdata.sql) depends on a source named 'staging.green_trip_external' which was not found)\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '179df18d'},\n",
       " {'text': '> in macro test_accepted_values (tests/generic/builtin.sql)\\n> called by test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)\\nRemember that you have to add to dbt_project.yml the vars:\\nvars:\\npayment_type_values: [1, 2, 3, 4, 5, 6]',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': \"'NoneType' object is not iterable\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '1ce1a275'},\n",
       " {'text': \"You will face this issue if you copied and pasted the exact macro directly from data-engineering-zoomcamp repo.\\nBigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]; reason: invalidQuery, location: query, message: No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]')\\nWhat you’d have to do is to change the data type of the numbers (1, 2, 3 etc.) to text by inserting ‘’, as the initial ‘payment_type’ data type should be string (Note: I extracted and loaded the green trips data using Google BQ Marketplace)\\n{#\\nThis macro returns the description of the payment_type\\n#}\\n{% macro get_payment_type_description(payment_type) -%}\\ncase {{ payment_type }}\\nwhen '1' then 'Credit card'\\nwhen '2' then 'Cash'\\nwhen '3' then 'No charge'\\nwhen '4' then 'Dispute'\\nwhen '5' then 'Unknown'\\nwhen '6' then 'Voided trip'\\nend\\n{%- endmacro %}\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'dbt macro errors with get_payment_type_description(payment_type)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'b529b0bc'},\n",
       " {'text': 'The dbt error  log contains a link to BigQuery. When you follow it you will see your query and the problematic line will be highlighted.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Troubleshooting in dbt:',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '2e51a111'},\n",
       " {'text': 'It is a default behaviour of dbt to append custom schema to initial schema. To override this behaviour simply create a macro named “generate_schema_name.sql”:\\n{% macro generate_schema_name(custom_schema_name, node) -%}\\n{%- set default_schema = target.schema -%}\\n{%- if custom_schema_name is none -%}\\n{{ default_schema }}\\n{%- else -%}\\n{{ custom_schema_name | trim }}\\n{%- endif -%}\\n{%- endmacro %}\\nNow you can override default custom schema in “dbt_project.yml”:',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Why changing the target schema to “marts” actually creates a schema named “dbt_marts” instead?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '6e1a0834'},\n",
       " {'text': 'There is a project setting which allows you to set `Project subdirectory` in dbt cloud:',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'How to set subdirectory of the github repository as the dbt project root',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a8657e65'},\n",
       " {'text': \"Remember that you should modify accordingly your .sql models, to read from existing table names in BigQuery/postgres db\\nExample: select * from {{ source('staging',<your table name in the database>') }}\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': \"Compilation Error : Model 'model.XXX' (models/<model_path>/XXX.sql) depends on a source named '<a table name>' which was not found\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '2678d8c2'},\n",
       " {'text': 'Make sure that you create a pull request from your Development branch to the Production branch (main by default). After that, check in your ‘seeds’ folder if the seed file is inside it.\\nAnother thing to check is your .gitignore file. Make sure that the .csv extension is not included.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': \"Compilation Error : Model '<model_name>' (<model_path>) depends on a node named '<seed_name>' which was not found   (Production Environment)\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'aa85c6ae'},\n",
       " {'text': '1. Go to your dbt cloud service account\\n1. Adding the  [Storage Object Admin,Storage Admin] role in addition tco BigQuery Admin.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'When executing dbt run after using fhv_tripdata as an external table: you get “Access Denied: BigQuery BigQuery: Permission denied”',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'de06929d'},\n",
       " {'text': 'Problem: when injecting data to bigquery, you may face the type error. This is because pandas by default will parse integer columns with missing value as float type.\\nSolution:\\nOne way to solve this problem is to specify/ cast data type Int64 during the data transformation stage.\\nHowever, you may be lazy to type all the int columns. If that is the case, you can simply use convert_dtypes to infer the data type\\n# Make pandas to infer correct data type (as pandas parse int with missing as float)\\ndf.fillna(-999999, inplace=True)\\ndf = df.convert_dtypes()\\ndf = df.replace(-999999, None)',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'How to automatically infer the column data type (pandas missing value issues)?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'b087fa95'},\n",
       " {'text': 'Seed files loaded from directory with name ‘seed’, that’s why you should rename dir with name ‘data’ to ‘seed’',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'When loading github repo raise exception that ‘taxi_zone_lookup’ not found',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '3c41892d'},\n",
       " {'text': 'Check the .gitignore file and make sure you don’t have *.csv in it\\n\\nDbt error 404 was not found in location\\nMy specific error:\\nRuntime Error in rpc request (from remote system.sql) 404 Not found: Table dtc-de-0315:trips_data_all.green_tripdata_partitioned was not found in location europe-west6 Location: europe-west6 Job ID: 168ee9bd-07cd-4ca4-9ee0-4f6b0f33897c\\nMake sure all of your datasets have the correct region and not a generalised region:\\nEurope-west6 as opposed to EU\\n\\nMatch this in dbt settings:\\ndbt -> projects -> optional settings -> manually set location to match',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': '‘taxi_zone_lookup’ not found',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '4842f3e8'},\n",
       " {'text': \"The easiest way to avoid these errors is by ingesting the relevant data in a .csv.gz file type. Then, do:\\nCREATE OR REPLACE EXTERNAL TABLE `dtc-de.trips_data_all.fhv_tripdata`\\nOPTIONS (\\nformat = 'CSV',\\nuris = ['gs://dtc_data_lake_dtc-de-updated/data/fhv_all/fhv_tripdata_2019-*.csv.gz']\\n);\\nAs an example. You should no longer have any data type issues for week 4.\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Data type errors when ingesting with parquet files',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '5eaf61fe'},\n",
       " {'text': 'This is due to the way the deduplication is done in the two staging files.\\nSolution: add order by in the partition by part of both staging files. Keep adding columns to order by until the number of rows in the fact_trips table is consistent when re-running the fact_trips model.\\nExplanation (a bit convoluted, feel free to clarify, correct etc.)\\nWe partition by vendor id and pickup_datetime and choose the first row (rn=1) from all these partitions. These partitions are not ordered, so every time we run this, the first row might be a different one. Since the first row is different between runs, it might or might not contain an unknown borough. Then, in the fact_trips model we will discard a different number of rows when we discard all values with an unknown borough.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Inconsistent number of rows when re-running fact_trips model',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '8ed36cea'},\n",
       " {'text': 'If you encounter data type error on trip_type column, it may due to some nan values that isn’t null in bigquery.\\nSolution: try casting it to FLOAT datatype instead of NUMERIC',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Data Type Error when running fact table',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '46aebc79'},\n",
       " {'text': \"This error could result if you are using some select * query without mentioning the name of table for ex:\\nwith dim_zones as (\\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`dim_zones`\\nwhere borough != 'Unknown'\\n),\\nfhv as (\\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`stg_fhv_tripdata`\\n)\\nselect * from fhv\\ninner join dim_zones as pickup_zone\\non fhv.PUlocationID = pickup_zone.locationid\\ninner join dim_zones as dropoff_zone\\non fhv.DOlocationID = dropoff_zone.locationid\\n);\\nTo resolve just replace use : select fhv.* from fhv\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'CREATE TABLE has columns with duplicate name locationid.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'e2d2bc58'},\n",
       " {'text': 'Some ehail fees are null and casting them to integer gives Bad int64 value: 0.0 error,\\nSolution:\\nUsing safe_cast returns NULL instead of throwing an error. So use safe_cast from dbt_utils function in the jinja code for casting into integer as follows:\\n{{ dbt_utils.safe_cast(\\'ehail_fee\\',  api.Column.translate_type(\"integer\"))}} as ehail_fee,\\nCan also just use safe_cast(ehail_fee as integer) without relying on dbt_utils.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Bad int64 value: 0.0 error',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '137aab88'},\n",
       " {'text': \"You might encounter this when building the fact_trips.sql model. The issue may be with the payment_type_description field.\\nUsing safe_cast as above, would cause the entire field to become null. A better approach is to drop the offending decimal place, then cast to integer.\\ncast(replace({{ payment_type }},'.0','') as integer)\\nBad int64 value: 1.0 error (again)\\n\\nI found that there are more columns causing the bad INT64: ratecodeid and trip_type on Green_tripdata table.\\nYou can use the queries below to address them:\\nCAST(\\nREGEXP_REPLACE(CAST(rate_code AS STRING), r'\\\\.0', '') AS INT64\\n) AS ratecodeid,\\nCAST(\\nCASE\\nWHEN REGEXP_CONTAINS(CAST(trip_type AS STRING), r'\\\\.\\\\d+') THEN NULL\\nELSE CAST(trip_type AS INT64)\\nEND AS INT64\\n) AS trip_type,\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Bad int64 value: 2.0/1.0 error',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a260e651'},\n",
       " {'text': 'The two solution above don’t work for me - I used the line below in `stg_green_trips.sql` to replace the original ehail_fee line:\\n`{{ dbt.safe_cast(\\'ehail_fee\\',  api.Column.translate_type(\"numeric\"))}} as ehail_fee,`',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'DBT - Error on building fact_trips.sql: Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64. File: gs://<gcs bucket>/<table>/green_taxi_2019-01.parquet\")',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'da8d9fcc'},\n",
       " {'text': \"Remember to add a space between the variable and the value. Otherwise, it won't be interpreted as a dictionary.\\nIt should be:\\ndbt run --var 'is_test_run: false'\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'The - vars argument must be a YAML dictionary, but was of type str',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '2314e3c4'},\n",
       " {'text': \"You don't need to change the environment type. If you are following the videos, you are creating a Production Deployment, so the only available option is the correct one.'\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Not able to change Environment Type as it is greyed out and inaccessible',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'e7bdbba6'},\n",
       " {'text': 'Database Error in model stg_yellow_tripdata (models/staging/stg_yellow_tripdata.sql)\\nAccess Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.\\ncompiled Code at target/run/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql\\nIn my case, I was set up in a different branch, so always check the branch you are working on. Change the 04-analytics-engineering/taxi_rides_ny/models/staging/schema.yml file in the\\nsources:\\n- name: staging\\ndatabase: your_database_name\\nIf this error will continue when running dbt job, As for changing the branch for your job, you can use the ‘Custom Branch’ settings in your dbt Cloud environment. This allows you to run your job on a different branch than the default one (usually main). To do this, you need to:\\nGo to an environment and select Settings to edit it\\nSelect Only run on a custom branch in General settings\\nEnter the name of your custom branch (e.g. HW)\\nClick Save\\nCould not parse the dbt project. please check that the repository contains a valid dbt project\\nRunning the Environment on the master branch causes this error, you must activate “Only run on a custom branch” checkbox and specify the branch you are  working when Environment is setup.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Access Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '52cccade'},\n",
       " {'text': 'Change to main branch, make a pull request from the development branch.\\nNote: this will take you to github.\\nApprove the merging and rerun you job, it would work as planned now',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Made change to your modelling files and commit the your development branch, but Job still runs on old file?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '11a814ea'},\n",
       " {'text': 'Before you can develop some data model on dbt, you should create development environment and set some parameter on it. After the model being developed, we should also create deployment environment to create and run some jobs.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Setup - I’ve set Github and Bigquery to dbt successfully. Why nothing showed in my Develop tab?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '0d1e02d5'},\n",
       " {'text': 'Error Message:\\nInvestigate Sentry error: ProtocolError \"Invalid input ConnectionInputs.SEND_HEADERS in state ConnectionState.CLOSED\"\\nSolution:\\nreference\\nRun it again because it happens sometimes. Or wait a few minutes, it will continue.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Prefect Agent retrieving runs from queue sometimes fails with httpx.LocalProtocolError',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '0a0cc4c3'},\n",
       " {'text': \"My taxi data was loaded into gcs with etl_web_to_gcs.py script that converts csv data into parquet. Then I placed raw data trips into external tables and when I executed dbt run I got an error message: Parquet column 'passenger_count' has type INT64 which does not match the target cpp_type DOUBLE. It is because several columns in files have different formats of data.\\nWhen I added df[col] = df[col].astype('Int64') transformation to the columns: passenger_count, payment_type, RatecodeID, VendorID, trip_type it went ok. Several people also faced this error and more about it you can read on the slack channel.\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'BigQuery returns an error when i try to run ‘dbt run’:',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'cb912983'},\n",
       " {'text': 'Use the syntax below instead if the code in the tutorial is not working.\\ndbt run --select stg_green_tripdata --vars \\'{\"is_test_run\": false}\\'',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': \"Running dbt run --models stg_green_tripdata --var 'is_test_run: false' is not returning anything:\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '2d4e434f'},\n",
       " {'text': \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': \"DBT - Error: No module named 'pytz' while setting up dbt with docker\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'bb6655b9'},\n",
       " {'text': \"If you have problems editing dbt_project.yml when using Docker after ‘docker-compose run dbt-bq-dtc init’, to change profile ‘taxi_rides_ny’ to 'bq-dbt-workshop’, just run:\\nsudo chown -R username path\\nDBT - Internal Error: Profile should not be None if loading is completed\\nWhen  running dbt debug, change the directory to the newly created subdirectory (e.g: the newly created `taxi_rides_ny` directory, which contains the dbt project).\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': '\\u200b\\u200bVS Code: NoPermissions (FileSystemError): Error: EACCES: permission denied (linux)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'fc2eb036'},\n",
       " {'text': 'When running a query on BigQuery sometimes could appear a this table is not on the specified location error.\\nFor this problem there is not a straightforward solution, you need to dig a little, but the problem could be one of these:\\nCheck the locations of your bucket, datasets and tables. Make sure they are all on the same one.\\nChange the query settings to the location you are in: on the query window select more -> query settings -> select the location\\nCheck if all the paths you are using in your query to your tables are correct: you can click on the table -> details -> and copy the path.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Google Cloud BigQuery Location Problems',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '25daead9'},\n",
       " {'text': 'This happens because we have moved the dbt project to another directory on our repo.\\nOr might be that you’re on a different branch than is expected to be merged from / to.\\nSolution:\\nGo to the projects window on dbt cloud -> settings -> edit -> and add directory (the extra path to the dbt project)\\nFor example:\\n/week5/taxi_rides_ny\\nMake sure your file explorer path and this Project settings path matches and there’s no files waiting to be committed to github if you’re running the job to deploy to PROD.\\nAnd that you had setup the PROD environment to check in the main branch, or whichever you specified.\\nIn the picture below, I had set it to ella2024 to be checked as “production-ready” by the “freshness” check mark at the PROD environment settings. So each time I merge a branch from something else into ella2024 and then trigger the PR, the CI check job would kick-in. But we still do need to Merge and close the PR manually, I believe, that part is not automated.\\nYou set up the PROD custom branch (if not default main) in the Environment setup screen.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'DBT Deploy - This dbt Cloud run was cancelled because a valid dbt project was not found.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '2221d75e'},\n",
       " {'text': 'When you are creating the pull request and running the CI, dbt is creating a new schema on BIgQuery. By default that new schema will be created on ‘US’ location, if you have your dataset, schemas and tables on ‘EU’ that will generate an error and the pull request will not be accepted. To change that location to ‘EU’ on the connection to BigQuery from dbt we need to add the location ‘EU’ on the connection optional settings:\\nDbt -> project -> settings -> connection BIgQuery -> OPtional Settings -> Location -> EU',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'DBT Deploy + CI - Location Problems on BigQuery',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '94524a9d'},\n",
       " {'text': 'When running trying to run the dbt project on prod there is some things you need to do and check on your own:\\nFirst Make the pull request and Merge the branch into the main.\\nMake sure you have the latest version, if you made changes to the repo in another place.\\nCheck if the dbt_project.yml file is accessible to the project, if not check this solution (Dbt: This dbt Cloud run was cancelled because a valid dbt project was not found.).\\nCheck if the name you gave to the dataset on BigQuery is the same you put on the dataset spot on the production environment created on dbt cloud.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'DBT Deploy - Error When trying to run the dbt project on Prod',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '1f1ecbb7'},\n",
       " {'text': 'In the step in this video (DE Zoomcamp 4.3.1 - Build the First dbt Models), after creating `stg_green_tripdata.sql` and clicking `build`, I encountered an error saying dataset not found in location EU. The default location for dbt Bigquery is the US, so when generating the new Bigquery schema for dbt, unless specified, the schema locates in the US.\\nSolution:\\nTurns out I forgot to specify Location to be `EU` when adding connection details.\\nDevelop -> Configure Cloud CLI -> Projects -> taxi_rides_ny -> (connection) Bigquery -> Edit -> Location (Optional) -> type `EU` -> Save',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'DBT - Error: “404 Not found: Dataset <dataset_name>:<dbt_schema_name> was not found in location EU” after building from stg_green_tripdata.sql',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c5af32ab'},\n",
       " {'text': 'Issue: If you’re having problems loading the FHV_20?? data from the github repo into GCS and then into BQ (input file not of type parquet), you need to do two things. First, append the URL Template link with ‘?raw=true’ like so:\\nURL_TEMPLATE = URL_PREFIX + \"/fhv_tripdata_{{ execution_date.strftime(\\\\\\'%Y-%m\\\\\\') }}.parquet?raw=true\"\\nSecond, update make sure the URL_PREFIX is set to the following value:\\n\\nURL_PREFIX = \"https://github.com/alexeygrigorev/datasets/blob/master/nyc-tlc/fhv\"\\nIt is critical that you use this link with the keyword blob. If your link has ‘tree’ here, replace it. Everything else can stay the same, including the curl -sSLf command. ‘',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Homework - Ingesting FHV_20?? data',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '1e6b7da1'},\n",
       " {'text': 'I found out that the easies way to upload datasets form github for the homework is utilising this script git_csv_to_gcs.py. Thank you Lidia!!\\nIt is similar to a script that Alexey provided us in 03-data-warehouse/extras/web_to_gcs.py',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Homework - Ingesting NYC TLC Data',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '259481c4'},\n",
       " {'text': 'If you have to securely put your credentials for a project and, probably, push it to a git repository then the best option is to use an environment variable\\nFor example for web_to_gcs.py or git_csv_to_gcs.py we have to set these variables:\\nGOOGLE_APPLICATION_CREDENTIALS\\nGCP_GCS_BUCKET\\nThe easises option to do it  is to use .env  (dotenv).\\nInstall it and add a few lines of code that inject these variables for your project\\npip install python-dotenv\\nfrom dotenv import load_dotenv\\nimport os\\n# Load environment variables from .env file\\nload_dotenv()\\n# Now you can access environment variables like GCP_GCS_BUCKET and GOOGLE_APPLICATION_CREDENTIALS\\ncredentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\\nBUCKET = os.environ.get(\"GCP_GCS_BUCKET\")',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'How to set environment variable easily for any credentials',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'edbae698'},\n",
       " {'text': \"If you uploaded manually the fvh 2019 csv files, you may face errors regarding date types. Try to create an the external table in bigquery but define the pickup_datetime and dropoff_datetime to be strings\\nCREATE OR REPLACE EXTERNAL TABLE `gcp_project.trips_data_all.fhv_tripdata`  (\\ndispatching_base_num STRING,\\npickup_datetime STRING,\\ndropoff_datetime STRING,\\nPUlocationID STRING,\\nDOlocationID STRING,\\nSR_Flag STRING,\\nAffiliated_base_number STRING\\n)\\nOPTIONS (\\nformat = 'csv',\\nuris = ['gs://bucket/*.csv']\\n);\\nThen when creating the fhv core model in dbt, use TIMESTAMP(CAST(()) to ensure it first parses as a string and then convert it to timestamp.\\nwith fhv_tripdata as (\\nselect * from {{ ref('stg_fhv_tripdata') }}\\n),\\ndim_zones as (\\nselect * from {{ ref('dim_zones') }}\\nwhere borough != 'Unknown'\\n)\\nselect fhv_tripdata.dispatching_base_num,\\nTIMESTAMP(CAST(fhv_tripdata.pickup_datetime AS STRING)) AS pickup_datetime,\\nTIMESTAMP(CAST(fhv_tripdata.dropoff_datetime AS STRING)) AS dropoff_datetime,\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': \"Invalid date types after Ingesting FHV data through CSV files: Could not parse 'pickup_datetime' as a timestamp\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '67217f4c'},\n",
       " {'text': \"If you uploaded manually the fvh 2019 parquet files manually after downloading from https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-*.parquet you may face errors regarding date types while loading the data in a landing table (say fhv_tripdata). Try to create an the external table with the schema defines as following and load each month in a loop.\\n-----Correct load with schema defination----will not throw error----------------------\\nCREATE OR REPLACE EXTERNAL TABLE `dw-bigquery-week-3.trips_data_all.external_tlc_fhv_trips_2019` (\\ndispatching_base_num STRING,\\npickup_datetime TIMESTAMP,\\ndropoff_datetime TIMESTAMP,\\nPUlocationID FLOAT64,\\nDOlocationID FLOAT64,\\nSR_Flag FLOAT64,\\nAffiliated_base_number STRING\\n)\\nOPTIONS (\\nformat = 'PARQUET',\\nuris = ['gs://project id/fhv_2019_8.parquet']\\n);\\nCan Also USE  uris = ['gs://project id/fhv_2019_*.parquet'] (THIS WILL remove the need for the loop and can be done for all month in single RUN )\\n– THANKYOU FOR THIS –\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Invalid data types after Ingesting FHV data through parquet files: Could not parse SR_Flag as Float64,Couldn’t parse datetime column as timestamp,couldn’t handle NULL values in PULocationID,DOLocationID',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '2aadd232'},\n",
       " {'text': 'When accessing Looker Studio through the Google Cloud Project console, you may be prompted to subscribe to the Pro version and receive the following errors:\\nInstead, navigate to https://lookerstudio.google.com/navigation/reporting which will take you to the free version.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'Google Looker Studio - you have used up your 30-day trial',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'adcd914a'},\n",
       " {'text': 'Ans: Dbt provides a mechanism called \"ref\" to manage dependencies between models. By referencing other models using the \"ref\" keyword in SQL, dbt automatically understands the dependencies and ensures the correct execution order.\\nLoading FHV Data goes into slumber using Mage?\\nTry loading the data using jupyter notebooks in a local environment. There might be bandwidth issues with Mage.\\nLoad the data into a pandas dataframe using the urls, make necessary transformations, upload the gcp bucket / alternatively download the parquet/csv files locally and then upload to GCP manually.\\nRegion Mismatch in DBT and BigQuery\\nIf you are using the datasets copied into BigQuery from BigQuery public datasets, the region will be set as US by default and hence it is much easier to set your dbt profile location as US while transforming the tables and views. \\nYou can change the location as follows:',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'How does dbt handle dependencies between models?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'bbf094b3'},\n",
       " {'text': \"Use the PostgreSQL COPY FROM feature that is compatible with csv files\\nCOPY table_name [ ( column_name [, ...] ) ]\\nFROM { 'filename' | PROGRAM 'command' | STDIN }\\n[ [ WITH ] ( option [, ...] ) ]\\n[ WHERE condition ]\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': 'What is the fastest way to upload taxi data to dbt-postgres?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '2fdc5057'},\n",
       " {'text': 'Update the line:\\nWith:',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'When configuring the profiles.yml file for dbt-postgres with jinja templates with environment variables, I\\'m getting \"Credentials in profile \"PROFILE_NAME\", target: \\'dev\\', invalid: \\'5432\\'is not of type \\'integer\\'',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '95e302f7'},\n",
       " {'text': 'Install SDKMAN:\\ncurl -s \"https://get.sdkman.io\" | bash\\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\\nUsing SDKMAN, install Java 11 and Spark 3.3.2:\\nsdk install java 11.0.22-tem\\nsdk install spark 3.3.2\\nOpen a new terminal or run the following in the same shell:\\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\\nVerify the locations and versions of Java and Spark that were installed:\\necho $JAVA_HOME\\njava -version\\necho $SPARK_HOME\\nspark-submit --version',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Setting up Java and Spark (with PySpark) on Linux (Alternative option using SDKMAN)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '1ac2c13c'},\n",
       " {'text': 'If you’re seriously struggling to set things up \"locally\" (here locally meaning non/partly-managed environment like own laptop, a VM or Codespaces) you can use the following guide to use Spark in Google Colab:\\nhttps://medium.com/gitconnected/launch-spark-on-google-colab-and-connect-to-sparkui-342cad19b304\\nStarter notebook:\\nhttps://github.com/aaalexlit/medium_articles/blob/main/Spark_in_Colab.ipynb\\nIt’s advisable to spend some time setting things up locally rather than jumping right into this solution.',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'PySpark - Setting Spark up in Google Colab',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '5cc0e4d9'},\n",
       " {'text': 'If after installing Java (either jdk or openjdk), Hadoop and Spark, and setting the corresponding environment variables you find the following error when spark-shell is run at CMD:\\njava.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c947bc5) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed\\nmodule @0x3c947bc5\\nSolution: Java 17 or 19 is not supported by Spark. Spark 3.x: requires Java 8/11/16. Install Java 11 from the website provided in the windows.md setup file.',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Spark-shell: unable to load native-hadoop library for platform - Windows',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '17090545'},\n",
       " {'text': 'I found this error while executing the user defined function in Spark (crazy_stuff_udf). I am working on Windows and using conda. After following the setup instructions, I found that the PYSPARK_PYTHON environment variable was not set correctly, given that conda has different python paths for each environment.\\nSolution:\\npip install findspark on the command line inside proper environment\\nAdd to the top of the script\\nimport findspark\\nfindspark.init()',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'PySpark - Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'd17e30c6'},\n",
       " {'text': 'This is because Python 3.11 has some inconsistencies with such an old version of Spark. The solution is a downgrade in the Python version. Python 3.9 using a conda environment takes care of it. Or install newer PySpark >= 3.5.1 works for me (Ella) [source].',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'PySpark - TypeError: code() argument 13 must be str, not int  , while executing `import pyspark`  (Windows/ Spark 3.0.3 - Python 3.11)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '1520b5bc'},\n",
       " {'text': 'If anyone is a Pythonista or becoming one (which you will essentially be one along this journey), and desires to have all python dependencies under same virtual environment (e.g. conda) as done with prefect and previous exercises, simply follow these steps\\nInstall OpenJDK 11,\\non MacOS: $ brew install java11\\nAdd export PATH=\"/opt/homebrew/opt/openjdk@11/bin:$PATH\"\\nto ~/.bashrc or ~/zshrc\\nActivate working environment (by pipenv / poetry / conda)\\nRun $ pip install pyspark\\nWork with exercises as normal\\nAll default commands of spark will be also available at shell session under activated enviroment.\\nHope this can help!\\nP.s. you won’t need findspark to firstly initialize.\\nPy4J - Py4JJavaError: An error occurred while calling (...)  java.net.ConnectException: Connection refused: no further information;\\nIf you\\'re getting `Py4JavaError` with a generic root cause, such as the described above (Connection refused: no further information). You\\'re most likely using incompatible versions of the JDK or Python with Spark.\\nAs of the current latest Spark version (3.5.0), it supports JDK 8 / 11 / 17. All of which can be easily installed with SDKMan! on macOS or Linux environments\\n\\n$ sdk install java 17.0.10-librca\\n$ sdk install spark 3.5.0\\n$ sdk install hadoop 3.3.5\\nAs PySpark 3.5.0 supports Python 3.8+ make sure you\\'re setting up your virtualenv with either 3.8 / 3.9 / 3.10 / 3.11 (Most importantly avoid using 3.12 for now as not all libs in the data-science/engineering ecosystem are fully package for that)\\n\\n\\n$ conda create -n ENV_NAME python=3.11\\n$ conda activate ENV_NAME\\n$ pip install pyspark==3.5.0\\nThis setup makes installing `findspark` and the likes of it unnecessary. Happy coding.\\nPy4J - Py4JJavaError: An error occurred while calling o54.parquet. Or any kind of Py4JJavaError that show up after run df.write.parquet(\\'zones\\')(On window)\\nThis assume you already correctly set up the PATH in the nano ~/.bashrc\\nHere my\\nexport JAVA_HOME=\"/c/tools/jdk-11.0.21\"\\nexport PATH=\"${JAVA_HOME}/bin:${PATH}\"\\nexport HADOOP_HOME=\"/c/tools/hadoop-3.2.0\"\\nexport PATH=\"${HADOOP_HOME}/bin:${PATH}\"\\nexport SPARK_HOME=\"/c/tools/spark-3.3.2-bin-hadoop3\"\\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"\\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\\nexport PYTHONPATH=\"${SPARK_HOME}spark-3.5.1-bin-hadoop3py4j-0.10.9.5-src.zip:$PYTHONPATH\"\\nYou also need to add environment variables correctly which paths to java jdk, spark and hadoop through\\nGo to Stephenlaye2/winutils3.3.0: winutils.exe hadoop.dll and hdfs.dll binaries for hadoop windows (github.com), download the right winutils for hadoop-3.2.0. Then create a new folder,bin and put every thing in side to make a /c/tools/hadoop-3.2.0/bin(You might not need to do this, but after testing it without the /bin I could not make it to work)\\nThen follow the solution in this video: How To Resolve Issue with Writing DataFrame to Local File | winutils | msvcp100.dll (youtube.com)\\nRemember to restart IDE and computer, After the error An error occurred while calling o54.parquet.  is fixed but new errors like o31.parquet. Or o35.parquet. appear.',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Java+Spark - Easy setup with miniconda env (worked on MacOS)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'e86ca928'},\n",
       " {'text': 'After installing all including pyspark (and it is successfully imported), but then running this script on the jupyter notebook\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder \\\\\\n.master(\"local[*]\") \\\\\\n.appName(\\'test\\') \\\\\\n.getOrCreate()\\ndf = spark.read \\\\\\n.option(\"header\", \"true\") \\\\\\n.csv(\\'taxi+_zone_lookup.csv\\')\\ndf.show()\\nit gives the error:\\nRuntimeError: Java gateway process exited before sending its port number\\n✅The solution (for me) was:\\npip install findspark on the command line and then\\nAdd\\nimport findspark\\nfindspark.init()\\nto the top of the script.\\nAnother possible solution is:\\nCheck that pyspark is pointing to the correct location.\\nRun pyspark.__file__. It should be list /home/<your user name>/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/__init__.py if you followed the videos.\\nIf it is pointing to your python site-packages remove the pyspark directory there and check that you have added the correct exports to you .bashrc file and that there are not any other exports which might supersede the ones provided in the course content.\\nTo add to the solution above, if the errors persist in regards to setting the correct path for spark,  an alternative solution for permanent path setting solve the error is  to set environment variables on system and user environment variables following this tutorial: Install Apache PySpark on Windows PC | Apache Spark Installation Guide\\nOnce everything is installed, skip to 7:14 to set up environment variables. This allows for the environment variables to be set permanently.',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'lsRuntimeError: Java gateway process exited before sending its port number',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '3b5b4eb3'},\n",
       " {'text': 'Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\nThe solution which worked for me(use following in jupyter notebook) :\\n!pip install findspark\\nimport findspark\\nfindspark.init()\\nThereafter , import pyspark and create spark contex<<t as usual\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\nFilter based on conditions based on multiple columns\\nfrom pyspark.sql.functions import col\\nnew_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\\nKrishna Anand',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Module Not Found Error in Jupyter Notebook .',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '489c366f'},\n",
       " {'text': 'You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\\nAdditionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '59381b15'},\n",
       " {'text': 'If below does not work, then download the latest available py4j version with\\nconda install -c conda-forge py4j\\nTake care of the latest version number in the website to replace appropriately.\\nNow add\\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\\nexport PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH\"\\nin your  .bashrc file.',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': \"Py4J Error - ModuleNotFoundError: No module named 'py4j' (Solve with latest version)\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '220b1cf3'},\n",
       " {'text': 'Even after we have exported our paths correctly you may find that  even though Jupyter is installed you might not have Jupyter Noteboopgak for one reason or another. Full instructions are found here (for my walkthrough) or here (where I got the original instructions from) but are included below. These instructions include setting up a virtual environment (handy if you are on your own machine doing this and not a VM):\\nFull steps:\\nUpdate and upgrade packages:\\nsudo apt update && sudo apt -y upgrade\\nInstall Python:\\nsudo apt install python3-pip python3-dev\\nInstall Python virtualenv:\\nsudo -H pip3 install --upgrade pip\\nsudo -H pip3 install virtualenv\\nCreate a Python Virtual Environment:\\nmkdir notebook\\ncd notebook\\nvirtualenv jupyterenv\\nsource jupyterenv/bin/activate\\nInstall Jupyter Notebook:\\npip install jupyter\\nRun Jupyter Notebook:\\njupyter notebook',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Exception: Jupyter command `jupyter-notebook` not found.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'd970a0da'},\n",
       " {'text': 'Code executed:\\ndf = spark.read.parquet(pq_path)\\n… some operations on df …\\ndf.write.parquet(pq_path, mode=\"overwrite\")\\njava.io.FileNotFoundException: File file:/home/xxx/code/data/pq/fhvhv/2021/02/part-00021-523f9ad5-14af-4332-9434-bdcb0831f2b7-c000.snappy.parquet does not exist\\nThe problem is that Sparks performs lazy transformations, so the actual action that trigger the job is df.write, which does delete the parquet files that is trying to read (mode=”overwrite”)\\n✅Solution: Write to a different directorydf\\ndf.write.parquet(pq_path_temp, mode=\"overwrite\")',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Error java.io.FileNotFoundException',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '5fa98bd0'},\n",
       " {'text': 'You need to create the Hadoop /bin directory manually and add the downloaded files in there, since the shell script provided for Windows installation just puts them in /c/tools/hadoop-3.2.0/ .',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Hadoop - FileNotFoundException: Hadoop bin directory does not exist , when trying to write (Windows)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'ce508f3c'},\n",
       " {'text': 'Actually Spark SQL is one independent “type” of SQL - Spark SQL.\\nThe several SQL providers are very similar:\\nSELECT [attributes]\\nFROM [table]\\nWHERE [filter]\\nGROUP BY [grouping attributes]\\nHAVING [filtering the groups]\\nORDER BY [attribute to order]\\n(INNER/FULL/LEFT/RIGHT) JOIN [table2]\\nON [attributes table joining table2] (...)\\nWhat differs the most between several SQL providers are built-in functions.\\nFor Built-in Spark SQL function check this link: https://spark.apache.org/docs/latest/api/sql/index.html\\nExtra information on SPARK SQL :\\nhttps://databricks.com/glossary/what-is-spark-sql#:~:text=Spark%20SQL%20is%20a%20Spark,on%20existing%20deployments%20and%20data.',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Which type of SQL is used in Spark? Postgres? MySQL? SQL Server?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'b7b9487d'},\n",
       " {'text': \"✅Solution: I had two notebooks running, and the one I wanted to look at had opened a port on localhost:4041.\\nIf a port is in use, then Spark uses the next available port number. It can be even 4044. Clean up after yourself when a port does not work or a container does not run.\\nYou can run spark.sparkContext.uiWebUrl\\nand result will be some like\\n'http://172.19.10.61:4041'\",\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'The spark viewer on localhost:4040 was not showing the current run',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a74de125'},\n",
       " {'text': '✅Solution: replace Java Developer Kit 11 with Java Developer Kit 8.\\nJava - RuntimeError: Java gateway process exited before sending its port number\\nShows java_home is not set on the notebook log\\nhttps://sparkbyexamples.com/pyspark/pyspark-exception-java-gateway-process-exited-before-sending-the-driver-its-port-number/\\nhttps://twitter.com/drkrishnaanand/status/1765423415878463839',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Java - java.lang.NoSuchMethodError: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner Error during repartition call (conda pyspark installation)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'e5270303'},\n",
       " {'text': '✅I got it working using `gcs-connector-hadoop-2.2.5-shaded.jar` and Spark 3.1\\nI also added the google_credentials.json and .p12 to auth with gcs. These files are downloadable from GCP Service account.\\nTo create the SparkSession:\\nspark = SparkSession.builder.master(\\'local[*]\\') \\\\\\n.appName(\\'spark-read-from-bigquery\\') \\\\\\n.config(\\'BigQueryProjectId\\',\\'razor-project-xxxxxxx) \\\\\\n.config(\\'BigQueryDatasetLocation\\',\\'de_final_data\\') \\\\\\n.config(\\'parentProject\\',\\'razor-project-xxxxxxx) \\\\\\n.config(\"google.cloud.auth.service.account.enable\", \"true\") \\\\\\n.config(\"credentialsFile\", \"google_credentials.json\") \\\\\\n.config(\"GcpJsonKeyFile\", \"google_credentials.json\") \\\\\\n.config(\"spark.driver.memory\", \"4g\") \\\\\\n.config(\"spark.executor.memory\", \"2g\") \\\\\\n.config(\"spark.memory.offHeap.enabled\",True) \\\\\\n.config(\"spark.memory.offHeap.size\",\"5g\") \\\\\\n.config(\\'google.cloud.auth.service.account.json.keyfile\\', \"google_credentials.json\") \\\\\\n.config(\"fs.gs.project.id\", \"razor-project-xxxxxxx\") \\\\\\n.config(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\\\\n.config(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\\\\n.getOrCreate()',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Spark fails when reading from BigQuery and using `.show()` on `SELECT` queries',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'cabe8a5b'},\n",
       " {'text': 'While creating a SparkSession using the config spark.jars.packages as com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\\nspark = SparkSession.builder.master(\\'local\\').appName(\\'bq\\').config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\").getOrCreate()\\nautomatically downloads the required dependency jars and configures the connector, removing the need to manage this dependency. More details available here',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Spark BigQuery connector Automatic configuration',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'e3c0f777'},\n",
       " {'text': 'Link to Slack Thread : has anyone figured out how to read from GCP data lake instead of downloading all the taxi data again?\\nThere’s a few extra steps to go into reading from GCS with PySpark\\n1.)  IMPORTANT: Download the Cloud Storage connector for Hadoop here: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters\\nAs the name implies, this .jar file is what essentially connects PySpark with your GCS\\n2.) Move the .jar file to your Spark file directory. I installed Spark using homebrew on my MacOS machine and I had to create a /jars directory under \"/opt/homebrew/Cellar/apache-spark/3.2.1/ (where my spark dir is located)\\n3.) In your Python script, there are a few extra classes you’ll have to import:\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.conf import SparkConf\\nfrom pyspark.context import SparkContext\\n4.) You must set up your configurations before building your SparkSession. Here’s my code snippet:\\nconf = SparkConf() \\\\\\n.setMaster(\\'local[*]\\') \\\\\\n.setAppName(\\'test\\') \\\\\\n.set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") \\\\\\n.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\\\\n.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\\nsc = SparkContext(conf=conf)\\nsc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\\nsc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\\n5.) Once you run that, build your SparkSession with the new parameters we’d just instantiated in the previous step:\\nspark = SparkSession.builder \\\\\\n.config(conf=sc.getConf()) \\\\\\n.getOrCreate()\\n6.) Finally, you’re able to read your files straight from GCS!\\ndf_green = spark.read.parquet(\"gs://{BUCKET}/green/202*/\")',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Spark Cloud Storage connector',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '50c009ef'},\n",
       " {'text': 'from pyarrow.parquet import ParquetFile\\npf = ParquetFile(\\'fhvhv_tripdata_2021-01.parquet\\')\\n#pyarrow builds tables, not dataframes\\ntbl_small = next(pf.iter_batches(batch_size = 1000))\\n#this function converts the table to a dataframe of manageable size\\ndf = tbl_small.to_pandas()\\nAlternatively without PyArrow:\\ndf = spark.read.parquet(\\'fhvhv_tripdata_2021-01.parquet\\')\\ndf1 = df.sort(\\'DOLocationID\\').limit(1000)\\npdf = df1.select(\"*\").toPandas()\\ngcsu',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'How can I read a small number of rows from the parquet file directly?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '3fe85b16'},\n",
       " {'text': 'Probably you’ll encounter this if you followed the video ‘5.3.1 - First Look at Spark/PySpark’ and used the parquet file from the TLC website (csv was used in the video).\\nWhen defining the schema, the PULocation and DOLocationID are defined as IntegerType. This will cause an error because the Parquet file is INT64 and you’ll get an error like:\\nParquet column cannot be converted in file [...] Column [...] Expected: int, Found: INT64\\nChange the schema definition from IntegerType to LongType and it should work',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'DataType error when creating Spark DataFrame with a specified schema?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '0fe0c76a'},\n",
       " {'text': 'df_finalx=df_finalw.select([col(x).alias(x.replace(\" \",\"\")) for x in df_finalw.columns])\\nKrishna Anand',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Remove white spaces from column names in Pyspark',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '18c5bafe'},\n",
       " {'text': 'This error comes up on the Spark video 5.3.1 - First Look at Spark/PySpark,\\nbecause as at the creation of the video, 2021 data was the most recent which utilised csv files but as at now its parquet.\\nSo when you run the command spark.createDataFrame(df1_pandas).show(),\\nYou get the Attribute error. This is caused by the pandas version 2.0.0 which seems incompatible with Spark 3.3.2, so to fix it you have to downgrade pandas to 1.5.3 using the command pip install -U pandas==1.5.3\\nAnother option is adding the following after importing pandas, if one does not want to downgrade pandas version (source) :\\npd.DataFrame.iteritems = pd.DataFrame.items\\nNote that this problem is solved with Spark versions from 3.4.1',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '59e86b40'},\n",
       " {'text': 'Another alternative is to install pandas 2.0.1 (it worked well as at the time of writing this), and it is compatible with Pyspark 3.5.1. Make sure to add or edit your environment variable like this:\\nexport SPARK_HOME=\"${HOME}/spark/spark-3.5.1-bin-hadoop3\"\\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '1ac3ea8f'},\n",
       " {'text': 'Open a CMD terminal in administrator mode\\ncd %SPARK_HOME%\\nStart a master node: bin\\\\spark-class org.apache.spark.deploy.master.Master\\nStart a worker node: bin\\\\spark-class org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> --host <IP_ADDR>\\nbin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host <IP_ADDR>\\nspark://<master_ip>:<port>: copy the address from the previous command, in my case it was spark://localhost:7077\\nUse --host <IP_ADDR> if you want to run the worker on a different machine. For now leave it empty.\\nNow you can access Spark UI through localhost:8080\\nHomework for Module 5:\\nDo not refer to the homework file located under /05-batch/code/. The correct file is located under\\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/05-batch/homework.md',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Spark Standalone Mode on Windows',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'e04529ac'},\n",
       " {'text': 'You can either type the export command every time you run a new session, add it to the .bashrc/ which you can find in /home or run this command at the beginning of your homebook:\\nimport findspark\\nfindspark.init()',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Export PYTHONPATH command in linux is temporary',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a602a7f8'},\n",
       " {'text': 'I solved this issue: unzip the file with:\\nf\\nbefore creating head.csv',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Compressed file ended before the end-of-stream marker was reached',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '9336ce2c'},\n",
       " {'text': 'In the code along from Video 5.3.3 Alexey downloads the CSV files from the NYT website and gzips them in their bash script. If we now (2023) follow along but download the data from the GH course Repo, it will already be zippes as csv.gz files. Therefore we zip it again if we follow the code from the video exactly. This then leads to gibberish outcome when we then try to cat the contents or count the lines with zcat, because the file is zipped twitch and zcat only unzips it once.\\n✅solution: do not gzip the files downloaded from the course repo. Just wget them and save them as they are as csv.gz files. Then the zcat command and the showSchema command will also work\\nURL=\"${URL_PREFIX}/${TAXI_TYPE}/${TAXI_TYPE}_tripdata_${YEAR}-${FMONTH}.csv.gz\"\\nLOCAL_PREFIX=\"data/raw/${TAXI_TYPE}/${YEAR}/${FMONTH}\"\\nLOCAL_FILE=\"${TAXI_TYPE}_tripdata_${YEAR}_${FMONTH}.csv.gz\"\\nLOCAL_PATH=\"${LOCAL_PREFIX}/${LOCAL_FILE}\"\\necho \"downloading ${URL} to ${LOCAL_PATH}\"\\nmkdir -p ${LOCAL_PREFIX}\\nwget ${URL} -O ${LOCAL_PATH}\\necho \"compressing ${LOCAL_PATH}\"\\n# gzip ${LOCAL_PATH} <- uncomment this line',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Compression Error: zcat output is gibberish, seems like still compressed',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'bac4e0f7'},\n",
       " {'text': 'Occurred while running : spark.createDataFrame(df_pandas).show()\\nThis error is usually due to the python version, since spark till date of 2 march 2023 doesn’t support python 3.11, try creating a new env with python version 3.8 and then run this command.\\nOn the virtual machine, you can create a conda environment (here called myenv) with python 3.10 installed:\\nconda create -n myenv python=3.10 anaconda\\nThen you must run conda activate myenv to run python 3.10. Otherwise you’ll still be running version 3.11. You can deactivate by typing conda deactivate.',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'PicklingError: Could not serialise object: IndexError: tuple index out of range.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '13dad632'},\n",
       " {'text': 'Make sure you have your credentials of your GCP in your VM under the location defined in the script.',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Connecting from local Spark to GCS - Spark does not find my google credentials as shown in the video?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'ddc3c75b'},\n",
       " {'text': 'To run spark in docker setup\\n1. Build bitnami spark docker\\na. clone bitnami repo using command\\ngit clone https://github.com/bitnami/containers.git\\n(tested on commit 9cef8b892d29c04f8a271a644341c8222790c992)\\nb. edit file `bitnami/spark/3.3/debian-11/Dockerfile` and update java and spark version as following\\n\"python-3.10.10-2-linux-${OS_ARCH}-debian-11\" \\\\\\n\"java-17.0.5-8-3-linux-${OS_ARCH}-debian-11\" \\\\\\nreference: https://github.com/bitnami/containers/issues/13409\\nc. build docker image by navigating to above directory and running docker build command\\nnavigate cd bitnami/spark/3.3/debian-11/\\nbuild command docker build -t spark:3.3-java-17 .\\n2. run docker compose\\nusing following file\\n```yaml docker-compose.yml\\nversion: \\'2\\'\\nservices:\\nspark:\\nimage: spark:3.3-java-17\\nenvironment:\\n- SPARK_MODE=master\\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n- SPARK_RPC_ENCRYPTION_ENABLED=no\\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n- SPARK_SSL_ENABLED=no\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8080:8080\\'\\n- \\'7077:7077\\'\\nspark-worker:\\nimage: spark:3.3-java-17\\nenvironment:\\n- SPARK_MODE=worker\\n- SPARK_MASTER_URL=spark://spark:7077\\n- SPARK_WORKER_MEMORY=1G\\n- SPARK_WORKER_CORES=1\\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n- SPARK_RPC_ENCRYPTION_ENABLED=no\\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n- SPARK_SSL_ENABLED=no\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8081:8081\\'\\nspark-nb:\\nimage: jupyter/pyspark-notebook:java-17.0.5\\nenvironment:\\n- SPARK_MASTER_URL=spark://spark:7077\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8888:8888\\'\\n- \\'4040:4040\\'\\n```\\nrun command to deploy docker compose\\ndocker-compose up\\nAccess jupyter notebook using link logged in docker compose logs\\nSpark master url is spark://spark:7077',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Spark docker-compose setup',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '095b667f'},\n",
       " {'text': 'To do this\\npip install gcsfs,\\nThereafter copy the uri path to the file and use \\ndf = pandas.read_csc(gs://path)',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'How do you read data stored in gcs on pandas with your local computer?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '56a67c23'},\n",
       " {'text': 'Error:\\nspark.createDataFrame(df_pandas).schema\\nTypeError: field Affiliated_base_number: Can not merge type <class \\'pyspark.sql.types.StringType\\'> and <class \\'pyspark.sql.types.DoubleType\\'>\\nSolution:\\nAffiliated_base_number is a mix of letters and numbers (you can check this with a preview of the table), so it cannot be set to DoubleType (only for double-precision numbers). The suitable type would be StringType. Spark  inferSchema is more accurate than Pandas infer type method in this case. You can set it to  true  while reading the csv, so you don’t have to take out any data from your dataset. Something like this can help:\\ndf = spark.read \\\\\\n.options(\\nheader = \"true\", \\\\\\ninferSchema = \"true\", \\\\\\n) \\\\\\n.csv(\\'path/to/your/csv/file/\\')\\nSolution B:\\nIt\\'s because some rows in the affiliated_base_number are null and therefore it is assigned the datatype String and this cannot be converted to type Double. So if you really want to convert this pandas df to a pyspark df only take the  rows from the pandas df that are not null in the \\'Affiliated_base_number\\' column. Then you will be able to apply the pyspark function createDataFrame.\\n# Only take rows that have no null values\\npandas_df= pandas_df[pandas_df.notnull().all(1)]',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'TypeError when using spark.createDataFrame function on a pandas df',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '7fed7813'},\n",
       " {'text': 'Default executor memory is 1gb. This error appeared when working with the homework dataset.\\nError: MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\\nScaling row group sizes to 95.00% for 8 writers\\nSolution:\\nIncrease the memory of the executor when creating the Spark session like this:\\nRemember to restart the Jupyter session (ie. close the Spark session) or the config won’t take effect.',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a0e7e259'},\n",
       " {'text': 'Change the working directory to the spark directory:\\nif you have setup up your SPARK_HOME variable, use the following;\\ncd %SPARK_HOME%\\nif not, use the following;\\ncd <path to spark installation>\\nCreating a Local Spark Cluster\\nTo start Spark Master:\\nbin\\\\spark-class org.apache.spark.deploy.master.Master --host localhost\\nStarting up a cluster:\\nbin\\\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'How to spark standalone cluster is run on windows OS',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '4ca14331'},\n",
       " {'text': 'I added PYTHONPATH, JAVA_HOME and SPARK_HOME to ~/.bashrc, import pyspark worked ok in iPython in terminal, but couldn’t be found in .ipynb opened in VS Code\\nAfter adding new lines to ~/.bashrc, need to restart the shell to activate the new lines, do either\\nsource ~/.bashrc\\nexec bash\\nInstead of configuring paths in ~/.bashrc, I created .env file in the root of my workspace:',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Env variables set in ~/.bashrc are not loaded to Jupyter in VS Code',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '6fdd09eb'},\n",
       " {'text': 'I don’t use visual studio, so I did it the old fashioned way: ssh -L 8888:localhost:8888 <my user>@<VM IP> (replace user and IP with the ones used by the GCP VM, e.g. : ssh -L 8888:localhost:8888 myuser@34.140.188.1',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'How to port forward outside VS Code',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '64bfb2c3'},\n",
       " {'text': 'If you are doing wc -l fhvhv_tripdata_2021-01.csv.gz  with the gzip file as the file argument, you will get a different result, obviously! Since the file is compressed.\\nUnzip the file and then do wc -l fhvhv_tripdata_2021-01.csv to get the right results.',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': '“wc -l” is giving a different result then shown in the video',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '33dd4516'},\n",
       " {'text': 'when trying to:\\nURL=\"spark://$HOSTNAME:7077\"\\nspark-submit \\\\\\n--master=\"{$URL}\" \\\\\\n06_spark_sql.py \\\\\\n--input_green=data/pq/green/2021/*/ \\\\\\n--input_yellow=data/pq/yellow/2021/*/ \\\\\\n--output=data/report-2021\\nand you get errors like the following (SUMMARIZED):\\nWARN Utils: Your hostname, <HOSTNAME> resolves to a loopback address..\\nWARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address Setting default log level to \"WARN\".\\nException in thread \"main\" org.apache.spark.SparkException: Master must either be yarn or start with spark, mesos, k8s, or local at …\\nTry replacing --master=\"{$URL}\"\\nwith --master=$URL (edited)\\nExtra edit for spark version 3.4.2 - if encountering:\\n`Error: Unrecognized option: --master=`\\n→ Replace `--master=\"{$URL}\"` with  `--master \"${URL}\"`',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': '`spark-submit` errors',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '504b8570'},\n",
       " {'text': 'If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\\nFor Windows, create a new User Variable “HADOOP_HOME” that points to your Hadoop directory. Then add “%HADOOP_HOME%\\\\bin” to the PATH variable.\\nAdditional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '42e933c5'},\n",
       " {'text': \"Change the hadoop version to 3.0.1.Replace all the files in the local hadoop bin folder with the files in this repo:  winutils/hadoop-3.0.1/bin at master · cdarlint/winutils (github.com)\\nIf this does not work try to change other versions found in this repository.\\nFor more information please see this link: This version of %1 is not compatible with the version of Windows you're running · Issue #20 · cdarlint/winutils (github.com)\",\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Java.io.IOException. Cannot run program “C:\\\\hadoop\\\\bin\\\\winutils.exe”. CreateProcess error=216, This version of 1% is not compatible with the version of Windows you are using.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'fe9240b0'},\n",
       " {'text': 'Fix is to set the flag like the error states. Get your project ID from your dashboard and set it like so:\\ngcloud dataproc jobs submit pyspark \\\\\\n--cluster=my_cluster \\\\\\n--region=us-central1 \\\\\\n--project=my-dtc-project-1010101 \\\\\\ngs://my-dtc-bucket-id/code/06_spark_sql.py\\n-- \\\\\\n…',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Dataproc - ERROR: (gcloud.dataproc.jobs.submit.pyspark) The required property [project] is not currently set. It can be set on a per-command basis by re-running your command with the [--project] flag.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c0a46e5d'},\n",
       " {'text': 'Go to %SPARK_HOME%\\\\bin\\nRun spark-class org.apache.spark.deploy.master.Master to run the master. This will give you a URL of the form spark://ip:port\\nRun spark-class org.apache.spark.deploy.worker.Worker spark://ip:port to run the worker. Make sure you use the URL you obtained in step 2.\\nCreate a new Jupyter notebook:\\nspark = SparkSession.builder \\\\\\n.master(\"spark://{ip}:7077\") \\\\\\n.appName(\\'test\\') \\\\\\n.getOrCreate()\\nCheck on Spark UI the master, worker and app.',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Run Local Cluster Spark in Windows 10 with CMD',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '943c2466'},\n",
       " {'text': 'This occurs because you are not logged in “gcloud auth login” and maybe the project id is not settled. Then type in a terminal:\\ngcloud auth login\\nThis will open a tab in the browser, accept the terms, after that close the tab if you want. Then set the project is like:\\ngcloud config set project <YOUR PROJECT_ID>\\nThen you can run the command to upload the pq dir to a GCS Bucket:\\ngsutil -m cp -r pq/ <YOUR URI from gsutil>/pq',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': \"lServiceException: 401 Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket. Permission 'storage.objects.list' denied on resource (or it may not exist).\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f41ef231'},\n",
       " {'text': \"When submit a job, it might throw an error about Java in log panel within Dataproc. I changed the Versioning Control when I created a cluster, so it means that I delete the cluster and created a new one, and instead of choosing Debian-Hadoop-Spark, I switch to Ubuntu 20.02-Hadoop3.3-Spark3.3 for Versioning Control feature, the main reason to choose this is because I have the same Ubuntu version in mi laptop, I tried to find documentation to sustent this but unfortunately I couldn't nevertheless it works for me.\",\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'py4j.protocol.Py4JJavaError  GCP',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '6b26d73c'},\n",
       " {'text': \"Use both repartition and coalesce, like so:\\ndf = df.repartition(6)\\ndf = df.coalesce(6)\\ndf.write.parquet('fhv/2019/10', mode='overwrite')\",\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Repartition the Dataframe to 6 partitions using df.repartition(6) - got 8 partitions instead',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '830e2936'},\n",
       " {'text': \"Possible solution - Try to forward the port using ssh cli instead of vs code.\\nRun > “ssh -L <local port>:<VM host/ip>:<VM port> <ssh hostname>”\\nssh hostname is the name you specified in the ~/.ssh/config file\\nIn case of Jupyter Notebook run\\n“ssh -L 8888:localhost:8888 gcp-vm”\\nfrom your local machine’s cli.\\nNOTE: If you logout from the session, the connection would break. Also while creating the spark session notice the block's log because sometimes it fails to run at 4040 and then switches to 4041.\\n~Abhijit Chakrborty: If you are having trouble accessing localhost ports from GCP VM consider adding the forwarding instructions to .ssh/config file as following:\\n```\\nHost <hostname>\\nHostname <external-gcp-ip>\\nUser xxxx\\nIdentityFile yyyy\\nLocalForward 8888 localhost:8888\\nLocalForward 8080 localhost:8080\\nLocalForward 5432 localhost:5432\\nLocalForward 4040 localhost:4040\\n```\\nThis should automatically forward all ports and will enable accessing localhost ports.\",\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Jupyter Notebook or SparkUI not loading properly at localhost after port forwarding from VS code?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '02007b7c'},\n",
       " {'text': '~ Abhijit Chakraborty\\n`sdk list java`  to check for available java sdk versions.\\n`sdk install java 11.0.22-amzn`  as  java-11.0.22-amzn was available for my codespace.\\nclick on Y if prompted to change the default java version.\\nCheck for java version using `java -version `.\\nIf working fine great, else `sdk default java 11.0.22-amzn` or whatever version you have installed.',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Installing Java 11 on codespaces',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '1ebb9a47'},\n",
       " {'text': 'Sometimes while creating a dataproc cluster on GCP, the following error is encountered.\\nSolution: As mentioned here, sometimes there might not be enough resources in the given region to allocate the request. Usually, gets freed up in a bit and one can create a cluster. – abhirup ghosh\\nSolution 2:  Changing the type of boot-disk from PD-Balanced to PD-Standard, in terraform, helped solve the problem.- Sundara Kumar Padmanabhan',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': \"Error: Insufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 470.0.\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '80125745'},\n",
       " {'text': \"Pyspark converts the difference of two TimestampType values to Python's native datetime.timedelta object. The timedelta object only stores the duration in terms of days, seconds, and microseconds. Each of the three units of time must be manually converted into hours in order to express the total duration between the two timestamps using only hours.\\nAnother way for achieving this is using the datediff (sql function). It receives this parameters\\nUpper Date: the closest date you have. For example dropoff_datetime\\nLower Date: the farthest date you have.  For example pickup_datetime\\nAnd the result is returned in terms of days, so you could multiply the result for 24 in order to get the hours.\",\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Homework - how to convert the time difference of two timestamps to hours',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f01df45b'},\n",
       " {'text': 'This version combination worked for me:\\nPySpark = 3.3.2\\nPandas = 1.5.3\\n\\nIf it still has an error,',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'PicklingError: Could not serialize object: IndexError: tuple index out of range',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '06014eec'},\n",
       " {'text': \"Run this before SparkSession\\nimport os\\nimport sys\\nos.environ['PYSPARK_PYTHON'] = sys.executable\\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\",\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Py4JJavaError: An error occurred while calling o180.showString. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '54653ca9'},\n",
       " {'text': \"import os\\nimport sys\\nos.environ['PYSPARK_PYTHON'] = sys.executable\\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\\nDataproc Pricing: https://cloud.google.com/dataproc/pricing#on_gke_pricing\",\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'RuntimeError: Python in worker has different version 3.11 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f95304db'},\n",
       " {'text': 'Ans: No, you can submit a job to DataProc from your local computer by installing gsutil (https://cloud.google.com/storage/docs/gsutil_install) and configuring it. Then, you can execute the following command from your local computer.\\ngcloud dataproc jobs submit pyspark \\\\\\n--cluster=de-zoomcamp-cluster \\\\\\n--region=europe-west6 \\\\\\ngs://dtc_data_lake_de-zoomcamp-nytaxi/code/06_spark_sql.py \\\\\\n-- \\\\\\n--input_green=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/green/2020/*/ \\\\\\n--input_yellow=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/yellow/2020/*/ \\\\\\n--output=gs://dtc_data_lake_de-zoomcamp-nytaxi/report-2020 (edited)',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Dataproc Qn: Is it essential to have a VM on GCP for running Dataproc and submitting jobs ?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '591df4e6'},\n",
       " {'text': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\\nthis is because the method inside the pyspark refers to a package that has been already deprecated\\n(https://stackoverflow.com/questions/76404811/attributeerror-dataframe-object-has-no-attribute-iteritems)\\nYou can do this code below, which is mentioned in the stackoverflow link above:\\nQ: DE Zoomcamp 5.6.3 - Setting up a Dataproc Cluster I cannot create a cluster and get this message. I tried many times as the FAQ said, but it didn't work. What can I do?\\nError\\nInsufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 250.0.\\nRequest ID: 17942272465025572271\\nA: The master and worker nodes are allocated a maximum of 250 GB of memory combined. In the configuration section, adhere to the following specifications:\\nMaster Node:\\nMachine type: n2-standard-2\\nPrimary disk size: 85 GB\\nWorker Node:\\nNumber of worker nodes: 2\\nMachine type: n2-standard-2\\nPrimary disk size: 80 GB\\nYou can allocate up to 82.5 GB memory for worker nodes, keeping in mind that the total memory allocated across all nodes cannot exceed 250 GB.\",\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'In module 5.3.1, trying to run spark.createDataFrame(df_pandas).show() returns error',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '5cb7f597'},\n",
       " {'text': 'The MacOS setup instruction (https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/macos.md#installing-java) for setting the JAVA_HOME environment variable is for Intel-based Macs which have a default install location at /usr/local/. If you have an Apple Silicon mac, you will have to set JAVA_HOME to /opt/homebrew/, specifically in your .bashrc or .zshrc:\\nexport JAVA_HOME=\"/opt/homebrew/opt/openjdk/bin\"\\nexport PATH=\"$JAVA_HOME:$PATH\"\\nConfirm that your path was correctly set by running the command: which java\\nYou should expect to see the output:\\n/opt/homebrew/opt/openjdk/bin/java\\nReference: https://docs.brew.sh/Installation',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Setting JAVA_HOME with Homebrew on Apple Silicon',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c5de1f96'},\n",
       " {'text': 'Check Docker Compose File:\\nEnsure that your docker-compose.yaml file is correctly configured with the necessary details for the \"control-center\" service. Check the service name, image name, ports, volumes, environment variables, and any other configurations required for the container to start.\\nOn Mac OSX 12.2.1 (Monterey) I could not start the kafka control center. I opened Docker Desktop and saw docker images still running from week 4, which I did not see when I typed “docker ps.” I deleted them in docker desktop and then had no problem starting up the kafka environment.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Could not start docker image “control-center” from the docker-compose.yaml file.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '70ac8e80'},\n",
       " {'text': \"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\nTo create a virtual env and install packages (run only once)\\npython -m venv env\\nsource env/bin/activate\\npip install -r ../requirements.txt\\nTo activate it (you'll need to run it every time you need the virtual env):\\nsource env/bin/activate\\nTo deactivate it:\\ndeactivate\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\",\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Module “kafka” not found when trying to run producer.py',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f6551ffb'},\n",
       " {'text': 'ImportError: DLL load failed while importing cimpl: The specified module could not be found\\nVerify Python Version:\\nMake sure you are using a compatible version of Python with the Avro library. Check the Python version and compatibility requirements specified by the Avro library documentation.\\n... you may have to load librdkafka-5d2e2910.dll in the code. Add this before importing avro:\\nfrom ctypes import CDLL\\nCDLL(\"C:\\\\\\\\Users\\\\\\\\YOUR_USER_NAME\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\dtcde\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\confluent_kafka.libs\\\\librdkafka-5d2e2910.dll\")\\nIt seems that the error may occur depending on the OS and python version installed.\\nALTERNATIVE:\\nImportError: DLL load failed while importing cimpl\\n✅SOLUTION: $env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1 in Powershell.\\nYou need to set this DLL manually in Conda Env.\\nSource: https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Error importing cimpl dll when running avro examples',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '0ec021de'},\n",
       " {'text': \"✅SOLUTION: pip install confluent-kafka[avro].\\nFor some reason, Conda also doesn't include this when installing confluent-kafka via pip.\\nMore sources on Anaconda and confluent-kafka issues:\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/590\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/1221\\nhttps://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka\",\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': \"ModuleNotFoundError: No module named 'avro'\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '1edd4630'},\n",
       " {'text': 'If you get an error while running the command python3 stream.py worker\\nRun pip uninstall kafka-python\\nThen run pip install kafka-python==1.4.6\\nWhat is the use of  Redpanda ?\\nRedpanda: Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles.\\nRedpanda is a powerful, yet simple, and cost-efficient streaming data platform that is compatible with Kafka® APIs while eliminating Kafka complexity.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Error while running python3 stream.py worker',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '4664ae28'},\n",
       " {'text': 'Got this error because the docker container memory was exhausted. The dta file was upto 800MB but my docker container does not have enough memory to handle that.\\nSolution was to load the file in chunks with Pandas, then create multiple parquet files for each dat file I was processing. This worked smoothly and the issue was resolved.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Negsignal:SIGKILL while converting dta files to parquet format',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '676e1b76'},\n",
       " {'text': 'Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a3c84279'},\n",
       " {'text': 'tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Kafka- python videos have low audio and hard to follow up',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '119c917d'},\n",
       " {'text': 'If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'kafka.errors.NoBrokersAvailable: NoBrokersAvailable',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f1284c1f'},\n",
       " {'text': 'Ankush said we can focus on horizontal scaling option.\\n“think of scaling in terms of scaling from consumer end. Or consuming message via horizontal scaling”',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Kafka homwork Q3, there are options that support scaling concept more than the others:',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '49a7db28'},\n",
       " {'text': 'If you get this error, know that you have not built your sparks and juypter images. This images aren’t readily available on dockerHub.\\nIn the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': \"How to fix docker compose error: Error response from daemon: pull access denied for spark-3.3.1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '196cb0f2'},\n",
       " {'text': 'Run this command in terminal in the same directory (/docker/spark):\\nchmod +x build.sh',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: ./build.sh: Permission denied Error',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '1e50eab7'},\n",
       " {'text': 'Restarting all services worked for me:\\ndocker-compose down\\ndocker-compose up',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: ‘KafkaTimeoutError: Failed to update metadata after 60.0 secs.’ when running stream-example/producer.py',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a7a6d0d7'},\n",
       " {'text': 'While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\\n…\\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\\n…\\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\\n…\\nSolution:\\nDowngrade your local PySpark to 3.3.1 (same as Dockerfile)\\nThe reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\\nSolution 2:\\nCheck what Spark version your local machine has\\npyspark –version\\nspark-submit –version\\nAdd your version to SPARK_VERSION in build.sh',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '0996213a'},\n",
       " {'text': 'Start a new terminal\\nRun: docker ps\\nCopy the CONTAINER ID of the spark-master container\\nRun: docker exec -it <spark_master_container_id> bash\\nRun: cat logs/spark-master.out\\nCheck for the log when the error happened\\nGoogle the error message from there',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '311bf368'},\n",
       " {'text': 'Make sure your java version is 11 or 8.\\nCheck your version by:\\njava --version\\nCheck all your versions by:\\n/usr/libexec/java_home -V\\nIf you already have got java 11 but just not selected as default, select the specific version by:\\nexport JAVA_HOME=$(/usr/libexec/java_home -v 11.0.22)\\n(or other version of 11)',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: ./spark-submit.sh streaming.py Error: py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'c1551650'},\n",
       " {'text': 'In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \"java-kafka-rides\"\\narchiveClassifier = \\'\\'\\n}\\nAnd then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f9b673cf'},\n",
       " {'text': 'confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '5479dce2'},\n",
       " {'text': 'In the project directory, run:\\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: How to run producer/consumer/kstreams/etc in terminal',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '02cf2317'},\n",
       " {'text': 'For example, when running JsonConsumer.java, got:\\nConsuming form kafka started\\nRESULTS:::0\\nRESULTS:::0\\nRESULTS:::0\\nOr when running JsonProducer.java, got:\\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\nSolution:\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '947c07a6'},\n",
       " {'text': 'Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn’t see it at first and had to do some fixes.\\nSolution:\\n(Source)\\nVS Code\\n→ Explorer (first icon on the left navigation bar)\\n→ JAVA PROJECTS (bottom collapsable)\\n→  icon next in the rightmost position to JAVA PROJECTS\\n→  clean Workspace\\n→ Confirm by clicking Reload and Delete\\nNow you will be able to see the triangle icon next to each test like what you normally see in python tests.\\nE.g.:\\nYou can also add classes and packages in this window instead of creating files in the project directory',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: Tests are not picked up in VSCode',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'bea22953'},\n",
       " {'text': 'In Confluent Cloud:\\nEnvironment → default (or whatever you named your environment as) → The right navigation bar →  “Stream Governance API” →  The URL under “Endpoint”\\nAnd create credentials from Credentials section below it',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Confluent Kafka: Where can I find schema registry URL?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a1603359'},\n",
       " {'text': 'You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'How do I check compatibility of local and container Spark versions?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a85a6a91'},\n",
       " {'text': 'According to https://github.com/dpkp/kafka-python/\\n“DUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING”\\nUse pip install kafka-python-ng instead',\n",
       "  'section': 'Project',\n",
       "  'question': 'How to fix the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\"?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '343864f5'},\n",
       " {'text': 'Each submitted project will be evaluated by 3 (three) randomly assigned students that have also submitted the project.\\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here.',\n",
       "  'section': 'Project',\n",
       "  'question': 'How is my capstone project going to be evaluated?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '6cb3b4a9'},\n",
       " {'text': 'There is only ONE project for this Zoomcamp. You do not need to submit or create two projects. There are simply TWO chances to pass the course. You can use the Second Attempt if you a) fail the first attempt b) do not have the time due to other engagements such as holiday or sickness etc. to enter your project into the first attempt.',\n",
       "  'section': 'Project',\n",
       "  'question': 'Project 1 & Project 2',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '5959ea3c'},\n",
       " {'text': 'See a list of datasets here: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_7_project/datasets.md',\n",
       "  'section': 'Project',\n",
       "  'question': 'Does anyone know nice and relatively large datasets?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '202af70b'},\n",
       " {'text': 'You need to redefine the python environment variable to that of your user account',\n",
       "  'section': 'Project',\n",
       "  'question': 'How to run python as start up script?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f2705fe7'},\n",
       " {'text': 'Initiate a Spark Session\\nspark = (SparkSession\\n.builder\\n.appName(app_name)\\n.master(master=master)\\n.getOrCreate())\\nspark.streams.resetTerminated()\\nquery1 = spark\\n.readStream\\n…\\n…\\n.load()\\nquery2 = spark\\n.readStream\\n…\\n…\\n.load()\\nquery3 = spark\\n.readStream\\n…\\n…\\n.load()\\nquery1.start()\\nquery2.start()\\nquery3.start()\\nspark.streams.awaitAnyTermination() #waits for any one of the query to receive kill signal or error failure. This is asynchronous\\n# On the contrary query3.start().awaitTermination() is a blocking ex call. Works well when we are reading only from one topic.',\n",
       "  'section': 'Project',\n",
       "  'question': 'Spark Streaming - How do I read from multiple topics in the same Spark Session',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '74f412c4'},\n",
       " {'text': 'Transformed data can be moved in to azure blob storage and then it can be moved in to azure SQL DB, instead of moving directly from databricks to Azure SQL DB.',\n",
       "  'section': 'Project',\n",
       "  'question': 'Data Transformation from Databricks to Azure SQL DB',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '5214eb93'},\n",
       " {'text': 'The trial dbt account provides access to dbt API. Job will still be needed to be added manually. Airflow will run the job using a python operator calling the API. You will need to provide api key, job id, etc. (be careful not committing it to Github).\\nDetailed explanation here: https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment\\nSource code example here: https://github.com/sungchun12/airflow-toolkit/blob/95d40ac76122de337e1b1cdc8eed35ba1c3051ed/dags/examples/dbt_cloud_example.py',\n",
       "  'section': 'Project',\n",
       "  'question': 'Orchestrating dbt with Airflow',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '3cfd16a7'},\n",
       " {'text': 'https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dataproc/index.html\\nhttps://airflow.apache.org/docs/apache-airflow-providers-google/stable/_modules/airflow/providers/google/cloud/operators/dataproc.html\\nGive the following roles to you service account:\\nDataProc Administrator\\nService Account User (explanation here)\\nUse DataprocSubmitPySparkJobOperator, DataprocDeleteClusterOperator and  DataprocCreateClusterOperator.\\nWhen using  DataprocSubmitPySparkJobOperator, do not forget to add:\\ndataproc_jars = [\"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.24.0.jar\"]\\nBecause DataProc does not already have the BigQuery Connector.',\n",
       "  'section': 'Project',\n",
       "  'question': 'Orchestrating DataProc with Airflow',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a7cecdf9'},\n",
       " {'text': 'You can trigger your dbt job in Mage pipeline. For this get your dbt cloud api key under settings/Api tokens/personal tokens. Add it safely to  your .env\\nFor example\\ndbt_api_trigger=dbt_**\\nNavigate to job page and find api trigger  link\\nThen create a custom mage Python block with a simple http request like here\\nfrom dotenv import load_dotenv\\nfrom pathlib import Path\\ndotenv_path = Path(\\'/home/src/.env\\')\\nload_dotenv(dotenv_path=dotenv_path)\\ndbt_api_trigger= os.getenv(dbt_api_trigger)\\nurl = f\"https://cloud.getdbt.com/api/v2/accounts/{dbt_account_id}/jobs/<job_id>/run/\"\\nheaders = {\\n        \"Authorization\": f\"Token {dbt_api_trigger}\",\\n        \"Content-Type\": \"application/json\" }\\nbody = {\\n        \"cause\": \"Triggered via API\"\\n    }\\n    response = requests.post(url, headers=headers, json=body)\\nvoila! You triggered dbt job form your mage pipeline.',\n",
       "  'section': 'Project',\n",
       "  'question': 'Orchestrating dbt cloud with Mage',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '2aad1011'},\n",
       " {'text': \"The slack thread : thttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1677678161866999\\nThe question is that sometimes even if you take plenty of effort to document every single step, and we can't even sure if the person doing the peer review will be able to follow-up, so how this criteria will be evaluated?\\nAlex clarifies: “Ideally yes, you should try to re-run everything. But I understand that not everyone has time to do it, so if you check the code by looking at it and try to spot errors, places with missing instructions and so on - then it's already great”\",\n",
       "  'section': 'Project',\n",
       "  'question': 'Project evaluation - Reproducibility',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'cb478996'},\n",
       " {'text': 'The key valut in Azure cloud is used to store credentials or passwords or secrets of different tech stack used in Azure. For example if u do not want to expose the password in SQL database, then we can save the password under a given name and use them in other Azure stack.',\n",
       "  'section': 'Project',\n",
       "  'question': 'Key Vault in Azure cloud stack',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'b4ef8ca7'},\n",
       " {'text': 'You can get the version of py4j from inside docker using this command\\ndocker exec -it --user airflow airflow-airflow-scheduler-1 bash -c \"ls /opt/spark/python/lib\"',\n",
       "  'section': 'Project',\n",
       "  'question': \"Spark docker - `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '8e74f943'},\n",
       " {'text': 'Either use conda or pip for managing venv, using both of them together will cause incompatibility.\\nIf you’re using conda, install psycopg2 using the conda-forge channel, which may handle the architecture compatibility automatically\\nconda install -c conda-forge psycopg2\\nIf pip, do the normal install\\npip install psycopg2',\n",
       "  'section': 'Project',\n",
       "  'question': 'psycopg2 complains of incompatible environment e.g x86 instead of amd',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'a73ed357'},\n",
       " {'text': 'This is not a FAQ but more of an advice if you want to set up dbt locally, I did it in the following way:\\nI had the postgres instance from week 2 (year 2024) up (the docker-compose)\\nmkdir dbt\\nvi dbt/profiles.yml\\nAnd here I attached this content (only the required fields) and replaced them with the proper values (for instance mine where in the .env file of the folder of week 2 docker stuff)\\ncd dbt && git clone https://github.com/dbt-labs/dbt-starter-project\\nmkdir project && cd project && mv dbt-starter-project/* .\\nMake sure that you align the profile name in profiles.yml with the dbt_project.yml file\\nAdd this line anywhere on the dbt_project.yml file:\\nconfig-version: 2\\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres ls\\nIf you have trouble run\\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres debug',\n",
       "  'section': 'Project',\n",
       "  'question': 'Setting up dbt locally with Docker and Postgres',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'd5b6ef5d'},\n",
       " {'text': 'The following line should be included in pyspark configuration\\n# Example initialization of SparkSession variable\\nspark = (SparkSession.builder\\n.master(...)\\n.appName(...)\\n# Add the following configuration\\n.config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-3.5-bigquery:0.37.0\")\\n)',\n",
       "  'section': 'Project',\n",
       "  'question': 'How to connect Pyspark with BigQuery?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'b406d90e'},\n",
       " {'text': 'Install the astronomer-cosmos package as a dependency. (see Terraform example).\\nMake a new folder, dbt/, inside the dags/ folder of your Composer GCP bucket and copy paste your dbt-core project there. (see example)\\nEnsure your profiles.yml is configured to authenticate with a service account key. (see BigQuery example)\\nCreate a new DAG using the DbtTaskGroup class and a ProfileConfig specifying a profiles_yml_filepath that points to the location of your JSON key file. (see example)\\nYour dbt lineage graph should now appear as tasks inside a task group like this:',\n",
       "  'section': 'Course Management Form for Homeworks',\n",
       "  'question': 'How to run a dbt-core project as an Airflow Task Group on Google Cloud Composer using a service account JSON key',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '0002ab8b'},\n",
       " {'text': 'The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname, or your real name, if you prefer. Your entry on the Leaderboard is the one highlighted in teal(?) / light green (?).\\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\\nThe \"Display on Leaderboard\" option indicates whether you want your name to be listed on the course leaderboard.\\nQuestion: Is it possible to create external tables in BigQuery using URLs, such as those from the NY Taxi data website?\\nAnswer: Not really, only Bigtable, Cloud Storage, and Google Drive are supported data stores.',\n",
       "  'section': 'Workshop 1 - dlthub',\n",
       "  'question': 'Edit Course Profile.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '138b55c7'},\n",
       " {'text': \"Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If you’re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).\",\n",
       "  'section': 'Workshop 1 - dlthub',\n",
       "  'question': 'How do I install the necessary dependencies to run the code?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '154d7705'},\n",
       " {'text': 'If you are running Jupyter Notebook on a fresh new Codespace or in local machine with a new Virtual Environment, you will need this package to run the starter Jupyter Notebook offered by the teacher. Execute this:\\npip install jupyter',\n",
       "  'section': 'Workshop 1 - dlthub',\n",
       "  'question': 'Other packages needed but not listed',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f96517d9'},\n",
       " {'text': 'Alternatively, you can switch to in-file storage with:',\n",
       "  'section': 'Workshop 1 - dlthub',\n",
       "  'question': 'How can I use DuckDB In-Memory database with dlt ?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '773587dd'},\n",
       " {'text': 'After loading, you should have a total of 8 records, and ID 3 should have age 33\\nQuestion: Calculate the sum of ages of all the people loaded as described above\\nThe sum of all eight records\\' respective ages is too big to be in the choices. You need to first filter out the people whose occupation is equal to None in order to get an answer that is close to or present in the given choices. 😃\\n----------------------------------------------------------------------------------------\\nFIXED = use a raw string and keep the file:/// at the start of your file path\\nI\\'m having an issue with the dlt workshop notebook. The \\'Load to Parquet file\\' section specifically. No matter what I change the file path to, it\\'s still saving the dlt files directly to my C drive.\\n# Set the bucket_url. We can also use a local folder\\nos.environ[\\'DESTINATION__FILESYSTEM__BUCKET_URL\\'] = r\\'file:///content/.dlt/my_folder\\'\\nurl = \"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\\n# Define your pipeline\\npipeline = dlt.pipeline(\\npipeline_name=\\'my_pipeline\\',\\ndestination=\\'filesystem\\',\\ndataset_name=\\'mydata\\'\\n)\\n# Run the pipeline with the generator we created earlier.\\nload_info = pipeline.run(stream_download_jsonl(url), table_name=\"users\", loader_file_format=\"parquet\")\\nprint(load_info)\\n# Get a list of all Parquet files in the specified folder\\nparquet_files = glob.glob(\\'/content/.dlt/my_folder/mydata/users/*.parquet\\')\\n# show parquet files\\nfor file in parquet_files:\\nprint(file)',\n",
       "  'section': 'Workshop 2 - RisingWave',\n",
       "  'question': 'Homework - dlt Exercise 3 - Merge a generator concerns',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '73aff710'},\n",
       " {'text': 'Check the contents of the repository with ls - the command.sh file should be in the root folder\\nIf it is not, verify that you had cloned the correct repository - https://github.com/risingwavelabs/risingwave-data-talks-workshop-2024-03-04',\n",
       "  'section': 'Workshop 2 - RisingWave',\n",
       "  'question': 'command.sh Error - source: no such file or directory: command.sh',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '0728ca67'},\n",
       " {'text': \"psql is a command line tool that is installed alongside PostgreSQL DB, but since we've always been running PostgreSQL in a container, you've only got `pgcli`, which lacks the feature to run a sql script into the DB. Besides, having a command line for each database flavor you'll have to deal with as a Data Professional is far from ideal.\\nSo, instead, you can use usql. Check the docs for details on how to install for your OS. On macOS, it supports `homebrew`, and on Windows, it supports scoop.\\nSo, to run the taxi_trips.sql script with usql:\",\n",
       "  'section': 'Workshop 2 - RisingWave',\n",
       "  'question': 'psql - command not found: psql (alternative install)',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '49a51e24'},\n",
       " {'text': 'If you encounter this error and are certain that you have docker compose installed, but typically run it as docker compose without the hyphen, then consider editing command.sh file by removing the hyphen from ‘docker-compose’. Example:\\nstart-cluster() {\\ndocker compose -f docker/docker-compose.yml up -d\\n}',\n",
       "  'section': 'Workshop 2 - RisingWave',\n",
       "  'question': 'Setup - source command.sh - error: “docker-compose” not found',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'f0d552a7'},\n",
       " {'text': 'ERROR: The Compose file \\'./docker/docker-compose.yml\\' is invalid because:\\nInvalid top-level property \"x-image\". Valid top-level sections for this Compose file are: version, services, networks, volumes, secrets, configs, and extensions starting with \"x-\".\\nYou might be seeing this error because you\\'re using the wrong Compose file version. Either specify a supported version (e.g \"2.2\" or \"3.3\") and place your service definitions under the `services` key, or omit the `version` key and place your service definitions at the root of the file to use version 1.\\nFor more on the Compose file format versions, see https://docs.docker.com/compose/compose-file/\\nIf you encounter the above error and have docker-compose installed, try updating your version of docker-compose. At the time of reporting this issue (March 17 2024), Ubuntu does not seem to support a docker-compose version high enough to run the required docker images. If you have this error and are on a Ubuntu machine, consider starting a VM with a Debian machine or look for an alternative way to download docker-compose at the latest version on your machine.',\n",
       "  'section': 'Workshop 2 - RisingWave',\n",
       "  'question': 'Setup - start-cluster error: Invalid top-level property x-image',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '9c750080'},\n",
       " {'text': 'Ans: [source] Yes, it is so that we can observe the changes as we’re working on the queries in real-time. The script is changing the date timestamp to the current time, so our queries with the now()filter would work. Open another terminal tab to copy+paste the queries while the stream-kafka script is running in the background.\\nNoel: I have recently increased this up to 100 at a time, you may pull the latest changes from the repository.',\n",
       "  'section': 'Workshop 2 - RisingWave',\n",
       "  'question': 'stream-kafka Qn: Is it expected that the records are being ingested 10 at a time?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '6f4998e6'},\n",
       " {'text': 'Ans: No, it is not.',\n",
       "  'section': 'Workshop 2 - RisingWave',\n",
       "  'question': 'Setup - Qn: Is kafka install required for the RisingWave workshop? [source]',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '97170587'},\n",
       " {'text': 'Ans: about 7GB free for all the containers to be provisioned and then the psql still needs to run and ingest the taxi data, so maybe 10gb in total?',\n",
       "  'section': 'Workshop 2 - RisingWave',\n",
       "  'question': 'Setup - Qn: How much free disk space should we have? [source]',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '4def6541'},\n",
       " {'text': 'Replace psycopg2==2.9.9 with psycopg2-binary in the requirements.txt file [source] [another]\\nWhen you open another terminal to run the psql, remember to do the source command.sh step for each terminal session\\n---------------------------------------------------------------------------------------------',\n",
       "  'section': 'Workshop 2 - RisingWave',\n",
       "  'question': 'Psycopg2 - issues when running stream-kafka script',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '66e117dd'},\n",
       " {'text': \"If you’re using an Anaconda installation:\\nCd home/\\nConda install gcc\\nSource back to your RisingWave Venv - source .venv/bin/activate\\nPip install psycopg2-binary\\nPip install -r requirements.txt\\nFor some reason this worked - the Conda base doesn’t have the GCC installed - (GNU Compiler Collection) a compiler system that supports various programming languages. Without this the it fails to install pyproject.toml-based projects\\n“It's possible that in your specific environment, the gcc installation was required at the system level rather than within the virtual environment. This can happen if the build process for psycopg2 tries to access system-level dependencies during installation.\\nInstalling gcc in your main Python installation (Conda) would make it available system-wide, allowing any Python environment to access it when necessary for building packages.”\\ngcc stands for GNU Compiler Collection. It is a compiler system developed by the GNU Project that supports various programming languages, including C, C++, Objective-C, and Fortran.\\nGCC is widely used for compiling source code written in these languages into executable programs or libraries. It's a key tool in the software development process, particularly in the compilation stage where source code is translated into machine code that can be executed by a computer's processor.\\nIn addition to compiling source code, GCC also provides various optimization options, debugging support, and extensive documentation, making it a powerful and versatile tool for developers across different platforms and architectures.\\n—-----------------------------------------------------------------------------------\",\n",
       "  'section': 'Workshop 2 - RisingWave',\n",
       "  'question': 'Psycopg2 - `Could not build wheels for psycopg2, which is required to install pyproject.toml-based projects`',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '94fd2476'},\n",
       " {'text': \"Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\\nUse the git bash terminal in windows.\\nActivate python venv from git bash: source .venv/Scripts/activate\\nModify the seed_kafka.py file: in the first line, replace python3 with python.\\nNow from git bash, run the seed-kafka cmd. It should work now.\\nAdditional Notes:\\nYou can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\\nThe equivalent of source commands.sh  in Powershell is . .\\\\commands.sh from the workshop directory.\\nHope this can save you from some trouble in case you're doing this workshop on Windows like I am.\\n—--------------------------------------------------------------------------------------\",\n",
       "  'section': 'Workshop 2 - RisingWave',\n",
       "  'question': 'Psycopg2 InternalError: Failed to run the query - when running the seed-kafka command after initial setup.',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '70d83d78'},\n",
       " {'text': 'In case the script gets stuck on\\n%3|1709652240.100|FAIL|rdkafka#producer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)gre\\nafter trying to load the trip data, check the logs of the message_queue container in docker. If it keeps restarting with Could not initialize seastar: std::runtime_error (insufficient physical memory: needed 4294967296 available 4067422208)  as the last message, then go to the docker-compose file in the docker folder of the project and change the ‘memory’ command for the message_queue service for some lower value.\\nSolution: lower the memory allocation of the service “message_queue” in your docker-compose file from 4GB. If you have the “insufficient physical memory” error message (try 3GB)\\nIssue: Running psql -f risingwave-sql/table/trip_data.sql after starting services with ‘default’ values using docker-compose up gives the error  “psql:risingwave-sql/table/trip_data.sql:61: ERROR:  syntax error at or near \".\" LINE 60:       properties.bootstrap.server=\\'message_queue:29092\\'”\\nSolution: Make sure you have run source commands.sh in each terminal window',\n",
       "  'section': 'Workshop 2 - RisingWave',\n",
       "  'question': 'Running stream-kafka script gets stuck on a loop with Connection Refused',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'accb7285'},\n",
       " {'text': 'Use seed-kafka instead of stream-kafka to get a static set of results.',\n",
       "  'section': 'Workshop 2 - RisingWave',\n",
       "  'question': 'For the homework questions is there a specific number of records that have to be processed to obtain the final answer?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'cbca4495'},\n",
       " {'text': 'It is best to use the order by and limit clause on the query to the materialized view instead of the materialized view creation in order to guarantee consistent results\\nHomework - The answers in the homework do not match the provided options: You must follow the following steps: 1. clean-cluster 2. docker volume prune and use seed-kafka instead of stream-kafka. Ensure that the number of records is 100K.',\n",
       "  'section': 'Workshop 2 - RisingWave',\n",
       "  'question': 'Homework - Materialized view does not guarantee order by warning',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '78fce6ad'},\n",
       " {'text': 'For this workshop, and if you are following the view from Noel (2024) this requires you to install postgres to use it on your terminal. Found this steps (commands) to get it done [source]:\\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\\nsudo sh -c \\'echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list\\'\\nsudo apt update\\napt install postgresql postgresql-contrib\\n(comment): now let’s check the service for postgresql\\nservice postgresql status\\n(comment) If down: use the next command\\nservice postgresql start\\n(comment) And your are done',\n",
       "  'section': 'Workshop 2 - RisingWave',\n",
       "  'question': 'How to install postgress on Linux like OS',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '68842c02'},\n",
       " {'text': 'Refer to the solution given in the first solution here:\\nhttps://stackoverflow.com/questions/24683221/xdg-open-no-method-available-even-after-installing-xdg-utils\\nInstead of w3m use any other browser of your choice.\\nIt is just trying to open the index.html file. Which you can do from your File Explorer/Finder. If you’re on wsl try using explorer.exe index.html',\n",
       "  'section': 'Workshop 2 - RisingWave',\n",
       "  'question': 'Unable to Open Dashboard as xdg-open doesn’t open any browser',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '71b1984b'},\n",
       " {'text': 'Example Error:\\nWhen attempting to execute a Python script named seed-kafka.py or server.py with the following shebang line specifying Python 3 as the interpreter:\\nUsers may encounter the following error in a Unix-like environment:\\nThis error indicates that there is a problem with the Python interpreter path specified in the shebang line. The presence of the \\\\r character suggests that the script was edited or created in a Windows environment, causing the interpreter path to be incorrect when executed in Unix-like environments.\\n2 Solutions:\\nEither one or the other\\nUpdate Shebang Line:\\nVerify Python Interpreter Path: Use the which python3 command to determine the path to the Python 3 interpreter available in the current environment.\\nUpdate Shebang Line: Open the script file in a text editor. Modify the shebang line to point to the correct Python interpreter path found in the previous step. Ensure that the shebang line is consistent with the Python interpreter path in the execution environment.\\nExample Shebang Line:\\nReplace /usr/bin/env python3 with the correct Python interpreter path found using which python3.\\nConvert Line Endings:\\nUse the dos2unix command-line tool to convert the line endings of the script from Windows-style to Unix-style.\\nThis removes the extraneous carriage return characters (\\\\r), resolving issues related to unexpected tokens and ensuring compatibility with Unix-like environments.\\nExample Command:',\n",
       "  'section': 'Workshop 2 - RisingWave',\n",
       "  'question': 'Resolving Python Interpreter Path Inconsistencies in Unix-like Environments',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'd452b490'},\n",
       " {'text': 'Ans : Windowing in streaming SQL involves defining a time-based or row-based boundary for data processing. It allows you to analyze and aggregate data over specific time intervals or based on the number of events received, providing a way to manage and organize streaming data for analysis.',\n",
       "  'section': 'Workshop 2 - RisingWave',\n",
       "  'question': 'How does windowing work in Sql?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '707cae8f'},\n",
       " {'text': 'Python 3.12.1, is not compatible with kafka-python-2.0.2. Therefore, instead of running \"pip install kafka-python\", you can resolve the issue by using \"pip install git+https://github.com/dpkp/kafka-python.git\". If you have already installed kafka-python, you need to run \"pip uninstall kafka-python\" before executing \"pip install git+https://github.com/dpkp/kafka-python.git\" to resolve the compatibility issue.\\nQ:In the Mage pipeline, individual blocks run successfully. However, when executing the pipeline as a whole, some blocks fail.\\nA: I have the following key-value pair in io_config.yaml file configured but still Mage blocks failed to generate OAuth and authenticate with GCP: GOOGLE_SERVICE_ACC_KEY_FILEPATH: \"{{ env_var(\\'GCP_CREDENTIALS\\') }}\". The GCP_CREDENTIALS variable holds the full path to the service account key\\'s JSON file. Adding the following line within the failed code block resolved the issue: os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = os.environ.get(\\'GCP_CREDENTIALS\\').\\nThis occurs because the path to profiles.yml is not correctly specified. You can rectify this by:\\n“export DBT_PROFILES_DBT=path/to/profiles.yml”\\nEg., /home/src/magic-zoomcamp/dbt/project_name/\\nDo the similar for DBT_PROJECT_DIR if getting similar issue with dbt_project.yml.\\nOnce DIRs are set,:\\n“dbt debug –config-dir”\\nThis would update your paths. To maintain same path across sessions, use the path variables in your .env file.\\nTo add triggers in mage pipelines via CLI, you can create a trigger of type API, and copy the API links.\\nEg. link: http://localhost:6789/api/pipeline_schedules/10/pipeline_runs/f3a1a4228fc64cfd85295b668c93f3b2\\nThen create a trigger.py as such:\\nimport os\\nimport requests\\nclass MageTrigger:\\nOPTIONS = {\\n\"<pipeline_name>\": {\\n\"trigger_id\": 10,\\n\"key\": \"f3a1a4228fc64cfd85295b668c93f3b2\"\\n}\\n}\\n@staticmethod\\ndef trigger_pipeline(pipeline_name, variables=None):\\ntrigger_id = MageTrigger.OPTIONS[pipeline_name][\"trigger_id\"]\\nkey = MageTrigger.OPTIONS[pipeline_name][\"key\"]\\nendpoint = f\"http://localhost:6789/api/pipeline_schedules/{trigger_id}/pipeline_runs/{key}\"\\nheaders = {\\'Content-Type\\': \\'application/json\\'}\\npayload = {}\\nif variables is not None:\\npayload[\\'pipeline_run\\'] = {\\'variables\\': variables}\\nresponse = requests.post(endpoint, headers=headers, json=payload)\\nreturn response\\nMageTrigger.trigger_pipeline(\"<pipeline_name>\")\\nFinally, after the mage server is up an running, simply this command:\\npython trigger.py from mage directory in terminal.\\nCan I do data partitioning & clustering run by dbt pipeline, or I would need to do this manually in BigQuery afterwards?\\nYou can use this configuration in your DBT model:\\n{\\n\"field\": \"<field name>\",\\n\"data_type\": \"<timestamp | date | datetime | int64>\",\\n\"granularity\": \"<hour | day | month | year>\"\\n# Only required if data_type is \"int64\"\\n\"range\": {\\n\"start\": <int>,\\n\"end\": <int>,\\n\"interval\": <int>\\n}\\n}\\nand for clustering\\n{{\\nconfig(\\nmaterialized = \"table\",\\ncluster_by = \"order_id\",\\n)\\n}}\\nmore details in: https://docs.getdbt.com/reference/resource-configs/bigquery-configs',\n",
       "  'section': 'Triggers in Mage via CLI',\n",
       "  'question': 'Encountering the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\" when running \"from kafka import KafkaProducer\" in Jupyter Notebook for Module 6 Homework?',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'ffbf3311'},\n",
       " {'text': 'Docker Commands\\n# Create a Docker Image from a base image\\nDocker run -it ubuntu bash\\n#List docker images\\nDocker images list\\n#List  Running containers\\nDocker ps -a\\n#List with full container ids\\nDocker ps -a --no-trunc\\n#Add onto existing image to create new image\\nDocker commit -a <User_Name> -m \"Message\" container_id New_Image_Name\\n# Create a Docker Image with an entrypoint from a base image\\nDocker run -it --entry_point=bash python:3.11\\n#Attach to a stopped container\\nDocker start -ai <Container_Name>\\n#Attach to a running container\\ndocker exec -it <Container_ID> bash\\n#copying from host to container\\nDocker cp <SRC_PATH/file> <containerid>:<dest_path>\\n#copying from container to host\\nDocker cp <containerid>:<Srct_path> <Dest Path on host/file>\\n#Create an image from a docker file\\nDocker build -t <Image_Name> <Location of Dockerfile>\\n#DockerFile Options and best practices\\nhttps://devopscube.com/build-docker-image/\\n#Docker delete all images forcefully\\ndocker rmi -f $(docker images -aq)\\n#Docker delete all containers forcefully\\ndocker rm -f $(docker ps -qa)\\n#docker compose creation\\nhttps://www.composerize.com/\\nGCP Commands\\n1.     Create SSH Keys\\n2.     Added to the Settings of Compute Engine VM Instance\\n3.     SSH-ed into the VM Instance with a config similar to following\\nHost my-website.com\\nHostName my-website.com\\nUser my-user\\nIdentityFile ~/.ssh/id_rsa\\n4.     Installed Anaconda by installing the sh file through bash <Anaconda.sh>\\n5.     Install Docker after\\na.     Sudo apt-get update\\nb.     Sudo apt-get docker\\n6.     To run Docker without SUDO permissions\\na.     https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md\\n7.     Google cloud remote copy\\na.     gcloud compute scp LOCAL_FILE_PATHVM_NAME:REMOTE_DIR\\nInstall GCP Cloud SDK on Docker Machine\\nhttps://stackoverflow.com/questions/23247943/trouble-installing-google-cloud-sdk-in-ubuntu\\nsudo apt-get install apt-transport-https ca-certificates gnupg && echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\"| sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list&& curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && sudo apt-get update && sudo apt-get install google-cloud-sdk && sudo apt-get install google-cloud-sdk-app-engine-java && sudo apt-get install google-cloud-sdk-app-engine-python && gcloud init\\nAnaconda Commands\\n#Activate environment\\nConda Activate <environment_name>\\n#DeActivate environment\\nConda DeActivate <environment_name>\\n#Start iterm without conda environment\\nconda config --set auto_activate_base false\\n# Using Conda forge as default (Community driven packaging recipes and solutions)\\nhttps://conda-forge.org/docs/user/introduction.html\\nconda --version\\nconda update conda\\nconda config --add channels conda-forge\\nconda config --set channel_priority strict\\n#Using Libmamba as Solver\\nconda install pgcli  --solver=libmamba\\nLinux/MAC Commands\\nStarting and Stopping Services on Linux\\n●  \\tsudo systemctl start postgresql\\n●  \\tsudo systemctl stop postgresql\\nStarting and Stopping Services on MAC\\n●      launchctl start postgresql\\n●      launchctl stop postgresql\\nIdentifying processes listening to a Port across MAC/Linux\\nsudo lsof -i -P -n | grep LISTEN\\n$ sudo netstat -tulpn | grep LISTEN\\n$ sudo ss -tulpn | grep LISTEN\\n$ sudo lsof -i:22 ## see a specific port such as 22 ##\\n$ sudo nmap -sTU -O IP-address-Here\\nInstalling a package on Debian\\nsudo apt install <packagename>\\nListing all package on Debian\\nDpkg -l | grep <packagename>\\nUnInstalling a package on Debian\\nSudo apt remove <packagename>\\nSudo apt autoclean  && sudo apt autoremove\\nList all Processes on Debian/Ubuntu\\nPs -aux\\napt-get update && apt-get install procps\\napt-get install iproute2 for ss -tulpn\\n#Postgres Install\\nsudo sh -c \\'echo \"deb https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list\\'\\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\\nsudo apt-get update\\nsudo apt-get -y install postgresql\\n#Changing Postgresql port to 5432\\n- sudo service postgresql stop - sed -e \\'s/^port.*/port = 5432/\\' /etc/postgresql/10/main/postgresql.conf > postgresql.conf\\n- sudo chown postgres postgresql.conf\\n- sudo mv postgresql.conf /etc/postgresql/10/main\\n- sudo systemctl restart postgresql',\n",
       "  'section': 'Triggers in Mage via CLI',\n",
       "  'question': 'Basic Commands',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': '3916f4a9'},\n",
       " {'text': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'How do I sign up?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '0227b872'},\n",
       " {'text': 'The course videos are pre-recorded, you can start watching the course right now.\\nWe will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\\nYou can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Is it going to be live? When?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '39fda9f0'},\n",
       " {'text': 'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'What if I miss a session?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '5170565b'},\n",
       " {'text': \"The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.: https://mlbookcamp.com/article/python\\nFor example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'How much theory will you cover?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'ecca790c'},\n",
       " {'text': \"Yes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code.\\nHere are some interesting videos covering linear algebra that you can already watch: ML Zoomcamp 1.8 - Linear Algebra Refresher from Alexey Grigorev or the excellent playlist from 3Blue1Brown Vectors | Chapter 1, Essence of linear algebra. Never hesitate to ask the community for help if you have any question.\\n(Mélanie Fouesnard)\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': \"I don't know math. Can I take the course?\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'c25b3de4'},\n",
       " {'text': \"The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.\\nIf you unsubscribed from our newsletter, you won't get course related updates too.\\nBut don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': \"I filled the form, but haven't received a confirmation email. Is it normal?\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '6ba259b1'},\n",
       " {'text': 'Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'How long is the course?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '67e2fd13'},\n",
       " {'text': 'Around ~10 hours per week. Timur Kamaliev did a detailed analysis of how much time students of the previous cohort needed to spend on different modules and projects. Full article',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'How much time do I need for this course?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'a6897e8c'},\n",
       " {'text': 'Yes, if you finish at least 2 out of 3 projects and review 3 peers’ Projects by the deadline, you will get a certificate. This is what it looks like: link. There’s also a version without a robot: link.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Will I get a certificate?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '2eba08e3'},\n",
       " {'text': \"Yes, it's possible. See the previous answer.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Will I get a certificate if I missed the midterm project?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '1d644223'},\n",
       " {'text': 'Check this article. If you know everything in this article, you know enough. If you don’t, read the article and join the coursIntroduction to Pythone too :)\\nIntroduction to Python – Machine Learning Bookcamp\\nYou can follow this English course from the OpenClassrooms e-learning platform, which is free and covers the python basics for data analysis: Learn Python Basics for Data Analysis - OpenClassrooms . It is important to know some basics such as: how to run a Jupyter notebook, how to import libraries (and what libraries are), how to declare a variable (and what variables are) and some important operations regarding data analysis.\\n(Mélanie Fouesnard)',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'How much Python should I know?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '14890cd2'},\n",
       " {'text': 'For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud (we use Saturn cloud but can be anything else).\\n(Rileen Sinha; based on response by Alexey on Slack)',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': \"Any particular hardware requirements for the course or everything is mostly cloud? TIA! Couldn't really find this in the FAQ.\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'a4fad482'},\n",
       " {'text': 'Here is an article that worked for me: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'How to setup TensorFlow with GPU support on Ubuntu?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '34b7fd35'},\n",
       " {'text': \"Here’s how you join a in Slack: https://slack.com/help/articles/205239967-Join-a-channel\\nClick “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.\\nBrowse the list of public channels in your workspace, or use the search bar to search by channel name or description.\\nSelect a channel from the list to view it.\\nClick Join Channel.\\nDo we need to provide the GitHub link to only our code corresponding to the homework questions?\\nYes. You are required to provide the URL to your repo in order to receive a grade\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'I’m new to Slack and can’t find the course channel. Where is it?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '4930aa19'},\n",
       " {'text': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'The course has already started. Can I still join it?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'ee58a693'},\n",
       " {'text': 'The course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'When does the next iteration start?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '636f55d5'},\n",
       " {'text': 'No, it’s not possible. The form is closed after the due date. But don’t worry, homework is not mandatory for finishing the course.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Can I submit the homework after the due date?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'c839b764'},\n",
       " {'text': 'Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\\nClick on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nOr you can just use this link: http://mlzoomcamp.com/#syllabus',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'I just joined. What should I do next? How can I access course materials?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '0a278fb2'},\n",
       " {'text': 'For the 2023 cohort, you can see the deadlines here (it’s taken from the 2023 cohort page)',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'What are the deadlines in this course?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '8de4fefd'},\n",
       " {'text': 'There’s not much difference. There was one special module (BentoML) in the previous iteration of the course, but the rest of the modules are the same as in 2022. The homework this year is different.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'What’s the difference between the previous iteration of the course (2022) and this one (2023)?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '94e86808'},\n",
       " {'text': 'We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\\nIf you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'The course videos are from the previous iteration. Will you release new ones or we’ll use the videos from 2021?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'e7ba6b8a'},\n",
       " {'text': 'When you post about what you learned from the course on your social media pages, use the tag #mlzoomcamp. When you submit your homework, there’s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).\\nFor posting the learning in public links, you get extra scores. But the number of scores is limited to 7 points: if you put more than 7 links in your homework form, you’ll get only 7 points.\\nThe same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week, see Alexey’s reply. (~ ellacharmed)\\nFor midterms/capstones, the awarded points are doubled as the duration is longer. So for projects the points are capped at 14 for 14 URLs.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Submitting learning in public links',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'f7bc2f65'},\n",
       " {'text': \"You can create your own github repository for the course with your notes, homework, projects, etc.\\nThen fork the original course repo and add a link under the 'Community Notes' section to the notes that are in your own repo.\\nAfter that's done, create a pull request to sync your fork with the original course repo.\\n(By Wesley Barreto)\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Adding community notes',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'ae52a907'},\n",
       " {'text': \"Leaderboard Links:\\n2023 - https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml\\n2022 - https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml\\nPython Code:\\nfrom hashlib import sha1\\ndef compute_hash(email):\\nreturn sha1(email.lower().encode('utf-8')).hexdigest()\\nYou need to call the function as follows:\\nprint(compute_hash('YOUR_EMAIL_HERE'))\\nThe quotes are required to denote that your email is a string.\\n(By Wesley Barreto)\\nYou can also use this website directly by entering your email: http://www.sha1-online.com. Then, you just have to copy and paste your hashed email in the “research” bar of the leaderboard to get your scores.\\n(Mélanie Fouesnard)\",\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'Computing the hash for the leaderboard and project review',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'dab5a24a'},\n",
       " {'text': 'If you get “wget is not recognized as an internal or external command”, you need to install it.\\nOn Ubuntu, run\\nsudo apt-get install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\nchoco install wget\\nOr you can download a binary from here and put it to any location in your PATH (e.g. C:/tools/)\\nOn Mac, the easiest way to install wget is to use brew.\\nBrew install wget\\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need eeeto use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAnd then in your python code, for example in your jupyter notebook, use:\\nimport wget\\nwget.download(\"URL\")\\nThis should download whatever is at the URL in the same directory as your code.\\n(Memoona Tahira)\\nAlternatively, you can read a CSV file from a URL directly with pandas:\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\ndf = pd.read_csv(url)\\nValid URL schemes include http, ftp, s3, gs, and file.\\nIn some cases you might need to bypass https checks:\\nimport ssl\\nssl._create_default_https_context = ssl._create_unverified_context\\nOr you can use the built-in Python functionality for downloading the files:\\nimport urllib.request\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\nUrllib.request.urlretrieve() is a standard Python library function available on all devices and platforms. URL requests and URL data retrieval are done with the urllib.request module.\\nThe urlretrieve() function allows you to download files from URLs and save them locally. Python programs use it to download files from the internet.\\nOn any Python-enabled device or platform, you can use the urllib.request.urlretrieve() function to download the file.\\n(Mohammad Emad Sharifi)',\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'wget is not recognized as an internal or external command',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '49f9bda9'},\n",
       " {'text': 'You can use\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nTo download the data too. The exclamation mark !, lets you execute shell commands inside your notebooks. This works generally for shell commands such as ls, cp, mkdir, mv etc . . .\\nFor instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:\\n!mkdir -p ../data/\\n!mv housing.csv ../data/',\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'Retrieving csv inside notebook',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'd44de7d1'},\n",
       " {'text': '(Tyler Simpson)',\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'Windows WSL and VS Code\\nIf you have a Windows 11 device and would like to use the built in WSL to access linux you can use the Microsoft Learn link Set up a WSL development environment | Microsoft Learn. To connect this to VS Code download the Microsoft verified VS Code extension ‘WSL’ this will allow you to remotely connect to your WSL Ubuntu instance as if it was a virtual machine.',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '314ebe32'},\n",
       " {'text': 'This is my first time using Github to upload a code. I was getting the below error message when I type\\ngit push -u origin master:\\nerror: src refspec master does not match any\\nerror: failed to push some refs to \\'https://github.com/XXXXXX/1st-Homework.git\\'\\nSolution:\\nThe error message got fixed by running below commands:\\ngit commit -m \"initial commit\"\\ngit push origin main\\nIf this is your first time to use Github, you will find a great & straightforward tutorial in this link https://dennisivy.com/github-quickstart\\n(Asia Saeed)\\nYou can also use the “upload file” functionality from GitHub for that\\nIf you write your code on Google colab you can also directly share it on your Github.\\n(By Pranab Sarma)',\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'Uploading the homework to Github',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '98cff602'},\n",
       " {'text': \"I'm trying to invert the matrix but I got error that the matrix is singular matrix\\nThe singular matrix error is caused by the fact that not every matrix can be inverted. In particular, in the homework it happens because you have to pay close attention when dealing with multiplication (the method .dot) since multiplication is not commutative! X.dot(Y) is not necessarily equal to Y.dot(X), so respect the order otherwise you get the wrong matrix.\",\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'Singular Matrix Error',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '54ec0de4'},\n",
       " {'text': 'I have a problem with my terminal. Command\\nconda create -n ml-zoomcamp python=3.9\\ndoesn’t work. Any of 3.8/ 3.9 / 3.10 should be all fine\\nIf you’re on Windows and just installed Anaconda, you can use Anaconda’s own terminal called “Anaconda Prompt”.\\nIf you don’t have Anaconda or Miniconda, you should install it first\\n(Tatyana Mardvilko)',\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'Conda is not an internal command',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'f81f4ecb'},\n",
       " {'text': 'How do I read the dataset with Pandas in Windows?\\nI used the code below but not working\\ndf = pd.read_csv(\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\nUnlike Linux/Mac OS, Windows uses the backslash (\\\\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the \\'\\\\\\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, \"\\\\n\" to add a new line or \"\\\\t\" to add spaces, etc. To avoid the issue we just need to add \"r\" before the file path and Python will treat it as a literal string (not an escape sequence).\\nHere’s how we should be loading the file instead:\\ndf = pd.read_csv(r\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\n(Muhammad Awon)',\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'Read-in the File in Windows OS',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'be760b92'},\n",
       " {'text': 'Type the following command:\\ngit config -l | grep url\\nThe output should look like this:\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\nChange this to the following format and make sure the change is reflected using command in step 1:\\ngit remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n(Added by Dheeraj Karra)',\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': \"'403 Forbidden' error message when you try to push to a GitHub repository\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'a2cfa1c9'},\n",
       " {'text': \"I had a problem when I tried to push my code from Git Bash:\\nremote: Support for password authentication was removed on August 13, 2021.\\nremote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\\nfatal: Authentication failed for 'https://github.com/username\\nSolution:\\nCreate a personal access token from your github account and use it when you make a push of your last changes.\\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\\nBruno Bedón\",\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': \"Fatal: Authentication failed for 'https://github.com/username\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '7b907071'},\n",
       " {'text': \"In Kaggle, when you are trying to !wget a dataset from github (or any other public repository/location), you get the following error:\\nGetting  this error while trying to import data- !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.\\nwget: unable to resolve host address 'raw.githubusercontent.com'\\nSolution:\\nIn your Kaggle notebook settings, turn on the Internet for your session. It's on the settings panel, on the right hand side of the Kaggle screen. You'll be asked to verify your phone number so Kaggle knows you are not a bot.\",\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': \"wget: unable to resolve host address 'raw.githubusercontent.com'\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'fc2e0a61'},\n",
       " {'text': 'I found this video quite helpful: Creating Virtual Environment for Python from VS Code\\n[Native Jupiter Notebooks support in VS Code] In VS Code you can also have a native Jupiter Notebooks support, i.e. you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled + run a ‘jupyter notebook ‘ command from a remote machine + have a remote connection configured in .ssh/config (as Alexey’s video suggests) - VS Code can execute remote Jupyter Notebooks files on a remote server from your local machine: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\\n[Git support from VS Code] You can work with Github from VSCode - staging and commits are easy from the VS Code’s UI:  https://code.visualstudio.com/docs/sourcecontrol/overview\\n(Added by Ivan Brigida)',\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'Setting up an environment using VS Code',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'd43e5742'},\n",
       " {'text': 'With regards to creating an environment for the project, do we need to run the command \"conda create -n .......\" and \"conda activate ml-zoomcamp\" everytime we open vs code to work on the project?\\nAnswer:\\n\"conda create -n ....\" is just run the first time to create the environment. Once created, you just need to run \"conda activate ml-zoomcamp\" whenever you want to use it.\\n(Added by Wesley Barreto)\\nconda env export > environment.yml will also allow you to reproduce your existing environment in a YAML file.  You can then recreate it with conda env create -f environment.yml',\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'Conda Environment Setup',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '32bc0538'},\n",
       " {'text': \"I was doing Question 7 from Week1 Homework and with step6: Invert XTX, I created the inverse. Now, an inverse when multiplied by the original matrix should return in an Identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:\\nInverse * Original:\\n[[ 1.00000000e+00 -1.38777878e-16]\\n[ 3.16968674e-13  1.00000000e+00]]\\nSolution:\\nIt's because floating point math doesn't work well on computers as shown here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken\\n(Added by Wesley Barreto)\",\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'Floating Point Precision',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'b6730228'},\n",
       " {'text': 'Answer:\\nIt prints the information about the dataset like:\\nIndex datatype\\nNo. of entries\\nColumn information with not-null count and datatype\\nMemory usage by dataset\\nWe use it as:\\ndf.info()\\n(Added by Aadarsha Shrestha & Emoghena Itakpe)',\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'What does pandas.DataFrame.info() do?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '3ce9bbb8'},\n",
       " {'text': \"Pandas and numpy libraries are not being imported\\nNameError: name 'np' is not defined\\nNameError: name 'pd' is not defined\\nIf you're using numpy or pandas, make sure you use the first few lines before anything else.\\nimport pandas as pd\\nimport numpy as np\\nAdded by Manuel Alejandro Aponte\",\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': \"NameError: name 'np' is not defined\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '4e584d06'},\n",
       " {'text': \"What if there were hundreds of columns? How do you get the columns only with numeric or object data in a more concise way?\\ndf.select_dtypes(include=np.number).columns.tolist()\\ndf.select_dtypes(include='object').columns.tolist()\\nAdded by Gregory Morris\",\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'How to select column by dtype',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'ff4da2b6'},\n",
       " {'text': 'There are many ways to identify the shape of dataset, one of them is using .shape attribute!\\ndf.shape\\ndf.shape[0] # for identify the number of rows\\ndf.shape[1] # for identify the number of columns\\nAdded by Radikal Lukafiardi',\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'How to identify the shape of dataset in Pandas',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '58c1c168'},\n",
       " {'text': 'First of all use np.dot for matrix multiplication. When you compute matrix-matrix multiplication you should understand that order of multiplying is crucial and affects the result of the multiplication!\\nDimension Mismatch\\nTo perform matrix multiplication, the number of columns in the 1st matrix should match the number of rows in the 2nd matrix. You can rearrange the order to make sure that this satisfies the condition.\\nAdded by Leah Gotladera',\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'How to avoid Value errors with array shapes in homework?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '96076a1a'},\n",
       " {'text': 'You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.\\nThis method is called imputing - when you have NaN/ null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns.\\nAdded by Anneysha Sarkar',\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'Question 5: How and why do we replace the NaN values with average of the column?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '3218389a'},\n",
       " {'text': 'In Question 7 we are asked to calculate\\nThe initial problem  can be solved by this, where a Matrix X is multiplied by some unknown weights w resulting in the target y.\\nAdditional reading and videos:\\nOrdinary least squares\\nMultiple Linear Regression in Matrix Form\\nPseudoinverse Solution to OLS\\nAdded by Sylvia Schmitt\\nwith commends from Dmytro Durach',\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'Question 7: Mathematical formula for linear regression',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '183a1c90'},\n",
       " {'text': 'This is most likely that you interchanged the first step of the multiplication\\nYou used  instead of\\nAdded by Emmanuel Ikpesu',\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'Question 7: FINAL MULTIPLICATION not having 5 column',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'f0bc1c19'},\n",
       " {'text': 'Note, that matrix multiplication (matrix-matrix, matrix-vector multiplication) can be written as * operator in some sources, but performed as @ operator or np.matmul() via numpy. * operator performs element-wise multiplication (Hadamard product).\\nnumpy.dot() or ndarray.dot() can be used, but for matrix-matrix multiplication @ or np.matmul() is preferred (as per numpy doc).\\nIf multiplying by a scalar numpy.multiply() or * is preferred.\\nAdded by Andrii Larkin',\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'Question 7: Multiplication operators.',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '735e6c78'},\n",
       " {'text': 'If you face an error kind of ImportError: cannot import name \\'contextfilter\\' from \\'jinja2\\' (anaconda\\\\lib\\\\site-packages\\\\jinja2\\\\__init__.py) when launching a new notebook for a brand new environment.\\nSwitch to the main environment and run \"pip install nbconvert --upgrade\".\\nAdded by George Chizhmak',\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'Error launching Jupyter notebook',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'b8ca1cd3'},\n",
       " {'text': 'If you face this situation and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. Then try again',\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv hangs on MacOS Ventura M1',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'efdb235f'},\n",
       " {'text': \"Wget doesn't ship with macOS, so there are other alternatives to use.\\nNo worries, we got curl:\\nexample:\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nExplanations:\\ncurl: a utility for retrieving information from the internet.\\n-o: Tell it to store the result as a file.\\nfilename: You choose the file's name.\\nLinks: Put the web address (URL) here, and cURL will extract data from it and save it under the name you provide.\\nMore about it at:\\nCurl Documentation\\nAdded by David Espejo\",\n",
       "  'section': '1. Introduction to Machine Learning',\n",
       "  'question': 'In case you are using mac os and having trouble with WGET',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '355348f0'},\n",
       " {'text': \"You can use round() function or f-strings\\nround(number, 4)  - this will round number up to 4 decimal places\\nprint(f'Average mark for the Homework is {avg:.3f}') - using F string\\nAlso there is pandas.Series. round idf you need to round values in the whole Series\\nPlease check the documentation\\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\\nAdded by Olga Rudakova\",\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'How to output only a certain number of decimal places',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '67afabf5'},\n",
       " {'text': 'Here are the crucial links for this Week 2 that starts September 18, 2023\\nAsk questions for Live Sessions: https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions\\nCalendar for weekly meetings: https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1\\nWeek 2 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md\\nSubmit HW Week 2: https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform (also available at the bottom of the above link)\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 2.X --- https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12\\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n~~Nukta Bhatia~~',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'How do I get started with Week 2?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '50d737e7'},\n",
       " {'text': 'We can use histogram:\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n# Load the data\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\ndf = pd.read_csv(url)\\n# EDA\\nsns.histplot(df[\\'median_house_value\\'], kde=False)\\nplt.show()\\nOR ceck skewness and describe:\\nprint(df[\\'median_house_value\\'].describe())\\n# Calculate the skewness of the \\'median_house_value\\' variable\\nskewness = df[\\'median_house_value\\'].skew()\\n# Print the skewness value\\nprint(\"Skewness of \\'median_house_value\\':\", skewness)\\n(Mohammad Emad Sharifi)',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Checking long tail of data',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'bbc0fca3'},\n",
       " {'text': 'It’s possible that when you follow the videos, you’ll get a Singular Matrix error. We will explain why it happens in the Regularization video. Don’t worry, it’s normal that you have it.\\nYou can also have an error because you did the inverse of X once in your code and you’re doing it a second time.\\n(Added by Cécile Guillot)',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'LinAlgError: Singular matrix',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '6f3bdd20'},\n",
       " {'text': 'You can find a detailed description of the dataset ere https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\\nKS',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'California housing dataset',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '27c2d90a'},\n",
       " {'text': 'I was using for loops to apply rmse to list of y_val and y_pred. But the resulting rmse is all nan.\\nI found out that the problem was when my data reached the mean step after squaring the error in the rmse function. Turned out there were nan in the array, then I traced the problem back to where I first started to split the data: I had only use fillna(0) on the train data, not on the validation and test data. So the problem was fixed after I applied fillna(0) to all the dataset (train, val, test). Voila, my for loops to get rmse from all the seed values work now.\\nAdded by Sasmito Yudha Husada',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Getting NaNs after applying .mean()',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '88e9600a'},\n",
       " {'text': 'Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?\\nOnly if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable.\\nThis can help to understand skewness and how it can be applied to the distribution of your data set.\\nhttps://en.wikipedia.org/wiki/Skewness\\nPastor Soto',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Target variable transformation',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'd59d8df7'},\n",
       " {'text': 'The dataset can be read directly to pandas dataframe from the github link using the technique shown below\\ndfh=pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\nKrishna Anand',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Reading the dataset directly from github',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '0b3eaf92'},\n",
       " {'text': \"For users of kaggle notebooks, the dataset can be loaded through widget using the below command. Please remember that ! before wget is essential\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nOnce the dataset is loaded to the kaggle notebook server, it can be read through the below pandas command\\ndf = pd.read_csv('housing.csv')\\nHarish Balasundaram\",\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Loading the dataset directly through Kaggle Notebooks',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '8fe56032'},\n",
       " {'text': 'We can filter a dataset by using its values as below.\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\nYou can use | for ‘OR’, and & for ‘AND’\\nAlternative:\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\nRadikal Lukafiardi',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Filter a dataset by using its values',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'af833e0a'},\n",
       " {'text': 'Above users showed how to load the dataset directly from github. Here is another useful way of doing this using the `requests` library:\\n# Get data for homework\\nimport requests\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\nif response.status_code == 200:\\nwith open(\\'housing.csv\\', \\'wb\\') as file:\\nfile.write(response.content)\\nelse:\\nprint(\"Download failed.\")\\nTyler Simpson',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Alternative way to load the data using requests',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '8d209d6d'},\n",
       " {'text': 'When creating a duplicate of your dataframe by doing the following:\\nX_train = df_train\\nX_val = df_val\\nYou’re still referencing the original variable, this is called a shallow copy. You can make sure that no references are attaching both variables and still keep the copy of the data do the following to create a deep copy:\\nX_train = df_train.copy()\\nX_val = df_val.copy()\\nAdded by Ixchel García',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Null column is appearing even if I applied .fillna()',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '0bc4c3da'},\n",
       " {'text': 'Yes, you can. Here we implement it ourselves to better understand how it works, but later we will only rely on Scikit-Learn’s functions. If you want to start using it earlier — feel free to do it',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Can I use Scikit-Learn’s train_test_split for this week?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'c0ee2665'},\n",
       " {'text': 'Yes, you can. We will also do that next week, so don’t worry, you will learn how to do it.',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Can I use LinearRegression from Scikit-Learn for this week?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '3f60871d'},\n",
       " {'text': 'What are equivalents in Scikit-Learn for the linear regression with and without regularization used in week 2.\\nCorresponding function for model without regularization:\\nsklearn.linear_model.LinearRegression\\nCorresponding function for model with regularization:\\nsklearn.linear_model.Ridge\\nThe linear model from Scikit-Learn are explained  here:\\nhttps://scikit-learn.org/stable/modules/linear_model.html\\nAdded by Sylvia Schmitt',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Corresponding Scikit-Learn functions for Linear Regression (with and without Regularization)',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'f30217a7'},\n",
       " {'text': '`r` is a regularization parameter.\\nIt’s similar to `alpha` in sklearn.Ridge(), as both control the \"strength\" of regularization (increasing both will lead to stronger regularization), but mathematically not quite, here\\'s how both are used:\\nsklearn.Ridge()\\n||y - Xw||^2_2 + alpha * ||w||^2_2\\nlesson’s notebook (`train_linear_regression_reg` function)\\nXTX = XTX + r * np.eye(XTX.shape[0])\\n`r` adds “noise” to the main diagonal to prevent multicollinearity, which “breaks” finding inverse matrix.',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Question 4: what is `r`, is it the same as `alpha` in sklearn.Ridge()?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '91fc573d'},\n",
       " {'text': 'Q: “In lesson 2.8 why is y_pred different from y? After all, we trained X_train to get the weights that when multiplied by X_train should give exactly y, or?”\\nA: linear regression is a pretty simple model, it neither can nor should fit 100% (nor any other model, as this would be the sign of overfitting). This picture might illustrate some intuition behind this, imagine X is a single feature:\\nAs our model is linear, how would you draw a line to fit all the \"dots\"?\\nYou could \"fit\" all the \"dots\" on this pic using something like scipy.optimize.curve_fit (non-linear least squares) if you wanted to, but imagine how it would perform on previously unseen data.\\nAdded by Andrii Larkin',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Why linear regression doesn’t provide a “perfect” fit?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'fe3139f6'},\n",
       " {'text': 'One of the questions on the homework calls for using a random seed of 42. When using 42, all my missing values ended up in my training dataframe and not my validation or test dataframes, why is that?\\nThe purpose of the seed value is to randomly generate the proportion split. Using a seed of 42 ensures that all learners are on the same page by getting the same behavior (in this case, all missing values ending up in the training dataframe). If using a different seed value (e.g. 9), missing values will then appear in all other dataframes.',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Random seed 42',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '48aac030'},\n",
       " {'text': 'It is possible to do the shuffling of the dataset with the pandas built-in function pandas.DataFrame.sample.The complete dataset can be shuffled including resetting the index with the following commands:\\nSetting frac=1 will result in returning a shuffled version of the complete Dataset.\\nSetting random_state=seed will result in the same randomization as used in the course resources.\\ndf_shuffled = df.sample(frac=1, random_state=seed)\\ndf_shuffled.reset_index(drop=True, inplace=True)\\nAdded by Sylvia Schmitt',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Shuffling the initial dataset using pandas built-in function',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '28321bc2'},\n",
       " {'text': 'That’s normal. We all have different environments: our computers have different versions of OS and different versions of libraries — even different versions of Python.\\nIf it’s the case, just select the option that’s closest to your answer',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': \"The answer I get for one of the homework questions doesn't match any of the options. What should I do?\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'edb92d22'},\n",
       " {'text': \"In question 3 of HW02 it is mentioned: ‘For computing the mean, use the training only’. What does that mean?\\nIt means that you should use only the training data set for computing the mean, not validation or  test data set. This is how you can calculate the mean\\ndf_train['column_name'].mean( )\\nAnother option:\\ndf_train[‘column_name’].describe()\\n(Bhaskar Sarma)\",\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Meaning of mean in homework 2, question 3',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'f488ce85'},\n",
       " {'text': 'When the target variable has a long tail distribution, like in prices, with a wide range, you can transform the target variable with np.log1p() method, but be aware if your target variable has negative values, this method will not work',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'When should we transform the target variable to logarithm distribution?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'bf395099'},\n",
       " {'text': 'If we try to perform an arithmetic operation between 2 arrays of different shapes or different dimensions, it throws an error like operands could not be broadcast together with shapes. There are some scenarios when broadcasting can occur and when it fails.\\nIf this happens sometimes we can use * operator instead of dot() method to solve the issue. So that the error is solved and also we get the dot product.\\n(Santhosh Kumar)',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'ValueError: shapes not aligned',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '01cd3b35'},\n",
       " {'text': 'Copy of a dataframe is made with X_copy = X.copy().\\nThis is called creating a deep copy.  Otherwise it will keep changing the original dataframe if used like this: X_copy = X.\\nAny changes to X_copy will reflect back to X. This is not a real copy, instead it is a “view”.\\n(Memoona Tahira)',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'How to copy a dataframe without changing the original dataframe?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '5551c92e'},\n",
       " {'text': 'One of the most important characteristics of the normal distribution is that mean=median=mode, this means that the most popular value, the mean of the distribution and 50% of the sample are under the same value, this is equivalent to say that the area under the curve (black) is the same on the left and on the right. The long tail (red curve) is the result of having a few observations with high values, now the behaviour of the distribution changes, first of all, the area is different on each side and now the mean, median and mode are different. As a consequence, the mean is no longer representative, the range is larger than before and the probability of being on the left or on the right is not the same.\\n(Tatiana Dávila)',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'What does ‘long tail’ mean?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '94f928d2'},\n",
       " {'text': 'In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. [Wikipedia] The formula to calculate standard deviation is:\\n(Aadarsha Shrestha)',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'What is standard deviation?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '266faa6d'},\n",
       " {'text': 'The application of regularization depends on the specific situation and problem. It is recommended to consider it when training machine learning models, especially with small datasets or complex models, to prevent overfitting. However, its necessity varies depending on the data quality and size. Evaluate each case individually to determine if it is needed.\\n(Daniel Muñoz Viveros)',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Do we need to apply regularization techniques always? Or only in certain scenarios?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'c21f99f5'},\n",
       " {'text': 'As it speeds up the development:\\nprepare_df(initial_df, seed, fill_na_type)  - that prepared all 3 dataframes and 3 y_vectors. Fillna() can be done before the initial_df is split.\\nOf course, you can reuse other functions: rmse() and train_linear_regression(X,y,r) from the class notebook\\n(Ivan Brigida)',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Shortcut: define functions for faster execution',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '13702957'},\n",
       " {'text': 'If we have a list or series of data for example x = [1,2,3,4,5]. We can use pandas to find the standard deviation. We can pass our list into panda series and call standard deviation directly on the series pandas.Series(x).std().\\n(Quinn Avila)',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'How to use pandas to find standard deviation',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '7cd652c5'},\n",
       " {'text': 'Numpy and Pandas packages use different equations to compute the standard deviation. Numpy uses  population standard deviation, whereas pandas uses sample standard deviation by default.\\nNumpy\\nPandas\\npandas default standard deviation is computed using one degree of freedom. You can change degree in of freedom in NumPy to change this to unbiased estimator by using ddof parameter:\\nimport numpy as np\\nnp.std(df.weight, ddof=1)\\nThe result will be similar if we change the dof = 1 in numpy\\n(Harish Balasundaram)',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Standard Deviation Differences in Numpy and Pandas',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'e1f93d10'},\n",
       " {'text': \"In pandas you can use built in Pandas function names std() to get standard deviation. For example\\ndf['column_name'].std() to get standard deviation of that column.\\ndf[['column_1', 'column_2']].std() to get standard deviation of multiple columns.\\n(Khurram Majeed)\",\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Standard deviation using Pandas built in Function',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '36b9d1b7'},\n",
       " {'text': 'Use ‘pandas.concat’ function (https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to combine two dataframes. To combine two numpy arrays use numpy.concatenate (https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) function. So the code would be as follows:\\ndf_train_combined = pd.concat([df_train, df_val])\\ny_train = np.concatenate((y_train, y_val), axis=0)\\n(George Chizhmak)',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'How to combine train and validation datasets',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '3c8b32a1'},\n",
       " {'text': 'The Root Mean Squared Error (RMSE) is one of the primary metrics to evaluate the performance of a regression model. It calculates the average deviation between the model\\'s predicted values and the actual observed values, offering insight into the model\\'s ability to accurately forecast the target variable. To calculate RMSE score:\\nLibraries needed\\nimport numpy as np\\nfrom sklearn.metrics import mean_squared_error\\nmse = mean_squared_error(actual_values, predicted_values)\\nrmse = np.sqrt(mse)\\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\\n(Aminat Abolade)',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Understanding RMSE and how to calculate RMSE score',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '05fb3a16'},\n",
       " {'text': 'If you would like to use multiple conditions as an example below you will get the error. The correct syntax for OR is |, and for AND is &\\n(Olga Rudakova)\\n–',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'What syntax use in Pandas for multiple conditions using logical AND and OR',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '225506b9'},\n",
       " {'text': 'I found this video pretty usual for understanding how we got the normal form with linear regression Normal Equation Derivation for Regression',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Deep dive into normal equation for regression',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'bd4a1395'},\n",
       " {'text': '(Hrithik Kumar Advani)',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Useful Resource for Missing Data Treatment\\nhttps://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python/notebook',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '81b8e8d0'},\n",
       " {'text': 'The instruction for applying log transformation to the ‘median_house_value’ variable is provided before Q3 in the homework for Week-2 under the ‘Prepare and split the dataset’ heading.\\nHowever, this instruction is absent in the subsequent questions of the homework, and I got stuck with Q5 for a long time, trying to figure out why my RMSE was so huge, when it clicked to me that I forgot to apply log transformation to the target variable. Please remember to apply log transformation to the target variable for each question.\\n(Added by Soham Mundhada)',\n",
       "  'section': '2. Machine Learning for Regression',\n",
       "  'question': 'Caution for applying log transformation in Week-2 2023 cohort homework',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'a7f6a33c'},\n",
       " {'text': 'Version 0.24.2 and Python 3.8.11\\n(Added by Diego Giraldo)',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'What sklearn version is Alexey using in the youtube videos?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '129b4ac0'},\n",
       " {'text': 'Week 3 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/03-classification/homework.md\\nSubmit HW Week 3: https://docs.google.com/forms/d/e/1FAIpQLSeXS3pqsv_smRkYmVx-7g6KIZDnG29g2s7pdHo-ASKNqtfRFQ/viewform\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 3.X --- https://www.youtube.com/watch?v=0Zw04wdeTQo&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=29\\n~~Nukta Bhatia~~',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'How do I get started with Week 3?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'b8cca8b7'},\n",
       " {'text': \"The error message “could not convert string to float: ‘Nissan’” typically occurs when a machine learning model or function is expecting numerical input, but receives a string instead. In this case, it seems like the model is trying to convert the car brand ‘Nissan’ into a numerical value, which isn’t possible.\\nTo resolve this issue, you can encode categorical variables like car brands into numerical values. One common method is one-hot encoding, which creates new binary columns for each category/label present in the original column.\\nHere’s an example of how you can perform one-hot encoding using pandas:\\nimport pandas as pd\\n# Assuming 'data' is your DataFrame and 'brand' is the column with car brands\\ndata_encoded = pd.get_dummies(data, columns=['brand'])\\nIn this code, pd.get_dummies() creates a new DataFrame where the ‘brand’ column is replaced with binary columns for each brand (e.g., ‘brand_Nissan’, ‘brand_Toyota’, etc.). Each row in the DataFrame has a 1 in the column that corresponds to its brand and 0 in all other brand columns.\\n-Mohammad Emad Sharifi-\",\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': \"Could not convert string to float:’Nissan’rt string to float: 'Nissan'\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '1091b10f'},\n",
       " {'text': 'Solution: Mutual Information score calculates the relationship between categorical variables or discrete variables. So in the homework, because the target which is median_house_value is continuous, we had to change it to binary format which in other words, makes its values discrete as either 0 or 1. If we allowed it to remain in the continuous variable format, the mutual information score could be calculated, but the algorithm would have to divide the continuous variables into bins and that would be highly subjective. That is why continuous variables are not used for mutual information score calculation.\\n—Odimegwu David—-',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'Why did we change the targets to binary format when calculating mutual information score in the homework?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '0c7715a1'},\n",
       " {'text': \"Q2 asks about correlation matrix and converting median_house_value from numeric to binary. Just to make sure here we are only dealing with df_train not df_train_full, right? As the question explicitly mentions the train dataset.\\nYes. I think it is only on df_train. The reason behind this is that df_train_full also contains the validation dataset, so at this stage we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data.\\nPastor Soto\",\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'What data should we use for correlation matrix',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'd2043cf5'},\n",
       " {'text': \"The background of any dataframe can be colored (not only the correlation matrix) based on the numerical values the dataframe contains by using the method pandas.io.formats.style.Styler.background_graident.\\nHere an example on how to color the correlation matrix. A color map of choice can get passed, here ‘viridis’ is used.\\n# ensure to have only numerical values in the dataframe before calling 'corr'\\ncorr_mat = df_numerical_only.corr()\\ncorr_mat.style.background_gradient(cmap='viridis')\\nHere is an example of how the coloring will look like using a dataframe containing random values and applying “background_gradient” to it.\\nnp.random.seed = 3\\ndf_random = pd.DataFrame(data=np.random.random(3*3).reshape(3,3))\\ndf_random.style.background_gradient(cmap='viridis')\\nAdded by Sylvia Schmitt\",\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'Coloring the background of the pandas.DataFrame.corr correlation matrix directly',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '44d22817'},\n",
       " {'text': 'data_corr = pd.DataFrame(data_num.corr().round(3).abs().unstack().sort_values(ascending=False))\\ndata_corr.head(10)\\nAdded by Harish Balasundaram\\nYou can also use seaborn to create a heatmap with the correlation. The code for doing that:\\nsns.heatmap(df[numerical_features].corr(),\\nannot=True,\\nsquare=True,\\nfmt=\".2g\",\\ncmap=\"crest\")\\nAdded by Cecile Guillot\\nYou can refine your heatmap and plot only a triangle, with a blue to red color gradient, that will show every correlation between your numerical variables without redundant information with this function:\\nWhich outputs, in the case of churn dataset:\\n(Mélanie Fouesnard)',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'Identifying highly correlated feature pairs easily through unstack',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '1f76dbeb'},\n",
       " {'text': \"Should we perform EDA on the base of train or train+validation or train+validation+test dataset?\\nIt's indeed good practice to only rely on the train dataset for EDA. Including validation might be okay. But we aren't supposed to touch the test dataset, even just looking at it isn't a good idea. We indeed pretend that this is the future unseen data\\nAlena Kniazeva\",\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'What data should be used for EDA?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'b8071a54'},\n",
       " {'text': 'Validation dataset helps to validate models and prediction on unseen data. This helps get an estimate on its performance on fresh data. It helps optimize the model.\\nEdidiong Esu\\nBelow is an extract of Alexey\\'s book explaining this point. Hope is useful\\nWhen we apply the fit method, this method is looking at the content of the df_train dictionaries we are passing to the DictVectorizer instance, and fit is figuring out (training) how to map the values of these dictionaries. If categorical, applies one-hot encoding, if numerical it will leave it as it is.\\nWith this context, if we apply the fit to the validation model, we are \"giving the answers\" and we are not letting the \"fit\" do its job for data that we haven\\'t seen. By not applying the fit to the validation model we can know how well it was trained.\\nBelow is an extract of Alexey\\'s book explaining this point.\\nHumberto Rodriguez\\nThere is no need to initialize another instance of dictvectorizer after fitting it on the train set as it will overwrite what it learnt from being fit on the train data.\\nThe correct way is to fit_transform the train set, and only transform the validation and test sets.\\nMemoona Tahira',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'Fitting DictVectorizer on validation',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'b8da9037'},\n",
       " {'text': 'For Q5 in homework, should we calculate the smallest difference in accuracy in real values (i.e. -0.001 is less than -0.0002) or in absolute values (i.e. 0.0002 is less than 0.001)?\\nWe should select the “smallest” difference, and not the “lowest”, meaning we should reason in absolute values.\\nIf the difference is negative, it means that the model actually became better when we removed the feature.',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'Feature elimination',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '467e0cec'},\n",
       " {'text': \"Instead use the method “.get_feature_names_out()” from DictVectorizer function and the warning will be resolved , but we need not worry about the waning as there won't be any warning\\nSanthosh Kumar\",\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'b69f32f6'},\n",
       " {'text': 'Fitting the logistic regression takes a long time / kernel crashes when calling predict() with the fitted model.\\nMake sure that the target variable for the logistic regression is binary.\\nKonrad Muehlberg',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'Logistic regression crashing Jupyter kernel',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '3b3b1989'},\n",
       " {'text': 'Ridge regression is a linear regression technique used to mitigate the problem of multicollinearity (when independent variables are highly correlated) and prevent overfitting in predictive modeling. It adds a regularization term to the linear regression cost function, penalizing large coefficients.\\nsag Solver: The sag solver stands for \"Stochastic Average Gradient.\" It\\'s particularly suitable for large datasets, as it optimizes the regularization term using stochastic gradient descent (SGD). sag can be faster than some other solvers for large datasets.\\nAlpha: The alpha parameter  controls the strength of the regularization in Ridge regression. A higher alpha value leads to stronger regularization, which means the model will have smaller coefficient values, reducing the risk of overfitting.\\nfrom sklearn.linear_model import Ridge\\nridge = Ridge(alpha=alpha, solver=\\'sag\\', random_state=42)\\nridge.fit(X_train, y_train)\\nAminat Abolade',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'Understanding Ridge',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'eb5771a0'},\n",
       " {'text': 'DictVectorizer(sparse=True) produces CSR format, which is both more memory efficient and converges better during fit(). Basically it stores non-zero values and indices instead of adding a column for each class of each feature (models of cars produced 900+ columns alone in the current task).\\nUsing “sparse” format like on the picture above, both via pandas.get_dummies() and DictVectorizer(sparse=False) - is slower (around 6-8min for Q6 task - Linear/Ridge Regression) for high amount of classes (like models of cars for eg) and gives a bit “worse” results in both Logistic and Linear/Ridge Regression, while also producing convergence warnings for Linear/Ridge Regression.\\nLarkin Andrii',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'pandas.get_dummies() and DictVectorizer(sparse=False) produce the same type of one-hot encodings:',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'bca10281'},\n",
       " {'text': 'Ridge with sag solver requires feature to be of the same scale. You may get the following warning: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\\nPlay with different scalers. See notebook-scaling-ohe.ipynb\\nDmytro Durach\\n(Oscar Garcia)  Use a StandardScaler for the numeric fields and OneHotEncoder (sparce = False) for the categorical features.  This help with the warning. Separate the features (num/cat) without using the encoder first and see if that helps.',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'Convergence Problems in W3Q6',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '34a8edb0'},\n",
       " {'text': \"When encountering convergence errors during the training of a Ridge regression model, consider the following steps:\\nFeature Normalization: Normalize your numerical features using techniques like MinMaxScaler or StandardScaler. This ensures that numerical features are on a \\tsimilar scale, preventing convergence issues.\\nCategorical Feature Encoding: If your dataset includes categorical features, apply \\tcategorical encoding techniques such as OneHotEncoder (OHE) to \\tconvert them into a numerical format. OHE is commonly used to represent categorical variables as binary vectors, making them compatible with regression models like Ridge.\\nCombine Features: After \\tnormalizing numerical features and encoding categorical features using OneHotEncoder, combine them to form a single feature matrix (X_train). This combined dataset serves as the input for training the Ridge regression model.\\nBy following these steps, you can address convergence errors and enhance the stability of your Ridge model training process. It's important to note that the choice of encoding method, such as OneHotEncoder, is appropriate for handling categorical features in this context.\\nYou can find an example here.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tOsman Ali\",\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'Dealing with Convergence in Week 3 q6',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'f625307b'},\n",
       " {'text': 'A sparse matrix is more memory-efficient because it only stores the non-zero values and their positions in memory. This is particularly useful when working with large datasets with many zero or missing values.\\nThe default DictVectorizer configuration is a sparse matrix. For week3 Q6 using the default sparse is an interesting option because of the size of the matrix. Training the model was also more performant and didn’t give an error message like dense mode.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tQuinn Avila',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'Sparse matrix compared dense matrix',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '7fa98526'},\n",
       " {'text': 'The warnings on the jupyter notebooks can be disabled/ avoided with the following comments:\\nImport warnings\\nwarnings.filterwarnings(“ignore”)\\nKrishna Anand',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'How  to Disable/avoid Warnings in Jupyter Notebooks',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '0807f0f3'},\n",
       " {'text': 'Question: Regarding RMSE, how do we decide on the correct score to choose? In the study group discussion    about week two homework, all of us got it wrong and one person had the lowest score selected as well.\\nAnswer: You need to find RMSE for each alpha. If RMSE scores  are equal, you will select the lowest alpha.\\nAsia Saeed',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'How to select the alpha parameter in Q6',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '6d0fb418'},\n",
       " {'text': 'Question: Could you please help me with HW3 Q3: \"Calculate the mutual information score with the (binarized) price for the categorical variable that we have. Use the training set only.\" What is the second variable that we need to use to calculate the mutual information score?\\nAnswer: You need to calculate the mutual info score between the binarized price (above_average) variable & ocean_proximity, the only original categorical variable in the dataset.\\nAsia Saeed',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'Second variable that we need to use to calculate the mutual information score',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'fbda1f40'},\n",
       " {'text': 'Do we need to train the model only with the features: total_rooms, total_bedrooms, population and households? or with all the available features and then pop once at a time each of the previous features and train the model to make the accuracy comparison?\\nYou need to create a list of all features in this question and evaluate the model one time to obtain the accuracy, this will be the original accuracy, and then remove one feature each time, and in each time, train the model, find the accuracy and the difference between the original accuracy and the found accuracy. Finally, find out which feature has the smallest absolute accuracy difference.\\nWhile calculating differences between accuracy scores while training on the whole model, versus dropping one feature at a time and comparing its accuracy to the model to judge impact of the feature on the accuracy of the model, do we take the smallest difference or smallest absolute difference?\\nSince order of subtraction between the two accuracy scores can result in a negative number, we will take its absolute value as we are interested in the smallest value difference, not the lowest difference value. Case in point, if difference is -4 and -2, the smallest difference is abs(-2), and not abs(-4)',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'Features for homework Q5',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '0f88b7ac'},\n",
       " {'text': 'Both work in similar ways. That is, to convert categorical features to numerical variables for use in training the model. But the difference lies in the input. OneHotEncoder uses an array as input while DictVectorizer uses a dictionary.\\nBoth will produce the same result. But when we use OneHotEncoder, features are sorted alphabetically. When you use DictVectorizer you stack features that you want.\\nTanya Mard',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'What is the difference between OneHotEncoder and DictVectorizer?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '9ffcc895'},\n",
       " {'text': 'They are basically the same. There are some key differences with regards to their input/output types, handling of missing values, etc, but they are both techniques to one-hot-encode categorical variables with identical results. The biggest difference is get_dummies are a convenient choice when you are working with Pandas Dataframes, while if you are building a scikit-learn-based machine learning pipeline and need to handle categorical data as part of that pipeline, OneHotEncoder is a more suitable choice. [Abhirup Ghosh]',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'What is the difference between pandas get_dummies and sklearn OnehotEncoder?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '94a3b2fb'},\n",
       " {'text': \"For the test_train_split question on week 3's homework, are we supposed to use 42 as the random_state in both splits or only the 1st one?\\nAnswer: for both splits random_state = 42 should be used\\n(Bhaskar Sarma)\",\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'Use of random seed in HW3',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'fb9a45d8'},\n",
       " {'text': 'Should correlation be calculated after splitting or before splitting. And lastly I know how to find the correlation but how do i find the two most correlated features.\\nAnswer: Correlation matrix of your train dataset. Thus, after splitting. Two most correlated features are the ones having the highest correlation coefficient in terms of absolute values.',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'Correlation before or after splitting the data',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'e31051f7'},\n",
       " {'text': 'Make sure that the features used in ridge regression model are only NUMERICAL ones not categorical.\\nDrop all categorical features first before proceeding.\\n(Aileah Gotladera)\\nWhile it is True that ridge regression accepts only numerical values, the categorical ones can be useful for your model. You have to transform them using one-hot encoding before training the model. To avoid the error of non convergence, put sparse=True when doing so.\\n(Erjon)',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'Features in Ridge Regression Model',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '493b7b59'},\n",
       " {'text': \"You need to use all features. and price for target. Don't include the average variable we created before.\\nIf you use DictVectorizer then make sure to use sparce=True to avoid convergence errors\\nI also used StandardScalar for numerical variable you can try running with or without this\\n(Peter Pan)\",\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'Handling Column Information for Homework 3 Question 6',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '4a55c510'},\n",
       " {'text': 'Use sklearn.preprocessing encoders and scalers, e.g. OneHotEncoder, OrdinalEncoder, and StandardScaler.',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'Transforming Non-Numerical Columns into Numerical Columns',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '3ca0b489'},\n",
       " {'text': 'These both methods receive the dictionary as an input. While the DictVectorizer will store the big vocabulary and takes more memory. FeatureHasher create a vectors with predefined length. They are both used for categorical features.\\nWhen you have a high cardinality for categorical features better to use FeatureHasher. If you want to preserve feature names in transformed data and have a small number of unique values is DictVectorizer. But your choice will dependence on your data.\\nYou can read more by follow the link https://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html\\nOlga Rudakova',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'What is the better option FeatureHasher or DictVectorizer',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '690d97f1'},\n",
       " {'text': '(Question by Connie S.)\\nThe reason it\\'s good/recommended practice to do it after splitting is to avoid data leakage - you don\\'t want any data from the test set influencing the training stage (similarly from the validation stage in the initial training). See e.g. scikit-learn documentation on \"Common pitfalls and recommended practices\": https://scikit-learn.org/stable/common_pitfalls.html\\nAnswered/added by Rileen Sinha',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': \"Isn't it easier to use DictVertorizer or get dummies before splitting the data into train/val/test? Is there a reason we wouldn't do this? Or is it the same either way?\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'eb5a25cb'},\n",
       " {'text': 'If you are getting 1.0 as accuracy then there is a possibility you have overfitted the model. Dropping the column msrp/price can help you solve this issue.\\nAdded by Akshar Goyal',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'HW3Q4 I am getting 1.0 as accuracy. Should I use the closest option?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '6d9e0a6f'},\n",
       " {'text': 'We can use sklearn & numpy packages to calculate Root Mean Squared Error\\nfrom sklearn.metrics import mean_squared_error\\nimport numpy as np\\nRmse = np.sqrt(mean_squared_error(y_pred, y_val/ytest)\\nAdded by Radikal Lukafiardi\\nYou can also refer to Alexey’s notebook for Week 2:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb\\nwhich includes the following code:\\ndef rmse(y, y_pred):\\nerror = y_pred - y\\nmse = (error ** 2).mean()\\nreturn np.sqrt(mse)\\n(added by Rileen Sinha)',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'How to calculate Root Mean Squared Error?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '618ad97a'},\n",
       " {'text': 'The solution is to use “get_feature_names_out” instead. See details: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\\nGeorge Chizhmak',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': \"AttributeError: 'DictVectorizer' object has no attribute 'get_feature_names'\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '683495d2'},\n",
       " {'text': 'To use RMSE without math or numpy, ‘sklearn.metrics’ has a mean_squared_error function with a squared kwarg (defaults to True). Setting squared to False will return the RMSE.\\nfrom sklearn.metrics import mean_squared_error\\nrms = mean_squared_error(y_actual, y_predicted, squared=False)\\nSee details: https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python\\nAhmed Okka',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'Root Mean Squared Error',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'dc1897b5'},\n",
       " {'text': 'This article explains different encoding techniques used https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02\\nHrithik Kumar Advani',\n",
       "  'section': '3. Machine Learning for Classification',\n",
       "  'question': 'Encoding Techniques',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '826098f2'},\n",
       " {'text': \"I got this error multiple times here is the code:\\n“accuracy_score(y_val, y_pred >= 0.5)”\\nTypeError: 'numpy.float64' object is not callable\\nI solve it using\\nfrom sklearn import metrics\\nmetrics.accuracy_score(y_train, y_pred>= 0.5)\\nOMAR Wael\",\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'Error in use of accuracy_score from sklearn in jupyter (sometimes)',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '821dfc08'},\n",
       " {'text': 'Week 4 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/04-evaluation/homework.md\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYouTube Link: 4.X --- https://www.youtube.com/watch?v=gmg5jw1bM8A&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=40\\nSci-Kit Learn on Evaluation:\\nhttps://scikit-learn.org/stable/model_selection.html\\n~~Nukta Bhatia~~',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'How do I get started with Week 4?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '27c8d5da'},\n",
       " {'text': 'https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696475675887119\\nMetrics can be used on a series or a dataframe\\n~~Ella Sahnan~~',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'Using a variable to score',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'a52d4739'},\n",
       " {'text': 'Ie particularly in module-04 homework Qn2 vs Qn5. https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696760905214979\\nRefer to the sklearn docs, random_state is to ensure the “randomness” that is used to shuffle dataset is reproducible, and it usually requires both random_state and shuffle params to be set accordingly.\\n~~Ella Sahnan~~',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'Why do we sometimes use random_state and not at other times?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'dc55359c'},\n",
       " {'text': 'How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\\nUse classification_report from sklearn. For more info check here.\\nAbhishek N',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'How to get all classification metrics?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '2ab49e43'},\n",
       " {'text': 'I am getting multiple thresholds with the same F1 score, does this indicate I am doing something wrong or is there a method for choosing? I would assume just pick the lowest?\\nChoose the one closest to any of the options\\nAdded by Azeez Enitan Edunwale\\nYou can always use scikit-learn (or other standard libraries/packages) to verify results obtained using your own code, e.g. you can use  “classification_report” (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to obtain precision, recall and F1-score.\\nAdded by Rileen Sinha',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'Multiple thresholds for Q4',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'b431e7eb'},\n",
       " {'text': \"Solution description: duplicating the\\ndf.churn = (df.churn == 'yes').astype(int)\\nThis is causing you to have only 0's in your churn column. In fact, match with the error you are getting:  ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.\\nIt is telling us that it only contains 0's.\\nDelete one of the below cells and you will get the accuracy\\nHumberto Rodriguez\",\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'c5fdeba9'},\n",
       " {'text': 'Use Yellowbrick. Yellowbrick in a library that combines scikit-learn with matplotlib to produce visualizations for your models. It produces colorful classification reports.\\nKrishna Annad',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'Method to get beautiful classification report',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'b8c9eaf1'},\n",
       " {'text': 'That’s fine, use the closest option',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'I’m not getting the exact result in homework',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'c54058a1'},\n",
       " {'text': 'Check the solutions from the 2021 iteration of the course. You should use roc_auc_score.',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'Use AUC to evaluate feature importance of numerical variables',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'b4b85c4b'},\n",
       " {'text': 'When calculating the ROC AUC score using sklearn.metrics.roc_auc_score the function expects two parameters “y_true” and “y_score”. So for each numerical value in the dataframe it will be passed as the “y_score” to the function and the target variable will get passed a “y_true” each time.\\nSylvia Schmitt',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'Help with understanding: “For each numerical value, use it as score and compute AUC”',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '7d40f6f6'},\n",
       " {'text': 'You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards, as you did in Question 2.\\nDiego Giraldo',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'What dataset should I use to compute the metrics in Question 3',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'f5dc446c'},\n",
       " {'text': \"What does this line do?\\nKFold(n_splits=n_splits, shuffle=True, random_state=1)\\nIf I do it inside the loop [0.01, 0.1, 1, 10] or outside the loop in Q6, HW04 it doesn't make any difference to my answers. I am wondering why and what is the right way, although it doesn't make a difference!\\nDid you try using a different random_state? From my understanding, KFold just makes N (which is equal to n_splits) separate pairs of datasets (train+val).\\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\\nIn my case changing random state changed results\\n(Arthur Minakhmetov)\\nChanging the random state makes a difference in my case too, but not whether it is inside or outside the for loop. I think I have got the answer. kFold = KFold(n_splits=n_splits, shuffle = True, random_state = 1)  is just a generator object and it contains only the information n_splits, shuffle and random_state. The k-fold splitting actually happens in the next for loop for train_idx, val_idx in kFold.split(df_full_train): . So it doesn't matter where we generate the object, before or after the first loop. It will generate the same information. But from the programming point of view, it is better to do it before the loop. No point doing it again and again inside the loop\\n(Bhaskar Sarma)\\nIn case of KFold(n_splits=n_splits, shuffle=True, random_state=1) and C= [0.01, 0.1, 1, 10], it is better to loop through the different values of Cs as the video explained. I had separate train() and predict() functions, which were reused after dividing the dataset via KFold. The model ran about 10 minutes and provided a good score.\\n(Ani Mkrtumyan)\",\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'What does KFold do?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'd30fc29d'},\n",
       " {'text': \"I’m getting “ValueError: multi_class must be in ('ovo', 'ovr')” when using roc_auc_score to evaluate feature importance of numerical variables in question 1.\\nI was getting this error because I was passing the parameters to roc_auc_score incorrectly (df_train[col] , y_train) . The correct way is to pass the parameters in this way: roc_auc_score(y_train, df_train[col])\\nAsia Saeed\",\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': \"ValueError: multi_class must be in ('ovo', 'ovr')\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '8eca9f73'},\n",
       " {'text': 'from tqdm.auto import tqdm\\nTqdm - terminal progress bar\\nKrishna Anand',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'Monitoring Wait times and progress of the code execution can be done with:',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '7b9eb7f7'},\n",
       " {'text': 'Inverting or negating variables with ROC AUC scores less than the threshold is a valuable technique to improve feature importance and model performance when dealing with negatively correlated features. It helps ensure that the direction of the correlation aligns with the expectations of most machine learning algorithms.\\nAileah Gotladera',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'What is the use of inverting or negating the variables less than the threshold?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'c4aaeed9'},\n",
       " {'text': 'In case of using predict(X) for this task we are getting the binary classification predictions which are 0 and 1. This may lead to incorrect evaluation values.\\nThe solution is to use predict_proba(X)[:,1], where we get the probability that the value belongs to one of the classes.\\nVladimir Yesipov\\nPredict_proba shows probailites per class.\\nAni Mkrtumyan',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'Difference between predict(X) and predict_proba(X)[:, 1]',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '3af31e2a'},\n",
       " {'text': 'For churn/not churn predictions, I need help to interpret the following scenario please, what is happening when:\\nThe threshold is 1.0\\nFPR is 0.0\\nAnd TPR is 0.0\\nWhen the threshold is 1.0, the condition for belonging to the positive class (churn class) is g(x)>=1.0 But g(x) is a sigmoid function for a binary classification problem. It has values between 0 and 1. This function  never becomes equal to outermost values, i.e. 0 and 1.\\nThat is why there is no object, for which churn-condition could be satisfied. And that is why there is no any positive  (churn) predicted value (neither true positive, nor false positive), if threshold is equal to 1.0\\nAlena Kniazeva',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'Why are FPR and TPR equal to 0.0, when threshold = 1.0?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '746342ff'},\n",
       " {'text': \"Matplotlib has a cool method to annotate where you could provide an X,Y point and annotate with an arrow and text. For example this will show an arrow pointing to the x,y point optimal threshold.\\nplt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\\\\nOptimal F1 Score: {optimal_f1_score:.2f}',\\nxy=(optimal_threshold, optimal_f1_score),\\nxytext=(0.3, 0.5),\\ntextcoords='axes fraction',\\narrowprops=dict(facecolor='black', shrink=0.05))\\nQuinn Avila\",\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'How can I annotate a graph?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'bda2c9b3'},\n",
       " {'text': \"It's a complex and abstract topic and it requires some time to understand. You can move on without fully understanding the concept.\\nNonetheless, it might be useful for you to rewatch the video, or even watch videos/lectures/notes by other people on this topic, as the ROC AUC is one of the most important metrics used in Binary Classification models.\",\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'I didn’t fully understand the ROC curve. Can I move on?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '41521c92'},\n",
       " {'text': 'One main reason behind that, is the way of splitting data. For example, we want to split data into train/validation/test with the ratios 60%/20%/20% respectively.\\nAlthough the following two options end up with the same ratio, the data itself is a bit different and not 100% matching in each case.\\n1)\\ndf_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\\ndf_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\\n2)\\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\\nTherefore, I would recommend using the second method which is more consistent with the lessons and thus the homeworks.\\nIbraheem Taha',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'Why do I have different values of accuracy than the options in the homework?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '25481ce5'},\n",
       " {'text': 'You can find the intercept between these two curves using numpy diff (https://numpy.org/doc/stable/reference/generated/numpy.diff.html ) and sign (https://numpy.org/doc/stable/reference/generated/numpy.sign.html):\\nI suppose here that you have your df_scores ready with your three columns ‘threshold’, ‘precision’ and ‘recall’:\\nYou want to know at which index (or indices) you have your intercept between precision and recall (namely: where the sign of the difference between precision and recall changes):\\nidx = np.argwhere(\\nnp.diff(\\nnp.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"]))\\n)\\n).flatten()\\nYou can print the result to easily read it:\\nprint(\\nf\"The precision and recall curves intersect at a threshold equal to {df_scores.loc[idx][\\'threshold\\']}.\"\\n)\\n(Mélanie Fouesnard)',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'How to find the intercept between precision and recall curves by using numpy?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '1427d567'},\n",
       " {'text': \"In the demonstration video, we are shown how to calculate the precision and recall manually. You can use the Scikit Learn library to calculate the confusion matrix. precision, recall, f1_score without having to first define true positive, true negative, false positive, and false negative.\\nfrom sklearn.metrics import precision_score, recall_score, f1_score\\nprecision_score(y_true, y_pred, average='binary')\\nrecall_score(y_true, y_pred, average='binary')\\nf1_score(y_true, y_pred, average='binary')\\nRadikal Lukafiardi\",\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'Compute Recall, Precision, and F1 Score using scikit-learn library',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '76c91dfb'},\n",
       " {'text': 'Cross-validation evaluates the performance of a model and chooses the best hyperparameters. Cross-validation does this by splitting the dataset into multiple parts (folds), typically 5 or 10. It then trains and evaluates your model multiple times, each time using a different fold as the validation set and the remaining folds as the training set.\\n\"C\" is a hyperparameter that is typically associated with regularization in models like Support Vector Machines (SVM) and logistic regression.\\nSmaller \"C\" values: They introduce more regularization, which means the model will try to find a simpler decision boundary, potentially underfitting the data. This is because it penalizes the misclassification of training examples more severely.\\nLarger \"C\" values: They reduce the regularization effect, allowing the model to fit the training data more closely, potentially overfitting. This is because it penalizes misclassification less severely, allowing the model to prioritize getting training examples correct.\\nAminat Abolade',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'Why do we use cross validation?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'e4dd91cf'},\n",
       " {'text': \"Model evaluation metrics can be easily computed using off the shelf calculations available in scikit learn library. This saves a lot of time and more precise compared to our own calculations from the scratch using numpy and pandas libraries.\\nfrom sklearn.metrics import (accuracy_score,\\nprecision_score,\\nrecall_score,\\nf1_score,\\nroc_auc_score\\n)\\naccuracy = accuracy_score(y_val, y_pred)\\nprecision = precision_score(y_val, y_pred)\\nrecall = recall_score(y_val, y_pred)\\nf1 = f1_score(y_val, y_pred)\\nroc_auc = roc_auc_score(y_val, y_pred)\\nprint(f'Accuracy: {accuracy}')\\nprint(f'Precision: {precision}')\\nprint(f'Recall: {recall}')\\nprint(f'F1-Score: {f1}')\\nprint(f'ROC AUC: {roc_auc}')\\n(Harish Balasundaram)\",\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'Evaluate the Model using scikit learn metrics',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'cc53ae94'},\n",
       " {'text': 'Scikit-learn offers another way: precision_recall_fscore_support\\nExample:\\nfrom sklearn.metrics import precision_recall_fscore_support\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n(Gopakumar Gopinathan)',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'Are there other ways to compute Precision, Recall and F1 score?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '403bbdd8'},\n",
       " {'text': '- ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\\n- The reason for this recommendation is that ROC curves present an optimistic picture of the model on datasets with a class imbalance.\\n-This is because of the use of true negatives in the False Positive Rate in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve.\\n- If the proportion of positive to negative instances changes in a test set, the ROC curves will not change. Metrics such as accuracy, precision, lift and F scores use values from both columns of the confusion matrix. As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not. ROC graphs are based upon TP rate and FP rate, in which each dimension is a strict columnar ratio, so cannot give an accurate picture of performance when there is class imbalance.\\n(Anudeep Vanjavakam)',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'When do I use ROC vs Precision-Recall curves?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '7c68ace0'},\n",
       " {'text': 'You can use roc_auc_score function from sklearn.metrics module and pass the vector of the target variable (‘above_average’) as the first argument and the vector of feature values as the second one. This function will return AUC score for the feature that was passed as a second argument.\\n(Denys Soloviov)',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'How to evaluate feature importance for numerical variables with AUC?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '147577f5'},\n",
       " {'text': 'Precision-recall curve, and thus the score, explicitly depends on the ratio  of positive to negative test cases. This means that comparison of the F-score across different problems with differing class ratios is problematic. One way to address this issue is to use a standard class ratio  when making such comparisons.\\n(George Chizhmak)',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'Dependence of the F-score on class imbalance',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'd3ffb802'},\n",
       " {'text': \"We can import precision_recall_curve from scikit-learn and plot the graph as follows:\\nfrom sklearn.metrics import precision_recall_curve\\nprecision, recall, thresholds = precision_recall_curve(y_val, y_predict)\\nplt.plot(thresholds, precision[:-1], label='Precision')\\nplt.plot(thresholds, recall[:-1], label='Recall')\\nplt.legend()\\nHrithik Kumar Advani\",\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'Quick way to plot Precision-Recall Curve',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'cc04d27a'},\n",
       " {'text': 'For multiclass classification it is important to keep class balance when you split the data set. In this case Stratified k-fold returns folds that contains approximately the sme percentage of samples of each classes.\\nPlease check the realisation in sk-learn library:\\nhttps://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold\\nOlga Rudakova',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'What is Stratified k-fold?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '927b5e09'},\n",
       " {'text': 'Week 5 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/05-deployment/homework.md\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nHW 3 Solution: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/03-classification/homework_3.ipynb\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYouTube Link: 5.X --- https://www.youtube.com/watch?v=agIFak9A3m8&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=49\\n~~~ Nukta Bhatia ~~~',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'How do I get started with Week 5?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'd22efea7'},\n",
       " {'text': 'While weeks 1-4 can relatively easily be followed and the associated homework completed with just about any default environment / local setup, week 5 introduces several layers of abstraction and dependencies.\\nIt is advised to prepare your “homework environment” with a cloud provider of your choice. A thorough step-by-step guide for doing so for an AWS EC2 instance is provided in an introductory video taken from the MLOPS course here:\\nhttps://www.youtube.com/watch?v=IXSiYkP23zo\\nNote that (only) small AWS instances can be run for free, and that larger ones will be billed hourly based on usage (but can and should be stopped when not in use).\\nAlternative ways are sketched here:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/01-intro/06-environment.md',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Errors related to the default environment: WSL, Ubuntu, proper Python version, installing pipenv etc.',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'd1409f67'},\n",
       " {'text': \"You’ll need a kaggle account\\nGo to settings, API and click `Create New Token`. This will download a `kaggle.json` file which contains your `username` and `key` information\\nIn the same location as your Jupyter NB, place the `kaggle.json` file\\nRun `!chmod 600 <ENTER YOUR FILEPATH>/kaggle.json`\\nMake sure to import os via `import os` and then run:\\nos.environ['KAGGLE_CONFIG_DIR'] = <STRING OF YOUR FILE PATH>\\nFinally you can run directly in your NB: `!kaggle datasets download -d kapturovalexander/bank-credit-scoring`\\nAnd then you can unzip the file and access the CSV via: `!unzip -o bank-credit-scoring.zip`\\n>>> Michael Fronda <<<\",\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'How to download CSV data via Jupyter NB and the Kaggle API, for one seamless experience',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'e07759e9'},\n",
       " {'text': 'Cd .. (go back)\\nLs (see current folders)\\nCd ‘path’/ (go to this path)\\nPwd (home)\\nCat “file name’ --edit txt file in ubuntu\\nAileah Gotladera',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Basic Ubuntu Commands:',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '620fb76e'},\n",
       " {'text': 'Open terminal and type the code below to check the version on your laptop\\npython3 --version\\nFor windows,\\nVisit the official python website at  https://www.python.org/downloads/ to download the python version you need for installation\\nRun the installer and  ensure to check the box that says “Add Python to PATH” during installation and complete the installation by following the prompts\\nOr\\nFor Python 3,\\nOpen your command prompt or terminal and run the following command:\\npip install --upgrade python\\nAminat Abolade',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Installing and updating to the python version 3.10 and higher',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '957280d8'},\n",
       " {'text': 'It is quite simple, and you can follow these instructions here:\\nhttps://www.youtube.com/watch?v=qYlgUDKKK5A&ab_channel=NeuralNine\\nMake sure that you have “Virtual Machine Platform” feature activated in your Windows “Features”. To do that, search “features” in the research bar and see if the checkbox is selected. You also need to make sure that your system (in the bios) is able to virtualize. This is usually the case.\\nIn the Microsoft Store: look for ‘Ubuntu’ or ‘Debian’ (or any linux distribution you want) and install it\\nOnce it is downloaded, open the app and choose a username and a password (secured one). When you type your password, nothing will show in the window, which is normal: the writing is invisible.\\nYou are now inside of your linux system. You can test some commands such as “pwd”. You are not in your Windows system.\\nTo go to your windows system: you need to go back two times with cd ../.. And then go to the “mnt” directory with cd mnt. If you list here your files, you will see your disks. You can move to the desired folder, for example here I moved to the ML_Zoomcamp folder:\\nPython should be already installed but you can check it by running sudo apt install python3 command.\\nYou can make your actual folder your default folder when you open your Ubuntu terminal with this command : echo \"cd ../../mnt/your/folder/path\" >> ~/.bashrc\\nYou can disable bell sounds (when you type something that does not exist for example) by modifying the inputrc file with this command: sudo vim /etc/inputrc\\nYou have to uncomment the set bell-style none line -> to do that, press the “i” keyboard letter (for insert) and go with your keyboard to this line. Delete the # and then press the Escape keyboard touch and finally press “:wq” to write (it saves your modifications) then quit.\\nYou can check that your modifications are taken into account by opening a new terminal (you can pin it to your task bar so you do not have to go to the Microsoft app each time).\\nYou will need to install pip by running this command sudo apt install python3-pip\\nNB: I had this error message when trying to install pipenv (https://github.com/microsoft/WSL/issues/5663):\\n/sbin/ldconfig.real: Can\\'t link /usr/lib/wsl/lib/libnvoptix_loader.so.1 to libnvoptix.so.1\\n/sbin/ldconfig.real: /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link\\nSo I had to create the following symbolic link:\\nsudo ln -s /usr/lib/wsl/lib/libcuda.so.1 /usr/lib64/libcuda.so\\n(Mélanie Fouesnard)',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'How to install WSL on Windows 10 and 11 ?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '185096ad'},\n",
       " {'text': \"Do you get errors building the Docker image on the Mac M1 chipset?\\nThe error I was getting was:\\nCould not open '/lib64/ld-linux-x86-64.so.2': No such file or directory\\nThe fix (from here): vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\\nOpen mlbookcamp-code/course-zoomcamp/01-intro/environment/Dockerfile\\nReplace line 1 with\\nFROM --platform=linux/amd64 ubuntu:latest\\nNow build the image as specified. In the end it took over 2 hours to build the image but it did complete in the end.\\nDavid Colton\",\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Error building Docker images on Mac with M1 silicon',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'ec88d101'},\n",
       " {'text': 'Import waitress\\nprint(waitress.__version__)\\nKrishna Anand',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Method to find the version of any install python libraries in jupyter notebook',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '7156679d'},\n",
       " {'text': 'Working on getting Docker installed - when I try running hello-world I am getting the error.\\nDocker: Cannot connect to the docker daemon at unix:///var/run/docker.sock. Is the Docker daemon running ?\\nSolution description\\nIf you’re getting this error on WSL, re-install your docker: remove the docker installation from WSL and install Docker Desktop on your host machine (Windows).\\nOn Linux, start the docker daemon with either of these commands:\\nsudo dockerd\\nsudo service docker start\\nAdded by Ugochukwu Onyebuchi',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Cannot connect to the docker daemon. Is the Docker daemon running?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '4b2a3181'},\n",
       " {'text': 'After using the command “docker build -t churn-prediction .” to build the Docker image, the above error is raised and the image is not created.\\nIn your Dockerfile, change the Python version in the first line the Python version installed in your system:\\nFROM python:3.7.5-slim\\nTo find your python version, use the command python --version. For example:\\npython --version\\n>> Python 3.9.7\\nThen, change it on your Dockerfile:\\nFROM python:3.9.7-slim\\nAdded by Filipe Melo',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': \"The command '/bin/sh -c pipenv install --deploy --system &&  rm -rf /root/.cache' returned a non-zero code: 1\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '73bd7fa1'},\n",
       " {'text': 'When the facilitator was adding sklearn to the virtual environment in the lectures, he used sklearn==0.24.1 and it ran smoothly. But while doing the homework and you are asked to use the 1.0.2 version of sklearn, it gives errors.\\nThe solution is to use the full name of sklearn. That is, run it as “pipenv install scikit-learn==1.0.2” and the error will go away, allowing you to install sklearn for the version in your virtual environment.\\nOdimegwu David\\nHomework asks you to install 1.3.1\\nPipenv install scikit-learn==1.3.1\\nUse Pipenv to install Scikit-Learn version 1.3.1\\nGopakumar Gopinathan',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Running “pipenv install sklearn==1.0.2” gives errors. What should I do?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'a4d3b1e5'},\n",
       " {'text': 'What is the reason we don’t want to keep the docker image in our system and why do we need to run docker containers with `--rm` flag?\\nFor best practice, you don’t want to have a lot of abandoned docker images in your system. You just update it in your folder and trigger the build one more time.\\nThey consume extra space on your disk. Unless you don’t want to re-run the previously existing containers, it is better to use the `--rm` option.\\nThe right way to say: “Why do we remove the docker container in our system?”. Well the docker image is still kept; it is the container that is not kept. Upon execution, images are not modified; only containers are.\\nThe option `--rm` is for removing containers. The images remain until you remove them manually. If you don’t specify a version when building an image, it will always rebuild and replace the latest tag. `docker images` shows you all the image you have pulled or build so far.\\nDuring development and testing you usually specify `--rm` to get the containers auto removed upon exit. Otherwise they get accumulated in a stopped state, taking up space. `docker ps -a` shows you all the containers you have in your host. Each time you change Pipfile (or any file you baked into the container), you rebuild the image under the same tag or a new tag. It’s important to understand the difference between the term “docker image” and “docker container”. Image is what we build with all the resources baked in. You can move it around, maintain it in a repository, share it. Then we use the image to spin up instances of it and they are called containers.\\nAdded by Muhammad Awon',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Why do we need the --rm flag',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '1d462fe0'},\n",
       " {'text': 'When you create the dockerfile the name should be dockerfile and needs to be without extension. One of the problems we can get at this point is to create the dockerfile as a dockerfile extension Dockerfile.dockerfile which creates an error when we build the docker image. Instead we just need to create the file without extension: Dockerfile and will run perfectly.\\nAdded by Pastor Soto',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Failed to read Dockerfile',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '366d7563'},\n",
       " {'text': 'Refer to the page https://docs.docker.com/desktop/install/mac-install/ remember to check if you have apple chip or intel chip.',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Install docker on MacOS',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'cef156d1'},\n",
       " {'text': 'Problem: When I am trying to pull the image with the docker pull svizor/zoomcamp-model command I am getting an error:\\nUsing default tag: latest\\nError response from daemon: manifest for svizor/zoomcamp-model:latest not found: manifest unknown: manifest unknown\\nSolution: The docker by default uses the latest tag to avoid this use the correct tag from image description. In our case use command:\\ndocker pull svizor/zoomcamp-model:3.10.12-slim\\nAdded by Vladimir Yesipov',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'I cannot pull the image with docker pull command',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'b632d2ea'},\n",
       " {'text': 'Using the command docker images or docker image ls will dump all information for all local Docker images. It is possible to dump the information only for a specified image by using:\\ndocker image ls <image name>\\nOr alternatively:\\ndocker images <image name>\\nIn action to that it is possible to only dump specific information provided using the option --format which will dump only the size for the specified image name when using the command below:\\ndocker image ls --format \"{{.Size}}\" <image name>\\nOr alternatively:\\ndocker images --format \"{{.Size}}\" <image name>\\nSylvia Schmitt',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Dumping/Retrieving only the size of for a specific Docker image',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '514e27bb'},\n",
       " {'text': \"It creates them in\\nOSX/Linux: ~/.local/share/virtualenvs/folder-name_cyrptic-hash\\nWindows: C:\\\\Users\\\\<USERNAME>\\\\.virtualenvs\\\\folder-name_cyrptic-hash\\nEg: C:\\\\Users\\\\Ella\\\\.virtualenvs\\\\code-qsdUdabf (for module-05 lesson)\\nThe environment name is the name of the last folder in the folder directory where we used the pipenv install command (or any other pipenv command). E.g. If you run any pipenv command in folder path ~/home/user/Churn-Flask-app, it will create an environment named Churn-Flask-app-some_random_characters, and it's path will be like this: /home/user/.local/share/virtualenvs/churn-flask-app-i_mzGMjX.\\nAll libraries of this environment will be installed inside this folder. To activate this environment, I will need to cd into the project folder again, and type pipenv shell. In short, the location of the project folder acts as an identifier for an environment, in place of any name.\\n(Memoona Tahira)\",\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Where does pipenv create environments and how does it name them?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '5c67e086'},\n",
       " {'text': 'Launch the container image in interactive mode and overriding the entrypoint, so that it starts a bash command.\\ndocker run -it --entrypoint bash <image>\\nIf the container is already running, execute a command in the specific container:\\ndocker ps (find the container-id)\\ndocker exec -it <container-id> bash\\n(Marcos MJD)',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'How do I debug a docker container?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '63a81b57'},\n",
       " {'text': \"$ docker exec -it 1e5a1b663052 bash\\nthe input device is not a TTY.  If you are using mintty, try prefixing the command with 'winpty'\\nFix:\\nwinpty docker exec -it 1e5a1b663052 bash\\nA TTY is a terminal interface that supports escape sequences, moving the cursor around, etc.\\nWinpty is a Windows software package providing an interface similar to a Unix pty-master for communicating with Windows console programs.\\nMore info on terminal, shell, console applications hi and so on:\\nhttps://conemu.github.io/en/TerminalVsShell.html\\n(Marcos MJD)\",\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'The input device is not a TTY when running docker in interactive mode (Running Docker on Windows in GitBash)',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '047f57fb'},\n",
       " {'text': 'Initially, I did not assume there was a model2. I copied the original model1.bin and dv.bin. Then when I tried to load using\\nCOPY [\"model2.bin\", \"dv.bin\", \"./\"]\\nthen I got the error above in MINGW64 (git bash) on Windows.\\nThe temporary solution I found was to use\\nCOPY [\"*\", \"./\"]\\nwhich I assume combines all the files from the original docker image and the files in your working directory.\\nAdded by Muhammed Tan',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Error: failed to compute cache key: \"/model2.bin\" not found: not found',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '11f7371c'},\n",
       " {'text': 'Create a virtual environment using the Cmd command (command) and use pip freeze command to write the requirements in the text file\\nKrishna Anand',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Failed to write the dependencies to pipfile and piplock file',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '45f39b76'},\n",
       " {'text': 'f-String not properly keyed in: does anyone knows why i am getting error after import pickle?\\nThe first error showed up because your f-string is using () instead of {} around C. So, should be: f’model_C={C}.bin’\\nThe second error as noticed by Sriniketh, your are missing one parenthesis it should be pickle.dump((dv, model), f_out)\\n(Humberto R.)',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'f-strings',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '94e17563'},\n",
       " {'text': \"This error happens because pipenv is already installed but you can't access it from the path.\\nThis error comes out if you run.\\npipenv  --version\\npipenv shell\\nSolution for Windows\\nOpen this option\\nClick here\\nClick in Edit Button\\nMake sure the next two locations are on the PATH, otherwise, add it.\\nC:\\\\Users\\\\AppData\\\\....\\\\Python\\\\PythonXX\\\\\\nC:\\\\Users\\\\AppData\\\\....\\\\Python\\\\PythonXX\\\\Scripts\\\\\\nAdded by Alejandro Aponte\\nNote: this answer assumes you don’t use Anaconda. For Windows, using Anaconda would be a better choice and less prone to errors.\",\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': \"'pipenv' is not recognized as an internal or external command, operable program or batch file.\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '9dd8efd2'},\n",
       " {'text': 'Following the instruction from video week-5.6, using pipenv to install python libraries throws below error\\nSolution to this error is to make sure that you are working with python==3.9 (as informed in the very first lesson of the zoomcamp) and not python==3.10.\\nAdded by Hareesh Tummala',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'AttributeError: module ‘collections’ has no attribute ‘MutableMapping’',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '9531dc92'},\n",
       " {'text': 'After entering `pipenv shell` don’t forget to use `exit` before `pipenv --rm`, as it may cause errors when trying to install packages, it is unclear whether you are “in the shell”(using Windows) at the moment as there are no clear markers for it.\\nIt can also mess up PATH, if that’s the case, here’s terminal commands for fixing that:\\n# for Windows\\nset VIRTUAL_ENV \"\"\\n# for Unix\\nexport VIRTUAL_ENV=\"\"\\nAlso manually re-creating removed folder at `C:\\\\Users\\\\username\\\\.virtualenvs\\\\removed-envname` can help, removed-envname can be seen at the error message.\\nAdded by Andrii Larkin',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': \"Q: ValueError: Path not found or generated: WindowsPath('C:/Users/username/.virtualenvs/envname/Scripts')\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '14e0e697'},\n",
       " {'text': 'Set the host to ‘0.0.0.0’ on the flask app and dockerfile then RUN the url using localhost.\\n(Theresa S.)',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': \"ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '6189375f'},\n",
       " {'text': 'Solution:\\nThis error occurred because I used single quotes around the filenames. Stick to double quotes',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'docker  build ERROR [x/y] COPY …',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '3419ee27'},\n",
       " {'text': 'I tried the first solution on Stackoverflow which recommended running `pipenv lock` to update the Pipfile.lock. However, this didn’t resolve it. But the following switch to the pipenv installation worked\\nRUN pipenv install --system --deploy --ignore-pipfile',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Fix error during installation of Pipfile inside Docker container',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '8b8c1603'},\n",
       " {'text': 'Solution\\nThis error was because there was another instance of gunicorn running. So I thought of removing this along with the zoomcamp_test image. However, it didn’t let me remove the orphan container. So I did the following\\nRunning the following commands\\ndocker ps -a <to list all docker containers>\\ndocker images <to list images>\\ndocker stop <container ID>\\ndocker rm <container ID>\\ndocker rmi image\\nI rebuilt the Docker image, and ran it once again; this time it worked correctly and I was able to serve the test script to the endpoint.',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'How to fix error after running the Docker run command',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'e54d5411'},\n",
       " {'text': 'I was getting the below error when I rebuilt the docker image although the port was not allocated, and it was working fine.\\nError message:\\nError response from daemon: driver failed programming external connectivity on endpoint beautiful_tharp (875be95c7027cebb853a62fc4463d46e23df99e0175be73641269c3d180f7796): Bind for 0.0.0.0:9696 failed: port is already allocated.\\nSolution description\\nIssue has been resolved running the following command:\\ndocker kill $(docker ps -q)\\nhttps://github.com/docker/for-win/issues/2722\\nAsia Saeed',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Bind for 0.0.0.0:9696 failed: port is already allocated',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'f7b38587'},\n",
       " {'text': 'I was getting the error on client side with this\\nClient Side:\\nFile \"C:\\\\python\\\\lib\\\\site-packages\\\\urllib3\\\\connectionpool.py\", line 703, in urlopen …………………..\\nraise ConnectionError(err, request=request)\\nrequests.exceptions.ConnectionError: (\\'Connection aborted.\\', RemoteDisconnected(\\'Remote end closed connection without response\\'))\\nSevrer Side:\\nIt showed error for gunicorn\\nThe waitress  cmd was running smoothly from server side\\nSolution:\\nUse the ip-address as 0.0.0.0:8000 or 0.0.0.0:9696.They are the ones which do work max times\\nAamir Wani',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Bind for 127.0.0.1:5000 showing error',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'be86b333'},\n",
       " {'text': 'Install it by using command\\n% brew install md5sha1sum\\nThen run command to check hash for file to check if they the same with the provided\\n% md5sum model1.bin dv.bin\\nOlga Rudakova',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Installing md5sum on Macos',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '4ea80460'},\n",
       " {'text': 'Problem description:\\nI started a web-server in terminal (command window, powershell, etc.). How can I run another python script, which makes a request to this server?\\nSolution description:\\nJust open another terminal (command window, powershell, etc.) and run a python script.\\nAlena Kniazeva',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'How to run a script while a web-server is working?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '8006b496'},\n",
       " {'text': \"Problem description:\\nIn video 5.5 when I do pipenv shell and then pipenv run gunicorn --bind 0.0.0.0:9696 predict:app, I get the following warning:\\nUserWarning: Trying to unpickle estimator DictVectorizer from version 1.1.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\\nSolution description:\\nWhen you create a virtual env, you should use the same version of Scikit-Learn that you used for training the model on this case it's 1.1.1. There is version conflicts so we need to make sure our model and dv files are created from the version we are using for the project.\\nBhaskar Sarma\",\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Version-conflict in pipenv',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '704f95d8'},\n",
       " {'text': \"If you install packages via pipenv install, and get an error that ends like this:\\npipenv.vendor.plette.models.base.ValidationError: {'python_version': '3.9', 'python_full_version': '3.9.13'}\\npython_full_version: 'python_version' must not be present with 'python_full_version'\\npython_version: 'python_full_version' must not be present with 'python_version'\\nDo this:\\nopen Pipfile in nano editor, and remove either the python_version or python_full_version line, press CTRL+X, type Y and click Enter to save changed\\nType pipenv lock to create the Pipfile.lock.\\nDone. Continue what you were doing\",\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Python_version and Python_full_version error after running pipenv install:',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'a5b3296b'},\n",
       " {'text': 'If during running the  docker build command, you get an error like this:\\nYour Pipfile.lock (221d14) is out of date. Expected: (939fe0).\\nUsage: pipenv install [OPTIONS] [PACKAGES]...\\nERROR:: Aborting deploy\\nOption 1: Delete the pipfile.lock via rm Pipfile, and then rebuild the lock via  pipenv lock from the terminal before retrying the docker build command.\\nOption 2:  If it still doesn’t work, remove the pipenv environment, Pipfile and Pipfile.lock, and create a new one before building docker again. Commands to remove pipenv environment and removing pipfiles:\\npipenv  --rm\\nrm Pipfile*',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Your Pipfile.lock (221d14) is out of date (during Docker build)',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'a23b276a'},\n",
       " {'text': 'Ans: Pip uninstall waitress mflow. Then reinstall just mlflow. By this time you should have successfully built your docker image so you dont need to reinstall waitress. All good. Happy learning.\\nAdded by 🅱🅻🅰🆀',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'You are using windows. Conda environment. You then use waitress instead of gunicorn. After a few runs, suddenly mlflow server fails to run.',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '3537eeee'},\n",
       " {'text': \"Ans: so you have created the env. You need to make sure you're in eu-west-1 (ireland) when you check the EB environments. Maybe you're in a different region in your console.\\nAdded by Edidiong Esu\",\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Completed creating the environment locally but could not find the environment on AWS.',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '1d6d5b51'},\n",
       " {'text': 'Running \\'pip install waitress\\' as a command on GitBash was not downloading the executable file \\'waitress-serve.exe\\'. You need this file to be able to run commands with waitress in Git Bash. To solve this:\\nopen a Jupyter notebook and run the same command \\' pip install waitress\\'. This way the executable file will be downloaded. The notebook may give you this warning : \\'WARNING: The script waitress-serve.exe is installed in \\'c:\\\\Users\\\\....\\\\anaconda3\\\\Scripts\\' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\\'\\nAdd the path where \\'waitress-serve.exe\\' is installed into gitbash\\'s PATH as such:\\nenter the following command in gitbash: nano ~/.bashrc\\nadd the path to \\'waitress-serve.exe\\' to PATH using this command: export PATH=\"/path/to/waitress:$PATH\"\\nclose gitbash and open it again and you should be good to go\\nAdded by Bachar Kabalan',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Installing waitress on Windows via GitBash: “waitress-serve” command not found',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '3a98b6b7'},\n",
       " {'text': 'Q2.1: Use Pipenv to install Scikit-Learn version 1.3.1\\nThis is an error I got while executing the above step in the ml-zoomcamp conda environment. The error is not fatal and just warns you that explicit language specifications are not set out in our bash profile. A quick-fix is here:\\nhttps://stackoverflow.com/questions/49436922/getting-error-while-trying-to-run-this-command-pipenv-install-requests-in-ma\\nBut one can proceed without addressing it.\\nAdded by Abhirup Ghosh',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Warning: the environment variable LANG is not set!',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'd42eb923'},\n",
       " {'text': 'The provided image FROM svizor/zoomcamp-model:3.10.12-slim has a model and dictvectorizer that should be used for question 6. \"model2.bin\", \"dv.bin\"\\nAdded by Quinn Avila',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Module5 HW Question 6',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '42aebe10'},\n",
       " {'text': 'https://apps.microsoft.com/detail/windows-terminal/9N0DX20HK701?hl=es-419&gl=CO\\nAdded by Dawuta Smit',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Terminal Used in Week 5 videos:',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'e4f62713'},\n",
       " {'text': \"Question:\\nWhen running\\npipenv run waitress-serve --listen=localhost:9696 q4-predict:app\\nI get the following:\\nThere was an exception (ValueError) importing your module.\\nIt had these arguments:\\n1. Malformed application 'q4-predict:app'\\nAnswer:\\nWaitress doesn’t accept a dash in the python file name.\\nThe solution is to rename the file replacing a dash with something else for instance with an underscore eg q4_predict.py\\nAdded by Alex Litvinov\",\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'waitress-serve shows Malformed application',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'c13d811f'},\n",
       " {'text': 'I wanted to have a fast and simple way to check if the HTTP POST requests are working just running a request from command line. This can be done running ‘curl’. \\n(Used with WSL2 on Windows, should also work on Linux and MacOS)\\ncurl --json \\'<json data>\\' <url>\\n# piping the structure to the command\\ncat <json file path> | curl --json @- <url>\\necho \\'<json data>\\' | curl --json @- <url>\\n# example using piping\\necho \\'{\"job\": \"retired\", \"duration\": 445, \"poutcome\": \"success\"}\\'\\\\\\n| curl --json @- http://localhost:9696/predict\\nAdded by Sylvia Schmitt',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Testing HTTP POST requests from command line using curl',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'dfb41f7e'},\n",
       " {'text': 'Question:\\nWhen executing\\neb local run  --port 9696\\nI get the following error:\\nERROR: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\\nAnswer:\\nThere are two options to fix this:\\nRe-initialize by running eb init -i and choosing the options from a list (the first default option for docker platform should be fine).\\nEdit the ‘.elasticbeanstalk/config.yml’ directly changing the default_platform from Docker to default_platform: Docker running on 64bit Amazon Linux 2023\\nThe disadvantage of the second approach is that the option might not be available the following years\\nAdded by Alex Litvinov',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'd04e77f8'},\n",
       " {'text': \"You need to include the protocol scheme: 'http://localhost:9696/predict'.\\nWithout the http:// part, requests has no idea how to connect to the remote server.\\nNote that the protocol scheme must be all lowercase; if your URL starts with HTTP:// for example, it won’t find the http:// connection adapter either.\\nAdded by George Chizhmak\",\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': \"Requests Error: No connection adapters were found for 'localhost:9696/predict'.\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '451c067f'},\n",
       " {'text': 'While running the docker image if you get the same result check which model you are using.\\nRemember you are using a model downloading model + python version so remember to change the model in your file when running your prediction test.\\nAdded by Ahmed Okka',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Getting the same result',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '9fbfcd61'},\n",
       " {'text': 'Ensure that you used pipenv to install the necessary modules including gunicorn. As pipfiles for virtual environments, you can use pipenv shell and then build+run your docker image. - Akshar Goyal',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'Trying to run a docker image I built but it says it’s unable to start the container process',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '1ed8cfde'},\n",
       " {'text': \"You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\\nTo copy a file or directory from your local machine into a running Docker container, you can use the `docker cp command`. The basic syntax is as follows:\\ndocker cp /path/to/local/file_or_directory container_id:/path/in/container\\nHrithik Kumar Advani\",\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'How do I copy files from my local machine to docker container?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '3f97f50f'},\n",
       " {'text': 'You can copy files from your local machine into a Docker container using the docker cp command. Here\\'s how to do it:\\nIn the Dockerfile, you can provide the folder containing the files that you want to copy over. The basic syntax is as follows:\\nCOPY [\"src/predict.py\", \"models/xgb_model.bin\", \"./\"]\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tGopakumar Gopinathan',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'How do I copy files from a different folder into docker container’s working directory?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'a24a874a'},\n",
       " {'text': 'I struggled with the command :\\neb init -p docker tumor-diagnosis-serving -r eu-west-1\\nWhich resulted in an error when running : eb local run --port 9696\\nERROR: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\\nI replaced it with :\\neb init -p \"Docker running on 64bit Amazon Linux 2\" tumor-diagnosis-serving -r eu-west-1\\nThis allowed the recognition of the Dockerfile and the build/run of the docker container.\\nAdded by Mélanie Fouesnard',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'I can’t create the environment on AWS Elastic Beanstalk with the command proposed during the video',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'bf563b1f'},\n",
       " {'text': \"I had this error when creating a AWS ElasticBean environment: eb create tumor-diagnosis-env\\nERROR   Instance deployment: Both 'Dockerfile' and 'Dockerrun.aws.json' are missing in your source bundle. Include at least one of them. The deployment failed.\\nI did not committed the files used to build the container, particularly the Dockerfile. After a git add and git commit of the modified files, the command works.\\nAdded by Mélanie Fouesnard\",\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'Dockerfile missing when creating the AWS ElasticBean environment',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '21e9facf'},\n",
       " {'text': 'Week 6 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/06-trees/homework.md\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nHW 4 Solution: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/04-evaluation/homework_4.ipynb\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYouTube Link: 6.X --- https://www.youtube.com/watch?v=GJGmlfZoCoU&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=57\\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n~~~Nukta Bhatia~~~',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'How to get started with Week 6?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'aef786aa'},\n",
       " {'text': 'During the XGBoost lesson, we created a parser to extract the training and validation auc from the standard output. However, we can accomplish that in a more straightforward way.\\nWe can use the evals_result  parameters, which takes an empty dictionary and updates it for each tree. Additionally, you can store the data in a dataframe and plot it in an easier manner.\\nAdded by Daniel Coronel',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'How to get the training and validation metrics from XGBoost?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '68858294'},\n",
       " {'text': 'You should create sklearn.ensemble.RandomForestRegressor object. It’s rather similar to sklearn.ensemble.RandomForestClassificator for classification problems. Check https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html for more information.\\nAlena Kniazeva',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'How to solve regression problems with random forest in scikit-learn?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '85ac722e'},\n",
       " {'text': 'In question 6, I was getting ValueError: feature_names must be string, and may not contain [, ] or < when I was creating DMatrix for train and validation\\nSolution description\\nThe cause of this error is that some of the features names contain special characters like = and <, and I fixed the error by removing them as  follows:\\nfeatures= [i.replace(\"=<\", \"_\").replace(\"=\",\"_\") for i in features]\\nAsia Saeed\\nAlternative Solution:\\nIn my case the equal sign “=” was not a problem, so in my opinion the first part of Asias solution features= [i.replace(\"=<\", \"_\") should work as well.\\nFor me this works:\\nfeatures = []\\nfor f in dv.feature_names_:\\nstring = f.replace(“=<”, “-le”)\\nfeatures.append(string)\\nPeter Ernicke',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'ValueError: feature_names must be string, and may not contain [, ] or <',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'b61d2e92'},\n",
       " {'text': 'If you’re getting this error, It is likely because the feature names in dv.get_feature_names_out() are a np.ndarray instead of a list so you have to convert them into a list by using the to_list() method.\\nAli Osman',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': \"`TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'> ` when training xgboost model.\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '8d7392cb'},\n",
       " {'text': \"If you’re getting TypeError:\\n“TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'>”,\\nprobably you’ve done this:\\nfeatures = dv.get_feature_names_out()\\nIt gets you np.ndarray instead of list. Converting to list list(features) will not fix this, read below.\\nIf you’re getting ValueError:\\n“ValueError: feature_names must be string, and may not contain [, ] or <”,\\nprobably you’ve either done:\\nfeatures = list(dv.get_feature_names_out())\\nor:\\nfeatures = dv.feature_names_\\nreason is what you get from DictVectorizer here looks like this:\\n['households',\\n'housing_median_age',\\n'latitude',\\n'longitude',\\n'median_income',\\n'ocean_proximity=<1H OCEAN',\\n'ocean_proximity=INLAND',\\n'population',\\n'total_bedrooms',\\n'total_rooms']\\nit has symbols XGBoost doesn’t like ([, ] or <).\\nWhat you can do, is either do not specify “feature_names=” while creating xgb.DMatrix or:\\nimport re\\nfeatures = dv.feature_names_\\npattern = r'[\\\\[\\\\]<>]'\\nfeatures = [re.sub(pattern, '  ', f) for f in features]\\nAdded by Andrii Larkin\",\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'Q6: ValueError or TypeError while setting xgb.DMatrix(feature_names=)',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'c920eef3'},\n",
       " {'text': 'To install Xgboost, use the code below directly in your jupyter notebook:\\n(Pip 21.3+ is required)\\npip install xgboost\\nYou can update your pip by using the code below:\\npip install --upgrade pip\\nFor more about xgbboost and installation, check here:\\nhttps://xgboost.readthedocs.io/en/stable/install.html\\nAminat Abolade',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'How to Install Xgboost',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '5017c9a4'},\n",
       " {'text': 'Sometimes someone might wonder what eta means in the tunable hyperparameters of XGBoost and how it helps the model.\\nETA is the learning rate of the model. XGBoost uses gradient descent to calculate and update the model. In gradient descent, we are looking for the minimum weights that help the model to learn the data very well. This minimum weights for the features is updated each time the model passes through the features and learns the features during training. Tuning the learning rate helps you tell the model what speed it would use in deriving the minimum for the weights.',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'What is eta in XGBoost',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '6ffe101d'},\n",
       " {'text': 'For ensemble algorithms, during the week 6, one bagging algorithm and one boosting algorithm were presented: Random Forest and XGBoost, respectively.\\nRandom Forest trains several models in parallel. The output can be, for example, the average value of all the outputs of each model. This is called bagging.\\nXGBoost trains several models sequentially: the previous model error is used to train the following model. Weights are used to ponderate the models such as the best models have higher weights and are therefore favored for the final output. This method is called boosting.\\nNote that boosting is not necessarily better than bagging.\\nMélanie Fouesnard\\nBagging stands for “Bootstrap Aggregation” - it involves taking multiple samples with replacement to derive multiple training datasets from the original training dataset (bootstrapping), training a classifier (e.g. decision trees or stumps for Random Forests) on each such training dataset, and then combining the the predictions (aggregation) to obtain the final prediction. For classification, predictions are combined via voting; for regression, via averaging. Bagging can be done in parallel, since the various classifiers are independent. Bagging decreases variance (but not bias) and is robust against overfitting.\\nBoosting, on the other hand, is sequential - each model learns from the mistakes of its predecessor. Observations are given different weights - observations/samples misclassified by the previous classifier are given a higher weight, and this process is continued until a stopping condition is reached (e.g. max. No. of models is reached, or error is acceptably small, etc.). Boosting reduces bias & is generally more accurate than bagging, but can be prone to overfitting.\\nRileen',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'What is the difference between bagging and boosting?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'a55b29ff'},\n",
       " {'text': 'I wanted to directly capture the output from the xgboost training for multiple eta values to a dictionary without the need to run the same cell multiple times and manually editing the eta value in between or copy the code for a second eta value.\\nUsing the magic cell command “%%capture output” I was only able to capture the complete output for all iterations for the loop, but. I was able to solve this using the following approach. This is just a code sample to grasp the idea.\\n# This would be the content of the Jupyter Notebook cell\\nfrom IPython.utils.capture import capture_output\\nimport sys\\ndifferent_outputs = {}\\nfor i in range(3):\\nwith capture_output(sys.stdout) as output:\\nprint(i)\\nprint(\"testing capture\")\\ndifferent_outputs[i] = output.stdout\\n# different_outputs\\n# {0: \\'0\\\\ntesting capture\\\\n\\',\\n#  1: \\'1\\\\ntesting capture\\\\n\\',\\n#  2: \\'2\\\\ntesting capture\\\\n\\'}\\nAdded by Sylvia Schmitt',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'Capture stdout for each iterations of a loop separately',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'eac70ce3'},\n",
       " {'text': 'Calling roc_auc_score() to get auc is throwing the above error.\\nSolution to this issue is to make sure that you pass y_actuals as 1st argument and y_pred as 2nd argument.\\nroc_auc_score(y_train, y_pred)\\nHareesh Tummala',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'ValueError: continuous format is not supported',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '5f91f8ca'},\n",
       " {'text': 'When rmse stops improving means, when it stops to decrease or remains almost similar.\\nPastor Soto',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'Question 3 of homework 6 if i see that rmse goes up at a certain number of n_estimators but then goes back down lower than it was before, should the answer be the number of n_estimators after which rmse initially went up, or the number after which it was its overall lowest value?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'a3be507a'},\n",
       " {'text': 'dot_data = tree.export_graphviz(regr, out_file=None,\\nfeature_names=boston.feature_names,\\nfilled=True)\\ngraphviz.Source(dot_data, format=\"png\")\\nKrishna Anand\\nfrom sklearn import tree\\ntree.plot_tree(dt,feature_names=dv.feature_names_)\\nAdded By Ryan Pramana',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'One of the method to visualize the decision trees',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '9a8faa50'},\n",
       " {'text': 'Solution: This problem happens because you use DecisionTreeClassifier instead of DecisionTreeRegressor. You should check if you want to use a Decision tree for classification or regression.\\nAlejandro Aponte',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': \"ValueError: Unknown label type: 'continuous'\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'a6e384fe'},\n",
       " {'text': 'When I run dt = DecisionTreeClassifier() in jupyter in same laptop, each time I re-run it or do (restart kernel + run) I get different values of auc. Some of them are 0.674, 0.652, 0.642, 0.669 and so on.  Anyone knows why it could be? I am referring to 7:40-7:45 of video 6.3.\\nSolution: try setting the random seed e.g\\ndt = DecisionTreeClassifier(random_state=22)\\nBhaskar Sarma',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'Different values of auc, each time code is re-run',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'ddc14ada'},\n",
       " {'text': \"They both do the same, it's just less typing from the script.\\nAsked by Andrew Katoch, Added by Edidiong Esu\",\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'Does it matter if we let the Python file create the server or if we run gunicorn directly?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '593f7569'},\n",
       " {'text': 'When I tried to run example from the video using function ping and can not import it. I use import ping and it was unsuccessful. To fix it I use the statement:\\n\\nfrom [file name] import ping\\nOlga Rudakova',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'No module named ‘ping’?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '6cb56405'},\n",
       " {'text': 'The DictVectorizer has a function to get the feature names get_feature_names_out(). This is helpful for example if you need to analyze feature importance but use the dict vectorizer for one hot encoding. Just keep in mind it does return a numpy array so you may need to convert this to a list depending on your usage for example dv.get_feature_names_out() will return a ndarray array of string objects. list(dv.get_feature_names_out()) will convert to a standard list of strings. Also keep in mind that you first need to fit the predictor and response arrays before you have access to the feature names.\\nQuinn Avila',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'DictVectorizer feature names',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'a22a93f1'},\n",
       " {'text': \"They both do the same, it's just less typing from the script.\",\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'Does it matter if we let the Python file create the server or if we run gunicorn directly?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '593f7569'},\n",
       " {'text': 'This error occurs because the list of feature names contains some characters like \"<\" that are not supported. To fix this issue, you can replace those problematic characters with supported ones. If you want to create a consistent list of features with no special characters, you can achieve it like this:\\nYou can address this error by replacing problematic characters in the feature names with underscores, like so:\\nfeatures = [f.replace(\\'=<\\', \\'_\\').replace(\\'=\\', \\'_\\') for f in features]\\nThis code will go through the list of features and replace any instances of \"=<\" with \"\", as well as any \"=\" with \"\", ensuring that the feature names only consist of supported characters.',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'ValueError: feature_names must be string, and may not contain [, ] or <',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'b6259dea'},\n",
       " {'text': \"To make it easier for us to determine which features are important, we can use a horizontal bar chart to illustrate feature importance sorted by value.\\n1. # extract the feature importances from the model\\nfeature_importances = list(zip(features_names, rdr_model.feature_importances_))\\nimportance_df = pd.DataFrame(feature_importances, columns=['feature_names', 'feature_importances'])\\n2. # sort descending the dataframe by using feature_importances value\\nimportance_df = importance_df.sort_values(by='feature_importances', ascending=False)\\n3. # create a horizontal bar chart\\nplt.figure(figsize=(8, 6))\\nsns.barplot(x='feature_importances', y='feature_names', data=importance_df, palette='Blues_r')\\nplt.xlabel('Feature Importance')\\nplt.ylabel('Feature Names')\\nplt.title('Feature Importance Chart')\\nRadikal Lukafiardi\",\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'Visualize Feature Importance by using horizontal bar chart',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'bcfdc6f4'},\n",
       " {'text': 'Instead of using np.sqrt() as the second step. You can extract it using like this way :\\nmean_squared_error(y_val, y_predict_val,squared=False)\\nAhmed Okka',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'RMSE using metrics.root_meas_square()',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'a7e7cdd2'},\n",
       " {'text': 'I like this visual implementation of features importance in scikit-learn library:\\nhttps://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\\nIt actually adds std.errors to features importance -> so that you can trace stability of features (important for a model’s explainability) over the different params of the model.\\nIvan Brigida',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'Features Importance graph',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '55477da8'},\n",
       " {'text': 'Expanded error says: xgboost.core.XGBoostError: sklearn needs to be installed in order to use this module. So, sklearn in requirements solved the problem.\\nGeorge Chizhmak',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'xgboost.core.XGBoostError: This app has encountered an error. The original error message is redacted to prevent data leaks.',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '6a245a05'},\n",
       " {'text': 'Information gain  in Y due to X, or the mutual information of Y and X\\nWhere  is the entropy of Y. \\n\\nIf X is completely uninformative about Y:\\nIf X is completely informative about Y: )\\nHrithik Kumar Advani',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'Information Gain',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '4405bfca'},\n",
       " {'text': 'Filling in missing values using an entire dataset before splitting for training/testing/validation causes',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'Data Leakage',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '3e0acc25'},\n",
       " {'text': 'Save model by calling ‘booster.save_model’, see eg\\nLoad model:\\nDawuta Smit\\nThis section is moved to Projects',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'Serialized Model Xgboost error',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'abaecdf8'},\n",
       " {'text': 'TODO',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'How to get started with Week 8?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'ff40f83b'},\n",
       " {'text': 'Create or import your notebook into Kaggle.\\nClick on the Three dots at the top right hand side\\nClick on Accelerator\\nChoose T4 GPU\\nKhurram Majeed',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'How to use Kaggle for Deep Learning?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '95a16746'},\n",
       " {'text': 'Create or import your notebook into Google Colab.\\nClick on the Drop Down at the top right hand side\\nClick on “Change runtime type”\\nChoose T4 GPU\\nKhurram Majeed',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'How to use Google Colab for Deep Learning?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '46acdd18'},\n",
       " {'text': 'Connecting your GPU on Saturn Cloud to Github repository is not compulsory, since you can just download the notebook and copy it to the Github folder. But if you like technology to do things for you, then follow the solution description below:\\nSolution description: Follow the instructions in these github docs to create an SSH private and public key:\\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-ke\\ny-and-adding-it-to-the-ssh-agenthttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account?tool=webui\\nThen the second video on this module about saturn cloud would show you how to add the ssh keys to secrets and authenticate through a terminal.\\nOr alternatively, you could just use the public keys provided by Saturn Cloud by default. To do so, follow these steps:\\nClick on your username and on manage\\nDown below you will see the Git SSH keys section.\\nCopy the default public key provided by Saturn Cloud\\nPaste these key into the SSH keys section of your github repo\\nOpen a terminal on Saturn Cloud and run this command “ssh -T git@github.com”\\nYou will receive a successful authentication notice.\\nOdimegwu David',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'How do I push from Saturn Cloud to Github?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'f721d54b'},\n",
       " {'text': 'This template is referred to in the video 8.1b Setting up the Environment on Saturn Cloud\\nbut the location shown in the video is no longer correct.\\nThis template has been moved to “python deep learning tutorials’ which is shown on the Saturn Cloud home page.\\nSteven Christolis',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'Where is the Python TensorFlow template on Saturn Cloud?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '69cd4897'},\n",
       " {'text': 'The above error happens since module scipy is not installed in the saturn cloud tensorflow image. While creating the Jupyter server resource, in the “Extra Packages” section under pip in the textbox write scipy. Below the textbox, the pip install scipy command will be displayed. This will ensure when the resource spins up, the scipy package will be automatically installed. This approach can also be followed for additional python packages.\\nSumeet Lalla',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'Getting error module scipy not found during model training in Saturn Cloud tensorflow image',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '346e799a'},\n",
       " {'text': 'Problem description: Uploading the data to saturn cloud from kaggle can be time saving, specially if the dataset is large.\\nYou can just download to your local machine and then upload to a folder on saturn cloud, but there is a better solution that needs to be set once and you have access to all kaggle datasets in saturn cloud.\\nOn your notebook run:\\n!pip install -q kaggle\\nGo to Kaggle website (you need to have an account for this):\\nClick on your profile image -> Account\\nScroll down to the API box\\nClick on Create New API token\\nIt will download a json file with the name kaggle.json store on your local computer. We need to upload this file in the .kaggle folder\\nOn the notebook click on folder icon on the left upper corner\\nThis will take you to the root folder\\nClick on the .kaggle folder\\nOnce inside of the .kaggle folder upload the kaggle.json file that you downloaded\\nRun this command on your notebook:\\n!chmod 600 /home/jovyan/.kaggle/kaggle.json\\nDownload the data using this command:\\n!kaggle datasets download -d agrigorev/dino-or-dragon\\nCreate a folder to unzip your files:\\n!mkdir data\\nUnzip your files inside that folder\\n!unzip dino-or-dragon.zip -d data\\nPastor Soto',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'How to upload kaggle data to Saturn Cloud?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '551461b2'},\n",
       " {'text': 'In order to run tensorflow with gpu on your local machine you’ll need to setup cuda and cudnn.\\nThe process can be overwhelming. Here’s a simplified guide\\nOsman Ali',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'How to install CUDA & cuDNN on Ubuntu 22.04',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'c3ba4459'},\n",
       " {'text': 'Problem description:\\nWhen loading saved model getting error: ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.\\nSolution description:\\nBefore loading model need to evaluate the model on input data: model.evaluate(train_ds)\\nAdded by Vladimir Yesipov',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'Error: (ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.) when loading model.',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'a114ad55'},\n",
       " {'text': 'Problem description:\\nWhen follow module 8.1b video to setup git in Saturn Cloud, run `ssh -T git@github.com` lead error `git@github.com: Permission denied (publickey).`\\nSolution description:\\nAlternative way, we can setup git in our Saturn Cloud env with generate SSH key in our Saturn Cloud and add it to our git account host. After it, we can access/manage our git through Saturn’s jupyter server. All steps detailed on this following tutorial: https://saturncloud.io/docs/using-saturn-cloud/gitrepo/\\nAdded by Ryan Pramana',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'Getting error when connect git on Saturn Cloud: permission denied',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'dd3c8000'},\n",
       " {'text': \"Problem description:\\nGetting an error using <git clone git@github.com:alexeygrigorev/clothing-dataset-small.git>\\nThe error:\\nCloning into 'clothing-dataset'...\\nHost key verification failed.\\nfatal: Could not read from remote repository.\\nPlease make sure you have the correct access rights\\nand the repository exists.\\nSolution description:\\nwhen cloning the repo, you can also chose https - then it should work. This happens when you don't have your ssh key configured.\\n<git clone https://github.com/alexeygrigorev/clothing-dataset-small.git>\\nAdded by Gregory Morris\",\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'Host key verification failed.',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '34b0ebfc'},\n",
       " {'text': \"Problem description\\nThe accuracy and the loss are both still the same or nearly the same while training.\\nSolution description\\nIn the homework, you should set class_mode='binary' while reading the data.\\nAlso, problem occurs when you choose the wrong optimizer, batch size, or learning rate\\nAdded by Ekaterina Kutovaia\",\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'The same accuracy on epochs',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '7d11d5ce'},\n",
       " {'text': 'Problem:\\nWhen resuming training after augmentation, the loss skyrockets (1000+ during first epoch) and accuracy settles around 0.5 – i.e. the model becomes as good as a random coin flip.\\nSolution:\\nCheck that the augmented ImageDataGenerator still includes the option “rescale” as specified in the preceding step.\\nAdded by Konrad Mühlberg',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'Model breaking after augmentation – high loss + bad accuracy',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'e4e45f15'},\n",
       " {'text': \"While doing:\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nmodel = tf.keras.models.load_model('model_saved.h5')\\nIf you get an error message like this:\\nValueError: The channel dimension of the inputs should be defined. The input_shape received is (None, None, None, None), where axis -1 (0-based) is the channel dimension, which found to be `None`.\\nSolution:\\nSaving a model (either yourself via model.save() or via checkpoint when save_weights_only = False) saves two things: The trained model weights (for example the best weights found during training) and the model architecture.  If the number of channels is not explicitly specified in the Input layer of the model, and is instead defined as a variable, the model architecture will not have the value in the variable stored. Therefore when the model is reloaded, it will complain about not knowing the number of channels. See the code below, in the first line, you need to specify number of channels explicitly:\\n# model architecture:\\ninputs = keras.Input(shape=(input_size, input_size, 3))\\nbase = base_model(inputs, training=False)\\nvectors = keras.layers.GlobalAveragePooling2D()(base)\\ninner = keras.layers.Dense(size_inner, activation='relu')(vectors)\\ndrop = keras.layers.Dropout(droprate)(inner)\\noutputs = keras.layers.Dense(10)(drop)\\nmodel = keras.Model(inputs, outputs)\\n(Memoona Tahira)\",\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'Missing channel value error while reloading model:',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'b3997e6f'},\n",
       " {'text': \"Problem:\\nA dataset for homework is in a zipped folder. If you unzip it within a jupyter notebook by means of ! unzip command, you’ll see a huge amount of output messages about unzipping of each image. So you need to suppress this output\\nSolution:\\nExecute the next cell:\\n%%capture\\n! unzip zipped_folder_name.zip -d destination_folder_name\\nAdded by Alena Kniazeva\\nInside a Jupyter Notebook:\\nimport zipfile\\nlocal_zip = 'data.zip'\\nzip_ref = zipfile.ZipFile(local_zip, 'r')\\nzip_ref.extractall('data')\\nzip_ref.close()\",\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'How to unzip a folder with an image dataset and suppress output?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'e414df91'},\n",
       " {'text': 'Problem:\\nWhen we run train_gen.flow_from_directory() as in video 8.5, it finds images belonging to 10 classes. Does it understand the names of classes from the names of folders? Or, there is already something going on deep behind?\\nSolution:\\nThe name of class is the folder name\\nIf you just create some random folder with the name \"xyz\", it will also be considered as a class!! The name itself is saying flow_from_directory\\na clear explanation below:\\nhttps://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720\\nAdded by Bhaskar Sarma',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'How keras flow_from_directory know the names of classes in images?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'f20a3479'},\n",
       " {'text': 'Problem:\\nI created a new environment in SaturnCloud and chose the image corresponding to Saturn with Tensorflow, but when I tried to fit the model it showed an error about the missing module: scipy\\nSolution:\\nInstall the module in a new cell: !pip install scipy\\nRestart the kernel and fit the model again\\nAdded by Erick Calderin',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'Error with scipy missing module in SaturnCloud',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'e7af4968'},\n",
       " {'text': 'The command to read folders in the dataset in the tensorflow source code is:\\nfor subdir in sorted(os.listdir(directory)):\\n…\\nReference: https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py, line 563\\nThis means folders will be read in alphabetical order. For example, in the case of a folder named dino, and another named dragon, dino will read first and will have class label 0, whereas dragon will be read in next and will have class label 1.\\nWhen a Keras model predicts binary labels, it will only return one value, and this is the probability of class 1 in case of sigmoid activation function in the last dense layer with 2 neurons. The probability of class 0 can be found out by:\\nprob(class(0)) = 1- prob(class(1))\\nIn case of using from_logits to get results, you will get two values for each of the labels.\\nA prediction of 0.8 is saying the probability that the image has class label 1 (in this case dragon), is 0.8, and conversely we can infer the probability that the image has class label 0 is 0.2.\\n(Added by Memoona Tahira)',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'How are numeric class labels determined in flow_from_directroy using binary class mode and what is meant by the single probability predicted by a binary Keras model:',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '9fad096e'},\n",
       " {'text': \"It's fine, some small changes are expected\\nAlexey Grigorev\",\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'Does the actual values matter after predicting with a neural network or it should be treated as like hood of falling in a class?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'bcdf7407'},\n",
       " {'text': 'Problem:\\nI found running the wasp/bee model on my mac laptop had higher reported accuracy and lower std deviation than the HW answers. This may be because of the SGD optimizer. Running this on my mac printed a message about a new and legacy version that could be used.\\nSolution:\\nTry running the same code on google collab or another way. The answers were closer for me on collab. Another tip is to change the runtime to use T4 and the model run’s faster than just CPU\\nAdded by Quinn Avila',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'What if your accuracy and std training loss don’t match HW?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '8d1e7e20'},\n",
       " {'text': 'When running “model.fit(...)” an additional parameter “workers” can be specified for speeding up the data loading/generation. The default value is “1”. Try out which value between 1 and the cpu count on your system performs best.\\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\\nAdded by Sylvia Schmitt',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'Using multi-threading for data generation in “model.fit()”',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '2023a9dc'},\n",
       " {'text': 'Reproducibility for training runs can be achieved following these instructions: \\nhttps://www.tensorflow.org/versions/r2.8/api_docs/python/tf/config/experimental/enable_op_determinism\\nseed = 1234\\ntf.keras.utils.set_random_seed(seed)\\ntf.config.experimental.enable_op_determinism()\\nThis will work for a script, if this gets executed multiple times.\\nAdded by Sylvia Schmitt',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'Reproducibility with TensorFlow using a seed point',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '468f69ff'},\n",
       " {'text': 'Pytorch is also a deep learning framework that allows to do equivalent tasks as keras. Here is a tutorial to create a CNN from scratch using pytorch :\\nhttps://blog.paperspace.com/writing-cnns-from-scratch-in-pytorch/\\nThe functions have similar goals. The syntax can be slightly different. For the lessons and the homework, we use keras, but one can feel free to make a pull request with the equivalent with pytorch for lessons and homework!\\nMélanie Fouesnard',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'Can we use pytorch for this lesson/homework ?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'c4ff26e5'},\n",
       " {'text': \"While training a Keras model you get the error “Failed to find data adapter that can handle input: <class 'keras.src.preprocessing.image.ImageDataGenerator'>, <class 'NoneType'>” you may have unintentionally passed the image generator instead of the dataset to the model\\ntrain_gen = ImageDataGenerator(rescale=1./255)\\ntrain_ds = train_gen.flow_from_directory(…)\\nhistory_after_augmentation = model.fit(\\ntrain_gen, # this should be train_ds!!!\\nepochs=10,\\nvalidation_data=test_gen # this should be test_ds!!!\\n)\\nThe fix is simple and probably obvious once pointed out, use the training and validation dataset (train_ds and val_ds) returned from flow_from_directory\\nAdded by Tzvi Friedman\",\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'Keras model training fails with “Failed to find data adapter”',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '62722d72'},\n",
       " {'text': 'The command ‘nvidia-smi’ has a built-in function which will run it in subsequently updating it every N seconds without the need of using the command ‘watch’.\\nnvidia-smi -l <N seconds>\\nThe following command will run ‘nvidia-smi’ every 2 seconds until interrupted using CTRL+C.\\nnvidia-smi -l 2\\nAdded by Sylvia Schmitt',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'Running ‘nvidia-smi’ in a loop without using ‘watch’',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'd1419be1'},\n",
       " {'text': 'The Python package ‘’ is an interactive GPU process viewer similar to ‘htop’ for CPU.\\nhttps://pypi.org/project//\\nImage source: https://pypi.org/project//\\nAdded by Sylvia Schmitt',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'Checking GPU and CPU utilization using ‘nvitop’',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'a5f6f439'},\n",
       " {'text': \"Let’s say we define our Conv2d layer like this:\\n>> tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3))\\nIt means our input image is RGB (3 channels, 150 by 150 pixels), kernel is 3x3 and number of filters (layer’s width) is 32.\\nIf we check model.summary() we will get this:\\n_________________________________________________________________\\nLayer (type)                Output Shape              Param #\\n=================================================================\\nconv2d (Conv2D)             (None, 148, 148, 32)      896\\nSo where does 896 params come from? It’s computed like this:\\n>>> (3*3*3 +1) * 32\\n896\\n# 3x3 kernel, 3 channels RGB, +1 for bias, 32 filters\\nWhat about the number of “features” we get after the Flatten layer?\\nFor our homework model.summary() for last MaxPooling2d and Flatten layers looked like this:\\n_________________________________________________________________\\nLayer (type)                Output Shape              Param #\\n=================================================================\\nmax_pooling2d_3       (None, 7, 7, 128)         0\\nflatten (Flatten)           (None, 6272)              0\\nSo where do 6272 vectors come from? It’s computed like this:\\n>>> 7*7*128\\n6272\\n# 7x7 “image shape” after several convolutions and poolings, 128 filters\\nAdded by Andrii Larkin\",\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'Q: Where does the number of Conv2d layer’s params come from? Where does the number of “features” we get after the Flatten layer come from?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '879c1ec0'},\n",
       " {'text': 'It’s quite useful to understand that all types of models in the course are a plain stack of layers where each layer has exactly one input tensor and one output tensor (Sequential model TF page, Sequential class).\\nYou can simply start from an “empty” model and add more and more layers in a sequential order.\\nThis mode is called “Sequential Model API”  (easier)\\nIn Alexey’s videos it is implemented as chained calls of different entities (“inputs”,“base”, “vectors”,  “outputs”) in a more advanced mode “Functional Model API”.\\nMaybe a more complicated way makes sense when you do Transfer Learning and want to separate “Base” model vs. rest, but in the HW you need to recreate the full model from scratch ⇒ I believe it is easier to work with a sequence of “similar” layers.\\nYou can read more about it in this TF2 tutorial.\\nA really useful Sequential model example is shared in the Kaggle’s “Bee or Wasp” dataset folder with code: notebook\\nAdded by Ivan Brigida\\nFresh Run on Neural Nets\\nWhile correcting an error on neural net architecture, it is advised to do fresh run by restarting kernel, else the model learns on top of previous runs.\\nAdded by Abhijit Chakraborty',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'Sequential vs. Functional Model Modes in Keras (TF2)',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '3ac604c3'},\n",
       " {'text': \"I found this code snippet fixed my OOM errors, as I have an Nvidia GPU. Can't speak to OOM errors on CPU, though.\\nhttps://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth\\n```\\nphysical_devices = tf.configlist_physical_devices('GPU')\\ntry:\\ntf.config.experimental.set_memory_growth(physical_devices[0],True)\\nexcept:\\n# Invalid device or cannot modify virtual devices once initialized.\\npass\\n```\",\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'Out of memory errors when running tensorflow',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '0315aa96'},\n",
       " {'text': 'When training the models, in the fit function, you can specify the number of workers/threads.\\nThe number of threads apparently also works for GPUs, and came very handy in google colab for the T4 GPU, since it was very very slow, and workers default value is 1.\\nI changed the workers variable to 2560, following this thread in stackoverflow. I am using the free T4 GPU.  (https://stackoverflow.com/questions/68208398/how-to-find-the-number-of-cores-in-google-colabs-gpu)\\nAdded by Ibai Irastorza',\n",
       "  'section': '8. Neural Networks and Deep Learning',\n",
       "  'question': 'Model training very slow in google colab with T4 GPU',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'daf84bc3'},\n",
       " {'text': 'From the keras documentation:\\nDeprecated: tf.keras.preprocessing.image.ImageDataGenerator is not recommended for new code. Prefer loading images with tf.keras.utils.image_dataset_from_directory and transforming the output tf.data.Dataset with preprocessing layers. For more information, see the tutorials for loading images and augmenting images, as well as the preprocessing layer guide.\\nHrithik Kumar Advani',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'Using image_dataset_from_directory instead of ImageDataGeneratorn for loading images',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '1e956ca7'},\n",
       " {'text': 'TODO',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'How to get started with Week 9?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '3ee083ab'},\n",
       " {'text': 'The week 9 uses a link to github to fetch the models.\\nThe original link was moved to here:\\nhttps://github.com/DataTalksClub/machine-learning-zoomcamp/releases',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'Where is the model for week 9?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'f826cba4'},\n",
       " {'text': 'Solution description\\nIn the unit 9.6, Alexey ran the command echo ${REMOTE_URI} which turned the URI address in the terminal. There workaround is to set a local variable (REMOTE_URI) and assign your URI address in the terminal and use it to login the registry, for instance, REMOTE_URI=2278222782.dkr.ecr.ap-south-1.amazonaws.com/clothing-tflite-images (fake address). One caveat is that you will lose this variable once the session is terminated.\\nI also had the same problem on Ubuntu terminal. I executed the following two commands:\\n$ export REMOTE_URI=1111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\\n$ echo $REMOTE_URI\\n111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\\nNote: 1. no curly brackets (e.g. echo ${REMOTE_URI}) needed unlike in video 9.6,\\n2. Replace REMOTE_URI with your URI\\n(Bhaskar Sarma)',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'Executing the command echo ${REMOTE_URI} returns nothing.',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '60fa95ed'},\n",
       " {'text': 'The command aws ecr get-login --no-include-email returns an invalid choice error:\\nThe solution is to use the following command instead:  aws ecr get-login-password\\nCould simplify the login process with, just replace the <ACCOUNT_NUMBER> and <REGION> with your values:\\nexport PASSWORD=`aws ecr get-login-password`\\ndocker login -u AWS -p $PASSWORD <ACCOUNT_NUMBER>.dkr.ecr.<REGION>.amazonaws.com/clothing-tflite-images\\nAdded by Martin Uribe',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'Getting a syntax error while trying to get the password from aws-cli',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '53f3ee10'},\n",
       " {'text': 'We can use the keras.models.Sequential() function to pass many parameters of the cnn at once.\\nKrishna Anand',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'Pass many parameters in the model at once',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '93aa4278'},\n",
       " {'text': 'This error is produced sometimes when building your docker image from the Amazon python base image.\\nSolution description: The following could solve the problem.\\nUpdate your docker desktop if you haven’t done so.\\nOr restart docker desktop and terminal and then build the image all over again.\\nOr if all else fails, first run the following command: DOCKER_BUILDKIT=0  docker build .  then build your image.\\n(optional) Added by Odimegwu David',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'Getting  ERROR [internal] load metadata for public.ecr.aws/lambda/python:3.8',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '0edeb016'},\n",
       " {'text': \"When trying to run the command  !ls -lh in windows jupyter notebook  , I was getting an error message that says “'ls' is not recognized as an internal or external command,operable program or batch file.\\nSolution description :\\nInstead of !ls -lh , you can use this command !dir , and you will get similar output\\nAsia Saeed\",\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': \"Problem: 'ls' is not recognized as an internal or external command, operable program or batch file.\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'ba186de6'},\n",
       " {'text': 'When I run   import tflite_runtime.interpreter as tflite , I get an error message says “ImportError: generic_type: type \"InterpreterWrapper\" is already registered!”\\nSolution description\\nThis error occurs when you import both tensorflow  and tflite_runtime.interpreter  “import tensorflow as tf” and “import tflite_runtime.interpreter as tflite” in the same notebook.  To fix the issue, restart the kernel and import only tflite_runtime.interpreter \" import tflite_runtime.interpreter as tflite\".\\nAsia Saeed',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'ImportError: generic_type: type \"InterpreterWrapper\" is already registered!',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'da2f1cf4'},\n",
       " {'text': 'Problem description:\\nIn command line try to do $ docker build -t dino_dragon\\ngot this Using default tag: latest\\n[2022-11-24T06:48:47.360149000Z][docker-credential-desktop][W] Windows version might not be up-to-date: The system cannot find the file specified.\\nerror during connect: This error may indicate that the docker daemon is not running.: Post\\n.\\nSolution description:\\nYou need to make sure that Docker is not stopped by a third-party program.\\nAndrei Ilin',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'Windows version might not be up-to-date',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '7fd648ca'},\n",
       " {'text': 'When running docker build -t dino-dragon-model it returns the above error\\nThe most common source of this error in this week is because Alex video shows a version of the wheel with python 8, we need to find a wheel with the version that we are working on. In this case python 9. Another common error is to copy the link, this will also produce the same error, we need to download the raw format:\\nhttps://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp39-cp39-linux_x86_64.whl\\nPastor Soto',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '42c09143'},\n",
       " {'text': 'Problem description:\\nIn video 9.6, after installing aswcli, we should configure it with aws configure . There it asks for Access Key ID, Secret Access Key, Default Region Name and also Default output format. What we should put for Default output format? Leaving it as  None is okay?\\nSolution description:\\nYes, in my I case I left everything as the provided defaults (except, obviously, the Access key and the secret access key)\\nAdded by Bhaskar Sarma',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'How to do AWS configure after installing awscli',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'd6d534fc'},\n",
       " {'text': 'Problem:\\nWhile passing local testing of the lambda function without issues, trying to test the same input with a running docker instance results in an error message like\\n{‘errorMessage’: ‘Unable to marshal response: Object of type float32 is not JSON serializable’, ‘errorType’: ‘Runtime.MarshalError’, ‘requestId’: ‘f155492c-9af2-4d04-b5a4-639548b7c7ac’, ‘stackTrace’: []}\\nThis happens when a model (in this case the dino vs dragon model) returns individual estimation values as numpy float32 values (arrays). They need to be converted individually to base-Python floats in order to become “serializable”.\\nSolution:\\nIn my particular case, I set up the dino vs dragon model in such a way as to return a label + predicted probability for each class as follows (below is a two-line extract of function predict() in the lambda_function.py):\\npreds = [interpreter.get_tensor(output_index)[0][0], \\\\\\n1-interpreter.get_tensor(output_index)[0][0]]\\nIn which case the above described solution will look like this:\\npreds = [float(interpreter.get_tensor(output_index)[0][0]), \\\\\\nfloat(1-interpreter.get_tensor(output_index)[0][0])]\\nThe rest can be made work by following the chapter 9 (and/or chapter 5!) lecture videos step by step.\\nAdded by Konrad Muehlberg',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'Object of type float32 is not JSON serializable',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'b2c0c554'},\n",
       " {'text': 'I had this error when running the command line : interpreter.set_tensor(input_index, x) that can be seen in the video 9.3 around 12 minutes.\\nValueError: Cannot set tensor: Got value of type UINT8 but expected type FLOAT32 for input 0, name: serving_default_conv2d_input:0\\nThis is because the X is an int but a float is expected.\\nSolution:\\nI found this solution from this question here https://stackoverflow.com/questions/76102508/valueerror-cannot-set-tensor-got-value-of-type-float64-but-expected-type-float :\\n# Need to convert to float32 before set_tensor\\nX = np.float32(X)\\nThen, it works. I work with tensorflow 2.15.0, maybe the fact that this version is more recent involves this change ?\\nAdded by Mélanie Fouesnard',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'Error with the line “interpreter.set_tensor(input_index, X”)',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '819afebc'},\n",
       " {'text': 'To check your file size using the powershell terminal, you can do the following command lines:\\n$File = Get-Item -Path path_to_file\\n$FileSize = (Get-Item -Path $FilePath).Length\\nNow you can check the size of your file, for example in MB:\\nWrite-host \"MB\":($FileSize/1MB)\\nSource: https://www.sharepointdiary.com/2020/10/powershell-get-file-size.html#:~:text=To%20get%20the%20size%20of,the%20file%2C%20including%20its%20size.\\nAdded by Mélanie Fouesnard',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'How to easily get file size in powershell terminal ?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '74551c54'},\n",
       " {'text': 'I wanted to understand how lambda container images work in depth and how lambda functions are initialized, for this reason, I found the following documentation\\nhttps://docs.aws.amazon.com/lambda/latest/dg/images-create.html\\nhttps://docs.aws.amazon.com/lambda/latest/dg/runtimes-api.html\\nAdded by Alejandro aponte',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'How do Lambda container images work?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '4d98cd09'},\n",
       " {'text': 'The docker image for aws lambda can be created and pushed to aws ecr and the same can be exposed as a REST API through APIGatewayService in a single go using AWS Serverless Framework. Refer the below article for a detailed walkthrough.\\nhttps://medium.com/hoonio/deploy-containerized-serverless-flask-to-aws-lambda-c0eb87c1404d\\nAdded by Sumeet Lalla',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'How to use AWS Serverless Framework to deploy on AWS Lambda and expose it as REST API through APIGatewayService?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '59a81fd5'},\n",
       " {'text': 'Problem:\\nWhile trying to build docker image in Section 9.5 with the command:\\ndocker build -t clothing-model .\\nIt throws a pip install error for the tflite runtime whl\\nERROR: failed to solve: process \"/bin/sh -c pip install https://github.com/alexeygrigorev/tflite-aws-lambda/blob/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\" did not complete successfully: exit code: 1\\nTry to use this link: https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\\nIf the link above does not work:\\nThe problem is because of the arm architecture of the M1. You will need to run the code on a PC or Ubuntu OS.\\nOr try the code bellow.\\nAdded by Dashel Ruiz Perez\\nSolution:\\nTo build the Docker image, use the command:\\ndocker build --platform linux/amd64 -t clothing-model .\\nTo run the built image, use the command:\\ndocker run -it --rm -p 8080:8080 --platform linux/amd64 clothing-model:latest\\nAdded by Daniel Egbo',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'Error building docker image on M1 Mac',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '35dbd6e2'},\n",
       " {'text': \"Problem: Trying to test API gateway in 9.7 - API Gateway: Exposing the Lambda Function, running: $ python test.py\\nWith error message:\\n{'message': 'Missing Authentication Token'}\\nSolution:\\nNeed to get the deployed API URL for the specific path you are invoking. Example:\\nhttps://<random string>.execute-api.us-east-2.amazonaws.com/test/predict\\nAdded by Andrew Katoch\",\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'Error invoking API Gateway deploy API locally',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'e5fe9efe'},\n",
       " {'text': 'Problem: When trying to install tflite_runtime with\\n!pip install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime\\none gets an error message above.\\nSolution:\\nfflite_runtime is only available for the os-python version combinations that can be found here: https://google-coral.github.io/py-repo/tflite-runtime/\\nyour combination must be missing here\\nyou can see if any of these work for you https://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite\\nand install the needed one using pip\\neg\\npip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\\nas it is done in the lectures code:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/09-serverless/code/Dockerfile#L4\\nAlternatively, use a virtual machine (with VM VirtualBox, for example) with a Linux system. The other way is to run a code at a virtual machine within cloud service, for example you can use Vertex AI Workbench at GCP (notebooks and terminals are provided there, so all tasks may be performed).\\nAdded by Alena Kniazeva, modified by Alex Litvinov',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'Error: Could not find a version that satisfies the requirement tflite_runtime (from versions:none)',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '5c043c62'},\n",
       " {'text': 'docker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system.\\nYou need to restart the docker services to get rid of the above error\\nKrishna Anand',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'Docker run error',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'af0739da'},\n",
       " {'text': 'The docker image can be saved/exported to tar format in local machine using the below command:\\ndocker image save <image-name> -o <name-of-tar-file.tar>\\nThe individual layers of the docker image for the filesystem content can be viewed by extracting the layer.tar present in the <name-of-tar-file.tar> created from above.\\nSumeet Lalla',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'Save Docker Image to local machine and view contents',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '451bc25d'},\n",
       " {'text': 'On vscode running jupyter notebook. After I ‘pip install pillow’, my notebook did not recognize using the import for example from PIL import image. After restarting the jupyter notebook the imports worked.\\nQuinn Avila',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'Jupyter notebook not seeing package',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'ea2e7458'},\n",
       " {'text': 'Due to experimenting back and forth so much without care for storage, I just ran out of it on my 30-GB AWS instance. It turns out that deleting docker images does not actually free up any space as you might expect. After removing images, you also need to run docker system prune',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'Running out of space for AWS instance.',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '6ce8e875'},\n",
       " {'text': 'Using the 2.14 version with python 3.11 works fine.\\nIn case it doesn’t work, I tried with tensorflow 2.4.4 whl, however, make sure to run it on top of supported python versions like 3.8, else there will be issues installing tf==2.4.4\\nAdded by Abhijit Chakraborty',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'Using Tensorflow 2.15 for AWS deployment',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'b50e9e2b'},\n",
       " {'text': 'see here',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…”',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '29311ef5'},\n",
       " {'text': 'Sign in to the AWS Console: Log in to the AWS Console.\\nNavigate to IAM: Go to the IAM service by clicking on \"Services\" in the top left corner and selecting \"IAM\" under the \"Security, Identity, & Compliance\" section.\\nCreate a new policy: In the left navigation pane, select \"Policies\" and click on \"Create policy.\"\\nSelect the service and actions:\\nClick on \"JSON\" and copy and paste the JSON policy you provided earlier for the specific ECR actions.\\nReview and create the policy:\\nClick on \"Review policy.\"\\nProvide a name and description for the policy.\\nClick on \"Create policy.\"\\nJSON policy:\\n{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"VisualEditor0\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"ecr:CreateRepository\",\\n\"ecr:GetAuthorizationToken\",\\n\"ecr:BatchCheckLayerAvailability\",\\n\"ecr:BatchGetImage\",\\n\"ecr:InitiateLayerUpload\",\\n\"ecr:UploadLayerPart\",\\n\"ecr:CompleteLayerUpload\",\\n\"ecr:PutImage\"\\n],\\n\"Resource\": \"*\"\\n}\\n]\\n}\\nAdded by: Daniel Muñoz-Viveros\\nERROR: failed to solve: public.ecr.aws/lambda/python:3.10: error getting credentials - err: exec: \"docker-credential-desktop.exe\": executable file not found in $PATH, out: ``\\n(WSL2 system)\\nSolved: Delete the file ~/.docker/config.json\\nYishan Zhan',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'What IAM permission policy is needed to complete Week 9: Serverless?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '1e0dc11c'},\n",
       " {'text': 'Add the next lines to vim /etc/docker/daemon.json\\n{\\n\"dns\": [\"8.8.8.8\", \"8.8.4.4\"]\\n}\\nThen, restart docker:  sudo service docker restart\\nIbai Irastorza',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'Docker Temporary failure in name resolution',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '1078aeb7'},\n",
       " {'text': \"Solution: add compile = False to the load_model function\\nkeras.models.load_model('model_name.h5', compile=False)\\nNadia Paz\",\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'Keras model *.h5 doesn’t load. Error: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '7daaca73'},\n",
       " {'text': 'This deployment setup can be tested locally using AWS RIE (runtime interface emulator).\\nBasically, if your Docker image was built upon base AWS Lambda image (FROM public.ecr.aws/lambda/python:3.10) - just use certain ports for “docker run” and a certain “localhost link” for testing:\\ndocker run -it --rm -p 9000:8080 name\\nThis command runs the image as a container and starts up an endpoint locally at:\\nlocalhost:9000/2015-03-31/functions/function/invocations\\nPost an event to the following endpoint using a curl command:\\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \\'{}\\'\\nExamples of curl testing:\\n* windows testing:\\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \"{\\\\\"url\\\\\": \\\\\"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\\\\\"}\"\\n* unix testing:\\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \\'{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}\\'\\nIf during testing you encounter an error like this:\\n# {\"errorMessage\": \"Unable to marshal response: Object of type float32 is not JSON serializable\", \"errorType\": \"Runtime.MarshalError\", \"requestId\": \"7ea5d17a-e0a2-48d5-b747-a16fc530ed10\", \"stackTrace\": []}\\njust turn your response at lambda_handler() to string - str(result).\\nAdded by Andrii Larkin',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': 'How to test AWS Lambda + Docker locally?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '0cfbe2e2'},\n",
       " {'text': 'Make sure all codes in test.py dont have any dependencies with tensorflow library. One of most common reason that lead the this error is tflite still imported from tensorflow. Change import tensorflow.lite as tflite to import tflite_runtime.interpreter as tflite\\nAdded by Ryan Pramana',\n",
       "  'section': '9. Serverless Deep Learning',\n",
       "  'question': '\"Unable to import module \\'lambda_function\\': No module named \\'tensorflow\\'\" when run python test.py',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '1460fb65'},\n",
       " {'text': 'I’ve tried to do everything in Google Colab. Here is a way to work with Docker in Google Colab:\\nhttps://gist.github.com/mwufi/6718b30761cd109f9aff04c5144eb885\\n\\uec03%%shell\\npip install udocker\\nudocker --allow-root install\\n\\uec02!udocker --allow-root run hello-world\\nAdded by Ivan Brigida\\nLambda API Gateway errors:\\n`Authorization header requires \\'Credential\\' parameter. Authorization header requires \\'Signature\\' parameter. Authorization header requires \\'SignedHeaders\\' parameter. Authorization header requires existence of either a \\'X-Amz-Date\\' or a \\'Date\\' header.`\\n`Missing Authentication Token`\\nimport boto3\\nclient = boto3.client(\\'apigateway\\')\\nresponse = client.test_invoke_method(\\nrestApiId=\\'your_rest_api_id\\',\\nresourceId=\\'your_resource_id\\',\\nhttpMethod=\\'POST\\',\\npathWithQueryString=\\'/test/predict\\', #depend how you set up the api\\nbody=\\'{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}\\'\\n)\\nprint(response[\\'body\\'])\\nYishan Zhan\\nUnable to run pip install tflite_runtime from github wheel links?\\nTo overcome this issue, you can download the whl file to your local project folder and in the Docker file add the following lines:\\nCOPY <file-name> .\\nRUN pip install <file-name>\\nAbhijit Chakraborty',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'Install Docker (udocker) in Google Colab',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'd4f9efdc'},\n",
       " {'text': 'TODO',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'How to get started with Week 10?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '6a417bfe'},\n",
       " {'text': 'Running a CNN on your CPU can take a long time and once you’ve run out of free time on some cloud providers, it’s time to pay up. Both can be tackled by installing tensorflow with CUDA support on your local machine if you have the right hardware.\\nI was able to get it working by using the following resources:\\nCUDA on WSL :: CUDA Toolkit Documentation (nvidia.com)\\nInstall TensorFlow with pip\\nStart Locally | PyTorch\\nI included the link to PyTorch so that you can get that one installed and working too while everything is fresh on your mind. Just select your options, and for Computer Platform, I chose CUDA 11.7 and it worked for me.\\nAdded by Martin Uribe',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'How to install Tensorflow in Ubuntu WSL2',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'ed8b300d'},\n",
       " {'text': 'If you are running tensorflow on your own machine and you start getting the following errors:\\nAllocator (GPU_0_bfc) ran out of memory trying to allocate 6.88GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\\nTry adding this code in a cell at the beginning of your notebook:\\nconfig = tf.compat.v1.ConfigProto()\\nconfig.gpu_options.allow_growth = True\\nsession = tf.compat.v1.Session(config=config)\\nAfter doing this most of my issues went away. I say most because there was one instance when I still got the error once more, but only during one epoch. I ran the code again, right after it finished, and I never saw the error again.\\nAdded by Martin Uribe',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'Getting: Allocator ran out of memory errors?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'a64aed6b'},\n",
       " {'text': 'In session 10.3, when creating the virtual environment with pipenv and trying to run the script gateway.py, you might get this error:\\nTypeError: Descriptors cannot not be created directly.\\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\\n1. Downgrade the protobuf package to 3.20.x or lower.\\n2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\\nThis will happen if your version of protobuf is one of the newer ones. As a workaround, you can fix the protobuf version to an older one. In my case I got around the issue by creating the environment with:\\npipenv install --python 3.9.13 requests grpcio==1.42.0 flask gunicorn \\\\\\nkeras-image-helper tensorflow-protobuf==2.7.0 protobuf==3.19.6\\nAdded by Ángel de Vicente',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'Problem with recent version of protobuf',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '727238ee'},\n",
       " {'text': 'Due to the uncertainties associated with machines, sometimes you can get the error message like this when you try to run a docker command:\\n”Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?”\\nSolution: The solution is simple. The Docker Desktop might no longer be connecting to the WSL Linux distro. What you need to do is go to your Docker Desktop setting and then click on resources. Under resources, click on WSL Integration. You will get a tab like the image below:\\nJust enable additional distros. That’s all. Even if the additional distro is the same as the default WSL distro.\\nOdimegwu David',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'WSL Cannot Connect To Docker Daemon',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '85d4901d'},\n",
       " {'text': 'In case the HPA instance does not run correctly even after installing the latest version of Metrics Server from the components.yaml manifest with:\\n>>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\\nAnd the targets still appear as <unknown>\\nRun >>kubectl edit deploy -n kube-system metrics-server\\nAnd search for this line:\\nargs:\\n- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\\nAdd this line in the middle:  - --kubelet-insecure-tls\\nSo that it stays like this:\\nargs:\\n- --kubelet-insecure-tls\\n- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\\nSave and run again >>kubectl get hpa\\nAdded by Marilina Orihuela',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'HPA instance doesn’t run properly',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'df023a13'},\n",
       " {'text': 'In case the HPA instance does not run correctly even after installing the latest version of Metrics Server from the components.yaml manifest with:\\n>>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\\nAnd the targets still appear as <unknown>\\nRun the following command:\\nkubectl apply -f https://raw.githubusercontent.com/Peco602/ml-zoomcamp/main/10-kubernetes/kube-config/metrics-server-deployment.yaml\\nWhich uses a metrics server deployment file already embedding the - --kubelet-insecure-tls option.\\nAdded by Giovanni Pecoraro',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'HPA instance doesn’t run properly (easier solution)',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '48e92d65'},\n",
       " {'text': \"When I run pip install grpcio==1.42.0 tensorflow-serving-api==2.7.0 to install the libraries in windows machine,  I was getting the below error :\\nERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\\\\\Users\\\\\\\\Asia\\\\\\\\anaconda3\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\google\\\\\\\\protobuf\\\\\\\\internal\\\\\\\\_api_implementation.cp39-win_amd64.pyd'\\nConsider using the `--user` option or check the permissions.\\nSolution description :\\nI was able to install the libraries using below command:\\npip --user install grpcio==1.42.0 tensorflow-serving-api==2.7.0\\nAsia Saeed\",\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'Could not install packages due to an OSError: [WinError 5] Access is denied',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '1685cae4'},\n",
       " {'text': 'Problem description\\nI was getting the below error message when I run gateway.py after modifying the code & creating virtual environment in  video 10.3 :\\nFile \"C:\\\\Users\\\\Asia\\\\Data_Science_Code\\\\Zoompcamp\\\\Kubernetes\\\\gat.py\", line 9, in <module>\\nfrom tensorflow_serving.apis import predict_pb2\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow_serving\\\\apis\\\\predict_pb2.py\", line 14, in <module>\\nfrom tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow\\\\core\\\\framework\\\\tensor_pb2.py\", line 14, in <module>\\nfrom tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow\\\\core\\\\framework\\\\resource_handle_pb2.py\", line 14, in <module>\\nfrom tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow\\\\core\\\\framework\\\\tensor_shape_pb2.py\", line 36, in <module>\\n_descriptor.FieldDescriptor(\\nFile \"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\google\\\\protobuf\\\\descriptor.py\", line 560, in __new__\\n_message.Message._CheckCalledFromGeneratedFile()\\nTypeError: Descriptors cannot not be created directly.\\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\\n1. Downgrade the protobuf package to 3.20.x or lower.\\n2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\\nSolution description:\\nIssue has been resolved by downgrading protobuf to version 3.20.1.\\npipenv install protobuf==3.20.1\\nAsia Saeed',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'TypeError: Descriptors cannot not be created directly.',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '4fb7b21e'},\n",
       " {'text': 'To install kubectl on windows using the terminal in vscode (powershell), I followed this tutorial: https://medium.com/@ggauravsigra/install-kubectl-on-windows-af77da2e6fff\\nI first downloaded kubectl with curl, with these command lines: https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/#install-kubectl-binary-with-curl-on-windows\\nAt step 3, I followed the tutorial with the copy of the exe file in a specific folder on C drive.\\nThen I added this folder path to PATH in my environment variables.\\nKind can be installed the same way with the curl command on windows, by specifying a folder that will be added to the path environment variable.\\nAdded by Mélanie Fouesnard',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'How to install easily kubectl on windows ?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '8bd3bfc2'},\n",
       " {'text': \"First you need to launch a powershell terminal with administrator privilege.\\nFor this we need to install choco library first through the following syntax in powershell:\\nSet-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\\nKrishna Anand\",\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'Install kind through choco library',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '03b5fc59'},\n",
       " {'text': 'If you are having challenges installing Kind through the Windows Powershell as provided on the website and Choco Library as I did, you can simply install Kind through Go.\\n> Download and Install Go (https://go.dev/doc/install)\\n> Confirm installation by typing the following in Command Prompt -  go version\\n> Proceed by installing Kind by following this command - go install sigs.k8s.io/kind@v0.20.0\\n>Confirm Installation kind --version\\nIt works perfectly.',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'Install Kind via Go package',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '7c31bc9a'},\n",
       " {'text': \"I ran into an issue where kubectl wasn't working.\\nI kept getting the following error:\\nkubectl get service\\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\\nI searched online for a resolution, but everyone kept talking about creating an environment variable and creating some admin.config file in my home directory.\\nAll hogwash.\\nThe solution to my problem was to just start over.\\nkind delete cluster\\nrm -rf ~/.kube\\nkind create cluster\\nNow when I try the same command again:\\nkubectl get service\\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\\nkubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   53s\\nAdded by Martin Uribe\",\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'The connection to the server localhost:8080 was refused - did you specify the right host or port?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '605efc12'},\n",
       " {'text': 'Problem description\\nDue to experimenting back and forth so much without care for storage, I just ran out of it on my 30-GB AWS instance.\\nMy first reflex was to remove some zoomcamp directories, but of course those are mostly code so it didn’t help much.\\nSolution description\\n> docker images\\nrevealed that I had over 20 GBs worth of superseded / duplicate models lying around, so I proceeded to > docker rmi\\na bunch of those — but to no avail!\\nIt turns out that deleting docker images does not actually free up any space as you might expect. After removing images, you also need to run\\n> docker system prune\\nSee also: https://stackoverflow.com/questions/36799718/why-removing-docker-containers-and-images-does-not-free-up-storage-space-on-wind\\nAdded by Konrad Mühlberg',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'Running out of storage after building many docker images',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'c5cde96c'},\n",
       " {'text': 'Yes, the question does require for you to specify values for CPU and memory in the yaml file, however the question that it is use in the form only refers to the port which do have a define correct value for this specific homework.\\nPastor Soto',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'In HW10 Q6 what does it mean “correct value for CPU and memory”? Aren’t they arbitrary?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'd45d2da6'},\n",
       " {'text': 'In Kubernetes resource specifications, such as CPU requests and limits, the \"m\" stands for milliCPU, which is a unit of computing power. It represents one thousandth of a CPU core.\\ncpu: \"100m\" means the container is requesting 100 milliCPUs, which is equivalent to 0.1 CPU core.\\ncpu: \"500m\" means the container has a CPU limit of 500 milliCPUs, which is equivalent to 0.5 CPU core.\\nThese values are specified in milliCPUs to allow fine-grained control over CPU resources. It allows you to express CPU requirements and limits in a more granular way, especially in scenarios where your application might not need a full CPU core.\\nAdded by Andrii Larkin',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'Why cpu vals for Kubernetes deployment.yaml look like “100m” and “500m”? What does \"m\" mean?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '59823c72'},\n",
       " {'text': 'Problem: Failing to load docker-image to cluster (when you’ved named a cluster)\\nkind load docker-image zoomcamp-10-model:xception-v4-001\\nERROR: no nodes found for cluster \"kind\"\\nSolution: Specify cluster name with -n\\nkind -n clothing-model load docker-image zoomcamp-10-model:xception-v4-001\\nAndrew Katoch',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'Kind cannot load docker image',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '665f7b27'},\n",
       " {'text': \"Problem: I download kind from the next command:\\ncurl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.17.0/kind-windows-amd64\\nWhen I try\\nkind --version\\nI get: 'kind' is not recognized as an internal or external command, operable program or batch file\\nSolution: The default name of executable is kind-windows-amd64.exe, so that you have to rename this file to  kind.exe. Put this file in specific folder, and add it to PATH\\nAlejandro Aponte\",\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': \"'kind' is not recognized as an internal or external command, operable program or batch file. (In Windows)\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '0a406fe0'},\n",
       " {'text': 'Using kind with Rootless Docker or Rootless Podman requires some changes on the system (Linux), see kind – Rootless (k8s.io).\\nSylvia Schmitt',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'Running kind on Linux with Rootless Docker or Rootless Podman',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '64b209b0'},\n",
       " {'text': 'Deploy and Access the Kubernetes Dashboard\\nLuke',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'Kubernetes-dashboard',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '518c4cb8'},\n",
       " {'text': 'Make sure you are on AWS CLI v2 (check with aws --version)\\nhttps://docs.aws.amazon.com/cli/latest/userguide/cliv2-migration-instructions.html',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'Correct AWS CLI version for eksctl',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '00882c83'},\n",
       " {'text': 'Problem Description:\\nIn video 10.3, when I was testing a flask service, I got the above error. I ran docker run .. in one terminal. When in second terminal I run python gateway.py, I get the above error.\\nSolution: This error has something to do with versions of Flask and Werkzeug. I got the same error, if I just import flask with from flask import Flask.\\nBy running pip freeze > requirements.txt,I found that their versions are Flask==2.2.2 and Werkzeug==2.2.2. This error appears while using an old version of werkzeug (2.2.2) with new version of flask (2.2.2). I solved it by pinning version of Flask into an older version with pipenv install Flask==2.1.3.\\nAdded by Bhaskar Sarma',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': \"TypeError: __init__() got an unexpected keyword argument 'unbound_message' while importing Flask\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'd6d483ce'},\n",
       " {'text': 'As per AWS documentation:\\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html\\nYou need to do: (change the fields in red)\\naws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com\\nAlternatively you can run the following command without changing anything given you have a default region configured\\naws ecr get-login-password --region $(aws configure get region) | docker login --username AWS --password-stdin \"$(aws sts get-caller-identity --query \"Account\" --output text).dkr.ecr.$(aws configure get region).amazonaws.com\"\\nAdded by Humberto Rodriguez',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…”',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'f9711723'},\n",
       " {'text': 'While trying to run the docker code on M1:\\ndocker run --platform linux/amd64 -it --rm \\\\\\n-p 8500:8500 \\\\\\n-v $(pwd)/clothing-model:/models/clothing-model/1 \\\\\\n-e MODEL_NAME=\"clothing-model\" \\\\\\ntensorflow/serving:2.7.0\\nIt outputs the error:\\nError:\\nStatus: Downloaded newer image for tensorflow/serving:2.7.0\\n[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/generated_message_reflection.cc:2345] CHECK failed: file != nullptr:\\nterminate called after throwing an instance of \\'google::protobuf::FatalException\\'\\nwhat():  CHECK failed: file != nullptr:\\nqemu: uncaught target signal 6 (Aborted) - core dumped\\n/usr/bin/tf_serving_entrypoint.sh: line 3:     8 Aborted                 tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\\nSolution\\ndocker pull emacski/tensorflow-serving:latest\\ndocker run -it --rm \\\\\\n-p 8500:8500 \\\\\\n-v $(pwd)/clothing-model:/models/clothing-model/1 \\\\\\n-e MODEL_NAME=\"clothing-model\" \\\\\\nemacski/tensorflow-serving:latest-linux_arm64\\nSee more here: https://github.com/emacski/tensorflow-serving-arm\\nAdded by Daniel Egbo',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'Error downloading  tensorflow/serving:2.7.0 on Apple M1 Mac',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '5bda3b94'},\n",
       " {'text': 'Similar to the one above but with a different solution the main reason is that emacski doesn’t seem to maintain the repo any more, the latest image is from 2 years ago at the time of writing (December 2023)\\nProblem:\\nWhile trying to run the docker code on Mac M2 apple silicon:\\ndocker run --platform linux/amd64 -it --rm \\\\\\n-p 8500:8500 \\\\\\n-v $(pwd)/clothing-model:/models/clothing-model/1 \\\\\\n-e MODEL_NAME=\"clothing-model\" \\\\\\ntensorflow/serving\\nYou get an error:\\n/usr/bin/tf_serving_entrypoint.sh: line 3:     7 Illegal instruction     tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\\nSolution:\\nUse bitnami/tensorflow-serving base image\\nLaunch it either using docker run\\ndocker run -d \\\\\\n--name tf_serving \\\\\\n-p 8500:8500 \\\\\\n-p 8501:8501 \\\\\\n-v $(pwd)/clothing-model:/bitnami/model-data/1 \\\\\\n-e TENSORFLOW_SERVING_MODEL_NAME=clothing-model \\\\\\nbitnami/tensorflow-serving:2\\nOr the following docker-compose.yaml\\nversion: \\'3\\'\\nservices:\\ntf_serving:\\nimage: bitnami/tensorflow-serving:2\\nvolumes:\\n- ${PWD}/clothing-model:/bitnami/model-data/1\\nports:\\n- 8500:8500\\n- 8501:8501\\nenvironment:\\n- TENSORFLOW_SERVING_MODEL_NAME=clothing-model\\nAnd run it with\\ndocker compose up\\nAdded by Alex Litvinov',\n",
       "  'section': '10. Kubernetes and TensorFlow Serving',\n",
       "  'question': 'Illegal instruction error when running tensorflow/serving image on Mac M2 Apple Silicon (potentially on M1 as well)',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'cccd31cf'},\n",
       " {'text': 'Problem: CPU metrics Shows Unknown\\nNAME         REFERENCE           TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\\ncredit-hpa   Deployment/credit   <unknown>/20%   1         3         1          18s\\nFailedGetResourceMetric       2m15s (x169 over 44m)  horizontal-pod-autoscaler  failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API:\\nSolution:\\n-> Delete HPA (kubectl delete hpa credit-hpa)\\n-> kubectl apply -f https://raw.githubusercontent.com/pythianarora/total-practice/master/sample-kubernetes-code/metrics-server.yaml\\n-> Create HPA\\nThis should solve the cpu metrics report issue.\\nAdded by Priya V',\n",
       "  'section': '11. KServe',\n",
       "  'question': 'HPA doesn’t show CPU metrics',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '57f49999'},\n",
       " {'text': 'Problem description:\\nRunning this:\\ncurl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.9/hack/quick_install.sh\" | bash\\nFails with errors because of istio failing to update resources, and you are on kubectl > 1.25.0.\\nCheck kubectl version with kubectl version\\nSolution description\\nEdit the file “quick_install.bash” by downloading it with curl without running bash. Edit the versions of Istio and Knative as per the matrix on the KServe website.\\nRun the bash script now.\\nAdded by Andrew Katoch',\n",
       "  'section': '11. KServe',\n",
       "  'question': 'Errors with istio during installation',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '5cb58698'},\n",
       " {'text': 'Problem description\\nSolution description\\n(optional) Added by Name',\n",
       "  'section': 'Projects (Midterm and Capstone)',\n",
       "  'question': 'Problem title',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'de650b41'},\n",
       " {'text': 'Answer: You can see them here (it’s taken from the 2022 cohort page). Go to the cohort folder for your own cohort’s deadline.',\n",
       "  'section': 'Projects (Midterm and Capstone)',\n",
       "  'question': 'What are the project deadlines?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '9ffacaac'},\n",
       " {'text': 'Answer: All midterms and capstones are meant to be solo projects. [source @Alexey]',\n",
       "  'section': 'Projects (Midterm and Capstone)',\n",
       "  'question': 'Are projects solo or collaborative/group work?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '4dfb5d4f'},\n",
       " {'text': 'Answer: Ideally midterms up to module-06, capstones include all modules in that cohort’s syllabus. But you can include anything extra that you want to feature. Just be sure to document anything not covered in class.\\nAlso watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nMore discussions:\\n[source1] [source2] [source3]',\n",
       "  'section': 'Projects (Midterm and Capstone)',\n",
       "  'question': 'What modules, topics, problem-sets should a midterm/capstone project cover? Can I do xyz?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '0b8739b7'},\n",
       " {'text': \"These links apply to all projects, actually. Again, for some cohorts, the modules/syllabus might be different, so always check in your cohort’s folder as well for additional or different instructions, if any.\\nMidterm Project Sample: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/cohorts/2021/07-midterm-project\\nMidTerm Project Deliverables: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/projects\\nSubmit MidTerm Project: https://docs.google.com/forms/d/e/1FAIpQLSfgmOk0QrmHu5t0H6Ri1Wy_FDVS8I_nr5lY3sufkgk18I6S5A/viewform\\nDatasets:\\nhttps://www.kaggle.com/datasets and https://www.kaggle.com/competitions\\nhttps://archive.ics.uci.edu/ml/index.php\\nhttps://data.europa.eu/en\\nhttps://www.openml.org/search?type=data\\nhttps://newzealand.ai/public-data-sets\\nhttps://datasetsearch.research.google.com\\nWhat to do and Deliverables\\nThink of a problem that's interesting for you and find a dataset for that\\nDescribe this problem and explain how a model could be used\\nPrepare the data and doing EDA, analyze important features\\nTrain multiple models, tune their performance and select the best model\\nExport the notebook into a script\\nPut your model into a web service and deploy it locally with Docker\\nBonus points for deploying the service to the cloud\",\n",
       "  'section': 'Projects (Midterm and Capstone)',\n",
       "  'question': 'Crucial Links',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '9eb52679'},\n",
       " {'text': 'Answer: Previous cohorts projects page has instructions (youtube).\\nhttps://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2022/projects.md#midterm-project\\nAlexey and his team will compile a g-sheet with links to submitted projects with our hashed emails (just like when we check leaderboard for homework) that are ours to review within the evaluation deadline.\\n~~~ Added by Nukta Bhatia ~~~',\n",
       "  'section': 'Projects (Midterm and Capstone)',\n",
       "  'question': 'How to conduct peer reviews for projects?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '7a1fcfd9'},\n",
       " {'text': 'See the answer here.',\n",
       "  'section': 'Projects (Midterm and Capstone)',\n",
       "  'question': 'Computing the hash for project review',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '1cfa62c5'},\n",
       " {'text': 'For the learning in public for this midterm project it seems that has a total value of 14!, Does this mean that we need make 14 posts?, Or the regular seven posts for each module and each one with a value of 2?, Or just one with a total value of 14?\\n14 posts, one for each day',\n",
       "  'section': 'Projects (Midterm and Capstone)',\n",
       "  'question': 'Learning in public links for the projects',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '2a78f52e'},\n",
       " {'text': 'You can use git-lfs (https://git-lfs.com/) for upload large file to github repository.\\nRyan Pramana',\n",
       "  'section': 'Projects (Midterm and Capstone)',\n",
       "  'question': \"My dataset is too large and I can't loaded in GitHub , do anyone knows about a solution?\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '68aeab64'},\n",
       " {'text': 'If you have submitted two projects (and peer-reviewed at least 3 course-mates’ projects for each submission), you will get the certificate for the course. According to the course coordinator, Alexey Grigorev, only two projects are needed to get the course certificate.\\n(optional) David Odimegwu',\n",
       "  'section': 'Projects (Midterm and Capstone)',\n",
       "  'question': 'What If I submitted only two projects and failed to submit the third?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '9a7c26e0'},\n",
       " {'text': 'Yes. You only need to review peers when you submit your project.\\nConfirmed on Slack by Alexey Grigorev (added by Rileen Sinha)',\n",
       "  'section': 'Projects (Midterm and Capstone)',\n",
       "  'question': \"I did the first two projects and skipped the last one so I wouldn't have two peer review in second capstone right?\",\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '1fd83eb9'},\n",
       " {'text': 'Regarding Point 4 in the midterm deliverables, which states, \"Train multiple models, tune their performance, and select the best model,\" you might wonder, how many models should you train? The answer is simple: train as many as you can. The term \"multiple\" implies having more than one model, so as long as you have more than one, you\\'re on the right track.',\n",
       "  'section': 'Projects (Midterm and Capstone)',\n",
       "  'question': 'How many models should I train?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'fbaa5b20'},\n",
       " {'text': 'I am not sure how the project evaluate assignment works? Where do I find this? I have access to all the capstone 2 project, perhaps, I can randomly pick any to review.\\nAnswer:\\nThe link provided for example (2023/Capstone link ): https://docs.google.com/forms/d/e/1FAIpQLSdgoepohpgbM4MWTAHWuXa6r3NXKnxKcg4NDOm0bElAdXdnnA/viewform contains a list of all submitted projects to be evaluated. More specific, you are to review 3 assigned peer projects. In the spreadsheet are 3 hash values of your assigned peer projects. However, you need to derive the your hash value of your email address and find the value on the spreadsheet under the (reviewer_hash) heading.\\nTo calculate your hash value run the python code below:\\nfrom hashlib import sha1\\ndef compute_hash(email):\\nreturn sha1(email.lower().encode(\\'utf-8\\')).hexdigest()\\n# Example usage **** enter your email below (Example1@gmail.com)****\\nemail = \"Example1@gmail.com\"\\nhashed_email = compute_hash(email)\\nprint(\"Original Email:\", email)\\nprint(\"Hashed Email (SHA-1):\", hashed_email)\\nEdit the above code to replace Example1@gmail.com as your email address\\nStore and run the above python code from your terminal. See below as the Hashed Email (SHA-1) value\\nYou then go to the link: https://docs.google.com/spreadsheets/d/e/2PACX-1vR-7RRtq7AMx5OzI-tDbkzsbxNLm-NvFOP5OfJmhCek9oYcDx5jzxtZW2ZqWvBqc395UZpHBv1of9R1/pubhtml?gid=876309294&single=true\\nLastly, copy the “Hashed Email (SHA-1): bd9770be022dede87419068aa1acd7a2ab441675” value and search for 3 identical entries. There you should see your peer project to be reviewed.\\nBy Emmanuel Ayeni',\n",
       "  'section': 'Projects (Midterm and Capstone)',\n",
       "  'question': 'How does the project evaluation work for you as a peer reviewer?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '37eab341'},\n",
       " {'text': 'Alexey Grigorev: “It’s based on all the scores to make sure most of you pass.”                                                   By Annaliese Bronz\\nOther course-related questions that don’t fall into any of the categories above or can apply to more than one category/module',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Do you pass a project based on the average of everyone else’s scores or based on the total score you earn?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '57754faf'},\n",
       " {'text': 'Answer: The train.py file will be used by your peers to review your midterm project. It is for them to cross-check that your training process works on someone else’s system. It should also be included in the environment in conda or with pipenv.\\nOdimegwu David',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Why do I need to provide a train.py file when I already have the notebook.ipynb file?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '6979c5d1'},\n",
       " {'text': \"Pip install pillow - install pillow library\\nfrom PIL import Image\\nimg = Image.open('aeroplane.png')\\nFrom numpy import asarray\\nnumdata=asarray(img)\\nKrishna Anand\",\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Loading the Image with PILLOW library and converting to numpy array',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'a1bd8c34'},\n",
       " {'text': \"Ans: train.py has to be a python file. This is because running a python script for training a model is much simpler than running a notebook and that's how training jobs usually look like in real life.\",\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Is a train.py file necessary when you have a train.ipynb file in your midterm project directory?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'b2ab0fc1'},\n",
       " {'text': 'Yes, you can create a mobile app or interface that manages these forms and validations. But you should also perform validations on backend.\\nYou can also check Streamlit: https://github.com/DataTalksClub/project-of-the-week/blob/main/2022-08-14-frontend.md\\nAlejandro Aponte',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Is there a way to serve up a form for users to enter data for the model to crunch on?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '80c439a9'},\n",
       " {'text': \"Using model.feature_importances_ can gives you an error:\\nAttributeError: 'Booster' object has no attribute 'feature_importances_'\\nAnswer: if you train the model like this: model = xgb.train you should use get_score() instead\\nEkaterina Kutovaia\",\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'How to get feature importance for XGboost model',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'ff93b86e'},\n",
       " {'text': 'In the Elastic Container Service task log, error “[Errno 12] Cannot allocate memory” showed up.\\nJust increase the RAM and CPU in your task definition.\\nHumberto Rodriguez',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': '[Errno 12] Cannot allocate memory in AWS Elastic Container Service',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'fcd86c8f'},\n",
       " {'text': \"When running a docker container with waitress serving the app.py for making predictions, pickle will throw an error that can't get attribute <name_of_class> on module __main__.\\nThis does not happen when Flask is used directly, i.e. not through waitress.\\nThe problem is that the model uses a custom column transformer class, and when the model was saved, it was saved from the __main__ module (e.g. python train.py). Pickle will reference the class in the global namespace (top-level code): __main__.<custom_class>.\\nWhen using waitress, waitress will load the predict_app module and this will call pickle.load, that will try to find __main__.<custom_class> that does not exist.\\nSolution:\\nPut the class into a separate module and import it in both the script that saves the model (e.g. train.py) and the script that loads the model (e.g. predict.py)\\nNote: If Flask is used (no waitress) in predict.py, and predict.py has the definition of the class, When  it is run: python predict.py, it will work because the class is in the same namespace as the one used when the model was saved (__main__).\\nDetailed info: https://stackoverflow.com/questions/27732354/unable-to-load-files-using-pickle-and-multiple-modules\\nMarcos MJD\",\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Pickle error: can’t get attribute XXX on module __main__',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '236864c2'},\n",
       " {'text': 'There are different techniques, but the most common used are the next:\\nDataset transformation (for example, log transformation)\\nClipping high values\\nDropping these observations\\nAlena Kniazeva',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'How to handle outliers in a dataset?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'efc4a04f'},\n",
       " {'text': 'I was getting the below error message when I was trying to create docker image using bentoml\\n[bentoml-cli] `serve` failed: Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"service\": No module named \\'sklearn\\'\\nSolution description\\nThe cause was because , in bentofile.yaml, I wrote sklearn instead of scikit-learn. Issue was fixed after I modified the packages list as below.\\npackages: # Additional pip packages required by the service\\n- xgboost\\n- scikit-learn\\n- pydantic\\nAsia Saeed',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"service\": No module named \\'sklearn\\'',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '15f361b7'},\n",
       " {'text': \"You might see a long error message with something about sparse matrices, and in the swagger UI, you get a code 500 error with “” (empty string) as output.\\nPotential reason: Setting DictVectorizer or OHE to sparse while training, and then storing this in a pipeline or custom object in the benotml model saving stage in train.py. This means that when the custom object is called in service.py, it will convert each input to a different sized sparse matrix, and this can't be batched due to inconsistent length. In this case, bentoml model signatures should have batchable set to False for production during saving the bentoml mode in train.py.\\n(Memoona Tahira)\",\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'BentoML not working with –production flag at any stage: e.g. with bentoml serve and while running the bentoml container',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'dbbce78b'},\n",
       " {'text': 'Problem description:\\nDo we have to run everything?\\nYou are encouraged, if you can, to run them. As this provides another opportunity to learn from others.\\nNot everyone will be able to run all the files, in particular the neural networks.\\nSolution description:\\nAlternatively, can you see that everything you need to reproduce is there: the dataset is there, the instructions are there, are there any obvious errors and so on.\\nRelated slack conversation here.\\n(Gregory Morris)',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Reproducibility',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'f3a00e15'},\n",
       " {'text': \"If your model is too big for github one option is to try and compress the model using joblib. For example joblib.dump(model, model_filename, compress=('zlib', 6) will use zlib to compress the model. Just note this could take a few moments as the model is being compressed.\\nQuinn Avila\",\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Model too big',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '9102b3c0'},\n",
       " {'text': \"When you try to push the docker image to Google Container Registry and get this message “unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials.”, type this below on console, but first install https://cloud.google.com/sdk/docs/install, this is to be able to use gcloud in console:\\ngcloud auth configure-docker\\n(Jesus Acuña)\",\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Permissions to push docker to Google Container Registry',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '70d89fdf'},\n",
       " {'text': 'I am getting this error message when I tried to install tflite in a pipenv environment\\nError:  An error occurred while installing tflite_runtime!\\nError text:\\nERROR: Could not find a version that satisfies the requirement tflite_runtime (from versions: none)\\nERROR: No matching distribution found for tflite_runtime\\nThis version of tflite do not run on python 3.10, the way we can make it work is by install python 3.9, after that it would install the tflite_runtime without problem.\\nPastor Soto\\nCheck all available versions here:\\nhttps://google-coral.github.io/py-repo/tflite-runtime/\\nIf you don’t find a combination matching your setup, try out the options at\\nhttps://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite\\nwhich you can install as shown in the lecture, e.g.\\npip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\\nFinally, if nothing works, use the TFLite included in TensorFlow for local development, and use Docker for testing Lambda.\\nRileen Sinha (based on discussions on Slack)',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Tflite_runtime unable to install',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'c5d6a804'},\n",
       " {'text': \"Error: ImageDataGenerator name 'scipy' is not defined.\\nCheck that scipy is installed in your environment.\\nRestart jupyter kernel and try again.\\nMarcos MJD\",\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Error when running ImageDataGenerator.flow_from_dataframe',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '8c7f089f'},\n",
       " {'text': 'Tim from BentoML has prepared a dedicated video tutorial wrt this use case here:\\nhttps://www.youtube.com/watch?v=7gI1UH31xb4&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=97\\nKonrad Muehlberg',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'How to pass BentoML content / docker container to Amazon Lambda',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '739bcccf'},\n",
       " {'text': \"In deploying model part, I wanted to test my model locally on a test-image data and I had this silly error after the following command:\\nurl = 'https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg'\\nX = preprocessor.from_url(url)\\nI got the error:\\nUnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7f797010a590>\\nSolution:\\nAdd ?raw=true after .jpg in url. E.g. as below\\nurl = ‘https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg?raw=true’\\nBhaskar Sarma\",\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Error UnidentifiedImageError: cannot identify image file',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '4603e4e5'},\n",
       " {'text': 'Problem: If you run pipenv install and get this message. Maybe manually change Pipfile and Pipfile.lock.\\nSolution: Run: ` pipenv lock` for fix this problem and dependency files\\nAlejandro Aponte',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': '[pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '0a7c328e'},\n",
       " {'text': 'Problem: In the course this function worked to get the features from the dictVectorizer instance: dv.get_feature_names(). But in my computer did not work. I think it has to do with library versions and but apparently that function will be deprecated soon:\\nOld: https://scikit-learn.org/0.22/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\\nNew: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\\nSolution: change the line dv.get_feature_names() to list(dv.get_feature_names_out))\\nIbai Irastorza',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Get_feature_names() not found',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '77efd069'},\n",
       " {'text': 'Problem happens when contacting the server waiting to send your predict-test and your data here in the correct shape.\\nThe problem was the format input to the model wasn’t in the right shape. Server receives the data in json format (dict) which is not suitable for the model. U should convert it to like numpy arrays.\\nAhmed Okka',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Error decoding JSON response: Expecting value: line 1 column 1 (char 0)',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'cc60f7bc'},\n",
       " {'text': \"Q: Hii folks, I tried deploying my docker image on Render, but it won't I get SIGTERM everytime.\\nI think .5GB RAM is not enough, is there any other free alternative available ?\\nA: aws (amazon), gcp (google), saturn.\\nBoth aws and gcp give microinstance for free for a VERY long time, and a bunch more free stuff.\\nSaturn even provides free GPU instances. Recent promo link from mlzoomcamp for Saturn:\\n“You can sign up here: https://bit.ly/saturn-mlzoomcamp\\nWhen you sign up, write in the chat box that you're an ML Zoomcamp student and you should get extra GPU hours (something like 150)”\\nAdded by Andrii Larkin\",\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Free cloud alternatives',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'aa13dd66'},\n",
       " {'text': \"Problem description: I have one column day_of_the_month . It has values 1, 2, 20, 25 etc. and int . I have a second column month_of_the_year. It has values jan, feb, ..dec. and are string. I want to convert these two columns into one column day_of_the_year and I want them to be int. 2 and jan should give me 2, i.e. 2nd day of the year, 1 and feb should give me 32, i.e. 32 nd day of the year. What is the simplest pandas-way to do it?\\nSolution description:\\nconvert dtype in day_of_the_month column from int to str with df['day_of_the_month'] = df['day_of_the_month'].map(str)\\nconvert month_of_the_year column in jan, feb ...,dec into 1,2, ..,12 string using map()\\nconvert day and month into a datetime object with:\\ndf['date_formatted'] = pd.to_datetime(\\ndict(\\nyear='2055',\\nmonth=df['month'],\\nday=df['day']\\n)\\n)\\nget day of year with: df['day_of_year']=df['date_formatted'].dt.dayofyear\\n(Bhaskar Sarma)\",\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Getting day of the year from day and month column',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'c41e479c'},\n",
       " {'text': 'How to visualize the predictions per classes after training a neural net\\nSolution description\\nclasses, predictions = zip(*dict(zip(classes, predictions)).items())\\nplt.figure(figsize=(12, 3))\\nplt.bar(classes, predictions)\\nLuke',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Chart for classes and predictions',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '2f28dcf1'},\n",
       " {'text': 'You can convert the prediction output values to a datafarme using \\ndf = pd.DataFrame.from_dict(dict, orient=\\'index\\' , columns=[\"Prediction\"])\\nEdidiong Esu',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Convert dictionary values to Dataframe table',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '7a69cccf'},\n",
       " {'text': 'The image dataset for the competition was in a different layout from what we used in the dino vs dragon lesson. Since that’s what was covered, some folks were more comfortable with that setup, so I wrote a script that would generate it for them\\nIt can be found here: kitchenware-dataset-generator | Kaggle\\nMartin Uribe',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Kitchenware Classification Competition Dataset Generator',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '20174c95'},\n",
       " {'text': 'Install Nvidia drivers: https://www.nvidia.com/download/index.aspx.\\nWindows:\\nInstall Anaconda prompt https://www.anaconda.com/\\nTwo options:\\nInstall package ‘tensorflow-gpu’ in Anaconda\\nInstall the Tensorflow way https://www.tensorflow.org/install/pip#windows-native\\nWSL/Linux:\\nWSL: Use the Windows Nvida drivers, do not touch that.\\nTwo options:\\nInstall the Tensorflow way https://www.tensorflow.org/install/pip#linux_1\\nMake sure to follow step 4 to install CUDA by environment\\nAlso run:\\necho ‘export XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib/> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\\nInstall CUDA toolkit 11.x.x https://developer.nvidia.com/cuda-toolkit-archive\\nInstall https://developer.nvidia.com/rdp/cudnn-download\\nNow you should be able to do training/inference with GPU in Tensorflow\\n(Learning in public links Links to social media posts where you share your progress with others (LinkedIn, Twitter, etc). Use #mlzoomcamp tag. The scores for this part will be capped at 7 points. Please make sure the posts are valid URLs starting with \"https://\" Does it mean that I should provide my linkedin link? or it means that I should write a post that I have completed my first assignement? (\\nANS (by ezehcp7482@gmail.com): Yes, provide the linkedIN link to where you posted.\\nezehcp7482@gmail.com:\\nPROBLEM: Since I had to put up a link to a public repository, I had to use Kaggle and uploading the dataset therein was a bit difficult; but I had to ‘google’ my way out.\\nANS: See this link for a guide (https://www.kaggle.com/code/dansbecker/finding-your-files-in-kaggle-kernels/notebook)',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'CUDA toolkit and cuDNN Install for Tensorflow',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'f2cd48b6'},\n",
       " {'text': 'When multiplying matrices, the order of multiplication is important.\\nFor example:\\nA (m x n) * B (n x p) = C (m x p)\\nB (n x p) * A (m x n) = D (n x n)\\nC and D are matrices of different sizes and usually have different values. Therefore the order is important in matrix multiplication and changing the order changes the result.\\nBaran Akın',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'About getting the wrong result when multiplying matrices',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '59b4324f'},\n",
       " {'text': 'Refer to https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/01-intro/06-environment.md\\n(added by Rileen Sinha)',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'None of the videos have how to install the environment in Mac, does someone have instructions for Mac with M1 chip?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'e1dc1ed9'},\n",
       " {'text': \"Depends on whether the form will still be open. If you're lucky and it's open, you can submit your homework and it will be evaluated. if closed - it's too late.\\n(Added by Rileen Sinha, based on answer by Alexey on Slack)\",\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'I may end up submitting the assignment late. Would it be evaluated?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'fc60bf3b'},\n",
       " {'text': 'Yes. Whoever corrects the homework will only be able to access the link if the repository is public.\\n(added by Tano Bugelli)\\nHow to install Conda environment in my local machine?\\nWhich ide is recommended for machine learning?',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Does the github repository need to be public?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '1e60e888'},\n",
       " {'text': 'Install w get:\\n!which wget\\nDownload data:\\n!wget -P /content/drive/My\\\\ Drive/Downloads/ URL\\n(added by Paulina Hernandez)',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'How to use wget with Google Colab?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '44552c2e'},\n",
       " {'text': \"Features (X) must always be formatted as a 2-D array to be accepted by scikit-learn.\\nUse reshape to reshape a 1D array to a 2D.\\n\\t\\t\\t\\t\\t\\t\\t(-Aileah) :>\\n(added by Tano\\nfiltered_df = df[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]\\n# Select only the desired columns\\nselected_columns = [\\n'latitude',\\n'longitude',\\n'housing_median_age',\\n'total_rooms',\\n'total_bedrooms',\\n'population',\\n'households',\\n'median_income',\\n'median_house_value'\\n]\\nfiltered_df = filtered_df[selected_columns]\\n# Display the first few rows of the filtered DataFrame\\nprint(filtered_df.head())\",\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Features in scikit-learn?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '7116b3be'},\n",
       " {'text': 'FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'When I plotted using Matplot lib to check if median has a tail, I got the error below how can one bypass?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '5d4d206e'},\n",
       " {'text': 'When trying to rerun the docker file in Windows, as opposed to developing in WSL/Linux, I got the error of:\\n```\\nWarning: Python 3.11 was not found on your system…\\nNeither ‘pipenv’ nor ‘asdf’ could be found to install Python.\\nYou can specify specific versions of Python with:\\n$ pipenv –python path\\\\to\\\\python\\n```\\nThe solution was to add Python311 installation folder to the PATH and restart the system and run the docker file again. That solved the error.\\n(Added by Abhijit Chakraborty)',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Reproducibility in different OS',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '387093cc'},\n",
       " {'text': 'You may quickly deploy your project to DigitalOcean App Cloud. The process is relatively straightforward. The deployment costs about 5 USD/month. The container needs to be up until the end of the project evaluation.\\nSteps:\\nRegister in DigitalOcean\\nGo to Apps -> Create App.\\nYou will need to choose GitHub as a service provider.\\nEdit Source Directory (if your project is not in the repo root)\\nIMPORTANT: Go to settings -> App Spec and edit the Dockerfile path so it looks like ./project/Dockerfile path relative to your repo root\\nRemember to add model files if they are not built automatically during the container build process.\\nBy Dmytro Durach',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Deploying to Digital Ocean',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'd12a2657'},\n",
       " {'text': \"I’m just looking back at the lessons in week 3 (churn prediction project), and lesson 3.6 talks about Feature Importance for categorical values. At 8.12, the mutual info scores show that the some features are more important than others, but then in lesson 3.10 the Logistic Regression model is trained on all of the categorical variables (see 1:35). Once we have done feature importance, is it best to train your model only on the most important features?\\nNot necessarily - rather, any feature that can offer additional predictive value should be included (so, e.g. predict with & without including that feature; if excluding it drops performance, keep it, else drop it). A few individually important features might in fact be highly correlated with others, & dropping some might be fine. There are many feature selection algorithms, it might be interesting to read up on them (among the methods we've learned so far in this course, L1 regularization (Lasso) implicitly does feature selection by shrinking some weights all the way to zero).\\nBy Rileen Sinha\",\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Is it best to train your model only on the most important features?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'eb7a57a6'},\n",
       " {'text': 'You can consider several different approaches:\\nSampling: In the exploratory phase, you can use random samples of the data.\\nChunking: When you do need all the data, you can read and process it in chunks that do fit in the memory.\\nOptimizing data types: Pandas’ automatic data type inference (when reading data in) might result in e.g. float64 precision being used to represent integers, which wastes space. You might achieve substantial memory reduction by optimizing the data types.\\nUsing Dask, an open-source python project which parallelizes Numpy and Pandas.\\n(see, e.g. https://www.vantage-ai.com/en/blog/4-strategies-how-to-deal-with-large-datasets-in-pandas)\\nBy Rileen Sinha',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'How can I work with very large datasets, e.g. the New York Yellow Taxi dataset, with over a million rows?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'd6f0c6ea'},\n",
       " {'text': 'Technically, yes. Advisable? Not really. Reasons:\\nSome homework(s) asks for specific python library versions.\\nAnswers may not match in MCQ options if using different languages other than Python 3.10 (the recommended version for 2023 cohort)\\nAnd as for midterms/capstones, your peer-reviewers may not know these other languages. Do you want to be penalized for others not knowing these other languages?\\nYou can create a separate repo using course’s lessons but written in other languages for your own learnings, but not advisable for submissions.\\ntx[source]',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Can I do the course in other languages, like R or Scala?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '9f261648'},\n",
       " {'text': 'Yes, it’s allowed (as per Alexey).\\nAdded By Rileen Sinha',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Is use of libraries like fast.ai or huggingface allowed in the capstone and competition, or are they considered to be \"too much help\"?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'aa7ff0f7'},\n",
       " {'text': 'The TF and TF Serving versions have to match (as per solution from the slack channel)\\nAdded by Chiedu Elue',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Flask image was built and tested successfully, but tensorflow serving image was built and unable to test successfully. What could be the problem?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '387bdc5f'},\n",
       " {'text': 'I’ve seen LinkedIn users list DataTalksClub as Experience with titles as:\\nMachine Learning Fellow\\nMachine Learning Student\\nMachine Learning Participant\\nMachine Learning Trainee\\nPlease note it is best advised that you do not list the experience as an official “job” or “internship” experience since DataTalksClub did not hire you, nor financially compensate you.\\nOther ways you can incorporate the experience in the following sections:\\nOrganizations\\nProjects\\nSkills\\nFeatured\\nOriginal posts\\nCertifications\\nCourses\\nBy Annaliese Bronz\\nInteresting question, I put the link of my project into my CV as showcase and make posts to show my progress.\\nBy Ani Mkrtumyan',\n",
       "  'section': 'Miscellaneous',\n",
       "  'question': 'Any advice for adding the Machine Learning Zoomcamp experience to your LinkedIn profile?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': 'c6a22665'},\n",
       " {'text': 'MLOps Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course, and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\n[Problem description]\\n[Solution description]\\n(optional) Added by Name',\n",
       "  'section': '+-General course questions',\n",
       "  'question': 'Format for questions: [Problem title]',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '0560e827'},\n",
       " {'text': 'Approximately 3 months. For each module, about 1 week with possible deadline extensions (in total 6~9 weeks), 2 weeks for working on the capstone project and 1 week for peer review.',\n",
       "  'section': '+-General course questions',\n",
       "  'question': 'What is the expected duration of this course or that for each module?',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '59812e77'},\n",
       " {'text': 'The difference is the Orchestration and Monitoring modules. Those videos will be re-recorded. The rest should mostly be the same.\\nAlso all of the homeworks will be changed for the 2023 cohort.',\n",
       "  'section': '+-General course questions',\n",
       "  'question': 'What’s the difference between the 2023 and 2022 course?',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'dce0bb09'},\n",
       " {'text': 'Yes, it will start in May 2024',\n",
       "  'section': '+-General course questions',\n",
       "  'question': 'Will there be a 2024 Cohort? When will the 2024 cohort start?',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '4920d4e9'},\n",
       " {'text': 'Please choose the closest one to your answer. Also do not post your answer in the course slack channel.',\n",
       "  'section': '+-General course questions',\n",
       "  'question': 'What if my answer is not exactly the same as the choices presented?',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '0f1d2765'},\n",
       " {'text': 'Please pick up a problem you want to solve yourself. Potential datasets can be found on either Kaggle, Hugging Face, Google, AWS, or the UCI Machine Learning Datasets Repository.',\n",
       "  'section': '+-General course questions',\n",
       "  'question': 'Are we free to choose our own topics for the final project?',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '4eef2f81'},\n",
       " {'text': 'In order to obtain the certificate, completion of the final capstone project is mandatory. The completion of weekly homework assignments is optional, but they can contribute to your overall progress and ranking on the top 100 leaderboard.',\n",
       "  'section': '+-General course questions',\n",
       "  'question': 'Can I still graduate when I didn’t complete homework for week x?',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '7f93c032'},\n",
       " {'text': 'You can get a few cloud points by using kubernetes even if you deploy it only locally. Or you can use local stack too to mimic AWS\\nAdded by Ming Jun, Asked by Ben Pacheco, Answered by Alexey Grigorev',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'For the final project, is it required to be put on the cloud?',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'ee6f7c89'},\n",
       " {'text': 'For those who are not using VSCode (or other similar IDE), you can automate port-forwarding for Jupyter Notebook by adding the following line of code to your\\n~/.ssh/config file (under the mlops-zoomcamp host):\\nLocalForward 127.0.0.1:8899 127.0.0.1:8899\\nThen you can launch Jupyter Notebook using the following command: jupyter notebook --port=8899 --no-browser and copy paste the notebook URL into your browser.\\nAdded by Vishal',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Port-forwarding without Visual Studio',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'b63b12e0'},\n",
       " {'text': 'You can install the Jupyter extension to open notebooks in VSCode.\\nAdded by Khubaib',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Opening Jupyter in VSCode',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '892c22c1'},\n",
       " {'text': 'In case one would like to set a github repository (e.g. for Homeworks), one can follow 2 great tutorials that helped a lot\\nSetting up github on AWS instance - this\\nSetting up keys on AWS instance - this\\nThen, one should be able to push to its repo\\nAdded by Daniel Hen (daniel8hen@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Configuring Github to work from the remote VM',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '13d38e8d'},\n",
       " {'text': \"Faced issue while setting up JUPYTER NOTEBOOK on AWS. I was unable to access it from my desktop. (I am not using visual studio and hence faced problem)\\nRun\\njupyter notebook --generate-config\\nEdit file /home/ubuntu/.jupyter/jupyter_notebook_config.py to add following line:\\nNotebookApp.ip = '*'\\nAdded by Atul Gupta (samatul@gmail.com)\",\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Opening Jupyter in AWS',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '7d64e9e0'},\n",
       " {'text': 'If you wish to use WSL on your windows machine, here are the setup instructions:\\nCommand: Sudo apt install wget\\nGet Anaconda download address here. wget <download address>\\nTurn on Docker Desktop WFree Download | AnacondaSL2\\nCommand: git clone <github repository address>\\nVSCODE on WSL\\nJupyter: pip3 install jupyter\\nAdded by Gregory Morris (gwm1980@gmail.com)\\nAll in all softwares at one shop:\\nYou can use anaconda which has all built in services like pycharm, jupyter\\nAdded by Khaja Zaffer (khajazaffer@aln.iseg.ulisboa.pt)\\nFor windows “wsl --install” in Powershell\\nAdded by Vadim Surin (vdmsurin@gmai.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'WSL instructions',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '645f0a55'},\n",
       " {'text': 'If you create a folder data and download datasets or raw files in your local repository. Then to push all your code to remote repository without this files or folder please use gitignore file. The simple way to create it do the following steps\\n1. Create empty .txt file (using text editor or command line)\\n2. Safe as .gitignore (. must use the dot symbol)\\n3. Add rules\\n *.parquet - to ignore all parquet files\\ndata/ - to ignore all files in folder data\\n\\nFor more pattern read GIT documentation\\nhttps://git-scm.com/docs/gitignore\\nAdded by Olga Rudakova (olgakurgan@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': '.gitignore how-to',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '7297b7fc'},\n",
       " {'text': \"Make sure when you stop an EC2 instance that it actually stops (there's a meme about it somewhere). There are green circles (running), orange (stopping), and red (stopped). Always refresh the page to make sure you see the red circle and status of stopped.\\nEven when an EC2 instance is stopped, there WILL be other charges that are incurred (e.g. if you uploaded data to the EC2 instance, this data has to be stored somewhere, usually an EBS volume and this storage incurs a cost).\\nYou can set up billing alerts. (I've never done this, so no advice on how to do this).\\n(Question by: Akshit Miglani (akshit.miglani09@gmail.com) and Answer by Anna Vasylytsya)\",\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'AWS suggestions',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '68154f64'},\n",
       " {'text': 'You can get invitation code by coursera and use it in account to verify it it has different characteristics.\\nI really love it\\nhttps://www.youtube.com/watch?v=h_GdX6KtXjo',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'IBM Cloud an alternative for AWS',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'dc7b6f51'},\n",
       " {'text': \"I am worried about the cost of keeping an AWS instance running during the course.\\nWith the instance specified during working environment setup, if you remember to Stop Instance once you finished your work for the day.  Using that strategy, in a day with about 5 hours of work you will pay around $0.40 USD which will account for $12 USD per month, which seems to be an affordable amount.\\nYou must remember that you would have a different IP public address every time you Restart your instance, and you would need to edit your ssh Config file.  It's worth the time though.\\nAdditionally, AWS enables you to set up an automatic email alert if a predefined budget is exceeded.\\nHere is a tutorial to set this up.\\nAlso, you can estimate the cost yourself, using AWS pricing calculator (to use it you don’t even need to be logged in).\\nAt the time of writing (20.05.2023) t3a.xlarge instance with 2 hr/day usage (which translates to 10 hr/week that should be enough to complete the course) and 30GB EBS monthly cost is 10.14 USD\\nHere’s a link to the estimate\\nAdded by Alex Litvinov (aaalex.lit@gmail.com)\",\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'AWS costs',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'b25c6ca3'},\n",
       " {'text': 'For many parts - yes. Some things like kinesis are not in AWS free tier, but you can do it locally with localstack.',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Is the AWS free tier enough for doing this course?',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '9f69ca26'},\n",
       " {'text': 'When I click an open IP-address in an AWS EC2 instance I get an error: “This site can’t be reached”. What should I do?\\nThis ip-address is not required to be open in a browser. It is needed to connect to the running EC2 instance via terminal from your local machine or via terminal from a remote server with such command, for example if:\\nip-address is 11.111.11.111\\ndownloaded key name is razer.pem (the key should be moved to a hidden folder .ssh)\\nyour user name is user_name\\nssh -i /Users/user_name/.ssh/razer.pem ubuntu@11.111.11.111',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'AWS EC2: this site can’t be reached',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '0f1ddc9e'},\n",
       " {'text': 'After this command `ssh -i ~/.ssh/razer.pem ubuntu@XX.XX.XX.XX` I got this error: \"unprotected private key file\". This page (https://99robots.com/how-to-fix-permission-error-ssh-amazon-ec2-instance/) explains how to fix this error. Basically you need to change the file permissions of the key file with this command: chmod 400 ~/.ssh/razer.pem',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Unprotected private key file!',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '01f61154'},\n",
       " {'text': 'My SSH connection to AWS cannot last more than a few minutes, whether via terminal or VS code.\\nMy config:\\n# Copy Configuration in local nano editor, then Save it!\\nHost mlops-zoomcamp                                         # ssh connection calling name\\nUser ubuntu                                             # username AWS EC2\\nHostName <instance-public-IPv4-addr>                    # Public IP, it changes when Source EC2 is turned off.\\nIdentityFile ~/.ssh/name-of-your-private-key-file.pem   # Private SSH key file path\\nLocalForward 8888 localhost:8888                        # Connecting to a service on an internal network from the outside, static forward or set port user forward via on vscode\\nStrictHostKeyChecking no\\nAdded by Muhammed Çelik\\nThe disconnection will occur whether I SSH via WSL2 or via VS Code, and usually occurs after I run some code, i.e. “import mlflow”, so not particularly intense computation.\\nI cannot reconnect to the instance without stopping and restarting with a new IPv4 address.\\nI’ve gone through steps listed on this page: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-linux-resolve-ssh-connection-errors/\\nInbound rule should allow all incoming IPs for SSH.\\nWhat I expect to happen:\\nSSH connection should remain while I’m actively using the instance, and if it does disconnect, I should be able to reconnect back.\\nSolution: sometimes the hang ups are caused by the instance running out of memory. In one instance, using EC2 feature to view screenshot of the instance as a means to troubleshoot, it was the OS out-of-memory feature which killed off some critical processes. In this case, if we can’t use a higher compute VM with more RAM, try adding a swap file, which uses the disk as RAM substitute and prevents the OOM error. Follow Ubuntu’s documentation here: https://help.ubuntu.com/community/SwapFaq.\\nAlternatively follow AWS’s own doc, which mirrors Ubuntu’s: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-memory-swap-file/',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'AWS EC2 instance constantly drops SSH connection',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'd43c32ba'},\n",
       " {'text': 'Everytime I restart my EC2 instance I keep getting different IP and need to update the config file manually.\\n\\nSolution: You can create a script like this to automatically update the IP address of your EC2 instance.https://github.com/dimzachar/mlops-zoomcamp/blob/master/notes/Week_1/update_ssh_config.md',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'AWS EC2 IP Update',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'a044d267'},\n",
       " {'text': 'Make sure to use an instance with enough compute capabilities such as a t2.xlarge. You can check the monitoring tab in the EC2 dashboard to monitor your instance.',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'VS Code crashes when connecting to Jupyter',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'abf8ccdc'},\n",
       " {'text': 'Error “ValueError: X has 526 features, but LinearRegression is expecting 525 features as input.” when running your Linear Regression Model on the validation data set:\\nSolution: The DictVectorizer creates an initial mapping for the features (columns). When calling the DictVecorizer again for the validation dataset transform should be used as it will ignore features that it did not see when fit_transform was last called. E.g.\\nX_train = dv.fit_transform(train_dict)\\nX_test = dv.transform(test_dict)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'X has 526 features, but expecting 525 features',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '26918af3'},\n",
       " {'text': 'If some dependencies are missing\\nInstall following packages\\npandas\\nmatplotlib\\nscikit-learn\\nfastparquet\\npyarrow\\nseaborn\\npip install -r requirements.txt\\nI have seen this error when using pandas.read_parquet(), the solution is to install pyarrow or fastparquet by doing !pip install pyarrow in the notebook\\nNOTE: if you’re using Conda instead of pip, install fastparquet rather than pyarrow, as it is much easier to install and it’s functionally identical to pyarrow for our needs.',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Missing dependencies',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'a5234ac0'},\n",
       " {'text': 'The evaluation RMSE I get doesn’t figure within the options!\\nIf you’re evaluating the model on the entire February data, try to filter outliers using the same technique you used on the train data (0≤duration≤60) and you’ll get a RMSE which is (approximately) in the options. Also don’t forget to convert the columns data types to str before using the DictVectorizer.\\nAnother option: Along with filtering outliers, additionally filter on null values by replacing them with -1.  You will get a RMSE which is (almost same as) in the options. Use ‘.round(2)’ method to round it to 2 decimal points.\\nWarning deprecation\\nThe python interpreter warning of modules that have been deprecated  and will be removed in future releases as well as making suggestion how to go about your code.\\nFor example\\nC:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\seaborn\\\\distributions.py:2619:\\nFutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\\nwarnings.warn(msg, FutureWarning)\\nTo suppress the warnings, you can include this code at the beginning of your notebook\\nimport warnings\\nwarnings.filterwarnings(\"ignore\")',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'No RMSE value in the options',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'af22c52a'},\n",
       " {'text': 'sns.distplot(df_train[\"duration\"])\\nCan be replaced with\\nsns.histplot(\\ndf_train[\"duration\"] , kde=True,\\nstat=\"density\", kde_kws=dict(cut=3), bins=50,\\nalpha=.4, edgecolor=(1, 1, 1, 0.4),\\n)\\nTo get almost identical result',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'How to replace distplot with histplot',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '2aaac94c'},\n",
       " {'text': 'You need to replace the capital letter “L” with a small one “l”',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': \"KeyError: 'PULocationID'  or  'DOLocationID'\",\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '9d15c9e9'},\n",
       " {'text': 'I have faced a problem while reading the large parquet file. I tried some workarounds but they were NOT successful with Jupyter.\\nThe error message is:\\nIndexError: index 311297 is out of bounds for axis 0 with size 131743\\nI solved it by performing the homework directly as a python script.\\nAdded by Ibraheem Taha (ibraheemtaha91@gmail.com)\\nYou can try using the Pyspark library\\nAnswered by kamaldeen (kamaldeen32@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Reading large parquet files',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '79b88d0b'},\n",
       " {'text': 'First remove the outliers (trips with unusual duration) before plotting\\nAdded by Ibraheem Taha (ibraheemtaha91@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Distplot takes too long',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '45485322'},\n",
       " {'text': 'Problem: RMSE on test set was too high when hot encoding the validation set with a previously fitted OneHotEncoder(handle_unknown=’ignore’) on the training set, while DictVectorizer would yield the correct RMSE.\\nIn principle both transformers should behave identically when treating categorical features (at least in this week’s homework where we don’t have sequences of strings in each row):\\nFeatures are put into binary columns encoding their presence (1) or absence (0)\\nUnknown categories are imputed as zeroes in the hot-encoded matrix',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'RMSE on test set too high',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'd5eab395'},\n",
       " {'text': 'A: Alexey’s answer https://www.youtube.com/watch?v=8uJ36ZZr_Is&t=13s\\nIn summary,\\npd.get_dummies or OHE can come up with result in different orders and handle missing data differently, so train and val set would have different columns during train and validation\\nDictVectorizer would ignore missing (in train) and new (in val) datasets\\nOther sources:\\nhttps://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor\\nhttps://scikit-learn.org/stable/modules/feature_extraction.html\\nhttps://innovation.alteryx.com/encode-smarter/\\n~ ellacharmed',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Q: Using of OneHotEncoder instead of DictVectorizer',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '282957fb'},\n",
       " {'text': \"Why didn't get_dummies in pandas library or OneHotEncoder in scikit-learn library be used for one-hot encoding? I know OneHotEncoder is the most common and useful. One-hot coding can also be done using the eye or identity components of the NumPy library.\\nM.Sari\\nOneHotEncoder has the option to output a row column tuple matrix. DictVectorizer is a one step method to encode and support row column tuple matrix output.\\nHarinder(sudwalh@gmail.com)\",\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Q: Why did we not use OneHotEncoder(sklearn) instead of DictVectorizer ?',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '39ad14fd'},\n",
       " {'text': 'How to check that we removed the outliers?\\nUse the pandas function describe() which can provide a report of the data distribution along with the statistics to describe the data. For example, after clipping the outliers using boolean expression, the min and max can be verified using\\ndf[‘duration’].describe()',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Clipping outliers',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'e34df2a5'},\n",
       " {'text': 'pd.get_dummies and DictVectorizer both create a one-hot encoding on string values. Therefore you need to convert the values in PUlocationID and DOlocationID to string.\\nIf you convert the values in PUlocationID and DOlocationID from numeric to string, the NaN values get converted to the string \"nan\".  With DictVectorizer the RMSE is the same whether you use \"nan\" or \"-1\" as string representation for the NaN values. Therefore the representation doesn\\'t have to be \"-1\" specifically, it could also be some other string.',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Replacing NaNs for pickup location and drop off location with -1 for One-Hot Encoding',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'c91b6b57'},\n",
       " {'text': 'Problem: My LinearRegression RSME is very close to the answer but not exactly the same. Is this normal?\\nAnswer: No, LinearRegression is an deterministic model, it should always output the same results when given the same inputs.\\nAnswer:\\nCheck if you have treated the outlier properly for both train and validation sets\\nCheck if the one hot encoding has been done properly by looking at the shape of one hot encoded feature matrix. If it shows 2 features, there is wrong with one hot encoding. Hint: the drop off and pick up codes need to be converted to proper data format and then DictVectorizer is fitted.\\nHarshit Lamba (hlamba19@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Slightly different RSME',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '4aa8eafc'},\n",
       " {'text': 'Problem: I’m facing an extremely low RMSE score (eg: 4.3451e-6) - what shall I do?\\nAnswer: Recheck your code to see if your model is learning the target prior to making the prediction. If the target variable is passed in as a parameter while fitting the model, chances are the model would score extremely low. However, that’s not what you would want and would much like to have your model predict that. A good way to check that is to make sure your X_train doesn’t contain any part of your y_train. The same stands for validation too.\\nSnehangsu De (desnehangsu@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Extremely low RSME',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'a9daaab0'},\n",
       " {'text': 'Problem: how to enable auto completion in jupyter notebook? Tab doesn’t work for me\\nSolution: !pip install --upgrade jedi==0.17.2\\nChristopher R.J.(romanjaimesc@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Enabling Auto-completion in jupyter notebook',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '931f9626'},\n",
       " {'text': \"Problem: While following the steps in the videos you may have problems trying to download with wget the files. Usually it is a 403 error type (Forbidden access).\\nSolution: The links point to files on cloudfront.net, something like this:\\nhttps://d37ci6vzurychx.cloudfront.net/tOSError: Could not open parquet input source '<Buffer>': Invalid: Parquet OSError: Could not open parquet input source '<Buffer>': Invalid: Parquet rip+data/green_tripdata_2021-01.parquet\\nI’m not download the dataset directly, i use dataset URL and run this in the file.\\nUpdate(27-May-2023): Vikram\\nI am able to download the data from the below link. This is from the official  NYC trip record page (https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). Copy link from page directly as the below url might get changed if the NYC decides to move away from this. Go to the page , right click and use copy link.\\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-01.parquet\\n(Asif)\\nCopy the link address and replace the cloudfront.net part with s3.amazonaws.com/nyc-tlc/, so it looks like this:\\nhttps://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-01.parquet\\nMario Tormo (mario@tormo-romero.eu)\\nOSError: Could not open parquet input source '<Buffer>': Invalid: Parquet\",\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Downloading the data from the NY Taxis datasets gives error : 403 Forbidden',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '782e1723'},\n",
       " {'text': 'Problem: PyCharm (remote) doesn’t see conda execution path. So, I cannot use conda env (which is located on a remote server).\\nSolution: In remote server in command line write “conda activate envname”, after write “which python” - it gives you python execution path. After you can use this path when you will add new interpreter in PyCharm: add local interpreter -> system interpreter -> and put the path with python.\\nSalimov Ilnaz (salimovilnaz777@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Using PyCharm & Conda env in remote development',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '4e08c86a'},\n",
       " {'text': 'Problem: The output of DictVectorizer was taking up too much memory. So much so, that I couldn’t even fit the linear regression model before running out of memory on my 16 GB machine.\\nSolution: In the example for DictVectorizer in the scikit-learn website, they set the parameter “sparse” as False. Although this helps with viewing the results, this results in a lot of memory usage. The solution is to either use “sparse=True” instead, or leave it at the default which is also True.\\nAhmed Fahim (afahim03@yahoo.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Running out of memory',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '34bcad27'},\n",
       " {'text': 'Problem: For me, Installing anaconda didn’t modify the .bashrc profile. That means Anaconda env was not activated even after exiting and relaunching the unix shell.\\nSolution:\\nFor bash : Initiate conda again, which will add entries for anaconda in .bashrc file.\\n$ cd YOUR_PATH_ANACONDA/bin $ ./conda init bash\\nThat will automatically edit your .bashrc.\\nReload:\\n$ source ~/.bashrc\\nAhamed Irshad (daisyfuentesahamed@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Activating Anaconda env in .bashrc',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '96144e66'},\n",
       " {'text': 'While working through the HW1, you will realize that the training and the validation data set feature sizes are different. I was trying to figure out why and went down the entire rabbit hole only to see that I wasn’t doing ```transform``` on the premade dictionary vectorizer instead of ```fit_transform```. You already have the dictionary vectorizer made so no need to execute the fit pipeline on the model.\\nSam Lim(changhyeonlim@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'The feature size is different for training set and validation set',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '840f739d'},\n",
       " {'text': 'I found a good guide how to get acces to your machine again when you removed your public key.\\nUsing the following link you can go to Session Manager and log in to your instance and create public key again. https://repost.aws/knowledge-center/ec2-linux-fix-permission-denied-errors\\nThe main problem for me here was to get my old public key, so for doing this you should run the following command: ssh-keygen -y -f /path_to_key_pair/my-key-pair.pem\\nFor more information: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/describe-keys.html#retrieving-the-public-key\\nHanna Zhukavets (a.zhukovec1901@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Permission denied (publickey) Error (when you remove your public key on the AWS machine)',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'bf006ff9'},\n",
       " {'text': 'Problem: The February dataset has been used as a validation/test dataset and been stripped of the outliers in a similar manner to the train dataset (taking only the rows for the duration between 1 and 60, inclusive). The RMSE obtained afterward is in the thousands.\\nAnswer: The sparsematrix result from DictVectorizer shouldn’t be turned into an ndarray. After removing that part of the code, I ended up receiving a correct result .\\nTahina Mahatoky (tahinadanny@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Overfitting: Absurdly high RMSE on the validation dataset',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'f178d4a0'},\n",
       " {'text': 'more specific error line:\\nfrom sklearn.feature_extraction import DictVectorizer\\nI had this issue and to solve it I did\\n!pip install scikit-learn\\nJoel Auccapuclla (auccapuclla 2013@gmail.com)',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Can’t import sklearn',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'b80401a2'},\n",
       " {'text': 'Problem: Localhost:5000 Unavailable // Access to Localhost Denied // You don’t have authorization to view this page (127.0.0.1:5000)\\n\\nSolution: If you are on an chrome browser you need to head to `chrome://net-internals/#sockets` and press “Flush Socket Pools”',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Access Denied at Localhost:5000 - Authorization Issue',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '88002d35'},\n",
       " {'text': \"You have something running on the 5000 port. You need to stop it.\\nAnswer: On terminal in mac .\\nRun ps -A | grep gunicorn\\nLook for the number process id which is the 1st number after running the command\\nkill 13580\\nwhere 13580  represents the process number.\\nSource\\nwarrie.warrieus@gmail.com\\nOr by executing the following command it will kill all the processes using port 5000:\\n>> sudo fuser -k 5000/tcp\\nAnswered by Vaibhav Khandelwal\\nJust execute in the command below in he command line to kill the running port\\n->> kill -9 $(ps -A | grep python | awk '{print $1}')\\nAnswered by kamaldeen (kamaldeen32@gmail.com)\\nChange to different port (5001 in this case)\\n>> mlflow ui --backend-store-uri sqlite:///mlflow.db --port 5001\\nAnswered by krishna (nellaikrishna@gmail.com)\",\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': \"Connection in use: ('127.0.0.1', 5000)\",\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'fe61aa5b'},\n",
       " {'text': 'Running python register_model.py results in the following error:\\nValueError: could not convert string to float: \\'0 int\\\\n1   float\\\\n2     hyperopt_param\\\\n3       Literal{n_estimators}\\\\n4       quniform\\\\n5         Literal{10}\\\\n6         Literal{50}\\\\n7         Literal{1}\\'\\nFull Traceback:\\nTraceback (most recent call last):\\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 101, in <module>\\nrun(args.data_path, args.top_n)\\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 67, in run\\ntrain_and_log_model(data_path=data_path, params=run.data.params)\\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 41, in train_and_log_model\\nparams = space_eval(SPACE, params)\\nFile \"/Users/name/miniconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/hyperopt/fmin.py\", line 618, in space_eval\\nrval = pyll.rec_eval(space, memo=memo)\\nFile \"/Users/name/miniconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/hyperopt/pyll/base.py\", line 902, in rec_eval\\nrval = scope._impls[node.name](*args, **kwargs)\\nValueError: could not convert string to float: \\'0 int\\\\n1   float\\\\n2     hyperopt_param\\\\n3       Literal{n_estimators}\\\\n4       quniform\\\\n5         Literal{10}\\\\n6         Literal{50}\\\\n7         Literal{1}\\'\\nSolution: There are two plausible errors to this. Both are in the hpo.py file where the hyper-parameter tuning is run. The objective function should look like this.\\n\\n   def objective(params):\\n# It\\'s important to set the \"with\" statement and the \"log_params\" function here\\n# in order to properly log all the runs and parameters.\\nwith mlflow.start_run():\\n# Log the parameters\\nmlflow.log_params(params)\\nrf = RandomForestRegressor(**params)\\nrf.fit(X_train, y_train)\\ny_pred = rf.predict(X_valid)\\n# Calculate and log rmse\\nrmse = mean_squared_error(y_valid, y_pred, squared=False)\\nmlflow.log_metric(\\'rmse\\', rmse)\\nIf you add the with statement before this function, and just after the following line\\nX_valid, y_valid = load_pickle(os.path.join(data_path, \"valid.pkl\"))\\nand you log the parameters just after the search_space dictionary is defined, like this\\nsearch_space = {....}\\n# Log the parameters\\nmlflow.log_params(search_space)\\nThen there is a risk that the parameters will be logged in group. As a result, the\\nparams = space_eval(SPACE, params)\\nregister_model.py file will receive the parameters in group, while in fact it expects to receive them one by one. Thus, make sure that the objective function looks as above.\\nAdded by Jakob Salomonsson',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Could not convert string to float - ValueError',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'b9adeb39'},\n",
       " {'text': 'Make sure you launch the mlflow UI from the same directory as thec that is running the experiments (same directory that has the mlflow directory and the database that stores the experiments).\\nOr navigate to the correct directory when specifying the tracking_uri.\\nFor example:\\nIf the mlflow.db is in a subdirectory called database, the tracking uri would be ‘sqllite:///database/mlflow.db’\\nIf the mlflow.db is a directory above your current directory: the tracking uri would be ‘sqlite:///../mlflow.db’\\nAnswered by Anna Vasylytsya\\nAnother alternative is to use an absolute path to mlflow.db rather than relative path\\nAnd yet another alternative is to launch the UI from the same notebook by executing the following code cell\\nimport subprocess\\nMLFLOW_TRACKING_URI = \"sqlite:///data/mlflow.db\"\\nsubprocess.Popen([\"mlflow\", \"ui\", \"--backend-store-uri\", MLFLOW_TRACKING_URI])\\nAnd then using the same MLFLOW_TRACKING_URI when initializing mlflow or the client\\nclient = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Experiment not visible in MLflow UI',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'ebc13686'},\n",
       " {'text': \"Problem:\\nGetting\\nERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE\\nduring MLFlow's installation process, particularly while installing the Numpy package using pip\\nWhen I installed mlflow using ‘pip install mlflow’ on 27th May 2022, I got the following error while numpy was getting installed through mlflow:\\n\\nCollecting numpy\\nDownloading numpy-1.22.4-cp310-cp310-win_amd64.whl (14.7 MB)\\n|██████████████              \\t| 6.3 MB 107 kB/s eta 0:01:19\\nERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE.\\nIf you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\\nnumpy from https://files.pythonhosted.org/packages/b5/50/d7978137464251c393df28fe0592fbb968110f752d66f60c7a53f7158076/numpy-1.22.4-cp310-cp310-win_amd64.whl#sha256=3e1ffa4748168e1cc8d3cde93f006fe92b5421396221a02f2274aab6ac83b077 (from mlflow):\\nExpected sha256 3e1ffa4748168e1cc8d3cde93f006fe92b5421396221a02f2274aab6ac83b077\\nGot    \\t15e691797dba353af05cf51233aefc4c654ea7ff194b3e7435e6eec321807e90\\nSolution:\\nThen when I install numpy separately (and not as part of mlflow), numpy gets installed (same version), and then when I do 'pip install mlflow', it also goes through.\\nPlease note that the above may not be consistently simulatable, but please be aware of this issue that could occur during pip install of mlflow.\\nAdded by Venkat Ramakrishnan\",\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Hash Mismatch Error with Package Installation',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '939f9c33'},\n",
       " {'text': 'After deleting an experiment from UI, the deleted experiment still persists in the database.\\nSolution: To delete this experiment permanently, follow these steps.\\nAssuming you are using sqlite database;\\nInstall ipython sql using the following command: pip install ipython-sql\\nIn your jupyter notebook, load the SQL magic scripts with this: %load_ext sql\\nLoad the database with this: %sql sqlite:///nameofdatabase.db\\nRun the following SQL script to delete the experiment permanently: check link',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'How to Delete an Experiment Permanently from MLFlow UI',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'b5c3e6af'},\n",
       " {'text': 'Problem: I cloned the public repo, made edits, committed and pushed them to my own repo. Now I want to get the recent commits from the public repo without overwriting my own changes to my own repo. Which command(s) should I use?\\nThis is what my config looks like (in case this might be useful):\\n[core]\\nrepositoryformatversion = 0\\nfilemode = true\\nbare = false\\nlogallrefupdates = true\\nignorecase = true\\nprecomposeunicode = true\\n[remote \"origin\"]\\nurl = git@github.com:my_username/mlops-zoomcamp.git\\nfetch = +refs/heads/*:refs/remotes/origin/*\\n[branch \"main\"]\\nremote = origin\\nmerge = refs/heads/main\\nSolution: You should fork DataClubsTak’s repo instead of cloning it. On GitHub, click “Fetch and Merge” under the menu “Fetch upstream” at the main page of your own',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'How to Update Git Public Repo Without Overwriting Changes',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '80554fc2'},\n",
       " {'text': 'This is caused by ```mlflow.xgboost.autolog()``` when version 1.6.1 of xgboost\\nDowngrade to 1.6.0\\n```pip install xgboost==1.6.0``` or update requirements file with xgboost==1.6.0 instead of xgboost\\nAdded by Nakul Bajaj',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Image size of 460x93139 pixels is too large. It must be less than 2^16 in each direction.',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '943df153'},\n",
       " {'text': 'Since the version 1.29 the list_experiments method was deprecated and then removed in the later version\\nYou should use search_experiments instead\\nAdded by Alex Litvinov',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': \"MlflowClient object has no attribute 'list_experiments'\",\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'b8d3c55e'},\n",
       " {'text': 'Make sure `mlflow.autolog()` ( or framework-specific autolog ) written BEFORE `with mlflow.start_run()` not after.\\nAlso make sure that all dependencies for the autologger are installed, including matplotlib. A warning about uninstalled dependencies will be raised.\\nMohammed Ayoub Chettouh',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'MLflow Autolog not working',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '67bf60c6'},\n",
       " {'text': 'If you’re running MLflow on a remote VM, you need to forward the port too like we did in Module 1 for Jupyter notebook port 8888. Simply connect your server to VS Code, as we did, and add 5000 to the PORT like in the screenshot:\\nAdded by Sharon Ibejih\\nIf you are running MLflow locally and 127.0.0.1:5000 shows a blank page navigate to localhost:5000 instead.',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'MLflow URL (http://127.0.0.1:5000), doesn’t open.',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '336f5e36'},\n",
       " {'text': 'Got the same warning message as Warrie Warrie when using “mlflow.xgboost.autolog()”\\nIt turned out that this was just a warning message and upon checking MLflow UI (making sure that no “tag” filters were included), the model was actually automatically tracked in the MLflow.\\nAdded by Bengsoon Chuah, Asked by Warrie Warrie, Answered by Anna Vasylytsya & Ivan Starovit',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'MLflow.xgboost Autolog Model Signature Failure',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'fd2b9972'},\n",
       " {'text': \"mlflow.exceptions.MlflowException: Cannot set a deleted experiment 'cross-sell' as the active experiment. You can restore the experiment, or permanently delete the  experiment to create a new one.\\nThere are many options to solve in this link: https://stackoverflow.com/questions/60088889/how-do-you-permanently-delete-an-experiment-in-mlflow\",\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'MlflowException: Unable to Set a Deleted Experiment',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '75cd9b7a'},\n",
       " {'text': 'You do not have enough disk space to install the requirements. You can either increase the base EBS volume by following this link or add an external disk to your instance and configure conda installation to happen on the external disk.\\nAbinaya Mahendiran\\nOn GCP: I added another disk to my vm and followed this guide to mount the disk. Confirm the mount by running df -H (disk free) command in bash shell. I also deleted Anaconda and instead used miniconda. I downloaded miniconda in the additional disk that I mounted and when installing miniconda, enter the path to the extra disk instead of the default disk, this way conda is installed on the extra disk.\\nYang Cao',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'No Space Left on Device - OSError[Errno 28]',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '51c99586'},\n",
       " {'text': 'I was using an old version of sklearn due to which I got the wrong number of parameters because in the latest version min_impurity_split for randomforrestRegressor was deprecated. Had to upgrade to the latest version to get the correct number of params.',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Parameters Mismatch in Homework Q3',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '089c8c18'},\n",
       " {'text': \"Error: I installed all the libraries from the requirements.txt document in a new environment as follows:\\npip install -r requirementes.txt\\nThen when I run mlflow from my terminal like this:\\nmlflow\\nI get this error:\\nSOLUTION: You need to downgrade the version of 'protobuf' module to 3.20.x or lower. Initially, it was version=4.21, I installed protobuf==3.20\\npip install protobuf==3.20\\nAfter which I was able to run mlflow from my terminal.\\n-Submitted by Aashnna Soni\",\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Protobuf error when installing MLflow',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'f4b82056'},\n",
       " {'text': 'Please check your current directory while running the mlflow ui command. You need to run mlflow ui or mlflow server command in the right directory.',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Setting up Artifacts folders',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'dd2e7dc9'},\n",
       " {'text': 'If you have problem with setting up MLflow for experiment tracking on GCP, you can check these two links:\\nhttps://kargarisaac.github.io/blog/mlops/data%20engineering/2022/06/15/MLFlow-on-GCP.html\\nhttps://kargarisaac.github.io/blog/mlops/2022/08/26/machine-learning-workflow-orchestration-zenml.html',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Setting up MLflow experiment tracker on GCP',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '3fcbd80e'},\n",
       " {'text': 'Solution: Downgrade setuptools (I downgraded 62.3.2 -> 49.1.0)',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Setuptools Replacing Distutils - MLflow Autolog Warning',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '924fcf47'},\n",
       " {'text': 'I can’t sort runs in MLFlow\\nMake sure you are in table view (not list view) in the MLflow UI.\\nAdded and Answered by Anna Vasylytsya',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Sorting runs in MLflow UI',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '58240887'},\n",
       " {'text': 'Problem: When I ran `$ mlflow ui` on a remote server and try to open it in my local browser I got an exception  and the page with mlflow ui wasn’t loaded.\\nSolution: You should `pip uninstall flask` on your remote server on conda env and after it install Flask `pip install Flask`. It is because the base conda env has ~flask<1.2, and when you clone it to your new work env, you are stuck with this old version.\\nAdded by Salimov Ilnaz',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': \"TypeError: send_file() unexpected keyword 'max_age' during MLflow UI Launch\",\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '67d343f2'},\n",
       " {'text': 'Problem: After successfully installing mlflow using pip install mlflow on my Windows system, I am trying to run the mlflow ui command but it throws the following error:\\nFileNotFoundError: [WinError 2] The system cannot find the file specified\\nSolution: Add C:\\\\Users\\\\{User_Name}\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts to the PATH\\nAdded by Alex Litvinov',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'mlflow ui on Windows FileNotFoundError: [WinError 2] The system cannot find the file specified',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '6de95c2a'},\n",
       " {'text': 'Running “python hpo.py --data_path=./your-path --max_evals=50” for the homework leads to the following error: TypeError: unsupported operand type(s) for -: \\'str\\' and \\'int\\'\\nFull Traceback:\\nFile \"~/repos/mlops/02-experiment-tracking/homework/hpo.py\", line 73, in <module>\\nrun(args.data_path, args.max_evals)\\nFile \"~/repos/mlops/02-experiment-tracking/homework/hpo.py\", line 47, in run\\nfmin(\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 540, in fmin\\nreturn trials.fmin(\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/base.py\", line 671, in fmin\\nreturn fmin(\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 586, in fmin\\nrval.exhaust()\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 364, in exhaust\\nself.run(self.max_evals - n_done, block_until_done=self.asynchronous)\\nTypeError: unsupported operand type(s) for -: \\'str\\' and \\'int\\'\\nSolution:\\nThe --max_evals argument in hpo.py has no defined datatype and will therefore implicitly be treated as string. It should be an integer, so that the script can work correctly. Add type=int to the argument definition:\\nparser.add_argument(\\n\"--max_evals\",\\ntype=int,\\ndefault=50,\\nhelp=\"the number of parameter evaluations for the optimizer to explore.\"\\n)',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Unsupported Operand Type Error in hpo.py',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '2ff28e5b'},\n",
       " {'text': 'Getting the following warning when running mlflow.sklearn:\\n\\n2022/05/28 04:36:36 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow. […]\\nSolution: use 0.22.1 <= scikit-learn <= 1.1.0\\nReference: https://www.mlflow.org/docs/latest/python_api/mlflow.sklearn.html',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Unsupported Scikit-Learn version',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '29c6bbf1'},\n",
       " {'text': 'Problem: CLI commands (mlflow experiments list) do not return experiments\\nSolution description: need to set environment variable for the Tracking URI:\\n$ export MLFLOW_TRACKING_URI=http://127.0.0.1:5000\\nAdded and Answered by Dino Vitale',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Mlflow CLI does not return experiments',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'bd09df94'},\n",
       " {'text': 'Problem: After starting the tracking server, when we try to use the mlflow cli commands as listed here, most of them can’t seem to find the experiments that have been run with the tracking server\\nSolution: We need to set the environment variable MLFLOW_TRACKING_URI to the URI of the sqlite database. This is something like “export MLFLOW_TRACKING_URI=sqlite:///{path to sqlite database}” . After this, we can view the experiments from the command line using commands like “mlflow experiments search”\\nEven after this commands like “mlflow gc” doesn’t seem to get the tracking uri, and they have to be passed explicitly as an argument every time the command is run.\\nAhmed Fahim (afahim03@yahoo.com)',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Viewing MLflow Experiments using MLflow CLI',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'af887c59'},\n",
       " {'text': 'All the experiment and other tracking information in mlflow are stored in sqllite database provided while initiating the mlflow ui command. This database can be inspected using Pycharm’s Database tab by using the SQLLite database type. Once the connection is created as below, the tables can be queried and inspected using regular SQL. The same applies for any SQL backed database such as postgres as well.\\nThis is very useful to understand the entity structure of the data being stored within mlflow and useful for any kind of systematic archiving of model tracking for longer periods.\\nAdded by Senthilkumar Gopal',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Viewing SQLlite Data Raw & Deleting Experiments Manually',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'ee7c59ea'},\n",
       " {'text': 'Solution : It is another way to start it for remote hosting a mlflow server. For example, if you are multiple colleagues working together on something you most likely would not run mlflow on one laptop but rather everyone would connect to the same server running mlflow\\nAnswer by Christoffer Added by Akshit Miglani (akshit.miglani09@gmail.com)',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'What does launching the tracking server locally mean?',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'a2531c75'},\n",
       " {'text': 'Problem: parameter was not recognized during the model registry\\nSolution: parameters should be added in previous to the model registry. The parameters can be added by mlflow.log_params(params) so that the dictionary can be directly appended to the data.run.params.\\nAdded and Answered by Sam Lim',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Parameter adding in case of max_depth not recognized',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'bc4b2320'},\n",
       " {'text': 'Problem: Max_depth is not recognize even when I add the mlflow.log_params\\nSolution: the mlflow.log_params(params) should be added to the hpo.py script, but if you run it it will append the new model to the previous run that doesn’t contain the parameters, you should either remove the previous experiment or change it\\nPastor Soto',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Max_depth is not recognize even when I add the mlflow.log_params',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'f69fb077'},\n",
       " {'text': \"Problem: About week_2 homework: The register_model.py  script, when I copy it into a jupyter notebook fails and spits out the following error. AttributeError: 'tuple' object has no attribute 'tb_frame'\\nSolution: remove click decorators\",\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': \"AttributeError: 'tuple' object has no attribute 'tb_frame'\",\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'e223524c'},\n",
       " {'text': 'Problem: when running the preprocess_data.py file you get the following error:\\n\\nwandb: ERROR api_key not configured (no-tty). call wandb.login(key=[your_api_key])\\nSolution: Go to your WandB profile (top RHS) → user settings → scroll down to “Danger Zone” and copy your API key. \\n\\nThen before running preprocess_data.py, add and run the following cell in your notebook:\\n\\n%%bash\\n\\nWandb login <YOUR_API_KEY_HERE>.\\nAdded and Answered by James Gammerman (jgammerman@gmail.com)',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'WandB API error',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '0f08bec7'},\n",
       " {'text': 'Please make sure you following the order below nd enabling the autologging before constructing the dataset. If you still have this issue check that your data is in format compatible with XGBoost.\\n# Enable MLflow autologging for XGBoost\\nmlflow.xgboost.autolog()\\n# Construct your dataset\\nX_train, y_train = ...\\n# Train your XGBoost model\\nmodel = xgb.XGBRegressor(...)\\nmodel.fit(X_train, y_train)\\nAdded by Olga Rudakova',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '8b4b1685'},\n",
       " {'text': 'Problem\\nUsing wget command to download either data or python scripts on Windows, I am using the notebook provided by Visual Studio and despite having a python virtual env, it did not recognize the pip command.\\nSolution: Use python -m pip, this same for any other command. Ie. python -m wget\\nAdded by Erick Calderin',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'wget not working',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'ecfc5c07'},\n",
       " {'text': \"Problem: Open/run github notebook(.ipynb) directly in Google Colab\\nSolution: Change the domain from 'github.com' to 'githubtocolab.com'. The notebook will open in Google Colab.\\nOnly works with Public repo.\\nAdded by Ming Jun\\nNavigating in Wandb UI became difficult to me, I had to intuit some options until I found the correct one.\\nSolution: Go to the official doc.\\nAdded by Erick Calderin\",\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Open/run github notebook(.ipynb) directly in Google Colab',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'a1b68c52'},\n",
       " {'text': 'Problem: Someone asked why we are using this type of split approach instead of just a random split.\\nSolution: For example, I have some models at work that train on Jan 1 2020 — Aug 1 2021 time period, and then test on Aug 1 - Dec 31 2021, and finally validate on Jan - March or something\\nWe do these “out of time”  validations to do a few things:\\nCheck for seasonality of our data\\nWe know if the RMSE for Test is 5 say, and then RMSE for validation is 20, then there’s serious seasonality to the data we are looking at, and now we might change to Time Series approaches\\nIf I’m predicting on Mar 30 2023 the outcomes for the next 3 months, the “random sample” in our train/test would have caused data leakage, overfitting, and poor model performance in production. We mustn’t take information about the future and apply it to the present when we are predicting in a model context.\\nThese are two of, I think, the biggest points for why we are doing jan/feb/march. I wouldn’t do it any other way.\\nTrain: Jan\\nTest: Feb\\nValidate: March\\nThe point of validation is to report out model metrics to leadership, regulators, auditors, and record the models performance to then later analyze target drift\\nAdded by Sam LaFell\\nProblem: If you get an error while trying to run the mlflow server on AWS CLI with S3 bucket and POSTGRES database:\\nReproducible Command:\\nmlflow server -h 0.0.0.0 -p 5000 --backend-store-uri postgresql://<DB_USERNAME>:<DB_PASSWORD>@<DB_ENDPOINT>:<DB_PORT>/<DB_NAME> --default-artifact-root s3://<BUCKET_NAME>\\nError:\\n\"urllib3 v2.0 only supports OpenSSL 1.1.1+, currently \"\\nImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the \\'ssl\\' module is compiled with \\'OpenSSL 1.0.2k-fips  26 Jan 2017\\'. See: https://github.com/urllib3/urllib3/issues/2168\\nSolution: Upgrade mlflow using\\nCode: pip3 install --upgrade mlflow\\nResolution: It downgrades urllib3 2.0.3 to 1.26.16 which is compatible with mlflow and ssl 1.0.2\\nInstalling collected packages: urllib3\\nAttempting uninstall: urllib3\\nFound existing installation: urllib3 2.0.3\\nUninstalling urllib3-2.0.3:\\nSuccessfully uninstalled urllib3-2.0.3\\nSuccessfully installed urllib3-1.26.16\\nAdded by Sarvesh Thakur',\n",
       "  'section': 'Module 3: Orchestration',\n",
       "  'question': 'Why do we use Jan/Feb/March for Train/Test/Validation Purposes?',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '483e7d61'},\n",
       " {'text': 'Problem description\\nSolution description\\n(optional) Added by Name',\n",
       "  'section': 'Module 3: Orchestration',\n",
       "  'question': 'Problem title',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'e5c33f50'},\n",
       " {'text': 'Here',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Where is the FAQ for Prefect questions?',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'cbf13b19'},\n",
       " {'text': 'Windows with AWS CLI already installed\\nAWS CLI version:\\naws-cli/2.4.24 Python/3.8.8 Windows/10 exe/AMD64 prompt/off\\nExecuting\\n$(aws ecr get-login --no-include-email)\\nshows error\\naws.exe: error: argument operation: Invalid choice, valid choices are…\\nUse this command instead. More info here:\\nhttps://docs.aws.amazon.com/cli/latest/reference/ecr/get-login-password.html\\naws ecr get-login-password \\\\\\n--region <region> \\\\\\n| docker login \\\\\\n--username AWS \\\\\\n--password-stdin <aws_account_id>.dkr.ecr.<region>.amazonaws.com\\nAdded by MarcosMJD',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'aws.exe: error: argument operation: Invalid choice — Docker can not login to ECR.',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '39861d6e'},\n",
       " {'text': 'Use ` at the end of each line except the last. Note that multiline string does not need `.\\nEscape “ to “\\\\ .\\nUse $env: to create env vars (non-persistent). E.g.:\\n$env:KINESIS_STREAM_INPUT=\"ride_events\"\\naws kinesis put-record --cli-binary-format raw-in-base64-out `\\n--stream-name $env:KINESIS_STREAM_INPUT `\\n--partition-key 1 `\\n--data \\'{\\n\\\\\"ride\\\\\": {\\n\\\\\"PULocationID\\\\\": 130,\\n\\\\\"DOLocationID\\\\\": 205,\\n\\\\\"trip_distance\\\\\": 3.66\\n},\\n\\\\\"ride_id\\\\\": 156\\n}\\'\\nAdded by MarcosMJD',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Multiline commands in Windows Powershell',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '3dac15ff'},\n",
       " {'text': \"If one gets pipenv failures for pipenv install command -\\nAttributeError: module 'collections' has no attribute 'MutableMapping'\\nIt happens because you use the system Python (3.10) for pipenv.\\nIf you previously installed pipenv with apt-get, remove it - sudo-apt remove pipenv\\nMake sure you have a non-system Python installed in your environment. The easiest way to do it is to install anaconda or miniconda\\nNext, install pipenv to your non-system Python. If you use the setup from the lectures, it’s just this: pip install pipenv\\nNow re-run pipenv install XXXX (relevant dependencies) - should work\\nTested and worked on AWS instance, similar to the config Alexey presented in class.\\nAdded by Daniel HenSSL\",\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': \"Pipenv installation not working (AttributeError: module 'collections' has no attribute 'MutableMapping')\",\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '32686722'},\n",
       " {'text': 'First check if SSL module configured with following command:\\nPython -m ssl\\n\\nIf the output of this is empty there is no problem with SSL configuration.\\n\\nThen you should upgrade your pipenv package in your current environment to resolve the problem.\\nAdded by Kenan Arslanbay',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': \"module is not available (Can't connect to HTTPS URL)\",\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '22521751'},\n",
       " {'text': \"During scikit-learn installation via the command:\\npipenv install scikit-learn==1.0.2\\nThe following error is raised:\\nModuleNotFoundError: No module named 'pip._vendor.six'\\nThen, one should:\\nsudo apt install python-six\\npipenv --rm\\npipenv install scikit-learn==1.0.2\\nAdded by Giovanni Pecoraro\",\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': \"No module named 'pip._vendor.six'\",\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '81ad4784'},\n",
       " {'text': 'Problem description. How can we use Jupyter notebooks with the Pipenv environment?\\nSolution: Refer to this stackoverflow question. Basically install jupyter and ipykernel using pipenv. And then register the kernel with `python -m ipykernel install --user --name=my-virtualenv-name` inside the Pipenv shell. If you are using Jupyter notebooks in VS Code, doing this will also add the virtual environment in the list of kernels.\\nAdded by Ron Medina',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Pipenv with Jupyter',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '29b5651e'},\n",
       " {'text': \"Problem: I tried to run starter notebook on pipenv environment but had issues with no output on prints. \\nI used scikit-learn==1.2.2 and python==3.10\\nTornado version was 6.3.2\\n\\nSolution: The error you're encountering seems to be a bug related to Tornado, which is a Python web server and networking library. It's used by Jupyter under the hood to handle networking tasks.\\nDowngrading to tornado==6.1 fixed the issue\\nhttps://stackoverflow.com/questions/54971836/no-output-jupyter-notebook\",\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Pipenv with Jupyter no output',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'ca79bbe8'},\n",
       " {'text': 'Problem description:  You might get an error ‘Invalid base64’ after running the ‘aws kinesis put-record’ command on your local machine. This might be the case if you are using the AWS CLI version 2 (note that in the video 4.4, around 57:42, you can see a warning since the instructor is using v1 of the CLI.\\nSolution description: To get around this, pass the argument ‘--cli-binary-format raw-in-base64-out’. This will encode your data string into base64 before passing it to kinesis\\nAdded by M',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': '‘Invalid base64’ error after running `aws kinesis put-record`',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '668f1ad9'},\n",
       " {'text': 'Problem description:   Running starter.ipynb in homework’s Q1 will show up this error.\\nSolution description: Update pandas (actually pandas version was the latest, but several dependencies are updated).\\nAdded by Marcos Jimenez',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Error index 311297 is out of bounds for axis 0 with size 131483 when loading parquet file.',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '7a6f23eb'},\n",
       " {'text': 'Use command $pipenv lock to force the creation of Pipfile.lock\\nAdded by Bijay P.',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Pipfile.lock was not created along with Pipfile',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '232e5557'},\n",
       " {'text': 'This issue is usually due to the pythonfinder module in pipenv.\\nThe solution to this involves manually changing the scripts as describe here python_finder_fix\\nAdded by Ridwan Amure',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Permission Denied using Pipenv',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'e44ec04a'},\n",
       " {'text': 'When passing arguments to a script via command line and converting it to a 4 digit number using f’{year:04d}’, this error showed up.\\nThis happens because all inputs from the command line are read as string by the script. They need to be converted to numeric/integer before transformation in fstring.\\nyear = int(sys.argv[1])\\nf’{year:04d}’\\nIf you use click library just edit a decorator\\n@click.command()\\n@click.option( \"--year\",  help=\"Year for evaluation\",   type=int)\\ndef  your_function(year):\\n<<Your code>>\\nAdded by Taras Sh',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': \"Error while parsing arguments via CLI  [ValueError: Unknown format code 'd' for object of type 'str']\",\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '55fdb8b9'},\n",
       " {'text': 'Ensure the correct image is being used to derive from.\\nCopy the data from local to the docker image using the COPY command to a relative path. Using absolute paths within the image might be troublesome.\\nUse paths starting from /app and don’t forget to do WORKDIR /app before actually performing the code execution.\\nMost common commands\\nBuild container using docker build -t mlops-learn .\\nExecute the script using docker run -it --rm mlops-learn\\n<mlops-learn> is just a name used for the image and does not have any significance.',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Dockerizing tips',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'bf9082a2'},\n",
       " {'text': 'If you are trying to run Flask gunicorn & MLFlow server from the same container, defining both in Dockerfile with CMD will only run MLFlow & not Flask.\\nSolution: Create separate shell script with server run commands, for eg:\\n> \\tscript1.sh\\n#!/bin/bash\\ngunicorn --bind=0.0.0.0:9696 predict:app\\nAnother script with e.g. MLFlow server:\\n>\\tscript2.sh\\n#!/bin/bash\\nmlflow server -h 0.0.0.0 -p 5000 --backend-store-uri=sqlite:///mlflow.db --default-artifact-root=g3://zc-bucket/mlruns/\\nCreate a wrapper script to run above 2 scripts:\\n>\\twrapper_script.sh\\n#!/bin/bash\\n# Start the first process\\n./script1.sh &\\n# Start the second process\\n./script2.sh &\\n# Wait for any process to exit\\nwait -n\\n# Exit with status of process that exited first\\nexit $?\\nGive executable permissions to all scripts:\\nchmod +x *.sh\\nNow we can define last line of Dockerfile as:\\n> \\tDockerfile\\nCMD ./wrapper_script.sh\\nDont forget to expose all ports defined by services!',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Running multiple services in a Docker container',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'e7906e44'},\n",
       " {'text': 'Problem description cannot generate pipfile.lock raise InstallationError( pip9.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with error code 1\\nSolution: you need to force and upgrade wheel and pipenv\\nJust run the command line :\\npip install --user --upgrade --upgrade-strategy eager pipenv wheel',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Cannot generate pipfile.lock raise InstallationError( pip9.exceptions.InstallationError)',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '76d8892e'},\n",
       " {'text': \"Problem description. How can we connect s3 bucket to MLFLOW?\\nSolution: Use boto3 and AWS CLI to store access keys. The access keys are what will be used by boto3 (AWS' Python API tool) to connect with the AWS servers. If there are no Access Keys how can they make sure that they have the right to access this Bucket? Maybe you're a malicious actor (Hacker for ex). The keys must be present for boto3 to talk to the AWS servers and they will provide access to the Bucket if you possess the right permissions. You can always set the Bucket as public so anyone can access it, now you don't need access keys because AWS won't care.\\nRead more here: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\\nAdded by Akshit Miglani\",\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Connecting s3 bucket to MLFLOW',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'c5c2c82a'},\n",
       " {'text': 'Even though the upload works using aws cli and boto3 in Jupyter notebook.\\nSolution set the AWS_PROFILE environment variable (the default profile is called default)',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Uploading to s3 fails with An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The AWS Access Key Id you provided does not exist in our records.\"',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '82b6c143'},\n",
       " {'text': 'Problem description: lib_lightgbm.so Reason: image not found\\nSolution description: Add “RUN apt-get install libgomp1” to your docker. (change installer command based on OS)\\nAdded by Kazeem Hakeem',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Dockerizing lightgbm',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '77d9a742'},\n",
       " {'text': 'When the request is processed in lambda function, mlflow library raises:\\n2022/09/19 21:18:47 WARNING mlflow.pyfunc: Encountered an unexpected error (AttributeError(\"module \\'dataclasses\\' has no attribute \\'__version__\\'\")) while detecting model dependency mismatches. Set logging level to DEBUG to see the full traceback.\\nSolution: Increase the memory of the lambda function.\\nAdded by MarcosMJD',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Error raised when executing mlflow’s pyfunc.load_model in lambda function.',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '1667e95d'},\n",
       " {'text': 'Just a note if you are following the video but also using the repo’s notebook The notebook is the end state of the video which eventually uses mlflow pipelines.\\nJust watch the video and be patient. Everything will work :)\\nAdded by Quinn Avila',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': '4.3 FYI Notebook is end state of Video -',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '624a3525'},\n",
       " {'text': 'Problem description: I was having issues because my python script was not reading AWS credentials from env vars, after building the image I was running it like this:\\ndocker run -it homework-04 -e AWS_ACCESS_KEY_ID=xxxxxxxx -e AWS_SECRET_ACCESS_KEY=xxxxxx\\nSolution 1:\\n\\nEnvironment Variables: \\nYou can set the AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN (if you are using AWS STS) environment variables. You can set these in your shell, or you can include them in your Docker run command like this:\\nI found out by myself that those variables must be passed before specifying the name of the image, as follow:\\ndocker run -e AWS_ACCESS_KEY_ID=xxxxxxxx -e AWS_SECRET_ACCESS_KEY=xxxxxx -it homework-04\\nAdded by Erick Cal\\nSolution 2 (if AWS credentials were not found):\\nAWS Configuration Files: \\nThe AWS SDKs and CLI will check the ~/.aws/credentials and ~/.aws/config files for credentials if they exist. You can map these files into your Docker container using volumes:\\n\\ndocker run -it --rm -v ~/.aws:/root/.aws homework:v1',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Passing envs to my docker image',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '1db86601'},\n",
       " {'text': 'If anyone is troubleshooting or just interested in seeing the model listed on the image svizor/zoomcamp-model:mlops-3.10.0-slim.\\nCreate a dockerfile. (yep thats all) and build “docker build -t zoomcamp_test .”\\nFROM svizor/zoomcamp-model:mlops-3.10.0-slim\\nRun “docker run -it zoomcamp_test ls /app” output -> model.bin\\nThis will list the contents of the app directory and “model.bin” should output. With this you could just copy your files, for example “copy myfile .” maybe a requirements file and this can be run for example “docker run -it myimage myscript arg1 arg2 ”. Of course keep in mind a build is needed everytime you change the Dockerfile.\\nAnother variation is to have it run when you run the docker file.\\n“””\\nFROM svizor/zoomcamp-model:mlops-3.10.0-slim\\nWORKDIR /app\\nCMD ls\\n“””\\nJust keep in mind CMD is needed because the RUN commands are used for building the image and the CMD is used at container runtime. And in your example you probably want to run a script or should we say CMD a script.\\nQuinn Avila',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'How to see the model in the docker container in app/?',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '047baefe'},\n",
       " {'text': 'To resolve this make sure to build the docker image with the platform tag, like this:\\n“docker build -t homework:v1 --platform=linux/arm64 .”',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': \"WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\",\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '4f240372'},\n",
       " {'text': \"Solution: instead of input_file = f'https://s3.amazonaws.com/nyc-tlc/trip+data/{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet'  use input_file = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet'\\nIlnaz Salimov\\nsalimovilnaz777@gmail.com\",\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'HTTPError: HTTP Error 403: Forbidden when call apply_model() in score.ipynb',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '7aef625b'},\n",
       " {'text': 'i\\'m getting this error ModuleNotFoundError: No module named \\'pipenv.patched.pip._vendor.urllib3.response\\'\\nand Resolved from this command pip install pipenv --force-reinstall\\ngetting this errror site-packages\\\\pipenv\\\\patched\\\\pip\\\\_vendor\\\\urllib3\\\\connectionpool.py\"\\nResolved from this command pip install -U pip and pip install requests\\nAsif',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': \"ModuleNotFoundError: No module named 'pipenv.patched.pip._vendor.urllib3.response'\",\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'a3aa3a7d'},\n",
       " {'text': 'Problem description: When running docker-compose up as shown in the video 5.2 if you go to http://localhost:3000/ you get asked for a username and a password.\\nSolution: for both of them the default is “admin”. Then you can enter your new password. \\nSee also here\\nAdded by JaimeRV',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'Login window in Grafana',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'd2719204'},\n",
       " {'text': 'Problem Description : In Linux, when starting services using docker compose up --build  as shown in video 5.2, the services won’t start and instead we get message unknown flag: --build in command prompt.\\nSolution : Since we install docker-compose separately in Linux, we have to run docker-compose up --build instead of docker compose up --build\\nAdded by Ashish Lalchandani',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'Error in starting monitoring services in Linux',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '30b8e8e6'},\n",
       " {'text': 'Problem: When running prepare.py getting KeyError: ‘content-length’\\nSolution: From Emeli Dral:\\nIt seems to me that the link we used in prepare.py to download taxi data does not work anymore. I substituted the instruction:\\nurl = f\"https://nyc-tlc.s3.amazonaws.com/trip+data/{file}\\nby the\\nurl = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{file}\"\\nin the prepare.py and it worked for me. Hopefully, if you do the same you will be able to get those data.',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'KeyError ‘content-length’ when running prepare.py',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'f33fc6e9'},\n",
       " {'text': 'Problem description\\nWhen I run the command “docker-compose up –build” and send the data to the real-time prediction service. The service will return “Max retries exceeded with url: /api”.\\nIn my case it because of my evidently service exit with code 2 due to the “app.py” in evidently service cannot import “from pyarrow import parquet as pq”.\\nSolution description\\nThe first solution is just install the pyarrow module “pip install pyarrow”\\nThe second solution is restart your machine.\\nThe third solution is if the first and second one didn’t work with your machine. I found that “app.py” of evidently service didn’t use that module. So comment the pyarrow module out and the problem was solved for me.\\nAdded by Surawut Jirasaktavee',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'Evidently service exit with code 2',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'd828de2a'},\n",
       " {'text': 'When using evidently if you get this error.\\nYou probably forgot to and parentheses () just and opening and closing and you are good to go.\\nQuinn Avila',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'ValueError: Incorrect item instead of a metric or metric preset was passed to Report',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '03f20ec1'},\n",
       " {'text': 'You will get an error if you didn’t add a target=’duration_min’\\nIf you want to use RegressionQualityMetric() you need a target=’duration_min and you need this added to you current_data[‘duration_min’]\\nQuinn Avila',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'For the report RegressionQualityMetric()',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '249726fe'},\n",
       " {'text': 'Problem description\\nValueError: Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by LinearRegression.\\nSolution description\\nThis happens because the generated data is based on an early date therefore the training dataset would be empty.\\nAdjust the following\\nbegin = datetime.datetime(202X, X, X, 0, 0)\\nAdded by Luke',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'Found array with 0 sample(s)',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '4e492af0'},\n",
       " {'text': 'Problem description\\nGetting “target columns” “prediction columns” not present errors after adding a metric\\nSolution description\\nMake sure to read through the documentation on what is required or optional when adding the metric. I added DatasetCorrelationsMetric which doesn’t require any parameters because the metric evaluates for correlations among the features.\\nSam Lim',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'Adding additional metric',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '10011dc1'},\n",
       " {'text': 'When you try to login in Grafana with standard requisites (admin/admin) it throw up an error.\\nAfter run grafana-cli admin reset-admin-password admin in Grafana container the problem will be fixed\\nAdded by Artem Glazkov',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'Standard login in Grafana does not work',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '92fb909a'},\n",
       " {'text': 'Problem description. While my metric generation script was still running, I noticed that the charts in Grafana don’t get updated.\\nSolution description. There are two things to pay attention to:\\nRefresh interval: set it to a small value: 5-10-30 seconds\\nUse your local timezone in a call to `pytz.timezone` – I couldn’t get updates before changing this from the original value “Europe/London” to my own zone',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'The chart in Grafana doesn’t get updates',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '2b8cb640'},\n",
       " {'text': 'Problem description. Prefect server was not running locally, I ran `prefect server start` command but it stopped immediately..\\nSolution description. I used Prefect cloud to run the script, however I created an issue on the Prefect github.\\nBy Erick Calderin',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'Prefect server was not running locally',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'd4ceab0b'},\n",
       " {'text': 'Solution. Using docker CLI run docker system prune to remove unused things (build cache, containers, images etc)\\nAlso, to see what’s taking space before pruning you can run docker system df\\nBy Alex Litvinov',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'no disk space left error when doing docker compose up',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '482e575f'},\n",
       " {'text': 'Problem: when run docker-compose up –build, you may see this error. To solve, add `command: php -S 0.0.0.0:8080 -t /var/www/html` in adminer block in yml file like:\\nadminer:\\ncommand: php -S 0.0.0.0:8080 -t /var/www/html\\nimage: adminer\\n…\\nIlnaz Salimov\\nsalimovilnaz777@gmail.com',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'Failed to listen on :::8080 (reason: php_network_getaddresses: getaddrinfo failed: Address family for hostname not supported)',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '33e775eb'},\n",
       " {'text': 'Problem: Can we generate charts like Evidently inside Grafana?\\nSolution: In Grafana that would be a stat panel (just a number) and scatter plot panel (I believe it requires a plug-in). However, there is no native way to quickly recreate this exact Evidently dashboard. You\\'d need to make sure you have all the relevant information logged to your Grafana data source, and then design your own plots in Grafana.\\nIf you want to recreate the Evidently visualizations externally, you can export the Evidently output in JSON with include_render=True\\n(more details here https://docs.evidentlyai.com/user-guide/customization/json-dict-output) and then parse information from it for your external visualization layer. To include everything you need for non-aggregated visuals, you should also add \"raw_data\": True  option (more details here https://docs.evidentlyai.com/user-guide/customization/report-data-aggregation).\\nOverall, this specific plot with under- and over-performance segments is more useful during debugging, so might be easier to access it ad hoc using Evidently.\\nAdded by Ming Jun, Asked by Luke, Answered by Elena Samuylova',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Generate Evidently Chart in Grafana',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '19a3d34a'},\n",
       " {'text': \"You may get an error ‘{'errorMessage': 'Unable to locate credentials', …’ from the print statement in test_docker.py after running localstack with kinesis.\\nTo fix this, in the docker-compose.yaml file, in addition to the environment variables like AWS_DEFAULT_REGION, add two other variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. Their value is not important; anything like abc will suffice\\nAdded by M\\nOther possibility is just to run\\naws --endpoint-url http://localhost:4566 configure\\nAnd providing random values for AWS Access Key ID , AWS Secret Access Key, Default region name, and Default output format.\\nAdded by M.A. Monjas\",\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Get an error ‘Unable to locate credentials’ after running localstack with kinesis',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '55c68f23'},\n",
       " {'text': \"You may get an error while creating a bucket with localstack and the boto3 client:\\nbotocore.exceptions.ClientError: An error occurred (IllegalLocationConstraintException) when calling the CreateBucket operation: The unspecified location constraint is incompatible for the region specific endpoint this request was sent to.\\nTo fix this, instead of creating a bucket via\\ns3_client.create_bucket(Bucket='nyc-duration')\\nCreate it with\\ns3_client.create_bucket(Bucket='nyc-duration', CreateBucketConfiguration={\\n'LocationConstraint': AWS_DEFAULT_REGION})\\nyam\\nAdded by M\",\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Get an error ‘ unspecified location constraint is incompatible ’',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '54020f0a'},\n",
       " {'text': 'When executing an AWS CLI command (e.g., aws s3 ls), you can get the error <botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>.\\nTo fix it, simply set the AWS CLI environment variables:\\nexport AWS_DEFAULT_REGION=eu-west-1\\nexport AWS_ACCESS_KEY_ID=foobar\\nexport AWS_SECRET_ACCESS_KEY=foobar\\nTheir value is not important; anything would be ok.\\nAdded by Giovanni Pecoraro',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Get an error “<botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>” after running an AWS CLI command',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'b6249d2c'},\n",
       " {'text': 'At every commit the above error is thrown and no pre-commit hooks are ran.\\nMake sure the indentation in .pre-commit-config.yaml is correct. Especially the 4 spaces ahead of every `repo` statement\\nAdded by M. Ayoub C.',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Pre-commit triggers an error at every commit: “mapping values are not allowed in this context”',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '31543d95'},\n",
       " {'text': 'No option to remove pytest test\\nRemove .vscode folder located on the folder you previously used for testing, e.g. folder code (from week6-best-practices) was chosen to test, so you may remove .vscode inside the folder.\\nAdded by Rizdi Aprilian',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Could not reconfigure pytest from zero after getting done with previous folder',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'e147bbb6'},\n",
       " {'text': 'Problem description\\nFollowing video 6.3, at minute 11:23, get records command returns empty Records.\\nSolution description\\nAdd --no-sign-request to Kinesis get records call:\\n aws --endpoint-url=http://localhost:4566 kinesis get-records --shard-iterator […] --no-sign-request',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Empty Records in Kinesis Get Records with LocalStack',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'dc55657f'},\n",
       " {'text': \"Problem description\\ngit commit -m 'Updated xxxxxx'\\nAn error has occurred: InvalidConfigError:\\n==> File .pre-commit-config.yaml\\n=====> 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\\nSolution description\\nSet uft-8 encoding when creating the pre-commit yaml file:\\npre-commit sample-config | out-file .pre-commit-config.yaml -encoding utf8\\nAdded by MarcosMJD\",\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'In Powershell, Git commit raises utf-8 encoding error after creating pre-commit yaml file',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'f6979915'},\n",
       " {'text': \"Problem description\\ngit commit -m 'Updated xxxxxx'\\n[INFO] Initializing environment for https://github.com/pre-commit/pre-commit-hooks.\\n[INFO] Installing environment for https://github.com/pre-commit/pre-commit-hooks.\\n[INFO] Once installed this environment will be reused.\\nAn unexpected error has occurred: CalledProcessError: command:\\n…\\nreturn code: 1\\nexpected return code: 0\\nstdout:\\nAttributeError: 'PythonInfo' object has no attribute 'version_nodot'\\nSolution description\\nClear app-data of the virtualenv\\npython -m virtualenv api -vvv --reset-app-data\\nAdded by MarcosMJD\",\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': \"Git commit with pre-commit hook raises error ‘'PythonInfo' object has no attribute 'version_nodot'\",\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '1076a121'},\n",
       " {'text': 'Problem description\\nProject structure:\\n/sources/production/model_service.py\\n/sources/tests/unit_tests/test_model_service.py (“from production.model_service import ModelService)\\nWhen running python test_model_service.py from the sources directory, it works.\\nWhen running pytest ./test/unit_tests fails. ‘No module named ‘production’’\\nSolution description\\nUse python -m pytest ./test/unit_tests\\nExplanation: pytest does not add to the sys.path the path where pytest is run.\\nYou can run python -m pytest, or alternatively export PYTHONPATH=. Before executing pytest\\nAdded by MarcosMJD',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Pytest error ‘module not found’ when if using custom packages in the source code',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'aa203ca7'},\n",
       " {'text': 'Problem description\\nProject structure:\\n/sources/production/model_service.py\\n/sources/tests/unit_tests/test_model_service.py (“from production.model_service import ModelService)\\ngit commit -t ‘test’ raises ‘No module named ‘production’’ when calling pytest hook\\n- repo: local\\nhooks:\\n- id: pytest-check\\nname: pytest-check\\nentry: pytest\\nlanguage: system\\npass_filenames: false\\nalways_run: true\\nargs: [\\n\"tests/\"\\n]\\nSolution description\\nUse this hook instead:\\n- repo: local\\nhooks:\\n- id: pytest-check\\nname: pytest-check\\nentry: \"./sources/tests/unit_tests/run.sh\"\\nlanguage: system\\ntypes: [python]\\npass_filenames: false\\nalways_run: true\\nAnd make sure that run.sh sets the right directory and run pytest:\\ncd \"$(dirname \"$0\")\"\\ncd ../..\\nexport PYTHONPATH=.\\npipenv run pytest ./tests/unit_tests\\nAdded by MarcosMJD',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Pytest error ‘module not found’ when using pre-commit hooks if using custom packages in the source code',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '8b04605d'},\n",
       " {'text': 'Problem description\\nThis is the step in the ci yml file definition:\\n- name: Run Unit Tests\\nworking-directory: \"sources\"\\nrun: ./tests/unit_tests/run.sh\\nWhen executing github ci action, error raises:\\n…/tests/unit_test/run.sh Permission error\\nError: Process completed with error code 126\\nSolution description\\nAdd execution  permission to the script and commit+push:\\ngit update-index --chmod=+x .\\\\sources\\\\tests\\\\unit_tests\\\\run.sh\\nAdded by MarcosMJD',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Github actions: Permission denied error when executing script file',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'a3b9af04'},\n",
       " {'text': 'Problem description\\nWhen a docker-compose file contains a lot of containers, running the containers may take too much resource. There is a need to easily select only a group of containers while ignoring irrelevant containers during testing.\\nSolution description\\nAdd profiles: [“profile_name”] in the service definition.\\nWhen starting up the service, add `--profile profile_name` in the command.\\nAdded by Ammar Chalifah',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Managing Multiple Docker Containers with docker-compose profile',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'b16aae74'},\n",
       " {'text': 'Problem description\\nIf you are having problems with the integration tests and kinesis double check that your aws regions match on the docker-compose and local config. Otherwise you will be creating a stream in the wrong region\\nSolution description\\nFor example set ~/.aws/config region = us-east-1 and the docker-compose.yaml - AWS_DEFAULT_REGION=us-east-1\\nAdded by Quinn Avila',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'AWS regions need to match docker-compose',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '66326a87'},\n",
       " {'text': 'Problem description\\nPre-commit command was failing with isort repo.\\nSolution description\\nSet version to 5.12.0\\nAdded by Erick Calderin',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Isort Pre-commit',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': 'fb3c4150'},\n",
       " {'text': 'Problem description\\nInfrastructure created in AWS with CD-Deploy Action needs to be destroyed\\nSolution description\\nFrom local:\\nterraform init -backend-config=\"key=mlops-zoomcamp-prod.tfstate\" --reconfigure\\nterraform destroy --var-file vars/prod.tfvars\\nAdded by Erick Calderin',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'How to destroy infrastructure created via GitHub Actions',\n",
       "  'course': 'mlops-zoomcamp',\n",
       "  'id': '886d1617'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "be7d90d3-6054-47f8-b911-4b60848fdcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "doc = documents[2]\n",
    "prompt = prompt_template.format(**doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dc39cde2-ab01-4c98-bc14-2732baf321fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You emulate a student who's taking our course.\n",
      "Formulate 5 questions this student might ask based on a FAQ record. The record\n",
      "should contain the answer to the questions, and the questions should be complete and not too short.\n",
      "If possible, use as fewer words as possible from the record. \n",
      "\n",
      "The record:\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I still join the course after the start date?\n",
      "answer: Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\n",
      "\n",
      "Provide the output in parsable JSON without using code blocks. Do not add any text other than the list of questions:\n",
      "\n",
      "[\"question1\", \"question2\", ..., \"question5\"]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "51b8f259-eed8-4a50-b50c-2186fb154853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can I still take the course after the start date? What are the deadlines for final projects?',\n",
       " 'How do late submissions of homeworks affect the final grade?',\n",
       " 'Are there any penalties for missing the deadlines for final projects?',\n",
       " 'What consequences can I expect if I leave everything for the last minute?',\n",
       " 'Can I still join the course halfway through and still finish all the homeworks?']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since I don't have OpenAI I will use Groq. But then to sync up with the module lecture, I will use the results from the \n",
    "# binary file \"results.bin\"\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "\n",
    "# The generated questions are different between the 2 models though they are more or less trying to be similar to the \n",
    "# question from FAQ\n",
    "from groq import Groq\n",
    "import os\n",
    "import json\n",
    "# api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model='llama3-8b-8192', # We are using llama3 within Groq instead of 'gpt-4o'\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "response.choices[0].message\n",
    "\n",
    "json_response = response.choices[0].message.content\n",
    "json_response\n",
    "\n",
    "json.loads(json_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6f6771a8-e580-4cbc-a07d-e996f304a1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Can I still take the course after the start date? What are the deadlines for final projects?\", \"How do late submissions of homeworks affect the final grade?\", \"Are there any penalties for missing the deadlines for final projects?\", \"What consequences can I expect if I leave everything for the last minute?\", \"Can I still join the course halfway through and still finish all the homeworks?\"]\n"
     ]
    }
   ],
   "source": [
    "print(json_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "534ac1ae-b5e0-43a7-b8ad-103fd56ced54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will run this function using groq\n",
    "def generate_questions(doc):\n",
    "    prompt = prompt_template.format(**doc)\n",
    "\n",
    "    # response = client.chat.completions.create(\n",
    "    #     model='gpt-4o',\n",
    "    #     messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    # )\n",
    "    response = client.chat.completions.create(\n",
    "    model='llama3-8b-8192', # We are using llama3 within Groq instead of 'gpt-4o'\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    json_response = response.choices[0].message.content\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eb2353f8-411b-4ab9-a4c2-0d158495491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "25a48077-18c0-4d31-82ab-00a44d426279",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "742f4b7c-7632-4475-b6df-b3c02d343287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b192997ea445d094f80e085c96dc7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/948 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for doc in tqdm(documents): \n",
    "    doc_id = doc['id']\n",
    "    if doc_id in results: # Sometimes the for loop will fail if the rate limit is hit. therefore we skip those \n",
    "                          # doc_id next time this step is re-run\n",
    "        continue\n",
    "    questions = generate_questions(doc)\n",
    "    results[doc_id] = questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e531a979-2b64-4ece-b71e-5454b6fe695c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c02e79ef': '[\"When will the course start, exactly?\", \"What is the purpose of this document?\", \"When does the \\'Office Hours\\' live start?\\', \"How do I subscribe to the course public Google Calendar?\", \"How do I register before the course starts?\"]',\n",
       " '1f6520ca': '[\"What are the prerequisites for this course that I need to take before joining the data engineering zoomcamp course?\", \"What if I don\\'t have experience in data engineering yet?\", \"Can I take this course as a beginner?\", \"Are there any specific skills I need for this course?\", \"Do I need to have GitHub for this course?\"]',\n",
       " '7842b56a': '[\"Can I still register for the course if I missed the registration deadline?\",\\n\"Are there any deadlines for finishing assignments?\",\\n\"What if I miss the first few classes, will that affect my submissions?\",\\n\"Can I still join the course if I didn\\'t attend the first class?\",\\n\"Should I plan my final project completion in advance?\"]',\n",
       " '0bbf41ec': '[\"When will I receive the confirmation email for the Data Engineering Bootcamp course after registering?\", \"Does the registration process guarantee a spot in the course?\", \"Why do I need to register if it\\'s not checked against any registered list?\", \"Can I start learning and submitting homework without registering?\", \"What does \\'gauging interest\\' before the start date mean in the registration process?\"]',\n",
       " '63394d91': '[\\n\"Can I start preparing for the course before it starts? Specifically, what are the steps I can take to get ready?\", \\n\"What software and tools do I need to have installed and set up before the course begins?\", \\n\"Are there any specific prerequisites or subjects that I should brush up on before the course starts?\", \\n\"Can you explain the different parts of the Google Cloud account and how I can set it up?\", \\n\"What are the core skills and knowledge I should have before starting the course?\"',\n",
       " '2ed9b986': 'Here are the 5 questions the student might ask:\\n\\n[\"How many times does the course have its Zoom camps throughout a year?\", \"What are the specific dates for each Zoom camp?\", \"Is there only one Data-Engineering Zoom camp \\'live\\' cohort per year?\", \"Do I have the option to attend any Zoom camp at any time, or are there specific schedules?\", \"Are the Zoom camps for different courses, and if so, what are those courses?\"]',\n",
       " '93e2c8ed': '[\\n\"What are the main differences between the current cohort and the previous one, specifically in terms of the tools used?\",\\n\"Can you elaborate on why Prefect is no longer being used and what made Mage AI a better choice?\",\\n\"How does the switch to Mage AI affect the overall structure and pace of the course?\",\\n\"Are the new terraform videos recorded specifically for this cohort or can we access previous recordings?\",\\n\"What are some key takeaways from the shift to Mage AI that students from previous cohorts may not have experienced?\"',\n",
       " 'a482086d': '[\"Can I access all the course materials even after the course is over?\", \"Will I have access to homeworks after the course finishes?\", \"Can I continue working on the capstone project after the course finishes?\", \"Will I be able to follow the course at my own pace after it finishes?\", \"Will I be able to continue preparing for the next cohort after the course finishes?\"]',\n",
       " 'eb56ae98': '[\\n\"Can I get support if I take the course in the self-paced mode and not attend the live sessions?\",\\n\"Can I still ask questions in the slack channel if I\\'m in the self-paced mode?\",\\n\"Is it beneficial to search the channel before asking a question?\",\\n\"Will I get accurate answers if I tag the bot @ZoomcampQABot?\",\\n\"Can I just rely on the bot\\'s answers or should I still cross-check them?\"\\n]',\n",
       " '4292531b': '[\"What specific playlist on YouTube do I need to look at for the main course videos?\", \"Why is the GitHub repository updated to show each video with a thumbnail?\", \"What is the primary playlist I should be referring to?\", \"Can I find additional videos for specific years or topics like office hours?\", \"How do I access the MAIN PLAYLIST?\" ]',\n",
       " 'ea739c65': '[\"How many hours per week am I expected to spend on this course, and what factors influence this expectation?\", \"How do you determine the amount of time required for the course, and is it just based on background and previous experience?\", \"Is the expected 5-15 hours per week just an estimate, and what can I do to verify this?\", \"Can I use the provided data to calculate the expected time spent on the course, and how would I do that?\", \"Is the time commitment for the course consistent across all participants, or does it vary depending on individual circumstances?\"]',\n",
       " 'cb257ee5': '[\"What is the difference between finishing the course in a self-paced mode and getting a certificate compared to finishing it with a live cohort?\", \"Can I still participate in the self-paced mode if I want to get a certificate?\", \"Why is it necessary to peer-review projects during the course instead of after it\\'s finished?\", \"Will I be able to get a certificate if I\\'m part of a team that finishes the course with a live cohort?\", \"Do I have to be part of a live cohort to get a certificate for a specific capstone project?\"]',\n",
       " '04aa4897': '[{ \"question1\": \"What is the format of the Office Hours sessions, and how do students participate?\" },\\n{ \"question2\": \"Where can we find the link to the livestream for the Office Hours sessions?\" },\\n{ \"question3\": \"How can we submit questions to the instructors during the online sessions?\" },\\n{ \"question4\": \"Why can\\'t we post our questions in the chat during the livestream?\" },\\n{ \"question5\": \"Where are the livestream videos posted after the sessions?\" }]',\n",
       " '9681be3b': '[\"Will there be recordings for the Office Hours workshop if I miss the live session?\", \"What happens if I\\'m unable to attend the Office Hours workshop?\", \"Will the Office Hours sessions be available for rewatching?\", \"Can I attend a recording of Office Hours instead of the live session?\", \"Are the workshop recordings available for a set period or when do they expire?\"]',\n",
       " 'a1daf537': '[\"What are the homework and project deadlines?\", \"Where can I find the latest and up-to-date deadlines?\", \"What if I need an extension or there\\'s news about the deadlines?\", \"Can the instructor update the deadlines and if so, how can I see the updates?\", \"Are there any other resources I should check for deadline information?\"]',\n",
       " 'be5bfee4': '[\"What happens if I miss the homework deadline?\", \"Are there any exceptions to the no-late-submission rule?\", \"Can I still submit my homework if the form is still open but the due date has passed?\", \"Is there a specific process for confirming a late submission?\", \"What if the homework submission form remains open but I\\'ve already missed the deadline?\"]',\n",
       " '0e424a44': '[\"What exactly is considered a reasonable repository on GitHub, GitLab, or Bitbucket where I can store my homework code?\", \"Can I include any type of code besides the specific exercises in my homework repository? Should I include all my code?\", \"Where should I look for the homework URL if the link provided by the instructor doesn\\'t lead me to my repository?\", \"Can I use an alternative location to store my code instead of the recommended platforms like GitHub, GitLab, or Bitbucket?\", \"Will the instructor check each file in my repository to ensure I completed all exercises?\"]',\n",
       " '29865466': '[\"What is the system for points in the course management platform regarding homework submissions?\", \"How do I see my points on the course management platform?\", \"Do the points from different activities, such as homework, FAQs, and Learning in Public, get added up?\", \"How many points do I get for submitting something to FAQs?\", \"Can I see my total points earned from all activities on the leaderboard?\"]',\n",
       " '016d46a1': 'Here are 5 questions based on the FAQ record:\\n\\n[\"How can I know which leaderboard I\\'m on when I\\'m not showing up on it?\", \"How do I find out my Display name on the leaderboard?\", \"I\\'m not on the leaderboard, how do I join it?\", \"Why was I randomly assigned a name as my Display name?\", \"Can I change my Display name after setting up my account?\"]',\n",
       " '47972cb1': '[\\n\"Can I still use Python 3.9 for the course in 2024, or are there specific versions that are recommended?\",\\n\"Why is the recommended Python version 3.9, and what are the implications if I use a different version?\",\\n\"Is Python 3.9 required for the course, or can I use 3.10 or 3.11?\",\\n\"What was the reasoning behind choosing Python 3.9 as the recommended version?\",\\n\"Will I face any issues if I use a different version of Python, such as 3.10 or 3.11, and if so, how will I troubleshoot them?\"\\n]',\n",
       " 'ddf6c1b3': '[\"What are the options for setting up the course environment?\",\\n\"What are the potential challenges with setting up my local machine?\",\\n\"Can I work on my local machine with Docker from Week 1?\",\\n\"What alternatives are available for setting up a virtual machine?\",\\n\"Can I work on the course from different devices since I have multiple laptops and PCs?\"]',\n",
       " 'ac25d3af': '{\\n\"question1\": \"Can I use GitHub Codespaces as an alternative to using the command line interface or Git Bash to create a Dockerfile for the data ingestion process?\",\\n\"question2\": \"What kind of computing resources does GitHub Codespaces provide?\",\\n\"question3\": \"Are there any pre-installed tools available in GitHub Codespaces for creating Dockerfiles?\",\\n\"question4\": \"Can I directly open any GitHub repository in GitHub Codespaces?\",\\n\"question5\": \"Do GitHub Codespaces offer Linux resources that are compatible with the Docker-based data ingestion process?\"',\n",
       " '251218fc': '{\"questions\": [\"Can we use our own environment instead of GitHub codespaces?\", \"What are the specific requirements for our laptop to run the course?\", \"Are there any other platform options for the course, besides GitHub codespaces and GCP VM?\", \"Do we have to use a virtual machine for the course?\", \"Can we complete the entire course from our laptop without using GitHub codespaces or a virtual machine?\"]}',\n",
       " '3c0114ce': '[\"What are the options for setting up an environment for the course project, and how do I decide which one is best?\", \"Do I need to use GitHub Codespaces and GCP, or is one sufficient?\", \"Can I use BigQuery locally, or do I need a cloud-based GCP environment?\", \"How do I set up a local environment for the course tasks, and is it feasible?\", \"Is it possible to use both a local setup and GCloud for different parts of the project?\"]',\n",
       " 'f43f5fe7': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\\n  \"How do I open the Run command window on my Windows machine?\",\\n  \"What are the common locations of registry values in the Registry Editor?\",\\n  \"Why do I need to change registry values when connecting to GCP VM using VSCode?\",\\n  \"What is the alternative solution to changing registry values when connecting to GCP VM using VSCode?\",\\n  \"Where is the known_hosts file located on my Windows machine?\"',\n",
       " 'd061525d': '[\"Why are we using Google Cloud Platform (GCP) and not other cloud providers like Azure or AWS, even though we have accounts with all three platforms?, Can I use other cloud platforms if I\\'m comfortable with them?, Is there a free trial period for GCP?, How can I sign up for a free GCP account?, Do I need a credit card to sign up for a free GCP account?\"]',\n",
       " '1cd01b2c': '[\"What requirements do I need to meet to use GCP without paying for cloud services?\", \"How do I take advantage of the free trial for GCP?\", \"Are there any limitations to the free trial for GCP?\", \"Will I still have to pay for cloud services if I exceed the free trial limits?\", \"Can I upgrade to paid services if I need more from GCP?\"]',\n",
       " 'e4a7c3b0': '{\"questions\": [\\n\"How can I do most of the course without using a cloud provider since some are unavailable in my country?\",\\n\"Is it possible to install a home lab for the course, considering other cloud providers are also unavailable?\",\\n\"Can I run all the course materials locally without relying on Google Cloud Platform?\",\\n\"Are there local alternatives for all the tools and platforms used in the course?\",\\n\"What\\'s the minimum setup I need for a local environment to complete the course without any cloud provider?\"\\n]}',\n",
       " '7cd1912e': '[\"Can I use AWS instead of GCP for the course environment and what are the differences I should consider when adapting the video content?\", \"Will I have trouble getting help from the instructor or course mates if I use AWS instead of GCP?\", \"How will my final capstone project be evaluated if I use AWS instead of GCP?\", \"Are there any specific steps I should take to adapt the course material for my AWS environment?\", \"Will I have any limitations or disadvantages compared to my peers who use GCP?\"]',\n",
       " '52393fb3': '[\"Is there any other time besides the Office Hour when I can receive help from the instructor?\", \"What other opportunities are there to get clarification on any doubts I may have?\", \"Are there any live Zoom calls scheduled apart from the Office Hour?\", \"Will there be any additional support sessions during the Capstone period?\", \"Are there any live calls scheduled that will be announced in advance?\"]',\n",
       " '10515af5': '[\"Are we still using the same data as last year\\'s NYC Trip data for January 2021?\", \"Is the project\\'s scope remaining the same as last year\\'s?\", \"Will the 2021 NYC Trip data be available for download?\", \"Are we using the same NYC Trip data that\\'s available on the course website?\", \"Is the January 2021 NYC Trip data still the one being used in the project?\"]',\n",
       " 'cdb86a97': '[\"Why is the 2022 repository still available?\", \"Is the 2022 repository still up for download?\", \"Can I still access the 2022 content?\", \"Was the 2022 repository deleted?\", \"Is the 2022 content still maintained?\"]',\n",
       " '3e0114ad': '[\"What if I want to use Airflow instead of another tool for my final project\", \"What are some other tools I can use for my project\", \"Can I use a tool I\\'ve never used before for my project\", \"Do I need to use a tool that\\'s specifically listed for my project\", \"Can I use a tool I\\'ve used in another class for my project\"]',\n",
       " 'b2799574': '[\\n\"Can we substitute tool \\'X\\' for the one being used in the course, considering the multiple alternative data stacks mentioned?\",\\n\"What alternatives are available for the tools used in the course, according to the records?\",\\n\"Are we allowed to use our preferred tools instead of the ones used in the course for the capstone project?\",\\n\"Will the course support us if we choose to use a different data stack or tools?\",\\n\"Will we need to explain the choice of tools in the peer review of our capstone project if we decide to use different tools?\"',\n",
       " '2f19301f': '{\"questions\": [\"How can I contribute to the course in a meaningful way?\", \"Can I share the course with my friends?\", \"What kind of changes can I make to the course repository?\", \"How do I start contributing to the course?\", \"Can I make changes to the structure of the course?\"]}',\n",
       " '7c700adb': '[\"Is the course specifically designed for a particular operating system, such as Windows or Mac? Can I use other operating systems like Linux or Chrome OS?\", \"What if I\\'m using a different operating system than the recommended one?\", \"Are there any technical requirements that differ based on the operating system?\", \"Is there a specific operating system that I should use to avoid any issues?\", \"Can I use both a Windows and a Mac for different projects?\"]',\n",
       " '44b14808': '[\"What is the recommended solution for Windows users who encounter issues with shell scripts in modules with *.sh files?\", \"Can we install WSL after starting the course and continue with the rest of the material?\", \"Will we be able to use Git Bash or MINGW64 to run the shell scripts or is WSL the only option?\", \"Why can\\'t Windows users without WSL continue with the course after encountering issues with shell scripts?\", \"Will this issue be discussed in future modules or will it only be introduced in module-05 and the RisingWave workshop?\"]',\n",
       " '76e4baf6': ' [{\"What are the essential books I should read for this course?\",\"Are there any online articles or blog posts on data engineering I should check out?\",\"Is there a specific database or software I should focus on learning for this course?\",\"Can I use a different programming language for this course, or is there a specific one required?\",\"Are there any additional resources or cheat sheets I can use to help me understand the material better?\"}',\n",
       " '48b533a8': '[\"What is the exact difference between Project Attempt #1 and #2?\",\\n\"What happens if I submit my project late for the first time?\",\\n\"Are there any specific guidelines for the second project attempt?\",\\n\"If I fail the first project attempt, do I get another chance?\",\\n\"What if I fail my second attempt? Is there a third chance?\"]',\n",
       " '954044d1': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I troubleshoot issues on my own before asking for help?\", \"What are some common techniques for finding the solution to an error?\", \"Why should I search for answers before posting on Stackoverflow, and what if I don\\'t find anything?\", \"What are some common mistakes to avoid when asking for help on Slack or Stackoverflow?\", \"What should I do if I\\'m stuck and unable to figure out the solution to my problem?\"]',\n",
       " 'a820b9b3': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"When couldn\\'t the troubleshooting guide help with my coding issue and I need help from another student?\", \"What information should I include when asking a question about my coding issue?\", \"How can I provide the most relevant information when posting my question?\", \"What should I include when describing the error message I\\'m getting?\", \"What have I tried to solve the issue so far and what didn\\'t work?\"]',\n",
       " 'f2945cd2': '[\"How can I create a GitHub account to use for this course?\", \"Can I just upload my own notes to the course repository or do I need to create my own repository?\", \"How do I ignore large files like databases and CSV files in my repository?\", \"What should I do if I accidentally upload a password or key to my repository?\", \"Is it necessary to know how to do everything listed in the tutorials provided in the FAQ for this course?\"]',\n",
       " 'eb9d376f': '[\"Why do I get an Error: Makefile:2: *** missing separator.  Stop. when trying to use tabs in VS Code?\", \"How do I convert tabs in a document to actual Tab characters in VS Code?\", \"Is it possible to automatically format a document in VS Code to use Tab characters instead of spaces?\", \"What can I do if my Makefile is not properly formatted and I keep getting this error?\", \"Can I set VS Code to use spaces instead of Tab characters by default?\"]',\n",
       " '72f25f6d': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"Can I open HTML files from the Linux side without switching to the Windows side?\", \"How do I open an HTML file from the Linux WSL using a specific browser?\", \"Does wslview work only with Microsoft Edge or any browser?\", \"Can I configure the default browser for wslview to use?\", \"How do I customise the browser used by wslview without modifying the environment variable?\"]',\n",
       " 'a1e59afc': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\"How do I set up Chrome Remote Desktop on a Debian Linux virtual machine on Compute Engine?\", \"What is the alternative link to download the 2021 Yellow Taxi Trip Records data?\", \"Why do I get an error when trying to download the 2021 Yellow Taxi Trip Records data from the TLC website?\", \"Can I use the \\'unzip\\' command to extract the \\'gz\\' file?\", \"How do I extract the contents of a \\'gz\\' file if the \\'unzip\\' command doesn\\'t work?\"]',\n",
       " '71c10610': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\\n\"How do I store tax data files correctly, considering the files are available as *.csv.gz?\",\\n\"Is it safe to read *.csv.gz files directly with pandas without changing the file extension?\",\\n\"Why does using csv.gz file extension cause issues in storing data files?\",\\n\"Can I modify the file name using the URL instead of specifying it manually?\",\\n\"Is there a way to handle the data file name parsing dynamically when using pd.read_csv?\"',\n",
       " '17a5aea1': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What does the data dictionary for NY Taxi data specify for the Yellow Trips records?\",\\n\"Are there two separate data dictionaries provided for NY Taxi data?\",\\n\"Can I find the data dictionary for Ny Taxi data trips in a single place?\",\\n\"What does the source of the Yellow Trips data dictionary indicate?\",\\n\"Are the trip records data dictionaries for Yellow and Green taxis different?\"',\n",
       " '5a275db7': 'Here are five questions the student might ask based on the FAQ record:\\n\\n[\"How do I unzip a downloaded parquet file in the command line?\",\\n\"What should I do with the csv file after it\\'s created?\",\\n\"Can I directly use parquet files in my python script or do I need to convert them?\",\\n\"How do I download the parquet file in my python script using os.system?\",\\n\"What format should I save the output in when using df.to_csv()?\"',\n",
       " '7ec0f9b0': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How do I install wget on Ubuntu\",\\n\"How do I install wget on MacOS\",\\n\"How do I install wget on Windows\",\\n\"What if I\\'m using Python, how do I use wget\",\\n\"What is an alternative way to download files instead of using wget\"\\n]',\n",
       " 'bb1ba786': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"How can I execute the wget command in a Jupyter Notebook?\", \"Why am I getting an error with wget and a website\\'s certificate?\", \"Can you provide an alternative way to run the wget command?\", \"What\\'s the purpose of adding ! before the wget command?\", \"What\\'s the effect of adding --no-check-certificate at the end of the wget command?\"]',\n",
       " '2f83dbe7': '[\"What does the backslash mean in Git Bash on Windows?\", \"How do I use the backslash as an escape character in Git Bash on Windows?\", \"Do I need to include the command in .bashrc?\", \"Is it a requirement to always use the backslash as the escape character?\", \"What is the purpose of setting the bash escape character in Git Bash?\"]',\n",
       " '543ff080': '[\"What is the recommended method for storing account-specific secrets for GitHub Codespaces?\", \"How do I ensure the security of my secrets in GitHub Codespaces?\", \"Are there any limitations to managing secrets for GitHub Codespaces?\", \"Can I use existing secrets management tools with GitHub Codespaces?\", \"What are the implications of using GitHub Codespaces secrets for my project\\'s security?\"]',\n",
       " 'd407d65b': '[\"Can\\'t connect to Docker daemon at unix:///var/run/docker.sock, how do I resolve this issue?\", \"Why do I keep getting the error \\'Cannot connect to Docker daemon\\'?\", \"I\\'m having trouble starting the Docker daemon, what can I do?\", \"Do I need to update the WSL in PowerShell? What\\'s the command?\", \"Can you provide more information on how to resolve the \\'Cannot connect to Docker daemon\\' issue?\"]',\n",
       " 'c9375c56': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"Why do I need elevated privileges to run the Docker client on Windows?\", \"What are the constraints for using Hyper-V as the Docker engine backend?\", \"Are there any specific requirements for Windows 10 Home/11 Home users to use Docker?\", \"I\\'m using Windows 10 Home/11 Home, but I don\\'t see the Hyper-V option. What\\'s going on?\", \"What can I do if I encounter issues installing WSL2, such as WslRegisterDistribution failed with error: 0x800701bc?\"]',\n",
       " 'e866156b': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What does docker pull do when fetching a non-public Docker image?\", \"Why do I get denied access when trying to fetch a Docker image?\", \"Can I use Docker login to fetch private Docker images?\", \"How do I fix a \\'permission denied\\' error when creating a PostgreSQL Docker container on macOS M1?\", \"Why do I need to stop Rancher Desktop to create a PostgreSQL Docker container with a mounted volume?\"]',\n",
       " '16370470': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\\n  \"Why can\\'t I delete a local folder that was mounted to a Docker volume?\",\\n  \"How did the mounting process give the folder write and read protection?\",\\n  \"What user owns the folder, and why did that cause issues?\",\\n  \"Why didn\\'t the \\'drag to trash\\' method work for deleting the folder?\",\\n  \"What is the purpose of using the \\'-f\\' flag with the \\'rm\\' command?\"',\n",
       " '316df755': '[\\n\"How can I ensure I\\'m running the latest version of Docker for Windows? I\\'ve been having trouble getting it to start.\",\\n\"Why does switching containers help get Docker unstuck when it\\'s stuck on starting?\",\\n\"Certain of my friends are able to run Docker on Windows 10/11 without any issues, but I\\'m having trouble. Is there something specific to the Pro Edition that I need to check?\",\\n\"Can you guide me through the process of enabling Hyper-V on my Windows 10/11 system? I want to use it as the backend for Docker.\",\\n\"What are the steps to set up WSL2 as the backend for Docker on my Windows 10/11 system?\"',\n",
       " 'f3aa9252': '[\"Should I run Docker commands from the Windows file system or a virtual file system under WSL?\", \"What edition of Windows am I allowed to use WSL2 on to run Docker?\", \"Can I still run Docker if I have issues with WSL2 configuration?\", \"Is there a way to resolve Docker issues by resetting to factory defaults or reinstalling?\", \"Do I have another option to install Docker on my system besides using WSL?\"]',\n",
       " 'a4abe7a5': '[\"What is the best way to store all code in my default Linux distro to get the best out of file system performance, considering Docker runs on WSL2 backend by default for Windows 10 Home and Windows 11 Home users?\", \"Are there any specific practices I should follow to optimize file system performance when running Docker on WSL2?\", \"Is it recommended to store my code in the default Linux distro rather than a separate Docker container?\", \"How can I take advantage of WSL2\\'s backend to improve Docker\\'s file system performance?\", \"Are there any specific Docker Docs resources I should check out for best practices on file system performance?\"]',\n",
       " 'fb930700': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What is the error message I get when trying to run a Docker container on Windows?\", \"Why am I getting the \\'the input device is not a TTY\\' error while trying to use Docker on Windows?\", \"How can I resolve the \\'the input device is not a TTY\\' issue when running a Docker container on Windows?\", \"Is there a way to avoid prefixing each Docker command with \\'winpty\\'?\", \"Can I make a permanent change to fix this issue instead of specifying \\'winpty\\' each time I run a Docker command?\"]',\n",
       " 'aa187680': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"We\\'re trying to install a package using pip within a Docker container, but it keeps failing. Why are we getting an error about connection failure?\", \\n\"What does the \\'NewConnectionError\\' mean in the error message, and why is it related to name resolution?\", \\n\"Can you explain what the \\'Retry(total=4)\\' part of the error message means?\", \\n\"How does the \\'winpty docker\\' command work, and what\\'s the purpose of the \\'--dns=8.8.8.8\\' option?\", \\n\"Is there a way to solve this issue without having to use the \\'--dns=8.8.8.8\\' option in the Docker command?\"',\n",
       " 'b000e899': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"Even after running the docker script, why is the ny_taxi_postgres_data folder still empty in VS Code?\", \"Why do I need to use the absolute path in the -v parameter?\", \"Can you explain why this fix resolves the issue of files not being visible in VS Code?\", \"Does the solution only work for Windows?\", \"What happens if I try to use a relative path instead of an absolute one in the -v parameter?\"]',\n",
       " '9c66759f': '[\"What is the alternative method to set up Docker on Mac if the method provided in the FAQ is outdated?\", \"Is it recommended to download Docker from the official website if the method provided in the FAQ does not work?\", \"Are there any specific settings or configurations required after downloading Docker from the official website?\", \"Can you provide more information about why the method provided in the FAQ may be unreliable?\", \"Is there a recommended method for troubleshooting issues when setting up Docker on Mac?\"]',\n",
       " 'e3106e07': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What does the error message \"Could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted\" mean?\", \"How do I solve the problem when Docker can\\'t change permissions of the directory?\", \"Why are the files belonging to this database system owned by user \"postgres\"?\", \"Can you explain what \\'volume\\' means in the context of Docker and how it solves the problem?\", \"If I\\'ve already initialized a PostgreSQL database, how do I troubleshoot an error that says the directory already exists but is not empty?\"]',\n",
       " '72229da5': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why does the video show a different way to mount volumes on Windows?\", \\n\"Why do I need to replace spaces in the folder path with underscores or move the data to a folder without spaces?\", \\n\"What is the difference between using \\'-v\\' and \\'--volume\\' options in the Docker command?\", \\n\"Why does adding quotes around the volume path or using a volume name instead of the path solve the issue?\", \\n\"What is the purpose of the \\'winpty\\' command and when should it be used with Docker on Windows?\"]',\n",
       " '58c9f99f': '[\\n  \"What is causing the error response from the daemon in Docker, specifically the message about an invalid mode: \\\\\\\\Program Files\\\\\\\\Git\\\\\\\\var\\\\\\\\lib\\\\\\\\postgresql\\\\\\\\data?\",\\n  \"Why do we need to change the mounting path in Docker, and how does this resolve the error?\",\\n  \"When specifying the mounting path in Docker, should we include a trailing backslash at the end of the path?\",\\n  \"What types of paths can we use for the mounting path in Docker, and are there any specific directories that are recommended?\",\\n  \"In the provided example, what does the leading slash in front of \\'c:\\' mean when specifying the mounting path?\"',\n",
       " 'bc42139a': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n  \"What does the error message \\'Docker - Error response from daemon: error while creating buildmount source path \\'/run/desktop/mnt/host/c/<your path>\\': mkdir /run/desktop/mnt/host/c: file exists\\' mean when I run the Docker command?\",\\n  \"Why do I get a different error message when I run the Docker command for the second time compared to the first time?\",\\n  \"How do I avoid the error that occurs when I mount the volume on the second run of the Docker command?\",\\n  \"Can I mount the volume at all when I run the Docker command the second time?\",\\n  \"What is the corrected Docker command that I should use if I still get the error message when mounting the volume on the second run?\"',\n",
       " 'a146e3ee': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\\n  \"What does the \\'can\\'t stat\\' error message in Docker mean?\",\\n  \"Why did setting the user id to 999 in the ny_taxi_postgres_data directory cause a permission issue?\",\\n  \"How can I fix a permission issue in Docker when accessing a directory?\",\\n  \"Is it necessary to have all files in the same directory to run the docker build command?\",\\n  \"What is the difference between running the docker build command on a different directory versus the current one?\"',\n",
       " '593a85ba': '[\\n\"Can you help me understand why I\\'m getting a \\'context canceled\\' error when I try to run a Docker container? I\\'ve followed the instructions to install Docker but I\\'m still getting this error.\",\\n\"I\\'ve tried to install Docker via the official website, but the installation is failing with a \\'Bind for 0.0.0.0:5432 failed: port is already in use\\' error.\",\\n\"I\\'ve noticed that my Docker installation was via snap, can you explain why this might be causing issues with running containers?\",\\n\"What does it mean when I run \\'sudo snap status docker\\' and get the response \\'error: unknown command \"status\", see \\'snap help\\'\"?\",\\n\"I\\'ve tried to reinstall Docker via the official website after uninstalling it from snap, but I still can\\'t get it to work. Can you provide any additional troubleshooting steps I can try?\"',\n",
       " '50bd1a71': '[\"Why does the build error occur when I try to run Docker on Ubuntu?\", \"Why do I have to add permissions to a folder in PopOS?\", \"What is the permission level of 777 in the chmod command?\", \"Why does the issue happen when the folder appears to be empty but it\\'s not?\", \"How can I apply the chmod command to multiple folders at the same time?\"]',\n",
       " 'f409f751': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\"What happens on Ubuntu/Linux systems when trying to run the command to build the Docker container again?\", \"Why do I get an error when the build command is executed again to rebuild the pipeline or create a new one?\", \"Can you explain why I\\'m getting a permission denied error when trying to run the command?\", \"How do I grant permissions to the new folder when running the Docker build command again?\", \"Is 755 a secure permission setting for granting write access only to the owner?\"]',\n",
       " '7d217da3': '[\"How do I get the network name for a Docker network?\", \"In the \\'docker network ls\\' command, why is there a \\'-\\' sign between \\'Network\\' and \\'ID\\'?\", \"What is the function of \\'docker network ls\\' command in Docker?\", \"How do I create a Docker network using a specific name?\", \"Can I combine \\'docker network ls\\' and \\'docker container ls\\' commands to get container information?\"]',\n",
       " '09081824': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What do I do when I get the error response `Docker - Error response from daemon: Conflict. The container name \"pg-database\" is already in use by container “xxx”`?\", \"How do I restart a docker image configured with a network name without getting this error?\", \"What is the difference between using `docker run` and `docker start` to restart a docker image?\", \"What happens when I try to use `docker run` to restart a docker image that is already running?\", \"Why do I need to use `docker stop` before `docker rm` when a container is already running?\"]',\n",
       " '4df80c55': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\"What happens when Docker-Compose is used and the hostname cannot be translated?\", \"Why do we see a OperationalError when running docker-compose up -d?\", \"How do we resolve the \\'could not translate host name\\' issue when using docker-compose?\", \"What is the equivalent of pg-network when running docker-compose up -d?\", \"Why are we using 2docker_default and 2docker-pgdatabase-1 instead of pg-network and pgdatabase?\"]',\n",
       " '3aee7261': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"How can I install Docker on a MacOS/Windows 11 VM running on top of Linux that uses nested virtualization?\",\\n\"Why do I get an error when trying to install Docker on a Linux VM running on top of MacOS/Windows 11?\",\\n\"Is there a solution for installing Docker on a nested virtualization setup?\",\\n\"What are the specific commands I need to run to enable nested virtualization for Docker installation?\",\\n\"How can I enable the necessary kernel modules for Docker installation in a nested virtualization setup?\"\\n]',\n",
       " '6497b659': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"How can I easily manage my Docker container, images and compose projects from VS Code?\", \"What is the command to stop a running Docker container?\", \"Can I use VS Code to connect to Docker running on WSL2?\", \"How do I launch the Docker extension in VS Code?\", \"Can I get the output of Docker commands in JSON format?\"]',\n",
       " 'a02f2039': '[\"What happens when I see a PostgreSQL Database directory in the Docker logs saying the database system is shut down?\", \"Why does connecting to the container result in a connection failed error saying the server closed the connection unexpectedly?\", \"Why would the PostgreSQL server terminate abnormally before or while processing a request?\", \"How can I resolve the issue with the PostgreSQL server terminating abnormally?\", \"What should I do with the directory containing data if the PostgreSQL server terminates abnormally?\"]',\n",
       " 'c6db65aa': '[{ \"question\": \"How do I install Docker on Ubuntu?\" }, { \"question\": \"Will I need to use a special command to install Docker on all versions of Ubuntu?\" }, { \"question\": \"Can I use a different method to install Docker on Ubuntu other than the one mentioned in the course?\" }, { \"question\": \"What is the best way to install Docker on Ubuntu for those who are not familiar with the command line?\" }, { \"question\": \"Are there any compatibility issues with installing Docker on older versions of Ubuntu?\" }]',\n",
       " 'f476a606': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What error do I get when trying to use Docker-Compose and mounting a directory?\", \"How do I define a named volume in the Docker-Compose file?\", \"Why does Docker-Compose create a new volume with a different name than what I specified?\", \"Can you show me the command to inspect the location of a Docker volume?\", \"What steps do I take if I accidentally create a new volume and want to switch to the existing one?\"]',\n",
       " 'e41b100c': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"Why am I getting the error \\'Couldn\\'t translate host name to address\\' when using Docker-Compose?\", \"What does it mean when the FAQ says \\'Make sure postgres database is running\\'?\", \"What is the command to start containers in detached mode with Docker-Compose?\", \"How do I view the logs for a specific container using Docker?\", \"If I running into issues with Docker-Compose, what can I do to troubleshoot the problem?\"]',\n",
       " 'cd0f9300': 'Here are the 5 questions the student might ask:\\n\\n[\"What happens when Docker compose creates its own default network?\", \"How do I find the new network name emitted in the logs after executing docker compose up?\", \"Why am I still receiving the error after changing the network name argument in my Ingestion script?\", \"How can I troubleshoot further if problems persist with pgcli?\", \"Can HeidiSQL or usql be used as an alternative to pgcli?\"]',\n",
       " '7f845a1c': 'Here are the 5 questions:\\n\\n[\"What happens when I run \\'docker ps -a\\'?\", \"How do you suggest I deal with a container that is not responding?\", \"What are some best practices for naming host names in Docker?\", \"Can you explain how I should specify the network in my docker-compose.yml file?\", \"What should I do if I get an error saying \\'network X\\' not found?\"]',\n",
       " '36e54439': '[\"How can I persist the data of a PostgreSQL database in a PGAdmin container when running Docker-Compose on Google Cloud Platform (GCP)?\", \"Why doesn\\'t the \\'./pgadmin\\' directory persist the data when mounting it as a volume in the Docker-Compose file?\", \"What is the difference between the \\'Volumes\\' and \\'volumes\\' parameters in the Docker-Compose file?\", \"How can I use Docker Volumes to persist data from a PGAdmin container?\", \"What is the purpose of the \\'wr\\' permission flag when specifying a mount volume in the Docker-Compose file?\"]',\n",
       " '32e8450c': '[\\n  \"When trying to run the Docker engine, it keeps crashing continuously and not working even after restarting. What could be the cause?\",\\n  \"Why do I keep seeing the \\'Docker engine stopped and failed to fetch extensions\\' popup on my screen?\",\\n  \"How can I determine if the Docker engine is not working correctly after a restart?\",\\n  \"What is the difference between the Docker engine stopping and failing to fetch extensions?\",\\n  \"If the problem persists after updating Docker, what is the final solution to get it working again?\"\\n]',\n",
       " '96606db2': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\"How do I persist pgAdmin configuration in Docker-Compose?\", \"What is the purpose of the \\'volumes\\' section in the Docker-Compose configuration file?\", \"Why do I need to give the pgAdmin container access to write to the \\'pgAdmin_data\\' folder?\", \"How do I give access to the mounted volume in the terminal?\", \"Why is the \\'chown\\' command necessary to assign the correct ownership to the \\'pgAdmin_data\\' folder?\"]',\n",
       " '0882bfac': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What causes the \\'docker-Compose - dial unix /var/run/docker.sock: connect: permission denied\\' error?\",\\n\"Why do I need to create a docker group and add my user to resolve this issue?\",\\n\"How can I make it so I don\\'t have to set up my database connection every time I start the containers?\",\\n\"What is the purpose of creating a volume for pgAdmin in my docker-compose.yaml file?\",\\n\"What does \\'press ctrl+D to log-out and log-in again\\' mean?\"',\n",
       " '7d067f5c': '[\"What happens when I install Docker Compose and the docker-compose command is not available even after modifying the .bashrc file?\", \"Why is it necessary to install Docker Compose manually if we already have a downloaded docker-compose file?\", \"How can we make the Docker Compose command available in the terminal after installation?\", \"Is it possible to use the Docker Compose file downloaded from GitHub in its current form, or do we need to make modifications?\", \"What are the advantages of using the Docker Compose command over using the docker-compose file directly?\"]',\n",
       " 'ff352621': '[\"What is the solution to the error getting credentials after running docker-compose up -d?\", \"How can I fix the issue with Docker-Compose?\", \"Can you provide a brief explanation about the error getting credentials after running docker-compose up -d?\", \"Is there a possible solution for the error getting credentials after running docker-compose up -d?\", \"What is the recommended fix for the error getting credentials after running docker-compose up -d?\"]',\n",
       " '2d653208': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\\n\"Can I set up a volume for my Docker container manually, or do I have to use the Docker Desktop app?\",\\n\"How do I change the low_memory parameter when importing a CSV file in Pandas, and why is it necessary?\",\\n\"What are the correct steps for running the Docker compose command, and why do I need to ensure no other containers are running?\",\\n\"Can I use a different database name or configuration in my pgadmin setup, or do I need to follow the same configurations as in the docker-compose.yml file?\",\\n\"Why do I need to use the specific function provided in the upload-data.ipynb file for tracking the ingestion process, and what benefits does it provide?\"',\n",
       " 'f09ea61e': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What happens when I try to run \\'docker-compose up -d\\' and I get an error about getting credentials?\", \\n\"What is the solution to the \\'docker-credential-desktop\\' executable file not found error?\", \\n\"Why am I getting an error when I try to run \\'docker-compose up -d\\'?\", \\n\"What do I need to do to fix the \\'executable file not found\\' error when running \\'docker-compose up -d\\'?\", \\n\"What are the steps to resolve the error when trying to run \\'docker-compose up -d\\'?\"]',\n",
       " 'fbd3d2bb': 'Here are the 5 questions the student might ask:\\n\\n[\"How do I determine which Docker-Compose binary to use for WSL?\", \"What are the possible values for the \\'uname -s\\' command?\", \"Can I use a specific version of Docker-Compose for my project?\", \"Why do I need to specify the \\'uname -m\\' command?\", \"How can I download Docker-Compose from the releases page without using the curl command?\"]',\n",
       " '0b014d0c': '[\"What are the common mistakes that could cause an error like \\'undefined volume\\' when using Docker-Compose on Windows/WSL?\", \"Can I copy-paste the Docker-Compose file from the video to use it directly?\", \"Why do I need to add the volume declaration in the Docker-Compose file?\", \"Can you show me the correct placement of the volume declaration in the Docker-Compose file?\", \"What is the advantage of including the volume declaration in the Docker-Compose file?\"]',\n",
       " 'd21bff1d': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\"What is the root cause of the WSL Docker directory permissions error?\", \"Why does WSL and Windows conflict when it comes to file system permissions?\", \"Is it essential to use Docker volumes to resolve permission issues?\", \"How does using local volumes affect the \\'user:\\' parameter in the docker-compose.yaml file?\", \"What benefits does using Docker volumes bring to the management of volumes?\"]',\n",
       " '6afb7b55': '[\\n  \"What is the reason for pgadmin not working for querying in Postgres if I\\'m using Git Bash or a VM on Windows?\",\\n  \"What are the libraries required for pgadmin to work on my Windows machine?\",\\n  \"Can I still use pgadmin if the above-mentioned libraries are installed and the issue persists?\",\\n  \"What is an alternative to pgadmin for querying in Postgres?\",\\n  \"How do I install this alternative, \\'psql\\', and are there any specific steps I need to follow for Docker?\"\\n]',\n",
       " 'b51c3b82': '{\"question1\": \"What happens when I see an \\'Insufficient system resources exist to complete the requested service\\' error in WSL?\",\\n\"question2\": \"Why did it take an app update to resolve the issue?\",\\n\"question3\": \"How do I update Windows Terminal to fix the problem?\",\\n\"question4\": \"What should I do if I have pending Windows security updates?\",\\n\"question5\": \"Do I need to restart my system after applying Windows updates to see the changes?\"}',\n",
       " '326af690': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What can I do if WSL integration with distro Ubuntu unexpectedly stops with exit code 1?\", \"Why is DNS resolution not working in WSL?\", \"How can I resolvebash: conda: command not founderror in WSL?\", \"What can I do when initializing a database in WSL prompts for superuser password?\", \"How can I switch from Windows containers to Linux containers in Docker?\"]',\n",
       " 'c2ec9047': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What is the likely cause of the \\'Permissions too open\\' issue when running the GPC VM through SSH through WSL2?\",\\n\"What is the correct folder where WSL2 looks for .ssh keys?\",\\n\"Why do I need to use \\'sudo\\' before running the SSH command in the terminal?\",\\n\"How do I change the permissions of the private key SSH file?\",\\n\"What is the purpose of creating a .ssh folder in the home directory of WSL2 and copying the content of the Windows .ssh folder to that new folder?\"]',\n",
       " '3b711e73': 'Here are five questions that a student might ask based on the FAQ record:\\n\\n[\"How do I fix the issue where WSL2 cannot resolve the host name?\", \"Why does WSL2 not reference the correct .ssh/config path from Windows?\", \"How do I create a config file in WSL2 that references my private key?\", \"What is the format of the config file I need to create in WSL2?\", \"Why do I need to create a .ssh folder in the home directory of WSL2?\"]',\n",
       " 'cfe07c9d': '[\\n  \"Why do I get a \\'PGCLI - connection failed\\' error when trying to connect to Postgres using pgcli?\",\\n  \"How do I resolve a \\'could not receive data from server: Connection refused\\' error in pgcli?\",\\n  \"What is the correct syntax for using pgcli to connect to a Postgres database?\",\\n  \"How can I troubleshoot common issues with pgcli and Postgres connections?\",\\n  \"What are some common causes for \\'could not send SSL negotiation packet: Connection refused\\' error in pgcli?\"\\n]',\n",
       " 'acf42bb8': '[\\n  \"I\\'m trying to use PGCLI, but when I run PGCLI --help, I get an error message. How do I resolve this issue when the PGCLI --help error occurs?\",\\n  \"What could be the possible reasons for the error message I get when I run PGCLI --help, and how do I troubleshoot them?\",\\n  \"I think I might have a problem with my PGCLI installation. How do I identify and fix the installation error?\",\\n  \"Is there a way to verify that the PGCLI installation was successful without running PGCLI --help?\",\\n  \"What are some common mistakes people make when installing PGCLI, and how can I avoid them?\"',\n",
       " '176ce516': '[\\n  \"Can we run pgcli inside another Docker container or will it pose any conflicts with the mapped port in Module 1, since the answer suggests the port is mapped directly to our local computer?\",\\n  \"If we can\\'t run pgcli inside another container, can we run multiple pgcli instances directly on our local system, and are there any specific considerations we need to keep in mind?\",\\n  \"How does the mapping of the 5432 port from the pgsql container to our local system affect our ability to access the database when running pgcli, and what are some of the potential benefits of this approach?\",\\n  \"Are there any specific requirements or limitations for accessing the postgres database via pgcli, given that it\\'s being mapped to our local system\\'s 5432 port?\",\\n  \"What happens if we need to access the postgres database via pgcli while the Docker container is not running, and is there a workaround available for this scenario?\"\\n]',\n",
       " '3e5d1e9b': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n  \"What happens when I try to use the default port 5432 with Docker container and a local Postgres installation?\",\\n  \"How do I connect to the Docker container using pgcli when using a different port?\",\\n  \"Can I use any port for mapping, or are there specific rules I should follow?\",\\n  \"How do I debug and find out what\\'s taking up the default port 5432 on my Mac?\",\\n  \"Can you provide a detailed explanation of how to unload and start the PostgreSQL service on my Mac?\"\\n]',\n",
       " '78833f32': 'Here are the 5 questions:\\n\\n[\"What is the solution to the PGCLI PermissionError: [Errno 13] Permission denied: \\'/some/path/.config/pgcli\\'?\",\\n\"How do I avoid installing pgcli with sudo?\",\\n\"Why does pgcli require special installation when other tools don\\'t?\",\\n\"What is the conda install solution for packages that get stuck at \\'Solving environment\\'?\",\\n\"How can I avoid system python being affected during installation?\"',\n",
       " '63823f21': 'Here are the 5 questions this student might ask:\\n\\n[\\n\"What is the specific Python version that we need to use in this course?\",\\n\"Why are we unable to import psycopg \\'binary\\' implementation and is there a workaround?\",\\n\"How do we check the current Python version and what is the command to do so?\",\\n\"What are the possible solutions for the \\'no pq wrapper available\\' error in PGCLI and other alternatives if the above steps don\\'t work?\",\\n\"How do WE install the PostgreSQL library using pip install command and what are the specific commands to use?\"',\n",
       " 'b36ea564': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"How can I resolve the PGCLI stuck on password prompt issue?\", \"Why do I keep getting the \\'password authentication failed\\' error for user \\'root\\'?\",\\n\"What are the possible causes of the connection failed error for PGCLI?\",\\n\"How do I stop the PostgreSQL service on Windows if I\\'m stuck?\", \"What is the solution to the \\'connection failed\\' error after closing the connection to the Postgres:13 image?\"]',\n",
       " 'e2a46ce5': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What if I\\'ve already installed pgcli but it\\'s not recognized by bash?\", \"How do I add the Python path to the Windows PATH on Git bash?\", \"I\\'ve tried adding the path to PATH but it still doesn\\'t work, how do I troubleshoot this?\", \"Is there a specific location where Python is installed that might cause issues?\", \"How do I update the PATH system variable on Windows?\"]',\n",
       " '27bdbc3f': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\"How can I run pgcli without installing it locally?\", \"What is the command to run pgcli in a Docker container?\", \"What is the purpose of \\'--network pg-network\\' in the Docker command?\", \"Why are \\'postgres related variables for pgcli\\' needed?\", \"How do I use the pgcli commands to view the table schema in the database?\"]',\n",
       " 'f7c5d8da': 'Here are 5 questions that this student might ask based on the FAQ record:\\n\\n[\"What do I do if my CLI command using PGCLI is not recognized?\", \"How do I avoid issues with column names in my PGCLI queries?\", \"Can you explain the case sensitivity rules for PGCLI?\", \"Are unquoted column names in PGCLI always case-insensitive?\", \"Where in the documentation can I learn more about PGCLI case sensitivity rules?\"]',\n",
       " 'c91ad8f2': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why do I get the error column \\'c.relhasoids does not exist\\'?\",\\n\"What shall I do if I get an error when running a command like \\'\\\\\\\\d <database name>\\'?\",\\n\"What is the resolution for the \\'column c.relhasoids does not exist\\' error?\",\\n\"What database name do I replace with \\'ny_taxi\\' in the resolution steps?\",\\n\"Why do I need to restart my PC after uninstalling and reinstalling pgcli?\"]',\n",
       " '88bf31a0': 'Here are the 5 questions the student might ask:\\n\\n[\"What is causing the psycopg2.OperationalError when trying to connect to Postgres in Jupyter Notebook?\", \\n\"What is the impact of port 5432 being taken by another Postgres instance?\", \\n\"Why am I still getting the error after mapping a different port to my Docker container?\", \\n\"I\\'m running Windows, what services should I check to resolve the Postgres connection issue?\", \\n\"Should I update the engine creation code when uploading data to use the correct port number?\"]',\n",
       " '23524e6d': 'Here are the 5 questions that the student might ask based on the FAQ record:\\n\\n[\\n\"Can you provide more information on why I get a Postgres - OperationalError when connecting to PostgreSQL locally? I have Postgres installed on my computer.\",\\n\"Why do I get a \\'role \"root\" does not exist\\' error while uploading data in Jupyter notebook?\",\\n\"Can you explain why I need to change the port number when connecting to PostgreSQL?\",\\n\"How do I check if there is a root user with the ability to login in my Postgres container?\",\\n\"What are some potential solutions to resolve the \\'role \"root\" does not exist\\' error in case it persists after trying the aforementioned solutions?\"',\n",
       " '9211bbd6': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\\n\"What is the cause of the psycopg2.OperationalError when trying to connect to Postgres?\", \\n\"What should I do if I have Postgres software installed on my computer?\", \\n\"Can I use a different port number when building a Postgres instance?\", \\n\"How do I check if my Postgres instance is running?\", \\n\"Why do I get a fatal error when trying to connect to a non-existent database?\"\\n]',\n",
       " '5db86809': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"Is there a specific error message I should look out for when installing psycopg2-binary, and what does it mean if I get a ModuleNotFoundError: No module named \\'psycopg2\\'?\",\\n\"Can you provide more details on how to install the psycopg2-binary package using pip, especially if I already have it installed?\",\\n\"What are some additional steps I can take if I\\'m still getting the ModuleNotFoundError: No module named \\'psycopg2\\' even after installing the package?\",\\n\"I\\'m using conda instead of pip, will the installation process be different for me, and how do I update conda?\",\\n\"Is there a way I can also install the necessary components for Postgres, such as the PostgreSQL client library, at the same time as installing psycopg2-binary?\"',\n",
       " '20c604dd': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"What happens when we mention a column name directly or enclosed in single quotes in join queries with PostgreSQL?\", \"Can you provide an example of when the \\'column does not exist\\' error occurs in Docker?\", \"Why do I get an error when trying to query a Postgres database with PyScopg2 on my MacBook Pro M2?\", \"How do I resolve the \\'column does not exist\\' error in my SQL query with PyScopg2?\", \"What is the difference between single and double quotes when using column names in PostgreSQL queries?\"]',\n",
       " 'b11b8c15': '[\\n\"Can you explain why the Create server dialog does not appear when I try to create a new server in pgAdmin?\",\\n\"What is the alternative method to create a server in pgAdmin if the dialog does not appear?\",\\n\"Why did the version of pgAdmin change and what are the steps to adapt to this change?\",\\n\"What is meant by \\'register\\' in the context of pgAdmin and how does it help with creating a server?\",\\n\"How can I upgrade my pgAdmin to the new version and what benefits will I get from the upgrade?\"',\n",
       " 'a6475348': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What can cause a blank screen after logging into pgAdmin in the browser?\",\"Why is the terminal of the pgAdmin container showing a CSRFError message?\",\"How do I modify the docker run command to resolve the CSRFError issue?\",\"What is an alternative to GitHub Codespaces in the browser for using pgAdmin?\",\"How does using a locally installed VSCode affect the pgAdmin login experience?\"]',\n",
       " '1ea7680e': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"How do I modify the \\'docker run\\' command to access the pgAdmin address via my browser?\",\\n\"Why am I getting a \\'ModuleNotFoundError: No module named \\'pysqlite2\\'\\' and how do I fix it\",\\n\"Can I modify the docker-compose.yaml configuration to access the pgAdmin address via my browser?\",\\n\"How do I link the postgres-container to the pgAdmin-container using the \\'docker run\\' command?\",\\n\"What is the correct path for storing the sqlite3.dll file in Anaconda?\"',\n",
       " '10acd478': '[\\n    \"Why does the Jupyter notebook script only ingest 1.3 million rows of data instead of 1.4 million rows if I follow the video?\",\\n    \"Why do I get missing records when I try to run the script for a second time from top to bottom?\",\\n    \"What is the purpose of the \\'df=next(df_iter)\\' cell in the notebook?\",\\n    \"Why can\\'t I run the script top to bottom without missing records?\",\\n    \"What is the difference between running the script in Jupyter notebook and running it in a .py file for the pipeline?\"',\n",
       " '752e8452': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I import a gzipped CSV file in Python?\", \"Why do I get a DeprecationWarning when reading a CSV file?\", \"How do I unzip a gzip file in Ubuntu?\", \"How do I load a CSV file in Python without errors?\", \"How can I quickly preview a CSV file in VSCode if it\\'s not already unzipped?\"]',\n",
       " 'aa6f52b8': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What does \\'parse_dates\\' parameter do in pandas\\' \\'read_csv\\' function?\", \\n\"How can we specify multiple columns for parsing dates in pandas?\", \\n\"What are the common data types that pandas\\' \\'read_csv\\' function can parse?\", \\n\"How can we limit the number of rows to be read from a csv file using pandas?\", \\n\"What is the default type of a column when pandas\\' \\'read_csv\\' function encounters a string value that can be interpreted as a date?\"]',\n",
       " '3dacbb98': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\"How do I use Python to download data from a GitHub link provided in the course?\", \"Can I use Docker to run Python code for data ingestion?\", \"Is there a way to automate the data ingestion process using a framework like Terraform?\", \"What if the data from the GitHub link is in JSON format instead of CSV?\", \"Can I modify the curl command in the answer to fetch data from a different URL?\"]',\n",
       " '8b71a398': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"What is a Gzip compressed CSV file and how is it different from a regular CSV file?\",\\n\"How can I read a Gzip compressed CSV file using Pandas?\",\\n\"Is there a specific function in Pandas that I should use to read a Gzip compressed CSV file?\",\\n\"What are the parameters I need to specify when reading a Gzip compressed CSV file using read_csv() function?\",\\n\"Can you provide an example of how to read a Gzip compressed CSV file using Pandas?\"',\n",
       " 'aa244fa0': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How can I iterate through a Parquet file and ingest the data, and how do I set a chunksize for this process?\", \"What library can I use to resolve the issue of not being able to iterate through and set a chunksize for Parquet files?\", \"Can you provide an example of how to use PyArrow to read a Parquet file and set a chunksize?\", \"In the example, why is the batch size set to 65536, and how does this affect the ingestion process?\", \"Can I use the same code to ingest Parquet files from a local directory instead of a URL, or do I need to modify the code for this?\"]',\n",
       " 'eac816d7': 'Here are 5 questions based on the FAQ record:\\n\\n[\\n    \"Why do I get an ImportError: cannot import name \\'TypeAliasType\\' from \\'typing_extensions\\' when using SQLAlchemy in Jupyter Notebook?\",\\n    \"What is causing the ImportError \\'TypeAliasType\\' in my Jupyter Notebook with SQLAlchemy?\",\\n    \"I\\'m getting an error with \\'TypeAliasType\\' when trying to import SQLAlchemy in Jupyter Notebook, what\\'s the solution?\",\\n    \"Why does my Jupyter Notebook keep throwing an ImportError \\'TypeAliasType\\' when I use SQLAlchemy?\",\\n    \"What do I need to do to fix the ImportError when importing SQLAlchemy in Jupyter Notebook with \\'TypeAliasType\\'?\"\\n]',\n",
       " 'd44d1c77': '[\\n\"What is the correct syntax for using sqlalchemy with postgresql in python? I get the error \\'module\\' object is not callable when trying to create an engine.\",\\n\"Why do I get a TypeError when trying to create an engine with sqlalchemy, even though I\\'ve correctly installed and imported the module?\",\\n\"Can you provide an example of a connection string for sqlalchemy to use when connecting to a postgresql database?\",\\n\"I\\'m trying to create an engine with sqlalchemy but I get the error \\'TypeError: \\'module\\' object is not callable\\'. How can I troubleshoot this issue?\",\\n\"What is the significance of the \\'+\\' character in the connection string for sqlalchemy when connecting to a postgresql database?\"',\n",
       " 'ed34766a': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"Why do I get a ModuleNotFoundError: No module named \\'psycopg2\\' when trying to execute a Python cell in Jupyter notebook?\",\"How do I install Python modules like \\'psycopg2\\' using pip?\", \"How do I install Python modules like \\'psycopg2\\' using Conda?\", \"Can \\'psycopg2\\' be installed using any method besides pip or Conda?\", \"What causes a ModuleNotFoundError: No module named \\'psycopg2\\' when I\\'m trying to connect to PostgreSQL using SQLAlchemy?\"',\n",
       " 'fd714677': 'Here are 5 questions that a student may ask based on the FAQ record:\\n\\n[\\n\"How to automatically update the system PATH on Windows when installing the Google Cloud SDK?\",\\n\"What is the Windows error message when trying to add the Google Cloud SDK PATH?\",\\n\"How do I add Gitbash to my Windows path?\",\\n\"What is the Anaconda Navigator and how does it relate to adding Google Cloud SDK PATH?\",\\n\"Can I use Gitbash as my default terminal and if so, how do I do it?\"',\n",
       " '9de2c3e9': 'Here are the 5 questions the student might ask:\\n\\n[\"I\\'m getting the error message \\'Requested entity alreadytpep_pickup_datetime exists\\' when trying to create a project in GCP. Can you explain what this error means?\", \"Why can\\'t I use a project ID like \\'testproject\\' because it\\'s already taken? Is it a common problem?\", \"How do I ensure I\\'m not using a project ID that already exists when creating a new project in GCP?\", \"What else should I check if I\\'m getting the same error message when trying to create a project in GCP?\", \"Why did the API request to create a project return a status of 409?\"]',\n",
       " '827dd4af': '[\"What is the most likely reason for the error \"Error 403: The project to be billed is associated with an absent billing account.\" when using GCP?\", \"What needs to be entered in the billing section to resolve this error?\", \"Where can I find a unique identifier for my GCP project ID?\", \"Why might I still see this error if I have a billing account?\", \"How do I resolve the error if my billing account is not linked to my current project?\"]',\n",
       " 'a42a7e8c': '{\"questions\": [\"I\\'m having trouble creating a GCP free trial account and keep getting the OR-CBAT-15 error. Can you suggest an alternative credit/debit card I can use?\", \"Why is it that some credit/debit cards are not accepted by GCP for free trial accounts?\", \"What are the chances of getting help from GCP support if I\\'m having issues with my payment method?\", \"Is there another prepaid card like Pyypl that I can use to create a GCP free trial account?\", \"I\\'m using a Kazakhstani banking system, Kaspi. Why didn\\'t it work for me and how can I resolve the issue?\"]}',\n",
       " '4eefdd01': 'Here are 5 questions based on the FAQ record:\\n\\n[\"Where can I find the configuration file for my GCP project?\",\\n\"How do I access the Service Accounts Keys tab in GCP?\",\\n\"Can I find the \\'ny-rides.json\\' file in a specific directory?\",\\n\"Is \\'ny-rides.json\\' a public or private file in GCP?\",\\n\"Can I use \\'ny-rides.json\\' as a key type in GCP?\"]',\n",
       " '0282578d': '[\\n    \"In the lecture, do I have to delete my instance in Google Cloud, just like Alexey did?\",\\n    \"Do I need to terminate or delete my instance in Google Cloud Platform for the week 1 readings?\",\\n    \"Do I have to delete my instance in Google Cloud or will it cause problems?\",\\n    \"For the week 1 readings, do I need to delete the instance Alexey deleted in the lecture?\",\\n    \"What if I accidentally delete my instance in Google Cloud Platform for the week 1 readings?\"',\n",
       " 'bd3e60fd': '[\\n\"How can I get real-time information about system resource usage?\",\\n\"Which command should I use to display information about system memory usage?\",\\n\"What is the command to show the disk space usage of all file systems?\",\\n\"Is there a command to show all running processes, including detailed information?\",\\n\"What command can I use to list all installed packages on my system?\"\\n]',\n",
       " 'c4e9bc60': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"What to do if I encounter an image error?\", \"Why does the error message say billing has not been enabled when I\\'m sure it is?\", \"How to resolve the issue when I\\'ve already enabled billing?\", \"Can I disable and re-enable billing to resolve the error?\", \"What is the default table expiration time in Terraform?\"]',\n",
       " 'f10b49be': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I troubleshoot the GCP Windows Google Cloud SDK installation issue?\",\\n\"Why do I get an error about quotas when trying to use the Google Cloud SDK?\",\\n\"Can you help me resolve the error \\'Cannot find a quota project to add to ADC\\'?\",\\n\"Is there a way to start a Virtual Machine that\\'s not starting because GCP has no resources?\",\\n\"How can I change the location of an existing Virtual Machine instance?\"]',\n",
       " '3184bd8b': '{\\n\"questions\": [\\n\"Why is a GCP VM necessary and when would it be more useful to use my own environment?\",\\n\"Why do many students struggle with configuring their environment?\",\\n\"In what cases would using a personal environment be more beneficial?\",\\n\"Why can\\'t I directly commit changes in a VM even if I\\'m the owner of the repo?\",\\n\"Can I still commit changes if I clone the repo in my own environment through HTTPS?\"\\n]}',\n",
       " '8bea4d53': 'Here is the list of questions the student might ask based on the FAQ record:\\n\\n[\"Why do I get a \\'Permission denied\\' error when creating a directory?\",\\n\"How do I create a directory in a specific location in my terminal?\",\\n\"What is the difference between my home directory and the root folder?\",\\n\"Why do I need to create a directory in my home directory rather than the root folder?\",\\n\"In Video 1.4.1, what is the method for creating the correct directory?\"]',\n",
       " '86d11cc0': '[\\n  \"Can the issue with saving a file in a GCP VM via VS Code be related to insufficient permissions?\",\\n  \"How do I need to change the owner of files I\\'m trying to edit via VS Code in a GCP VM?\",\\n  \"Why do I receive a \\'permission denied\\' error when trying to save a file in a GCP VM via VS Code?\",\\n  \"Is there a specific command I need to run to address the permission issue when saving a file in a GCP VM via VS Code?\",\\n  \"Can you provide more information about what the error message \\'<NoPermissions (FileSystemError): Error: EACCES: permission denied, open \\'/home/<user>/data_engineering_course/week_2/airflow/dags/<file>\\'>\\' means and how to resolve it?\"\\n]',\n",
       " '2cb48591': 'Here is the list of questions in JSON format:\\n\\n[\\n  \"Why did my VM connection request suddenly start timing out after it worked last week?\",\\n  \"What should I do when I cannot connect to my GCP VM via SSH?\",\\n  \"How do I troubleshoot issues with my VM connection request to the GCP VM?\",\\n  \"Can you help me fix the connection request timeout issue when trying to connect to my GCP VM?\",\\n  \"I\\'m unable to connect to my GCP VM, how do I resolve the connection request timeout error?\"\\n]',\n",
       " '9523c813': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What do I need to do if I\\'m getting a \\'no route to host\\' error when trying to connect to port 22 on my GCP VM?\",\\n\"How do I fix the issue of not being able to connect to my GCP VM\\'s port 22?\",\\n\"What\\'s causing the \\'no route to host\\' error when trying to SSH into my GCP VM?\",\\n\"In the GCP console, where can I find the option to add a startup script to my VM?\",\\n\"What specific command do I need to add to my VM\\'s startup script to allow SSH connections?\"',\n",
       " '4f8d9174': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How do I start the Jupyter Notebook in the correct folder on the GCP VM?\",\\n\"What is the purpose of the \\'pgAdmin\\' in the answer and how is it related to port forwarding?\",\\n\"What is the external IP of the VM and how do I obtain it?\",\\n\"Why do I need to specify the access token when trying to access Jupyter Notebook?\",\\n\"How do I forward both pgAdmin and PostgreSQL using a single ssh command?\"',\n",
       " '29f84a82': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"What happens when I run gcloud auth application-default login in MS VS Code using WSL2?\",\\n\"Why does the gcloud auth process hang when I run gcloud auth application-default login?\",\\n\"What\\'s the alternative to clicking on the long link when prompted to open it in a browser?\",\\n\"Why do I see an error message when trying to login to gcp via the gcloud cli?\",\\n\"How can I avoid getting stuck with the message and see the login page without issues?\"]',\n",
       " '20a01fd0': '[\\n  \"What could be the common reason for getting the error \\'Terraform - Error: Failed to query available provider packages\\'? Is it due to a problem with the provider registry or internet connectivity?\",\\n  \"Why is being unable to query the provider registry causing an error when trying to retrieve the list of available versions for a provider?\",\\n  \"Is it possible that the issue with querying the provider registry is related to VPN/Firewall settings, and if so, how can I resolve it?\",\\n  \"If I\\'m experiencing issues with querying the provider registry, will clearing cookies or restarting my network resolve the problem?\",\\n  \"What should I do if I\\'ve checked my VPN/Firewall settings and network connectivity, but I\\'m still getting the error, and how can I resolve it?\"',\n",
       " '5a712a20': '[\\n    \"What could be the reason for the error \\'oauth2: cannot fetch token: Post \"https://oauth2.googleapis.com/token\": dial tcp 172.217.163.42:443: i/o timeout\\' in Terraform?\",\\n    \"Why is Google not accessible from my country?\",\\n    \"Why doesn\\'t the terminal program automatically follow the system proxy?\",\\n    \"What could be the solution if I\\'m using a VPN and encountering an issue with Google?\",\\n    \"How can I troubleshoot and resolve issues related to VPN providers in Terraform?\"\\n]',\n",
       " '06021091': '[\"How do I install Terraform on Windows 10 Linux Subsystem following the Azure Developer Community Blog instructions provided in the FAQ link?\", \"What are the steps to set up Terraform on WSL as described in the Microsoft Azure Developer Community Blog article?\", \"Can I use the Microsoft Azure Developer Community Blog instructions to install Terraform on WSL as I have Windows 10 Linux Subsystem installed?\", \"Does the FAQ link guide explain how to configure Terraform on Windows 10 Linux Subsystem?\", \"What does the Microsoft Azure Developer Community Blog say about using Terraform on WSL in their instructions for setting up WSL?\"]',\n",
       " 'df8ea7e8': '[\\n\"Can you explain why I\\'m getting an error acquiring the state lock with Terraform?\",\\n\"What is causing this error when I\\'m trying to use Terraform?\",\\n\"Can you show me a solution for this common Terraform error?\",\\n\"I\\'m stuck on this Terraform error, how do I fix it?\",\\n\"Why does Terraform sometimes have trouble acquiring the state lock?\"',\n",
       " '1093daf5': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What causes the error \\'invalid_grant\\' when running \\'terraform apply\\' on WSL?\", \"Is this error related to timezone settings on my machine?\", \"What is the difference between \\'short-lived token\\' and a regular token in JWT?\", \"How does running the command \\'sudo hwclock -s\\' correct the error?\", \"Are there any other situations where system time desync can cause issues besides JWT tokens?\"]',\n",
       " '947213b1': 'Here are 5 questions the student may ask based on the FAQ record:\\n\\n[\\n  \"Why do I keep getting an error 403 when trying to run Terraform?\",\\n  \"What could be causing the \\'Access denied\\' error when using Google Cloud with Terraform?\",\\n  \"How can I fix the issue with Terraform unable to access Google Cloud due to a 403 error?\",\\n  \"What should I do if my $GOOGLE_APPLICATION_CREDENTIALS are not pointing to the correct file?\",\\n  \"Can you provide the correct command to export the GOOGLE_APPLICATION_CREDENTIALS variable?\"',\n",
       " '002d4943': '[\"What if I already have a service account, do I still need to make a new one specifically for Terraform?\", \"Do I need to set up multiple service accounts for all the resources in the course?\", \"Can I use the same service account for multiple services in the course?\", \"Is it better to create separate service accounts for each service or resource in the course?\", \"Will I need to create a service account for each file I generate in the course?\"]',\n",
       " '8dc77677': '[\"Where can I find the Terraform 1.1.3 Linux AMD 64 executables?\", \"How do I download Terraform 1.1.3 for Linux AMD 64?\", \"Can I use a specific version of Terraform for my project?\", \"Where can I find older versions of Terraform?\", \"Do you have any resource on how to use Terraform with Docker?\"]',\n",
       " '29d3d343': 'Here are the 5 questions:\\n\\n[\\n\"What does it mean when I get the message \\'Terraform initialized in an empty directory\\' even though there are no Terraform configuration files?\",\\n\"Why do I get an error when I run \\'terraform init\\' outside the working directory?\",\\n\"What command should I use to initialize Terraform correctly?\",\\n\"Why does navigating to the working directory fix the Terraform initialization issue?\",\\n\"What configuration files does Terraform require to be present in the directory in order to work correctly?\"',\n",
       " 'e2095203': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"How can I fix the \\'Access denied\\' error I\\'m getting when trying to create a dataset with Terraform?\", \"Why am I getting an \\'Error 403: Request had insufficient authentication scopes\\' error when running Terraform?\", \"What do I need to do to resolve the \\'Error creating Dataset: googleapi: Error 403\\' error\", \"How can I authenticate my Google service account credentials so I can use them with Terraform?\", \"What did Alexey do in the environment set-up video in week1 that I need to do to fix my Google application credentials?\"]',\n",
       " '22a2b9f2': '[\"What does the Project ID exactly refer to in the context of a Google Cloud project?\", \"How do I find the Project ID in the GCP console?\", \"Why does the error message suggest that the permission \\'storage.buckets.create\\' may not exist?\", \"What should I do if I\\'m still experiencing permission issues after declaring the correct Project ID?\", \"Can the Project name and Project ID be the same in a Google Cloud project?\"]',\n",
       " '5d7588f0': 'Here are 5 questions based on the FAQ record:\\n\\n[\"How can I ensure the sensitivity of the credentials file in my Docker and Terraform module?\",\"Why did I have to input the credentials as a file and not in plain text?\",\"What is the format to specify credentials in the provider block for Google services?\",\"Can I use a variable to store my credentials file path?\",\"How can I use the region and zone settings in conjunction with the project ID and credentials file?\"',\n",
       " '5276a695': '[\\n  \"Why do I need to enclose column names with double quotes when they start with uppercase letters?\",\\n  \"What\\'s the best way to handle uppercase columns in SQL queries?\",\\n  \"I got an error saying column \\'Zone\\' doesn\\'t exist, what could be the possible reason?\",\\n  \"How do I include single quotes in a SQL query if I\\'m using them to represent a value?\",\\n  \"What\\'s the correct zone name to use in the SQL query, is it \\'Astoria Zone\\' or just \\'Astoria\\'?\"',\n",
       " '70c159df': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How can I solve the SQL error Column Zone doesn\\'t exist even after selecting the correct column?\",\\n\"Why do I need to put all data in lowercase when storing it in the database?\",\\n\"How can I achieve the same result without rewriting the database schema?\",\\n\"What is the difference between using quotation marks and using lowercase for data in the database?\",\\n\"Why is it inconvenient to use quotation marks all the time despite being more readable?\"',\n",
       " 'f55efcf0': '[\\n\"How do I resolve the error \\'curl: (6) Could not resolve host\\' when using CURL in Mac OS?\",\\n\"What is the solution to get the output of CURL in Mac OS?\",\\n\"Can you elaborate on the correct syntax for using os.system in mac?\",\\n\"Is there a way to modify the output file name dynamically when using CURL in Mac OS?\",\\n\"Can you provide more information on how to handle errors when using CURL in Docker?\"',\n",
       " '2b7a8512': '{\"questions\": [\\n\"What are the necessary steps to resolve the SSH connection error \\'ssh: Could not resolve hostname linux: Name or service not known\\' when using Docker and Terraform?\",\\n\"What is the location of the config file that needs to be set up to resolve the SSH error?\",\\n\"Can you please provide more details on how to configure the config file in Ubuntu to resolve the SSH error?\",\\n\"What should I do if I have already created a config file and I am still getting the SSH error?\",\\n\"What is the role of the config file in resolving the SSH connection issue when using Docker and Terraform?\"}]',\n",
       " '1cd746c4': '[\\n\"Under what operating systems does the PATH modification differ for adding Anaconda’s Python to add pip?\",\\n\"Why do I need to restart my terminal after adding Anaconda to the PATH?\",\\n\"What is the default path of Anaconda installation on Windows?\",\\n\"Can I find Anaconda’s installation path without using the terminal/command prompt?\",\\n\"How do I add pip to the Path in Windows without using Git Bash?\"',\n",
       " '6d367222': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n    \"What could be the cause of the error \\'error starting userland proxy: listen tcp4 0.0.0.0:8080: bind: address already in use\\' and how can I resolve it?\",\\n    \"Why am I getting a \\'permission denied\\' error when trying to stop a container and how can I fix it?\",\\n    \"I\\'m getting an error when trying to import the psycopg2 module. What packages do I need to install and how can I install them?\",\\n    \"I\\'m trying to build a Docker image, but I\\'m getting an error message saying \\'can\\'t stat \\'<path-to-file>\\'. How can I resolve this issue?\",\\n    \"What could be causing the problem when trying to run \\'docker build ...\\' and how can I avoid it?\"\\n]',\n",
       " '84e601e1': 'Here are the 5 questions this student might ask:\\n\\n[\\n\"How do I get a pip-friendly requirements.txt file from Anaconda?\",\\n\"What is the command to install pip from Anaconda?\",\\n\"Is \\'conda list -d > requirements.txt\\' a correct way to get a pip-friendly requirements.txt file?\",\\n\"Will \\'pip freeze > requirements.txt\\' always provide a correct output?\",\\n\"Is there a difference between \\'conda install pip\\' and \\'pip install\\'?\"',\n",
       " '4cf83cc2': '[\"Where can I find the FAQ questions from previous cohorts for the orchestration module?\", \"Where can I find the answers to the orchestration module questions for other workflow tools?\", \"Can I see examples of FAQ questions specific to workflow orchestration?\"; \"Are there any FAQ questions related to orchestrating workflows with Prefect?\", \"Can I find the answers to questions about Airflow workflow orchestration in this FAQ record?\"]',\n",
       " '5adc5188': '{\\n\"questions\": [\\n\"How can Docker containers exit instantly with code 132 when I run \\'docker compose up\\'?\",\\n\"What could be the cause of Docker containers exiting instantly with code 132, as mentioned in the Mage documentation?\",\\n\"Is there a possible solution to fix Docker containers without purchasing a new computer?\",\\n\"I am running Ubuntu 22.04.3 LTS and Docker version 25.0.2 on a VirtualBox VM, what could be the potential hardware issue?\",\\n\"What are the details on how VirtualBox VM was set up (CPU, RAM, network, etc.) and could this affect Docker containers?\"\\n]',\n",
       " '3ef0bb96': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"Why am I experiencing unexpected kernel restarts and running out of memory with WSL, and how can I fix it? \\n\"What is happening when WSL 2 doesn\\'t dedicate enough cpu cores to Docker? \\n\"What is the .wslconfig file used for and how do I find it? \\n\"How do I edit the .wslconfig file to dedicate more CPU cores and memory to Docker? \\n\"Why do I need to restart WSL and Docker Desktop after editing the .wslconfig file?\"\\n]',\n",
       " 'a41ce360': '[\"What are the common issues that appear when configuring Postgres and how can they be solved?\", \"Is there a specific link that discusses the configuration of Postgres? Can I access it?\", \"In the module 2 discussion, are there any specific steps for configuring Postgres that I should follow?\", \"Can you provide more details on the troubleshooting steps mentioned in the link provided?\", \"How does the link provided by the module 2 discussion, which discusses Postgres configuration, relate to our current coursework?\"]',\n",
       " 'b1cf59e5': '[\"What if I already have a conflicting Postgres installation on my host machine and it\\'s using port 5431, but the POSTGRES_PORT variable in io_config.yml is set to 5431?\", \\n\"What is the default port for Postgres?\", \\n\"If there\\'s a conflicting installation on my host machine, will using a different port for the mage container solve the issue?\", \\n\"Why do I need to set the POSTGRES_PORT variable in the io_config.yml file?\", \\n\"What happens if I don\\'t set the POSTGRES_PORT variable and the mage container tries to connect to port 5431?\"]',\n",
       " 'f9d6f8bd': '[\"Why do I get a KeyError when executing SELECT 1; in MAGE?\", \"How do I fix the issue of forgetting the dev profile in PostgreSQL?\", \"What is the solution to the MAGE KeyError problem?\", \"Can you explain why I need to select the dev profile?\", \"What profile should I choose in the PostgreSQL connection dropdown?\"]',\n",
       " 'f3adb937': 'Here are 5 questions based on the FAQ record:\\n\\n[\"Why do I get a timeout error (\\'Connection aborted.\\', TimeoutError(\\'The write operation timed out\\')) when connecting to MAGE 2.2.4?\"]\\n[\"How can I resolve the \\'NotFound: 404 Not found\\' error when testing BigQuery connection using SQL in MAGE 2.2.4?\"]\\n[\"What should I do if I\\'m getting a \\'NotFound: 404 Not found\\' error even after giving all roles and permissions to the service account in MAGE 2.2.4?\"]\\n[\"What specific value should I set for the timeout value in the mage io_config.yaml file to resolve the ConnectionError?\"]\\n[\"Why do I need to untick the \\'Use raw SQL\\' box to resolve the \\'NotFound: 404 Not found\\' error when testing BigQuery connection in MAGE 2.2.4?\"',\n",
       " 'eb3d6d36': '[\"What is causing the \\'invalid_grant\\' error in my workflow orchestration flow?\", \"How do I verify the iat and exp values in the JWT claim are correct?\", \"Why is the error message saying that the token must be short-lived?\", \"What is the reasonable timeframe for a short-lived token?\", \"Can you explain more about how to check the JWT claim in the error message?\"]',\n",
       " 'a76e1f4d': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\"What is the most likely cause of the Mage 2.2.4 IndexError: list index out of range error?\", \"Is this issue specific to a particular version of Mage?\", \"How do I update my Docker image to resolve this issue?\", \"Will updating my Docker image to 0.9.62 fix the error mentioned in the FAQ?\", \"Can I use a different tag than \\'alpha\\' when updating my Docker image?\"]',\n",
       " '934facf8': '[\\n\"What does the \\'..\\\\\\\\..\\' represent in the code and why are we using it?\",\\n\"Why do we need to create the directory before saving the file?\",\\n\"I don\\'t understand why we\\'re using \\'as_posix()\\' after \\'path\\'. Can you explain?\",\\n\"When would the \\'/path.parent.is_dir()\\' condition be False, and what would happen then?\",\\n\"What\\'s the purpose of creating a directory recursively with \\'parents=True\\'?\"',\n",
       " 'a2c7b59f': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\"What is missing in the video DE Zoomcamp 2.2.7, and how can we deploy Mage to GCP?\", \"How do we set the default project ID in variables.tf?\", \"How do we enable the Cloud Filestore API in the Google Cloud Console?\", \"What are the steps to perform the deployment using Terraform?\", \"What happens during the terraform apply step, and how do we confirm the deployment?\"]',\n",
       " '997d4aaa': '[\"How can I run multiple Mage instances in Docker from different directories?\", \"What are the correct port mappings to use in the docker-compose.yml file?\", \"Why did I encounter an error while deploying Mage to Google Cloud using Terraform?\", \"What is the error message that I get while deploying Mage to Google Cloud using Terraform?\", \"How can I fix the \\'insufficient authentication scopes\\' error when using a VM inside GCS?\"]',\n",
       " 'bc269b95': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"What happens when trying to deploy infrastructures with Terraform on a free trial account on GCP?\", \"Why is the Load Balancer service not available on free trial accounts?\", \"How can I work around the Load Balancer issue on a free trial account on GCP?\", \"What code lines in the main.tf file need to be commented or deleted to fix the issue?\", \"What is the best way to delete any infrastructure created on failed attempts when dealing with Load Balancer issues?\"]',\n",
       " '10ea342e': '[\"What should I do if I\\'m getting an error when I run terraform apply?\", \"Why is it taking 20 minutes to deploy the MAGE Terraform files on GCP?\", \"Why are some resources not being deleted by terraform destroy?\", \"What should I do if I see remaining GCP resources after running terraform destroy?\", \"How can I check my GCP billing account for any unexpected charges?\"]',\n",
       " '4bd23594': 'Here are the 5 questions:\\n\\n[\\n\"What does the error \\'Permission \\'vpcaccess.connectors.create\\' denied on resource \\'//vpcaccess.googleapis.com/projects/<ommit>/locations/us-west1\\'\" mean?\",\\n\"What resource is blocking the creation of a connector?\",\\n\"What is the IAM permission that is being denied?\",\\n\"Why am I getting an error when trying to create a connector?\",\\n\"How can I resolve the IAM permission issue to create a connector successfully?\"',\n",
       " 'b0d48cd7': 'Here are the 5 questions that the student might ask based on the FAQ record:\\n\\n[\\n\"What happens if I try to save a file to a non-existent directory, and why is this causing an error?\",\\n\"Why can\\'t I push an empty folder to GitHub, and how can I fix this?\",\\n\"How do I create a new folder in Python using Pathlib as mentioned in the FAQ?\",\\n\"What\\'s the difference between a relative path and a full path when writing locally and uploading to a GCS bucket?\",\\n\"Can you give me an example of how to use two separate paths for local write and GCS bucket upload, as suggested in the FAQ?\"',\n",
       " '70a37f2c': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What columns do the green and yellow datasets contain? Why are there different columns?\", \"How do I modify the script for the green dataset?\", \"What if I need to use both columns, how do I handle this?\", \"Is it possible to use a single script for both datasets?\", \"Can I use a workaround or is the script modified necessary for this scenario?\"]',\n",
       " '8ab78bee': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I get the data downloaded using Pandas?\",\\n\"What does the iterator parameter in pd.read_csv do?\",\\n\"How can I use the fastparquet engine to append data to a Parquet file?\",\\n\"Can I increase the chunksize to download more data at once?\",\\n\"What compression method should I use when writing to the Parquet file?\"]',\n",
       " '54c6db2f': '[\"What are the possible reasons for a push to a docker image failure, and what is the error message when the request is denied?\", \"How do I ensure I\\'m logged in properly to Docker Desktop to avoid push to docker image failure?\", \"What can I do if I\\'ve used the wrong username when pushing to docker images, and how can I avoid this mistake?\", \"Can you provide an example of how to build a docker image with the correct username and tag, following best practices?\", \"In the event of a push to docker image failure, what command can I use to check if my Docker image name or tag is correct?\"]',\n",
       " 'c5b998f3': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why does my flow script fail with a \\'killed\\' message?\", \"What could be the reason for a \\'killed\\' error in my workflow orchestration?\", \"How can I resolve the \\'killed\\' issue I\\'m experiencing with my flow script?\", \"Is it possible to run out of memory on my VM while executing a flow script?\", \"How much RAM do I need to allocate to my VM for proper flow script execution?\"]',\n",
       " 'eec29536': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What happens when the disk space of a GCP VM is full after using Prefect?\",\\n\"Can I use the command-line to identify which directory is taking up the most space?\",\\n\"How do I delete older flows from the Prefect storage directory?\",\\n\"Why do I get an SSL certificate verify error when running flows on a Mac and how can I fix it?\",\\n\"Is there a way to manually install a certificate for python on a Mac?\"',\n",
       " '727e5a69': '[\\n\"What happens when my Docker container crashes with a status code of 137, and how can I fix the issue when it happens while completing homework, specifically Question #3?\",\\n\"Why is it so crucial to allocate more RAM to Docker when dealing with large datasets, and are there any alternative ways to resolve the issue if my workstation\\'s resources are limited?\",\\n\"What\\'s the significance of the recommendation to restart the computer and start only necessary processes to run the container, and how does this relate to the container crashing due to memory consumption?\",\\n\"Are there any specific troubleshooting steps I should take when experiencing container crashes with status code 137, especially when working with large datasets like the ones in Question #3?\",\\n\"In what cases might using an online compute environment service like GitPod be a viable solution to address Docker container issues, and are there any usage limits or restrictions I should be aware of?\"',\n",
       " 'da899638': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\"What are the possible causes of timeout due to slow upload internet in this workflow orchestration?\", \"Why does the ETL script take so long to upload to GCS?\", \"How can I prevent network errors when uploading data to GCS?\", \"Is there a way to resolve the WSL2 crash/hang issue when running the ETL script?\", \"Can I adjust the timeout value in the GCP Cloud Storage bucket upload method?\"]',\n",
       " 'dde58c8f': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What happens when I try to re-run the export block of transformed green_taxi data to PostgreSQL and I get an error saying column \", \"How do I resolve the error when the export block fails to run and gives me an undefined column error?\", \"Why am I getting an error saying that a column doesn\\'t exist when I\\'m trying to export data to PostgreSQL?\", \"What do I need to do to fix the error and re-run the block successfully?\", \"Can I export the transformed green_taxi data to PostgreSQL without getting this error?\"]',\n",
       " '207be93b': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\\n\"What does SettingWithCopyWarning exactly mean when it pops up in my code?\", \\n\"What\\'s the difference between data.loc[] and df[] when assigning values?\", \\n\"Can you provide an example of when SettingWithCopyWarning would occur and how to avoid it?\", \\n\"Why do I need to use \\'data.loc[]\\' instead of \\'df[]\\' when modifying a DataFrame?\", \\n\"How do I know if I\\'m trying to set a value on a copy of a slice from a DataFrame?\"',\n",
       " 'f0617e65': 'Here is the output in parsable JSON format:\\n\\n[\"What are the steps to switch to Pyspark Kernel in Mage if my laptop is slow and I\\'m dealing with big csv files?\", \"How do I know if Pyspark Kernel is suitable for handling big csv files in our NYC data?\", \"Can I use Pyspark Kernel in Mage for all big csv files, or is it only recommended for specific cases?\", \"Are there any potential issues or limitations when using Pyspark Kernel in Mage compared to Python kernel?\", \"What is the documentation for using Pyspark Kernel in Mage and where can I access it?\"]',\n",
       " '6290a1a6': '[\\n\"What happens when I encounter an error while deleting a block in a pipeline? I want to know the correct sequence of actions to follow.\",\\n\"Why do I need to delete connections between blocks before I can remove a block from a pipeline?\",\\n\"What specific issues arise when I try to delete a block in a pipeline, and how do I resolve them?\",\\n\"I\\'m having trouble removing a block from a pipeline due to an error. Can you provide a step-by-step guide to resolving this issue?\",\\n\"What common mistakes should I avoid when trying to delete a block in a pipeline, and how can I prevent them?\"',\n",
       " '5a06248c': '[\\n\"Why does the Mage UI throw a permission denied error when trying to edit the Pipeline name?\", \\n\"What can I do when the permission denied error occurs while trying to edit the Pipeline name in Mage UI?\", \\n\"Can I edit the Pipeline name directly in Mage UI if the warning message appears?\", \\n\"What is the workaround when the permission denied error prevents editing the Pipeline name in Mage UI?\", \\n\"Why does revisiting the edit later resolve the permission denied error when trying to edit the Pipeline name?\"',\n",
       " 'c46a2e9e': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"How do I apply multiple filters to partitioned files in Solution n°2?\", \"What is the purpose of pa_table.to_pandas() in Solution n°1?\", \"Can I use Solution n°1 to download specific dates instead of all files?\", \"What is the difference between pa_table and pa_dataset in the two solutions?\", \"How can I troubleshoot the ERROR: UndefinedColumn: column \\'vendor_id\\' of relation \\'green_taxi\\' does not exist and use one of the two solutions to fix it?\"]',\n",
       " '0513ab8a': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What files should I submit for Homework 2?\", \"How do I access my MAGE files locally?\", \"Can I download individual blocks from my pipeline?\", \"What files should I commit to my GitHub repo?\", \"How do I get metadata and other files from my pipeline?\"]',\n",
       " 'a9385356': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How do I include the files from the Mage repo in my personal copy of the Data Engineering Zoomcamp repo?\"\\n\"I need help moving files from the Mage repo to my main DE Zoomcamp repo. How do I do this?\",\\n\"I\\'m having trouble connecting my Mage repo to my DE Zoomcamp repo. What\\'s the best way to do this?\",\\n\"Can I add the files from the Mage repo to my DE Zoomcamp repo directly, or do I need to do something else first?\",\\n\"How do I avoid losing my work by combining the Mage repo with my main DE Zoomcamp repo?\"\\n]',\n",
       " 'c30468c0': 'Here are the 5 questions the student might ask:\\n\\n[\\n\"What are the limitations of using the \\'and\\' operator for filtering data in a DataFrame?\", \\n\"Can you provide more context on what \\'truth value of a Series is ambiguous\\' means?\", \\n\"Why did the \\'ValueError\\' occur in the first place?\", \\n\"How did changing the \\'and\\' operator to \\'&\\' fixed the issue?\", \\n\"What are some common mistakes students make when filtering data in a DataFrame?\"',\n",
       " '305aead7': '[\\n  \"Why do the Mage AI files disappear when I start my PC after working on the project from the previous day?\",\\n  \"What happens when I run docker compose up and the Mage web interface appears, but my files are missing?\",\\n  \"Is it necessary to properly shut down the mage docker compose using Ctrl + C to resolve the issue of missing files?\",\\n  \"Why is it important to verify that I am in the correct repository before running docker compose up?\",\\n  \"How many times did the issue of missing files persist with your own PC and how were you able to resolve it?\"\\n]',\n",
       " '77410975': '[\"What are the common errors that occur in the io.config.yaml file in Mage?\",\"Can incorrect quotes cause errors in io.config.yaml?\",\"Why do I need to modify the trailing side of the io.config.yaml file?\",\"How can I fix errors in the io.config.yaml file?\", \"Are there specific characters that trigger these errors in the io.config.yaml file?\"]',\n",
       " '0952abde': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What causes the Mage - ArrowException Cannot open credentials file error?\", \"Why do I need to have permissions to access the specified GCP credentials file?\", \"How do I update the code to point to the GCP credentials file?\", \"Do I need to create a separate credentials folder for the Mage app?\", \"What does the .json key file contain in the credentials folder?\"]',\n",
       " '7c4326eb': '[\"What could be the reason behind Mage\\'s OSError error as described in the Google Cloud authentication process?\", \"How can I troubleshoot the \\'couldn\\'t resolve host name\\' error mentioned in the underlying error message?\", \"Is there a specific retry policy that I need to implement for such errors to occur?\", \"What are the possible implications of an OAuth2 access token not being created?\", \"How do I resolve the underlying issue of Google Cloud authentication to complete the request successfully?\"]',\n",
       " 'a1fc1a14': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"Can I still attempt to export data from Mage to Google Cloud Storage if the service account doesn\\'t have the necessary permissions?\",\\n\"What error message do I see when the service account doesn\\'t have storage.buckets.get access to the Google Cloud Storage bucket?\",\\n\"Is the \\'Permission denied\\' error permanent or can it be resolved somehow?\",\\n\"Why do I need to add Cloud Storage Admin role to the service account for this task?\",\\n\"What are the exact steps to add Cloud Storage Admin role to the service account for Google Cloud Storage bucket access?\"',\n",
       " '6d67fba9': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What are the necessary steps to trigger a DataProc cluster from Mage?\", \"How can I ensure I have a PySpark script ready for sending to the DataProc cluster?\", \"Do I need to create a Dataproc cluster in GCP Console for this to work?\", \"Why do I need to add the Dataproc Editor role to my service account?\", \"How do I install the gcloud CLI for Mage to access Google Dataproc?\"]',\n",
       " '06876291': 'Here are the 5 questions the student might ask:\\n\\n[\"What are the possible solutions to make Docker-compose stop taking infinitely long to install zip unzip packages for Linux?\", \"How can I prevent Docker-compose from automatically agreeing to install additional packages when installing zip unzip packages for Linux?\", \"Can I use a programming language other than Docker-compose to unpack datasets?\", \"Are there any specific versions of python that I need to use Python ZipFile package?\", \"Why does Docker-compose need to install zip unzip packages for unpacking datasets in the first place?\"]',\n",
       " '690ba010': '[\\n\"Can you explain what happened if I don\\'t use Nullable dataTypes, such as Int64 when applicable, when writing data from the web to GCS, and how does this lead to errors?\",\\n\"Why are the Nullable dataTypes, like Int64, especially important when writing data from the web to GCS?\",\\n\"What are the consequences of not using NullPointerExceptions in this specific scenario?\",\\n\"How do I know when to use Int64 as a Nullable dataType when writing data to GCS?\",\\n\"Is there a general rule of thumb for using Nullable dataTypes, like Int64, when working with data warehousing and GCS?\"',\n",
       " 'b6fdd91d': 'Here are the 5 questions the student might ask based on the provided FAQ record:\\n\\n[\\n\"How do BigQuery tables handle files with different schemas in the same directory?\",\\n\"What happens when trying to ingest data with different data types in the same column from different files?\",\\n\"Can you provide an example of how to enforce data types for columns before uploading them to BigQuery?\",\\n\"Why is BigQuery unable to create a table when it encounters a file with a different schema?\",\\n\"How does the first file in a directory affect the schema of the table when importing Parquet files to BigQuery?\"',\n",
       " '155aa868': 'Here are the 5 questions:\\n\\n[\"What is causing the gzip.BadGzipFile: Not a gzipped file error when importing FHV data to GCS?\", \"How do I specify the correct URL for the FHV dataset when importing data to GCS?\", \"I\\'m getting an error while importing FHV data to GCS, is it related to the URL?\", \"Why do I need to use the \\'/releases/download\\' part of the URL when importing FHV data to GCS?\", \"Can you provide more information on how to fix the error when importing FHV data to GCS?\"]',\n",
       " 'e78cf960': 'Here are the 5 questions the student might ask:\\n\\n[\"What is the process for loading data from a URL list into a GCP Bucket?\", \"Is Krishna Anand the person who can help me with this topic?\", \"Can I load data from a URL list directly into a GCS bucket?\", \"What is required to load data from a URL list into a GCS bucket?\", \"How do I ensure data integrity while loading data from a URL list into a GCS bucket?\"]',\n",
       " '9afa1f74': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I troubleshoot the Bad character (ASCII 0) error when querying my dataset in GCS Bucket?\", \"What could be causing a wrong formatting issue in my dataset?\", \"I\\'m getting the Bad character error after uploading the CSV files using pandas. Can I still use this method?\", \"Is there a specific way to upload the CSV.GZ files to GCS Bucket to avoid the Bad character error?\", \"Where can I find more helpful tips to resolve the Bad character error from a previous Slack conversation?\"]',\n",
       " 'fac138a7': '[\"What does the \\'bq: command not found\\' error message mean in GCP BigQuery?\", \"How can I check if the BigQuery Command Line Tool is installed?\", \"What are the two ways to run the BigQuery command?\", \"Can I use a specific command to check if the BigQuery Command Line Tool is installed?\", \"Why is it necessary to use bq.cmd instead of just \\'bq\\' to make it work?\"]',\n",
       " '0174dde5': '[\\n  \"What should I be cautious about when using BigQuery, especially if I\\'m on a free trial? Are there any specific mistakes that could result in unexpected bills?\",\\n  \"What happened when you exceeded the free trial limits and got a bill of $80? How can I avoid that?\",\\n  \"How can I effectively use my free BigQuery credits? Are there any best practices to keep in mind?\",\\n  \"What should I do once I\\'ve finished creating a BigQuery dataset? Is there any way to permanently delete it?\",\\n  \"Why is it important to regularly check my Billing account, especially if I\\'ve set up a VM?\"',\n",
       " '1023ee65': 'Here are the 5 questions:\\n\\n[\\n\"How can I ensure that my GCS Bucket and BigQuery dataset are in the same region when creating resources on GCP?\",\\n\"What happens if I forget to create resources in the same region when loading data from GCS into BigQuery?\",\\n\"Can I still load data from GCS to BigQuery if my resources are in different regions?\",\\n\"Why do I need to create a new dataset in BigQuery if my GCS Bucket and dataset are in different regions?\",\\n\"Will data transfer be affected if I create a new dataset in the same region as my GCS Bucket?\"',\n",
       " 'effd2bfa': '[\\n\"Can you explain why I\\'m getting an error saying I cannot read and write in different locations when using GCP BigQuery?\",\\n\"What does it mean when it says GCS Bucket is created in a specific region?\",\\n\"In the FAQ, it\\'s mentioned to create the BigQuery dataset in the same region as the GCS Bucket, but what if I created the dataset before creating the bucket?\",\\n\"I\\'m trying to move my dataset to a different region, but the FAQ says it\\'s not allowed. Is there a way to do this?\",\\n\"What are the consequences if I ignore the error and try to write data to a different region?\"\\n]',\n",
       " '5b55273c': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why do I need to save my queries in BigQuery SQL Editor?\", \"How can I avoid losing my SQL script if my Chrome Tab freezes?\", \"What is the purpose of clicking on the button at the top bar to save my script?\", \"Can I save my queries in any plain text editor, or do I need a specific one?\", \"What is the benefit of using the .sql extension to save my queries?\"]',\n",
       " '1835bfe0': 'Here are the 5 questions this student might ask:\\n\\n[\"How does BigQuery handle real-time data streaming?\", \"Can we integrate BigQuery with other tools for real-time analytics?\", \"Do you use BigQuery for cloud-based data warehousing?\", \"Does BigQuery support real-time data loading?\", \"Can I analyze real-time data using BigQuery in this project?\"]',\n",
       " '04656af5': '[\"What can I do to resolve the \\'could not parse \\'pickup_datetime\\' as timestamp for field pickup_datetime\\' error when loading data into a materialized table in BigQuery?\", \"How can I identify and fix invalid data in the timestamp column of my external table?\", \"Why am I getting an invalid timestamp error when appending data to a file in Google Cloud Storage?\", \"Is it possible to filter out invalid rows when importing data into a materialized table?\", \"Can I specify a schema with a string datatype for an external table to resolve this issue?\"]',\n",
       " '2d6536d3': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How can I resolve error messages in BigQuery when importing data from Parquet files?\",\\n\"Are Parquet files created using PyArrow or PySpark compatible?\",\\n\"Why do errors occur when editing Parquet files with Python, causing format issues with datetime data?\",\\n\"How can I write timestamps to INT96 Parquet format in BigQuery?\",\\n\"What is the use of the `use_deprecated_int96_timestamps` parameter in the `pq.write_to_dataset` function?\"',\n",
       " '0516ccbe': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why do datetime columns in Parquet files created from Pandas show up as integer columns in BigQuery?\", \"How can I generate the Parquet file with the correct logical type for the datetime columns when using Mage?\", \"Can you provide an example of a PyArrow schema that defines the data types for the columns?\", \"What is the purpose of using \\'coerce_timestamps\\' parameter in the `pq.write_table` function?\", \"How do I specify the location of my service account key JSON file in the code?\"]',\n",
       " '6052513d': '[\\n\"Can you show me a simple example of creating an external table using Python in BigQuery, as provided in the FAQ record, and how does it connect to my own data in Google Cloud?\",\\n\"How do I identify the project_id, dataset_name, table_name, bucket_name, and object_key variables that are needed to create the external table, and are these variables hard-coded in the Python script?\",\\n\"Can I use other formats, like CSV or Avro, instead of PARQUET, and what are the implications for my data?\",\\n\"What is the \\'autodetect\\' parameter in the external_config object, and how does it affect the data loading process?\",\\n\"In the script, why is the \\'source_uris\\' list containing a wildcard character \\'*\\', and how does it relate to the object_key variable?\"',\n",
       " '7a71fa2c': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"How do I check if a BigQuery table already exists using the table ID?\", \"Why do I need to use the \\'try-except\\' block in the `tableExists` function?\", \"Is it necessary to use \\'Create External Table using Python\\' before calling the `client.create_table` function?\", \"What does the `client.get_table(tableID)` function do in the `tableExists` function?\", \"Can I directly create a new table in BigQuery if it already exists using the `client.create_table` function?\"]',\n",
       " 'f83d9435': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\"How do I resolve the error \\'Missing close double quote (\") character\\' when working with Google Cloud Platform BigQuery?\", \"What is the command to upload data from Google Cloud Storage to BigQuery?\", \"Is there a way to specify the source format of my data when uploading to BigQuery?\", \"How do I ensure that my data is uploaded correctly from Google Cloud Storage to BigQuery?\", \"What are the steps to load data into BigQuery using BigQuery Cloud Shell?\"]',\n",
       " 'dbf65e11': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"Why can\\'t I read and write in different locations in GCP BigQuery?\", \"What is the region in which my GCS and BigQuery storage should be located?\", \"How do I check the region of my Google Cloud Bucket?\", \"How do I create a new dataset in BigQuery with a specific region?\", \"Why is it necessary to have the same region for GCS and BigQuery storage?\"]',\n",
       " 'c489266b': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What is the purpose of using Cloud Functions to automate tasks in Google Cloud?\", \"How do I define the schema for the data in the CSV.gz files to prevent it from changing with every load?\", \"Can you explain the difference between WRITE_APPEND and WRITE_TRUNCATE in the BigQuery LoadJobConfig?\", \"How do I handle errors when downloading CSV.gz files from Github?\", \"Can I modify the Cloud Function script to load data from a different source, such as a local file instead of Github?\"]',\n",
       " 'ebd63566': '[\\n\"Can you please explain why unchecking cache preferences in query settings solves the issue when querying two different tables external and materialized using count(distinct(*))?\",\\n\"Why do I get the same result when querying two different tables external and materialized using count(distinct(*))? Is it related to the data itself or something else?\",\\n\"What are cache preferences in query settings, and how do they impact the results of count(distinct(*)) queries?\",\\n\"Can I trust the results of count(distinct(*)) queries when querying two different tables external and materialized if I uncheck cache preferences in query settings?\",\\n\"I\\'m still getting the same result when unchecking cache preferences in query settings. Is there anything else I can try to troubleshoot the issue?\"',\n",
       " 'f7252f17': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"Why do some datasets have missing values on DOlocationID and PUlocationID when using Pandas?\",\\n\"What is the difference between \\'int64\\' and \\'Int64\\' when casting columns in Pandas?\",\\n\"Can I still use astype and Int64 to cast columns even if they don\\'t have missing values?\",\\n\"How do I ensure consistent data type between parquet in GCS and schema defined in Big Query?\",\\n\"What is the best practice to define data types in the Transformation section of the ETL pipeline?\"',\n",
       " '47a43bb0': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What can be the reason for the error while reading the table \\'trips_data_all.external_fhv_tripdata\\'?\",\\n\"What is the kind of error that might occur when the project ID in BigQuery is wrong?\",\\n\"Why do I encounter an error in BigQuery when the target type is DOUBLE and the Parquet column type is INT64?\",\\n\"How do I fix the error when the project ID in BigQuery doesn\\'t meet the required criteria?\",\\n\"What can cause an invalid project ID in BigQuery, and how can I rectify it?\"]',\n",
       " 'f3f13def': '[\\n\"Can you explain why BigQuery does not support multiple columns for partitioning, given its potential benefits for data analysis and management?\",\\n\"Why is \\'DOlocationID\\' specific column causing an error in \\'trips_data_all\\' table during Parquet reading, and how can this error be resolved?\",\\n\"What are the implications of having only one column for partitioning on BigQuery, and how will this impact data querying and analysis?\",\\n\"Are there any alternatives to partitioning that can still help improve data retrieval and analysis performance in BigQuery?\",\\n\"In what specific section of the BigQuery documentation can I find the information about single-column partitioning limitation, and can you provide a link to the documentation?\"',\n",
       " '4fd37712': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What is causing the error when reading the Parquet table trips_data_all.external_fhv_tripdata?\", \"How do I solve the date formatting issue in BigQuery when using the DATE() function?\", \"What types of expressions are allowed in the PARTITION BY clause in BigQuery?\", \"How do I convert a column to datetime in Pandas?\", \"What are the correct ways to truncate a datetime column in BigQuery?\"]',\n",
       " '8abeca36': '[\\n  \"What is the difference between Parquet column types and target cpp_types in BigQuery?\",\\n  \"Can I resolve a Parquet column type mismatch by changing the column type in BigQuery?\",\\n  \"Is it possible to store data in both Native tables and External tables in BigQuery?\",\\n  \"What is the main advantage of using External tables over Native tables in BigQuery?\",\\n  \"Can you provide more information about the metadata stored for External tables in BigQuery?\"\\n]',\n",
       " '16c16ff9': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"How do I resolve the issue with exporting an ML model from BigQuery to Google Cloud Storage?\", \"What is causing the error \\'Not found: Dataset was not found in location US\\' when trying to export an ML model from BigQuery to GCS?\", \"Why is the command to export an ML model from BigQuery to GCS not working as shown in the video?\", \"What is the solution to the error \\'Parquet column \\'DOlocationID\\' has type INT64 which does not match the target cpp_type DOUBLE\\'?\", \"How do I enter the correct project_id and gcs_bucket folder address in the command to export an ML model from BigQuery to GCS?\"]',\n",
       " 'c65d8fd9': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What should I do when I encounter an error while reading a table and it says the column type does not match the target cpp type?\", \\n\"How do I resolve the issue where a dataset was not found in a specific location?\", \\n\"What is the purpose of the materialized=\\'table\\' command in SQL?\", \\n\"Why do I need to update the config section in the dim_zones table?\", \\n\"What is the correct location value I should use when creating the fact_trips table?\"]\\n\\nLet me know if this meets your requirements!',\n",
       " 'c1a95536': 'Here are 5 questions based on the FAQ record:\\n\\n{\"question1\": \"I\\'m having a problem exporting my ML model from GCP BigQuery ML to make predictions on my MacBook. It\\'s not working because of the arm architecture, what can I do?\"}\\n{\"question2\": \"Why am I getting an error saying that the Parquet column \\'DOlocationID\\' has a type that doesn\\'t match the target cpp_type when reading the table \\'trips_data_all.external_fhv_tripdata\\'?\"}\\n{\"question3\": \"Can you explain how to set up a serving directory as mentioned in the extract_model.md file?\"}\\n{\"question4\": \"What are the differences between \\'docker pull tensorflow/serving\\' and \\'docker pull emacski/tensorflow-serving\\' and why do I need to use the latter?\"}\\n{\"question5\": \"How do I run the curl command to get a prediction using the emacski/tensorflow-serving image and my serving directory?\"}',\n",
       " 'bba0da04': 'Here are the 5 questions this student might ask:\\n\\n[\\n\"What should I do if I encounter an error while reading a table?\", \\n\"What does VM mean in the context of the course?\", \\n\"If my VM runs out of space, what will happen to my files?\", \\n\"In the ETL process, what are steps I should take to recover from file deletion?\", \\n\"What is the significance of ncdu tool in troubleshooting VM issues?\"\\n]',\n",
       " 'a2120335': 'Here are 5 questions the student might ask:\\n\\n[\"What if I encounter an error like \\'Parquet column \\'DOlocationID\\' has type INT64 which does not match the target cpp_type DOUBLE.\\' when working on the error section?\", \\n\"Why do I need to create an external table based on the files I load into a bucket, as mentioned in the stop with loading files answer?\", \\n\"In the error section, can I try to fix the error by cleaning the data and putting it in parquet format, or should I just load the files into a bucket?\", \\n\"What does \\'Stop with loading the files into a bucket\\' exactly mean, and what should I do next after achieving this stop?\", \\n\"Is there a specific reason why the target cpp_type for the \\'DOlocationID\\' column is DOUBLE, and what are the implications of using INT64 instead?\"]',\n",
       " 'a4ba2478': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What is the exact cause of the \"Out of bounds\" error when reading parquet files from nyc.gov?\", \\n\"Why does pandas use \\'timestamp[ns]\\' and how does it affect the data?\", \\n\"Why do I get weird timestamps when reading the datetime columns separately?\", \\n\"How does the \\'errors=’coerce’\\' parameter convert out of bounds timestamps?\", \\n\"Can you provide a step-by-step example of how to use pyarrow to fix the error and remove the offending rows?\"]',\n",
       " '74c361fe': '[\\n  \"How do I refer to all 12 parquet files for the green taxi 2022 data in BigQuery since they are available separately for each month?\",\\n  \"Why does the Parquet column \\'DOlocationID\\' have type INT64 and not DOUBLE if we need to use DOUBLE as the target cpp_type?\",\\n  \"Do we need to put all 12 parquet files for the green taxi 2022 data in the same GCS bucket?\",\\n  \"Can we use the wildcard \"*\" to refer to all 12 parquet files in a single string when creating an external table in BigQuery?\",\\n  \"Why can\\'t we use a single parquet file for all the data instead of individual files for each month?\"\\n]',\n",
       " 'b9b3ef9f': '[\"What are schema issues that I should be aware of when doing the homework?\", \"Why am I getting an error while reading a table?\", \"Can I upload multiple files at once using the \\'upload files\\' button?\", \"How do I avoid getting errors when uploading files to GCS?\", \"What is the purpose of using the \\'upload files\\' button in GCS?\"]',\n",
       " '009ac612': '[\"What might be the cause of an error message indicating that a Parquet column\\'s data type doesn\\'t match the target C++ type DOUBLE, especially when dealing with a cluster/table?\", \"Can I rely solely on the partitioned/clustered table for accurate predictions, or is there something that could affect the outcome?\", \"What are some common issues that might arise when partitioning/ clustering a table, and how can I overcome them?\", \"When dealing with dates, particularly in the context of a partitioned/clustered table, are there any specific formatting requirements that I should be aware of?\", \"In the context of a cluster/table, what is the purpose of the partition, and how does it impact the accuracy of my predictions?\"]',\n",
       " '68815ec2': '[\"Why am I getting an error while reading the parquet file and how can I resolve it?\", \"What type of match am I supposed to get in the homework Q6, and why is it not exact?\", \"Is it acceptable to choose a nearby option if I don\\\\\\'t get an exact match in the homework Q6?\", \"How can I ensure the correct types are being used in the code?\", \"Is the failure to get an exact match in the homework Q6 common among my peers?\"]',\n",
       " 'c8ad08b3': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How do I handle errors while reading Parquet files?\",\\n\"What is causing a \\'utf-8\\' codec error when reading a CSV file?\",\\n\"How can I ensure that the encoding is correct when reading data from the web?\",\\n\"What is the recommended way to write a pandas dataframe to GCS as a CSV file?\",\\n\"How can I solve UnicodeDecodeError issues while reading data?\"',\n",
       " 'd68b433f': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What happens when we encounter an error in reading a table like trips_data_all.external_fhv_tripdata?\", \"Can generators be used to efficiently handle large datasets in Python?\", \"Why do we get an error message saying \\'Parquet column \\'DOlocationID\\' has type INT64 which does not match the target cpp_type DOUBLE\\'?\", \"How do generators differ from lists or tuples in Python?\", \"What is the purpose of the yield keyword in a generator function?\"]',\n",
       " 'e265ee5a': '[\\n\"How can I resolve the error message \\'Parquet column \\'DOlocationID\\' has type INT64 which does not match the target cpp_type DOUBLE.\\' while loading my trips_data_all table?\",\\n\"What\\'s the error in reading my trips_data_all external table: trips_data_all.external_fhv_tripdata?\",\\n\"What happens if I try to read multiple files at once with read_parquet?\",\\n\"Can I use a list of files as an argument for the read_parquet function?\",\\n\"How does the read_parquet function merge multiple files into a single table?\"',\n",
       " '0e7dfddc': '[\"How can I solve the error when the column type in a Parquet file does not match the target cpp_type, specifically when encountering INT64?\", \"Why do I need to use Int64 when converting data types in Python?\", \"What are the incorrect methods to convert the data type of a Pandas column to INT64?\", \"How can I correctly convert the data type of a Pandas column to INT64?\", \"What is the correct way to cast INT64 data type in Pandas?\"]',\n",
       " '0a059700': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\"What does the error message \\'Parquet column \\'DOlocationID\\' has type INT64 which does not match the target cpp_type DOUBLE\\' mean?\",\\n\"What is the alternative to cache_key_fn=task_input_hash in Prefect?\",\\n\"What\\'s the purpose of the cache_key_fn parameter in Prefect?\",\\n\"Why does removing cache_key_fn=task_input_hash fix the Prefect error?\",\\n\"How can I avoid the Prefect error \\'ValueError: Path /Users/kt/.prefect/storage/44ccce0813ed4f24ab2d3783de7a9c3a does not exist\\' when running a flow?\"]',\n",
       " 'feca7402': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How do I download a file from a URL in a Prefect environment?\",\\n\"How can I write a function to download a CSV file from a URL?\",\\n\"What is the difference between \\'wb\\' and \\'w\\' in the open() function?\",\\n\"Can I use this snippet to download any type of file from a URL, or just CSV files?\",\\n\"How do I integrate this code snippet with the Prefect flow architecture?\"',\n",
       " '1f519b1a': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why do I get a \\'not found in location US\\' error when deploying to DBT Cloud production?\", \"How can I fix the WARN 404 error when running a model in production?\", \"What is the simplest solution to resolve the location issue when creating a dataset in BigQuery?\", \"Why is the prod dataset created in US instead of EU when scheduling and running a job?\", \"Can I create the prod dataset manually in BigQuery instead of relying on DBT Cloud to create it?\"]',\n",
       " '43c454c7': '[\"How do I create a development environment since the error message doesn\\'t provide clear instructions?\", \"What is the guide mentioned in the error message\", \"How do I configure my development credentials to use the dbt IDE\", \"Is there a specific development environment that is recommended for this project\", \"Can I find more information about the setup process in the slack chat\"]',\n",
       " 'd7ad69da': '[\"How do I troubleshoot the \"Runtime Error: Access Denied\" issue when connecting dbt Cloud with BigQuery? Can I use any other role instead of BigQuery Job User? Are there any other ways to grant access in the Google Cloud IAM & Admin panel? What are the recommended roles to add for BigQuery and storage permissions in addition to BigQuery Job User? How do I test the connection in dbt Cloud after making the necessary changes to the service account permissions?\"]',\n",
       " '03fdb780': '[\\n\"What causes a Dbt build error saying that my project was not found?\",\\n\"What does a proper dbt_project.yml config file look like?\",\\n\"Why does my Dbt project need to be in the root directory of the repository?\",\\n\"Can I locate my Dbt project in a subdirectory and still use it with dbt Cloud?\",\\n\"How do I specify the location of my Dbt project in dbt Cloud\\'s Project settings page?\"',\n",
       " '9c85f3aa': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n \"What does \\'Permission denied (publickey)\\' mean when trying to clone the repository?\",\\n \"How do I solve the issue where I don\\'t have permissions to write to DataTalksClub/data-engineering-zoomcamp.git?\",\\n \"Is it necessary to create a separate repo for dbt-lessons or can I just use a subfolder in my existing repo?\",\\n \"Why is there a warning about \"known hosts\" when cloning the repository?\",\\n \"Is the HTTPS link a more reliable option for cloning the repository than the git link?\"\\n]',\n",
       " '63026349': '[\"What is the specific requirement for enabling CI Jobs in dbt Cloud?\", \"Why is my CI Job disabled even though I\\'m on the Team Plan?\", \"Can I leverage CI Jobs with the Developer Plan?\", \"How does the Team Plan trial period affect CI Job setup?\", \"Are there any workarounds to enable CI Jobs for the Developer Plan?\"]',\n",
       " '6ba02f77': '[\\n  \"Why does my IDE session not start and what can I do to resolve the issue?\",\\n  \"I have an issue with the DBT cloud IDE, it\\'s loading indefinitely and showing an error. What\\'s wrong?\",\\n  \"When I\\'m trying to clone a repository into my dbt project, I get stuck. How can I fix this?\",\\n  \"After setting up SSH Keys, I\\'m still having trouble importing my repo into dbt. Can you help me?\",\\n  \"I\\'ve checked the dbt_cloud_setup.md file, but I\\'m still not able to start my IDE session. What else can I try?\"',\n",
       " '8b14286c': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What causes columns datatype issues while running DBT/BigQuery and how can we avoid it?\", \"How do we define the schema while running the web_to_gcp.py pipeline?\", \"Why do we get a Parquet column type mismatch error when running dbt run for fact_trips.sql?\", \"How can we fix the Parquet column type mismatch error when running dbt run for fact_trips.sql?\", \"How can we specify dtypes when importing a CSV file to a pandas DataFrame?\"]',\n",
       " '14a876ea': 'Here are 5 questions based on the FAQ record:\\n\\n[\"How do I download the trip data from the GitHub repository if the provided URL is not working?\", \"Can I use the GitHub CLI to resolve the Access Denied issue while loading trip data into GCS?\", \"Are there any specific commands I need to use to download the trip data from the GitHub repository?\", \"How do I upload the downloaded files to a GCS bucket if I\\'m not using the GUI?\", \"What are some alternative ways to load trip data into GCS if the provided quick script doesn\\'t work?\"]',\n",
       " '1cf5be74': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What causes the CSV parse error when trying to convert fhv_tripdata_2020-01.csv using Airflow\\'s format_to_parquet_task?\", \"How do I fix the CSV parse error when converting fhv_tripdata_2020-01.csv?\", \"Why do I need to open a bash in the container executing the DAG to fix the CSV parse error?\", \"What command should I run to fix the CSV parse error when converting fhv_tripdata_2020-01.csv?\", \"How do I clear the failed task in Airflow to force re-execution after fixing the CSV parse error?\"]',\n",
       " '315ac3cc': 'Here are 5 questions this student might ask:\\n\\n[\"How do I load yellow and green trip data for 2019 and 2020?\", \"Why is the initial approach to load yellow trip data taking forever?\", \"What are some common issues when uploading parquet files directly to GCS?\", \"Can you explain the scheme inconsistency issue when creating a BigQuery table?\", \"Is there a hack to load data to BigQuery and what are the required schema changes?\"]',\n",
       " 'c5c3beba': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\"What are the steps to upload multiple files from a Google Cloud Storage bucket to BigQuery?\", \\n\"Is it possible to copy multiple files from Google Cloud Storage bucket to BigQuery at once?\", \\n\"Can you provide instructions on how to transfer multiple Parquet files from Google Cloud Storage to BigQuery?\", \\n\"How do I move multiple files from a Google Cloud Storage bucket to BigQuery, and what would I need to add in the destination folder?\", \\n\"Is there a specific command or syntax required to transfer multiple files from Google Cloud Storage to BigQuery, and if so, what would it be?\"]',\n",
       " 'f19be91b': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What is a common cause that may have led to my GCP VM\\'s ssh to stop working after a restart?\", \"Why is ssh no longer working after running prefect several times?\", \"What can I do to resolve the issue with my GCP VM\\'s ssh not working?\", \"What is the primary folder that causes the issue with my GCP VM\\'s ssh not working?\", \"How can I avoid the problem of ssh not working after restarting my GCP VM?\"]',\n",
       " '33db7dc7': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How can I regain SSH access to my GCP VM if I\\'ve lost it due to a lack of space?\", \"Why do I get a \\'permission denied\\' error when trying to access my GCP VM?\", \"What are the steps to resolve the \\'publickey\\' issue when accessing my GCP VM?\", \"Can you provide detailed instructions for recovering SSH access to my GCP VM?\", \"How do I free up space on my GCP VM to avoid losing SSH access again?\"]',\n",
       " '67ef8f87': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\\n\"What do I do if I get a 404 Not found error when trying to access a dataset in BigQuery?\",\\n\"How do I check the location of my source dataset and schema in BigQuery?\",\\n\"Why am I getting a mismatch between the location of my source data and the schema I\\'m trying to write to in BigQuery?\",\\n\"Can I specify a single-region location instead of a multi-regional location when setting up my DBT project?\",\\n\"How do I update the location in DBT Cloud to match the region where my BigQuery dataset is located?\"',\n",
       " '6acf2e77': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"When I execute dbt run after installing dbt-utils latest version, I get a warning with an error message. What does it mean and how do I fix it?\", \"I\\'m running dbt run after creating fact_trips.sql, but it fails with an \\'Access Denied\\' error. Why is this happening and how can I resolve the issue?\", \"I\\'m getting an error message when running dbt run. It says `dbt_utils.surrogate_key` has been replaced by `dbt_utils.generate_surrogate_key`. What\\'s going on and how do I fix it?\", \"I\\'m trying to fix the error in my stg_green_tripdata.sql file, but I\\'m not sure how to replace `dbt_utils.surrogate_key`. Can you provide more guidance?\", \"I\\'m having trouble with permissions when running dbt run. I need to add some roles to a service account in BigQuery and GCS. Can you explain how to do this?\"]',\n",
       " '18430f10': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What should I do when I\\'m getting an error that says \\'dbt_utils not found\\'?\",\\n\"I\\'m having trouble with \\'dbt_utils not found\\' error, can you help me fix it?\",\\n\"When I get a \\'dbt_utils not found\\' error, how do I resolve this issue?\",\\n\"Why do I keep getting an error that says \\'dbt_utils not found\\' and how can I fix it?\",\\n\"What\\'s the solution to the \\'dbt_utils not found\\' error I\\'m currently experiencing?\"',\n",
       " 'afb7a40a': '[\"My yml file is formatted correctly, but I still get the error \\'lineage is currently unavailable\\'. What are common compilation errors I should look out for?\", \\n\"What can I do if I\\'m still getting the error after checking for compilation errors?\", \\n\"Can I expand the command history console to see more information about the build process?\", \\n\"How do I check the build logs if the run was not completed successfully?\", \\n\"What should I do if I still can\\'t resolve the issue after checking the build logs and contacting support?\"]',\n",
       " 'd6a5b80e': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why do my Fact_trips models only include a few days of data?\", \"When running dbt, what is the purpose of the \\'--var\\' flag?\", \"How do I use multiple variables with \\'dbt build\\' command?\", \"Can you explain what \\'is_test_run\\' does in the dbt command?\", \"When running dbt, how do I ensure I\\'m getting all the data in my Fact_trips model?\"]',\n",
       " 'de426d2f': 'Here are 5 questions based on the FAQ record:\\n\\n[\"Why do I only see one month of data when I write to BigQuery?\", \"Can I overwrite data when I write to BigQuery?\", \"How do I append data when writing to BigQuery?\", \"What happens when I use \\'replace\\' in writing data to BigQuery?\", \"How do I ensure data from all months is present in BigQuery?\"]',\n",
       " '354f0e10': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\"When I run the dm_monthly_zone_revenue.sql model in BigQuery, I get an error message. What could be the problem?\",\"Why isn\\'t my model running in BigQuery, and what\\'s the solution?\",\"I\\'m trying to run a model in BigQuery and getting an error. Can you help me troubleshoot?\",\"I see an error when I run a model in BigQuery. Can you explain why and how to fix it?\",\"Why am I getting an error in BigQuery when I try to run a specific model?\"]',\n",
       " '98fae8d0': '[\"How do I write a surrogate key in DBT? Is there a difference between using surrogate_key and generate_surrogate_key functions?\", \"Can I use placeholder names like field_a, field_b, etc. in the generate_surrogate_key function? Are these names hardcoded?\", \"What if I have more than 26 fields in the list, can I still use generate_surrogate_key? Are there any limitations?\", \"How do I implement surrogate key when working with big tables? Is it necessary to divide the data into smaller chunks?\", \"Are there any performance considerations while using generate_surrogate_key function, as it contains multiple fields?\"]',\n",
       " 'cb678fde': '[\\n\"I changed the location in dbt, but running \\'dbt run\\' still gives me an error even after restarting the environment, how can I resolve this?\",\\n\"Why do I need to remove the dataset created by dbt from BigQuery to change the location\",\\n\"After changing the dbt location, does the dataset automatically get recreated in BigQuery with the new location\",\\n\"I removed the dataset from BigQuery, but now \\'dbt run\\' is still failing, what could be the issue\",\\n\"Is it necessary to recreate the dataset in BigQuery with the new location every time I change the dbt location\"',\n",
       " '39bfb043': '[\"Why is it possible that my table in BigQuery has the same number of rows after changing a variable as it did before?\", \"Why do I get a new dataset created in BigQuery when I run my CI/CD job?\", \"Why did dbt not merge all my models into one environment when I ran the CI/CD job?\", \"Why did my previous run of dbt create a new dataset, and how do I prevent this?\", \"How can I ensure that my CI/CD job reflects all the rows in my table after changing a variable?\"]',\n",
       " '351a078a': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"What is the purpose of the Staging dataset?\", \"Why did Vic not use the Staging dataset in the project?\", \"Why are datasets in Staging materialized as views and not tables?\", \"Can we skip creating the Staging dataset for our project?\", \"How does the Staging dataset fit between the raw datasets and the fact and dim tables?\"]',\n",
       " '61da1919': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What should I do if DBT docs are served but not accessible via browser?\", \"Why can\\'t I access DBT docs in my browser?\", \"I can see DBT docs being served, but how do I make them accessible?\", \"Can\\'t access DBT docs, any workarounds?\", \"Why do I have issues accessing DBT docs, and how can I solve it?\"]',\n",
       " '6528c6ae': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What should I do when I get a 404 Not found error for my dataset in BigQuery saying it was not found in the europe-west6 location?\", \"Why is my dataset not found in BigQuery when I\\'ve set up the connection?\", \"How do I change the location of my dataset in BigQuery after setting up the connection?\", \"What should I do if I need to reupload my GCP key for BigQuery?\", \"Can I rebuild my project in dbt to resolve issues with dataset location in BigQuery?\"]',\n",
       " 'c0d3a2e8': '{\"questions\": [\\n\"What do I do when I encounter a \\'read-only\\' main branch in dbt+git integration, and how can I start editing my project?\",\\n\"In module 4, how do I resolve the issue of \\'read-only\\' main branch when working with dbt and git?\",\\n\"Why is my dbt project\\'s main branch set to \\'read-only\\' and how can I edit files on it?\",\\n\"I am having trouble with my dbt+git integration and the main branch is \\'read-only\\', can I find more information on how to resolve this issue in the dbt docs?\",\\n\"How do I prevent the \\'read-only\\' main branch from locking me out of editing files in my dbt project, and what steps can I take to resolve this Issue?\"\\n]}',\n",
       " '859a97c5': '[\"How can I edit dbt files in the course repository since I am currently in read-only mode?\", \"What is the correct procedure to perform changes to our dbt project\", \"How do I transition from making changes on a new branch to committing and pushing to the main branch\", \"Why am I not allowed to edit files directly on the main branch in the repository\", \"What steps do I need to take to make temporary changes to the dbt project without affecting the main branch\"]',\n",
       " '32469a2d': 'Here are 5 questions based on the FAQ record:\\n\\n[\\n    \"Why can\\'t I create a CI checks job for deployment to Production in dbt deploy + Git CI?\",\\n    \"Does the dbt Cloud native integration with Github, Gitlab, or Azure DevOps affect the availability of the CI checks job?\",\\n    \"How do I resolve the error caused by pull requests when trying to create a CI checks job?\",\\n    \"What is the recommended alternative to the Git Clone option for linking my dbt repository with Github?\",\\n    \"Can I use the UN-LINK and RE-LINK method to resolve issues with dbt Cloud\\'s integration with Github?\"',\n",
       " 'c599b3a0': 'Here are 5 questions based on the FAQ record:\\n\\n[\"How do I reconnect with Github using a native connection instead of clone by SSH when setting up Continuous Integration (CI) with Github?\", \"Why can\\'t I see \\'Run on Pull Requests\\' on triggers in the job\\'s options when setting up CI with Github?\", \"How do I disconnect my current Github\\'s configuration from Account Settings > Projects (analytics) > Github connection?\", \"What happens if I don\\'t see \\'Run on Pull Requests\\' after reconnecting with Github?\", \"Where can I find more information on connecting and setting up Git in dbt?\"]',\n",
       " '179df18d': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What happens if my model depends on a source that was not found during compilation?\",\\n\"Why do I get a compilation error if my project schema.yml file is not saved?\",\\n\"In the video DE Zoomcamp 4.3.1, what is the problem at 14:25 that causes a Compilation Error?\",\\n\"Can I view my Lineage graph if I\\'m following the video DE Zoomcamp 4.3.1?\",\\n\"Do I need to do any specific steps if I encounter a Compilation Error in DBT?\"',\n",
       " '1ce1a275': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What does the error message \\'NoneType\\' object is not iterable mean in the context of dbt?\",\\n\"Why is the error occurring in the test_accepted_values macro in the tests/generic/builtin.sql file?\",\\n\"What is the role of the test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ test in dbt?\",\\n\"Why do I need to add variables to the dbt_project.yml file in this specific scenario?\",\\n\"What are the payment_type_values and how do they affect the dbt model or testing process?\"',\n",
       " 'b529b0bc': 'Here are the 5 questions the student might ask:\\n\\n[\"Why do I get a BadRequest error in BigQuery when running a dbt macro?\", \"What\\'s the cause of the \\'No matching signature for operator CASE\\' error?\", \"How do I fix the macro to get the correct description for payment type?\", \"What\\'s the syntax error in the CASE statement and how do I resolve it?\", \"Why do I need to change the data type of the numbers in the payment_type column to text?\"]',\n",
       " '2e51a111': '[\"What does the dbt error log contain information about?\", \"Can I see the problematic line of my query in the log?\", \"Is the link in the log related to my query results?\", \"What will I see when I follow the link to BigQuery?\", \"How can I quickly identify the issue in my query with the highlighted line?\"]',\n",
       " '6e1a0834': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why does dbt append the string \"dbt_\" to the target schema when I specify \"marts\" as the target schema?\", \"How does dbt determine what to append to the target schema?\", \"Why do I need to create a macro to override the default schema naming behaviour?\", \"What is the purpose of the \\'generate_schema_name\\' macro in dbt?\", \"How do I use the \\'generate_schema_name\\' macro to specify the target schema in my dbt_project.yml file?\"]',\n",
       " 'a8657e65': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What is the setting to set as project root in dbt cloud?\", \"How do I set my GitHub repository subdirectory as the root in dbt?\", \"Can I configure project root in dbt cloud?\", \"Is there a way to set custom root directory in dbt?\", \"How can I set the root directory in my dbt project on dbt cloud?\"]',\n",
       " '2678d8c2': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[{\"What if my model references a table that doesn\\'t exist? Will I still get an error during compilation?\"},\\n{\"How do I know which tables exist in my BigQuery/Postgres database?\"},\\n{\"Can I use nested sources, or do I have to flatten them out to the highest level?\"},\\n{\"What\\'s the correct syntax to use when referencing a source table? Is it really just \\'{{ source(\\'staging\\',<your table name in the database>\\') }}\\'?\"},\\n{\"I\\'m trying to reference a table from a specific schema. Do I need to include the schema name when using the \\'{{ source() }}\\' function?\"]',\n",
       " 'aa85c6ae': '[\"What happens if my model depends on a seed that is not found in the production environment?\", \"Why do I need to create a pull request before building the model?\", \"What is the significance of checking the \\'seeds\\' folder for a seed file?\", \"How does the presence of .gitignore file impact model compilation?\", \"Is there any specific rule for .csv extension inclusion in .gitignore file?\"]',\n",
       " 'de06929d': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What role do I need to add to execute dbt run after using fhv_tripdata as an external table?\", \\n\"What permissions do I need to get to avoid an \\'Access Denied\\' error?\", \\n\"I\\'m trying to use fhv_tripdata as an external table, but I get an error. How do I resolve this?\", \\n\"After using fhv_tripdata as an external table, I\\'m getting an error. Can you tell me why?\", \\n\"I\\'\\x80\\x99m struggling to execute dbt run after using fhv_tripdata. What should I do differently?\"]',\n",
       " 'b087fa95': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How can I prevent the error that occurs when injecting data into BigQuery due to pandas parsing integer columns with missing values as float type?\",\\n\"Why does pandas parse integer columns with missing values as float type by default?\",\\n\"Is there a way to avoid having to specify the data type for each integer column in the data transformation stage?\",\\n\"What is the purpose of the `convert_dtypes` function in pandas and when is it useful?\",\\n\"Can you provide more details on how the `fillna` method is used to replace missing values and how it affects the data type inference process?\"',\n",
       " '3c41892d': '[\\n\"What happens when loading a GitHub repo and getting an exception about \\'taxi_zone_lookup\\' not being found?\",\\n\"Why do I get an error when trying to load a GitHub repo when \\'taxi_zone_lookup\\' is not found?\",\\n\"I\\'m having trouble loading a GitHub repo because of an error about \\'taxi_zone_lookup\\' not being found, what\\'s going on?\",\\n\"What\\'s the reason for the \\'taxi_zone_lookup\\' not found error when loading a GitHub repo?\",\\n\"I\\'m seeing an exception when loading a GitHub repo because \\'taxi_zone_lookup\\' wasn\\'t found, what should I do?\"\\n]',\n",
       " '4842f3e8': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"What does the \\'.gitignore file\\' do in dbt? How does it affect my CSV files?\", \"Why am I getting a 404 error in dbt and how can I fix it?\", \"What do you mean by \\'generalized region\\'? How does it affect my dataset?\", \"How do I set the location manually in dbt settings?\", \"Is there a specific way to structure my datasets for dbt to find them correctly?\"]',\n",
       " '5eaf61fe': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\"When we\\'re ingesting data with Parquet files, why do we get data type errors?\", \"Why do you recommend avoiding Parquet files?\", \"Can we use other compression formats instead of .csv.gz?\", \"What\\'s the difference between creating an external table with .csv.gz vs. Parquet files?\", \"Why won\\'t the data type errors come up for week 4 if we use .csv.gz files?\"]',\n",
       " '8ed36cea': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How can I resolve the issue of inconsistent number of rows in my fact_trips table when re-running the fact_trips model?\",\\n\"I don\\'t understand why the deduplication process in my staging files is causing inconsistent results. Can someone explain it?\",\\n\"Is there a way to reorder the partitions in my staging files to get consistent results?\",\\n\"Why does the first row chosen in each partition affect the number of rows discarded in the fact_trips model?\",\\n\"What\\'s the best way to ensure that my order by columns in the partition by part of my staging files are correct to avoid inconsistent results?\"',\n",
       " '46aebc79': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n{\"question\": \"What could be the cause of a data type error when running a fact table?\"},\\n{\"question\": \"Why do I get a data type error on the trip_type column?\"},\\n{\"question\": \"How do I resolve the data type error on the trip_type column?\"},\\n{\"question\": \"What might be the reason for this error, as I don\\'t have any null values in BigQuery?\"},\\n{\"question\": \"Is there a specific data type that I should use instead of NUMERIC to resolve the issue?\"]',\n",
       " 'e2d2bc58': '[\"What causes the CREATE TABLE error when columns have duplicate names?\", \"Can you explain what happens when using a select * query without table name?\", \"How do we resolve the duplicate column name error in a SELECT statement?\", \"What is the best approach to avoid duplicate column names in a query?\", \"In the example provided, why is it necessary to mention the table name when using a SELECT query?\"]',\n",
       " '137aab88': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What happens when I try to cast a null value in my dbt model as an integer and get a Bad int64 value: 0.0 error?\", \"Why am I getting a Bad int64 value: 0.0 error when I try to cast ehail fees in my dbt code?\", \"How can I avoid getting a Bad int64 value: 0.0 error when casting integer values in dbt?\", \"Can I use a different function instead of safe_cast to avoid getting Bad int64 value: 0.0 errors?\", \"Will using safe_cast instead of casting directly to integer resolve the Bad int64 value: 0.0 error I\\'m experiencing?\"]',\n",
       " 'a260e651': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What causes the \\'Bad int64 value: 2.0/1.0 error\\' when building the fact_trips.sql model?\", \"Why does safe_cast cause the entire field to become null?\", \"How do I address the \\'Bad int64 value: 1.0 error\\' again in the fact_trips.sql model?\", \"Which columns other than payment_type_description are causing the \\'Bad int64 value\\' error?\", \"How do I use the queries to address the \\'Bad int64 value\\' error in the Green_tripdata table?\" ]',\n",
       " 'da8d9fcc': 'Here are the 5 questions the student might ask:\\n\\n[\\n\"How do I handle a data type mismatch between a Parquet column and the target type in a DBT model?\",\\n\"Why am I getting an error when building a fact table in DBT and how can I resolve it?\",\\n\"I am experiencing issues with Double data type in Parquet file not matching INT64 in DBT - what is the solution?\",\\n\"What is the best way to convert a Parquet column from Double to INT64 in a DBT model?\",\\n\"Can you provide an example of how to handle data type mismatches in a DBT model, specifically with Parquet files?\"',\n",
       " '2314e3c4': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"Why does the - vars argument have to be a YAML dictionary?\",\\n\"What is the correct syntax for using the - vars argument in dbt?\",\\n\"Why do I need to add a space between the variable and the value?\",\\n\"How do I specify multiple variables in the - vars argument?\",\\n\"What are the common mistakes to avoid when using the - vars argument?\"',\n",
       " 'e7bdbba6': '[\"What is the reason why Environment Type is greyed out and inaccessible in the platform?\", \"Why is it important to note that I don\\'t need to change the environment type?\", \"Is it necessary to configure the environment type in DBT?\", \"Why is the Production Deployment option the only available option?\", \"Are there any exceptions to not changing the environment type in the course?\"]',\n",
       " '52cccade': 'Here are 5 questions the student might ask:\\n\\n[\"Why do I get an \\'Access Denied\\' error when trying to query a table?\", \"What is the significance of the branch name when running dbt jobs?\", \"How can I resolve the \\'Database Error\\' in a model file?\", \"What does \\'compiled Code at target/run/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql\\' mean?\", \"How do I change the branch for my dbt job in dbt Cloud?\"]',\n",
       " '11a814ea': '[\\n  \"I made a change to my modelling files and committed them to my development branch, but the job is still running on the old file. Can I just refresh the job to use the new changes?\", \\n  \"How do I make sure the job runs with the updated modelling files after I made a change to them in the development branch?\", \\n  \"I\\'m trying to switch to the new model files, but the job is still holding on to the old ones. What\\'s the right way to do this?\", \\n  \"I\\'ve committed the changes to my development branch, but I\\'m still seeing the old models being used. Why is this happening and how do I fix it?\", \\n  \"What\\'s the process I need to follow to ensure my changes are reflected in the job and it starts using the updated model files?\"\\n]',\n",
       " '0d1e02d5': '[\"What are the necessary steps to set up a development environment in dbt?\", \"How do I develop data models in dbt and make them appear in the Develop tab?\", \"What is the first step to develop a data model on dbt?\", \"Why do I need to create both development and deployment environments for dbt jobs?\", \"Can I skip setting up the development environment and still successfully run a dbt job in Bigquery?\"]',\n",
       " '0a0cc4c3': '[\\n\"Can someone explain what causes the Prefect Agent to fail while retrieving runs from a queue sometimes?\", \\n\"What does the error message \\'Invalid input ConnectionInputs.SEND_HEADERS in state ConnectionState.CLOSED\\' mean in the context of Prefect Agent?\", \\n\"Is there a chance that the Prefect Agent will be able to reconnect and retrieve the runs if I wait a few minutes?\", \\n\"Why does the Prefect Agent sometimes fail to retrieve runs from the queue, and is there a way to prevent it?\", \\n\"Instead of running it again, are there any other options to troubleshoot the issue with the Prefect Agent retrieving runs from the queue?\"',\n",
       " 'cb912983': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What could be the cause of BigQuery returning an error when I try to run \\'dbt run\\'?\",\\n\"Does the error \\'Parquet column \\'passenger_count\\' has type INT64 which does not match the target cpp_type DOUBLE\\' occur only in my case, or is it a common issue?\",\\n\"How can I resolve the issue with different formats of data in my files?\",\\n\"Can I use the same transformation method to fix other columns with incorrect data formats, or is it specific to the ones mentioned in the answer?\",\\n\"Are there any other common errors or issues that have been discussed on the Slack channel for others to learn from?\"',\n",
       " '2d4e434f': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\"What is the correct syntax for running a specific set of models in dbt?\", \"How do I execute a SQL command in dbt with a specific variable?\", \"Why is my dbt command not returning any output?\", \"Is there an alternative syntax for dbt run if the code in the tutorial doesn\\'t work?\", \"Can I run dbt with a specific variable as part of the command?\"]',\n",
       " 'bb6655b9': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"Can I still set up dbt and BigQuery in a Docker environment if I encounter an error?\",\\n\"Why does the error \\'ModuleNotFoundError: No module named \\'pytz\\'\\' occur while setting up dbt with BigQuery on Docker?\",\\n\"What should I add to the Dockerfile to resolve the \\'No module named \\'pytz\\'\\' error when running dbt with BigQuery?\",\\n\"How do I fix the \\'No module named \\'pytz\\'\\' error when running `docker-compose build` and `docker-compose run dbt-bq-dtc init`?\",\\n\"Why do I need to install the \\'pytz\\' module using pip when setting up dbt with BigQuery in a Docker environment?\"',\n",
       " 'fc2eb036': 'Here are the 5 questions the student might ask:\\n\\n[\"What happens when I have permission issues editing dbt_project.yml in Docker on a Linux system?\", \"Why do I see a \\'NoPermissions\\' error when trying to use dbt in VS Code on Linux?\", \"Why isn\\'t my dbt project being loaded correctly when I run \\'dbt debug\\'?\", \"How can I resolve the \\'Profile should not be None if loading is completed\\' error when using dbt?\", \"How do I change the directory to the newly created subdirectory when running dbt debug?\"]',\n",
       " '25daead9': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\"What are the possible causes of the \\'table is not on the specified location\\' error in BigQuery?\", \"How do I check if the locations of my bucket, datasets, and tables in BigQuery are consistent?\", \"Why do I need to change the query settings to the location I\\'m in in order to resolve BigQuery location problems?\", \"What should I do if I\\'m getting a \\'table is not on the specified location\\' error and the locations of my files are correct?\", \"How can I ensure that the paths I\\'m using in my BigQuery query to my tables are accurate?\"]',\n",
       " '2221d75e': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\"What is the cause of the \\'DBT Deploy - This dbt Cloud run was cancelled because a valid dbt project was not found\\' error?\", \"Why do I need to add the directory path to my dbt Cloud project settings?\", \"How do I ensure that my file explorer path and the project settings path are matched?\", \"Why do I need to merge and close the pull request manually in dbt Cloud?\", \"How do I set up the PROD custom branch in the Environment setup screen?\"]',\n",
       " '94524a9d': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"When deploying and running CI in Module 4, why does dbt create a new schema on BigQuery and how can this cause errors?;\", \"Why would creating a new schema on the \\'US\\' location cause an issue if my dataset, schemas, and tables are in \\'EU\\'?;\", \"How do I change the default location that dbt uses when creating a new schema on BigQuery?;\", \"Why does the location setting need to be specified in the connection optional settings and not in the project settings?;\", \"Will changing the location setting affect the existing schema and data on my BigQuery dataset?\"]',\n",
       " '1f1ecbb7': '[\\n  \"When I try to run my dbt project on production, I get an error. What should I do?\",\\n  \"I made changes to my repo and now I\\'m getting errors when deploying to prod. How can I fix this?\",\\n  \"I\\'m getting a dbt Cloud run error saying no valid dbt project was found. What\\'s going on?\",\\n  \"Why do I need to check the dbt_project.yml file before deploying to prod?\",\\n  \"I set up a dataset on BigQuery with a certain name, but when I deploy to prod, I get an error. How do I resolve this?\"\\n]',\n",
       " 'c5af32ab': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\"What does the \\'404 Not found\\' error message in DBT mean?\", \"Why does DBT default to the US location when generating a Bigquery schema?\", \"Why did I get an error saying the dataset was not found in the EU location after building from stg_green_tripdata.sql?\", \"How do I specify the location for my Bigquery connection in dbt?\", \"Why did I have to edit the connection details for my Bigquery project to resolve this error?\"]',\n",
       " '1e6b7da1': '[\"What do I need to do if I\\'m having trouble loading FHV_20?? data from the github repo into GCS and then into BQ?\", \"How do I fix the issue with the input file not being of type parquet?\", \"What URL link do I need to use to load the data?\", \"Why do I need to append \\'?raw=true\\' to the URL Template link?\", \"How do I identify if my URL link has \\'tree\\' instead of \\'blob\\'?\"]',\n",
       " '259481c4': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"How can I upload datasets from GitHub for the homework?\", \"Is using the script git_csv_to_gcs.py the most efficient way to upload datasets?\", \"Can I use a similar script for other types of data?\", \"Do you have a specific script for uploading datasets to Google Cloud Storage?\", \"Can I get a copy of the script Alexey provided for uploading web data to Google Cloud Storage? \"]',\n",
       " 'edbae698': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n  \"Can you explain how to securely store project credentials?\",\\n  \"What is the easiest way to set environment variables for a project?\",\\n  \"How do I use .env files to inject environment variables?\",\\n  \"Is there a way to access environment variables like GCP_GCS_BUCKET and GOOGLE_APPLICATION_CREDENTIALS in my project?\",\\n  \"What if I have multiple environment variables to set for my project - is there an easier way than manually setting each one?\"',\n",
       " '67217f4c': 'Here are the 5 questions this student might ask:\\n\\n[\"What can I do if I\\'m experiencing errors with date types after ingesting FHV data through CSV files?\", \"How do I create an external table in BigQuery when I have date columns that are causing issues?\", \"Can I use the TIMESTAMP function in DBT to resolve date-related errors?\", \"Why do I need to define date columns as strings in the external table, and how does that help with parsing?\", \"In the DBT model, can I directly use TIMESTAMP on the pickup_datetime and dropoff_datetime columns without casting them as strings first?\"]',\n",
       " '2aadd232': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\"What data types should I use when creating an external table for FHV data in DBT?\", \"How do I resolve errors with date types when loading FHV parquet files?\", \"Can I load all months of FHV data at once without using a loop?\", \"How can I avoid errors with NULL values in PULocationID and DOLocationID when creating an external table?\", \"Is it possible to manually upload FHV 2019 parquet files and expect to avoid errors when loading the data in a landing table?\"]',\n",
       " 'adcd914a': '[\\n  \"What happens if I\\'ve used up my 30-day trial of Google Looker Studio?\",\\n  \"Can I still use Looker Studio if I haven\\'t paid for the Pro version?\",\\n  \"How do I switch to the free version of Looker Studio?\",\\n  \"Why am I being prompted to subscribe to Looker Studio Pro when accessing the console?\",\\n  \"Is there a way to get an error when using Looker Studio without paying for the Pro version?\"',\n",
       " 'bbf094b3': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"What is the mechanism used by dbt to manage dependencies between models? Can you explain the role of the \\'ref\\' keyword?\"]\\n[\"Is there any way to load data from FHV Data that\\'s currently going into slumber when using Mage? I\\'ve tried loading data in Jupyter notebooks, but I\\'m still having issues.\"]\\n[\"I\\'m having issues with region mismatch between DBT and BigQuery. How can I fix this? Specifically, how do I change the location in DBT to match BigQuery?\"]\\n[\"Are there any specific steps I need to take when loading FHV data into GCP, as opposed to loading local data?\"]\\n[\"Is there a way to output the results in parsable JSON format without having to use code blocks in DBT? If so, can you please provide more information on how to do this?\"',\n",
       " '2fdc5057': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\"What are some common issues that may arise when using COPY FROM feature in dbt-postgres?\", \\n\"How can I check if the taxi data is properly uploaded to dbt-postgres using COPY FROM feature?\", \\n\"Is it possible to upload multiple CSV files at once using COPY FROM feature in dbt-postgres?\", \\n\"Can I use COPY FROM feature to upload data from a source other than CSV files?\", \\n\"Are there any specific settings or configurations required to use COPY FROM feature in dbt-postgres for taxi data upload?\"]',\n",
       " '95e302f7': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What is the issue with configuring profiles.yml file for dbt-postgres when using jinja templates with environment variables?\",\\n\"Why am I getting an error when I try to configure the profiles.yml file for dbt-postgres with jinja templates and environment variables?\",\\n\"What is the correct way to configure profiles.yml file for dbt-postgres when using jinja templates with environment variables?\",\\n\"Can you provide an example of how to update the profiles.yml file for dbt-postgres with jinja templates and environment variables?\",\\n\"What is the issue with specifying port \\'5432\\' in the profiles.yml file for dbt-postgres with jinja templates and environment variables?\"',\n",
       " '1ac2c13c': '[\"How do I install and set up Java and Spark using SDKMAN on a Linux system, and what are the steps to verify that the installations were successful?\", \"What does the command \\'echo $JAVA_HOME\\' do and what information does it display?\", \"What is the purpose of running the command \\'sdk install java 11.0.22-tem\\'?\", \"What does \\'spark-submit --version\\' do and what information does it display?\", \"Can I run these commands in the same shell as my other commands, and do I need to start a new terminal for each step?\"]',\n",
       " '5cc0e4d9': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\"Is it normal to struggle with setting up PySpark locally?\", \"Can I use Spark in Google Colab without setting it up locally?\", \"What does \\'locally\\' mean in the context of setting up PySpark?\", \"Why should I prioritize setting up PySpark locally over using Google Colab?\", \"Are there any other alternative ways to use Spark besides Google Colab and local setup?\"]',\n",
       " '17090545': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[{\"question\": \"What should I do if I\\'m using Java 17 or 19 with Spark and it\\'s causing an error?\"},\\n {\"question\": \"Why do I get a Java error when trying to run spark-shell in Command Prompt?\"}, \\n {\"question\": \"Can I use Java 8, 11, or 16 with Spark 3.0?\"},\\n {\"question\": \"What\\'s the problem with the \\'native-hadoop library for platform\\' when running spark-shell?\"}, \\n {\"question\": \"Why is the spark-shell not loading after installing Hadoop and Spark on Windows?\"}]',\n",
       " 'd17e30c6': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"Why do I get the \\'Python was not found\\' error while trying to use PySpark in Windows?\",\\n\"What is the reason behind the \\'PYSPARK_PYTHON environment variable was not set correctly\\' issue in Windows?\",\\n\"Is it necessary to install \\'findspark\\' using pip for using PySpark in Python?\",\\n\"What are the \\'proper environment\\' and \\'command line\\' referred to in the solution for fixing the \\'Python was not found\\' error?\",\\n\"Can you provide an example of how to \\'add to the top of the script\\' the \\'import findspark\\' and \\'findspark.init()\\' commands?\"',\n",
       " '1520b5bc': 'Here are 5 questions based on the FAQ record:\\n\\n{\"question1\": \"I\\'m using Python 3.11 and Spark 3.0.3 on Windows, and I\\'m getting a TypeError saying that code() argument 13 must be str, not int when I try to import PySpark. Why is this happening?\"},\\n{\"question2\": \"I\\'ve tried to run my PySpark code, but it\\'s throwing a TypeError: code() argument 13 must be str, not int. Can someone help me figure out what\\'s going wrong?\" },\\n{\"question3\": \"I\\'m experiencing a TypeError when trying to import PySpark in my Python script. The error message says Python 3.11 tries to interpret an integer as a string. How do I fix this?\" },\\n{\"question4\": \"I\\'ve encountered a weird TypeError when running my PySpark code on a Windows machine with Python 3.11 and Spark 3.0.3. Can anyone tell me why this is happening?\" },\\n{\"question5\": \"I\\'m having trouble with PySpark on my Windows machine, where I\\'m using Python 3.11 and Spark 3.0.3. The error message says \\'TypeError: code() argument 13 must be str, not int\\', but I have no idea why it\\'s happening.\"}',\n",
       " 'e86ca928': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I set up Java with Miniconda environment on MacOS?\", \"What is the correct setup for Spark, Hadoop, and Java versions?\", \"Why am I getting a Py4JError with a generic root cause of Connection refused?\", \"Do I need to manually set environment variables for Java, Spark, and Hadoop?\", \"How can I resolve the error \\'An error occurred while calling o54.parquet\\' and subsequent errors like \\'o31.parquet\\' and \\'o35.parquet\\' when writing DataFrame to local file?\"]',\n",
       " '3b5b4eb3': 'Here are the 5 questions:\\n\\n[\"What does the RuntimeError: Java gateway process exited before sending its port number error mean?\", \"Why do I get this error even after installing pyspark?\", \"Why do I need to install findspark?\", \"How can I check if pyspark is pointing to the correct location?\", \"How can I set environment variables permanently to solve the error?\"]',\n",
       " '489c366f': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What do I do when I get a \\'Module Not Found Error\\' in Jupyter Notebook after installing pyspark correctly?\", \\n\"Why do I need to install findspark and initialize it before using pyspark in Jupyter Notebook?\", \\n\"How do I import pyspark and create spark context correctly in Jupyter Notebook?\", \\n\"What is the difference between using `!pip install pyspark` and `!pip3 install pyspark` in Jupyter Notebook?\", \\n\"How can I filter a pyspark dataframe based on multiple conditions from multiple columns?\"]',\n",
       " '59381b15': 'Here are the 5 questions:\\n\\n[\"How do I update the export command to include the correct Py4J file version?\", \"How do I find the version of the Py4J file in ${SPARK_HOME}/python/lib/?\", \"What should I do if adding the correct Py4J version to the export command does not solve the problem?\", \"Why am I getting \\'ModuleNotFoundError: No module named \\'py4j\\'\\' when trying to execute \\'import pyspark\\'?\", \"Is there an alternative solution to resolve the \\'ModuleNotFoundError: No module named \\'py4j\\'\\' issue other than updating the export command?\"]',\n",
       " '220b1cf3': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\"How do I solve the Py4J error if the conda install command does not work?\", \\n\"What do I do if the latest py4J version installation fails?\", \\n\"Why am I getting a Py4J error and how do I fix it?\", \\n\"What should I do if \\'conda install -c conda-forge py4j\\' command does not resolve the error?\", \\n\"Can you explain the command to modify my .bashrc file for Py4J setup?\"]',\n",
       " 'd970a0da': 'Here are the 5 questions this student might ask:\\n\\n[\\n\"How can I troubleshoot the problem when even after exporting the correct paths, I still encounter an error saying that the \\'jupyter-notebook\\' command is not found?\",\\n\"I installed Jupyter, but when I try to run \\'jupyter notebook\\', I\\'m prompted with a message saying the command is not found. What am I missing?\",\\n\"I\\'ve followed the instructions to set up a virtual environment, but I still can\\'t find the \\'jupyter-notebook\\' command. Is there something else I\\'m doing wrong?\",\\n\"What is the reason behind installing Python virtualenv and creating a Python Virtual Environment before installing Jupyter Notebook?\",\\n\"What are the potential issues that might cause \\'jupyter-notebook\\' not to be found, even after I\\'ve set up the virtual environment and installed Jupyter Notebook?\"',\n",
       " '5fa98bd0': 'Here are the 5 questions:\\n\\n[\"What do I need to do when I encounter a java.io.FileNotFoundException and Spark\\'s lazy transformations cause problems?\", \"Why do I need to write to a different directory when using mode=\\'overwrite\\'?\", \"Is there a specific reason why write.parquet() operation triggers the job?\", \"Why does Spark\\'s lazy transformation cause problems when reading parquet files?\", \"Can I overwrite the existing parquet files without getting a FileNotFoundException?\"]',\n",
       " 'ce508f3c': 'Here are 5 questions that a student might ask based on the provided FAQ record, in parsable JSON format:\\n\\n[\\n\"Can I please know why I\\'m getting a FileNotFoundException when trying to write with pyspark on Windows?\",\\n\"What needs to be done differently for a Windows installation compared to other operating systems?\",\\n\"Can you provide more details on the installation of Hadoop on Windows?\",\\n\"Why does the shell script not install the Hadoop /bin directory by default?\",\\n\"How can I ensure that the Hadoop /bin directory is correctly set up for pyspark in my Windows environment?\"',\n",
       " 'b7b9487d': '[\"Why is Spark SQL considered a separate type of SQL?\", \\n\"What does Spark SQL use for querying data?\", \\n\"Can I use MySQL or SQL Server syntax in Spark SQL?\", \\n\"What are the main differences between built-in functions in Spark SQL and other SQL providers?\", \\n\"Are there specific types of joins available in Spark SQL that are not found in other SQL providers?\"]',\n",
       " 'a74de125': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"What happens if the port is already in use when I try to access the Spark viewer?\", \"Why is the Spark viewer not showing the latest run?\", \"How do I find out which port the Spark context is using?\", \"What should I do if I can\\'t access the Spark viewer because it\\'s using a different port?\", \"How do I clean up after myself when a port or container is not working as expected?\"]',\n",
       " 'e5270303': '[\\n\"What are the possible causes of \\'java.lang.NoSuchMethodError: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner\\' during a repartition call in PySpark using Conda installation?\",\\n\"Why would I encounter the error \\'RuntimeError: Java gateway process exited before sending its port number\\' in PySpark, and what role does setting the \\'java_home\\' environment variable play in this scenario?\",\\n\"How do I resolve the issue where the Java gateway process exits before sending its port number in PySpark, and is setting the \\'java_home\\' environment variable sufficient?\",\\n\"What is the recommended solution to the \\'java.lang.NoSuchMethodError: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner\\' error I\\'m experiencing in PySpark during a repartition call, and how does it relate to my Conda installation?\",\\n\"Why are some JReleaser links referenced in the original answer, and how do they provide additional information about the reported errors?\"\\n]',\n",
       " 'cabe8a5b': 'Here are the 5 questions a student might ask based on the FAQ record:\\n\\n[\"How do I resolve the issue of Spark failing when reading from BigQuery and using the .show() method on SELECT queries?\", \"Can you provide more details about the jar file mentioned in the solution: gcs-connector-hadoop-2.2.5-shaded.jar?\", \"Why do I need to specify the BigQueryProjectId, BigQueryDatasetLocation, and parentProject in the SparkSession configuration?\", \"How do I create a SparkSession for reading data from BigQuery and using the .show() method?\", \"What are the recommended settings for spark.driver.memory, spark.executor.memory, and spark.memory.offHeap.size in the SparkSession configuration?\"]',\n",
       " 'e3c0f777': '[\\n  \"How can we configure the SparkSession to automatically download and set up the BigQuery connector when starting a new Spark application?\",\\n  \"Is it necessary to manually download and add the required jar files for the BigQuery connector when we use a SparkSession?\",\\n  \"What is the recommended way to create a SparkSession with the BigQuery connector using the config spark.jars.packages option?\",\\n  \"How can we retrieve more information about the Spark BigQuery connector automatic configuration process?\",\\n  \"Do we still need to manually manage the dependency jars for the BigQuery connector when using the config spark.jars.packages option?\"\\n]',\n",
       " '50c009ef': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How do I download the Cloud Storage connector for Hadoop?\",\\n\"What are the extra steps to read from GCS with PySpark?\",\\n\"Why do I need to create a /jars directory in my Spark directory?\",\\n\"What imports do I need to add to my Python script to use the Spark Cloud Storage connector?\",\\n\"How do I set up the configurations before building my SparkSession?\"',\n",
       " '3fe85b16': '[\"How can I read a small number of rows from a parquet file into a pandas dataframe using PyArrow?\", \"How do I sort the data after reading it from the parquet file?\", \"Can I use Spark to read the parquet file without converting it to a pandas dataframe?\", \"Is it possible to read only a specific column from the parquet file?\", \"What is the difference between using PyArrow to read the parquet file and using Spark to read it?\"]',\n",
       " '0fe0c76a': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What if I encounter a data type error when creating a Spark DataFrame with a specified schema?\", \"Why is this error occurring when using the parquet file from the TLC website?\", \"Why are the PULocation and DOLocationID defined as IntegerType?\", \"How can I resolve this data type error?\", \"What changes should I make to my schema definition to fix this issue?\"]',\n",
       " '18c5bafe': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\"Is there a way to remove white spaces from all column names in a Pyspark dataframe?\", \\n\"What is the best method to remove white spaces from specific column names in a Pyspark dataframe?\", \\n\"Can you provide an example of how to remove white spaces from a column name in a Pyspark dataframe?\", \\n\"Is it possible to remove multiple white spaces from a column name at once in Pyspark?\", \\n\"Is there a shortcut to remove white spaces from all column names in a Pyspark dataframe?\"',\n",
       " '59e86b40': 'Here are the questions the student might ask:\\n\\n[\\n\"What is the reason behind the AttributeError when creating a DataFrame object?\",\\n\"How do I resolve the issue with pandas version 1.5.3?\",\\n\"What alternative method can I use if downgrading pandas is not an option?\",\\n\"Is this problem specific to a particular Spark version?\",\\n\"What versions of Spark and pandas are compatible and do not produce the error?\"',\n",
       " '1ac3ea8f': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n{\"question1\": \"Why do I get an AttributeError when trying to use iteritems on a DataFrame?\", \\n\"question2\": \"How do I install pandas 2.0.1 to resolve this issue?\", \\n\"question3\": \"Can I use a newer version of pandas 2.1.0 instead of 2.0.1?\", \\n\"question4\": \"What should I put in my environment variable when setting SPARK_HOME?\", \\n\"question5\": \"Do I need to set PATH as well, and what does this do?\"}',\n",
       " 'e04529ac': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I start the Spark standalone mode on Windows?\", \"What command do I use to start a worker node in Spark standalone mode?\", \"Can I run the worker node on a different machine?\", \"What is the purpose of the --host option when starting a worker node?\", \"How do I access the Spark UI after setting up Spark standalone mode?\"]',\n",
       " 'a602a7f8': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\\n\"Can I automate the export command every time I start a new Linux session?\",\\n\"How do I add the export command to my .bashrc file?\",\\n\"What is the use of the .bashrc file in Linux?\",\\n\"Why do I need to run the init() function after importing findspark in Python?\",\\n\"Can I add the export command to the beginning of my home book instead of .bashrc file?\"',\n",
       " '9336ce2c': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What does it mean when my compressed file ends before the end-of-stream marker is reached?\", \"How can I fix compressed files that are incomplete?\", \"Is it common to encounter issues with compressed files?\", \"What should I do before trying to create a head.csv file?\", \"Can you give an example of a command I can use to unzip a file?\"]',\n",
       " 'bac4e0f7': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why does Alexey\\'s code from Video 5.3.3 gzip the CSV files when downloading them from the NYT website?\", \\n\"Why is zcat command not working properly when used on the downloaded CSV files?\", \\n\"What does it mean when the zcat output is gibberish?\", \\n\"Can we not gzip the files downloaded from the course repo?\", \\n\"Why do we need to avoid compressing the files downloaded from the course repo?\"]',\n",
       " '13dad632': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"Could you suggest why the error \\'PicklingError: Could not serialise object: IndexError: tuple index out of range\\' occurs while running spark.createDataFrame(df_pandas).show()?\",\\n\"What could be the possible reason for this error?\",\\n\"How do I ensure I\\'m running the correct Python version?\",\\n\"What is the minimum Python version supported by Spark?\",\\n\"How can I switch between Python versions on the virtual machine?\"',\n",
       " 'ddc3c75b': '[\"What location does the script specify for the GCP credentials?\",\\n\"What should I do if Spark cannot find my GCP credentials?\",\\n\"Why doesn\\'t Spark find my GCP credentials, even though I\\'m sure they\\'re in the correct location?\",\\n\"How do I ensure that my GCP credentials are in the correct location for Spark?\",\\n\"What happens if Spark can\\'t find my GCP credentials, and what are the possible consequences?\"',\n",
       " '095b667f': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What are the steps to build Bitnami Spark Docker image?\", \\n\"How do I update the Java and Spark version in the Dockerfile?\", \\n\"What services are included in the provided `docker-compose.yml` file?\", \\n\"What are the environment variables set for each service in the `docker-compose.yml` file?\", \\n\"How do I access Jupyter notebook in the Spark setup?\"]',\n",
       " '56a67c23': 'Here are 5 questions based on the FAQ record:\\n\\n[\"How do I install the necessary library to read data from GCS with pandas?\", \"Can I use a different method to connect to GCS instead of using the default uri path?\", \"Do I need to specify the filename explicitly when using pandas.read_csc?\", \"What if I encounter an error when trying to read data from GCS with pandas?\", \"Is it possible to read data from GCS without installing any additional libraries?\"]',\n",
       " '7fed7813': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why do I get a TypeError when using spark.createDataFrame on a pandas df?\",\"Could you clarify why DataFrame cannot merge types?\", \"How can I avoid the error when creating a pyspark DataFrame from a pandas df?\", \"Is there a way to handle null values in the pandas df before converting it to a pyspark df?\", \"How does setting inferSchema to true while reading a csv file help in this situation?\"]',\n",
       " 'a0e7e259': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What does the error memory manager total allocation exceeds 95.00% mean?\", \"Why did this error occur when working with the homework dataset?\", \"How can I solve the memory related error in pyspark?\", \"Is it possible to overwrite the default executor memory in pyspark?\", \"Do I need to restart the Jupyter session after increasing the executor memory?\"]',\n",
       " '4ca14331': 'Here are 5 questions that might be asked based on the FAQ record:\\n\\n[\"How can I run a standalone Spark cluster on Windows OS if I\\'ve set up my SPARK_HOME variable?\",\\n\"How do I run a standalone Spark cluster on Windows OS if I didn\\'t set up my SPARK_HOME variable?\",\\n\"Can I create a local Spark cluster on Windows OS?\",\\n\"What is the command to start the Spark Master on Windows OS?\",\\n\"Can I start a cluster by running org.apache.spark.deploy.worker.Worker on Windows OS?\"]',\n",
       " '6fdd09eb': 'Here are 5 questions this student might ask:\\n\\n[\"Why do I need to restart the shell after updating my .bashrc file?\", \"What is the difference between \\'source ~/.bashrc\\' and \\'exec bash\\'?\", \"Why didn\\'t my ENV variables get applied when I opened a new notebook in VS Code?\", \"How do I configure EVN variables in VS Code instead of .bashrc?\", \"Can I use both .bashrc and .env files at the same time?\"]',\n",
       " '64bfb2c3': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How do I connect to the GCP VM outside of VS Code?\", \\n\"What is the format of the ssh command for port forwarding?\", \\n\"Can I use the same user name and IP address as the ones provided in the answer?\", \\n\"Why do I need to port forward if I\\'m only accessing the VM from within VS Code?\", \\n\"Is there any specific reason why the answer does not mention using Visual Studio, instead suggesting port forwarding via ssh?\"\\n]',\n",
       " '33dd4516': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"Why is wc -l showing different results than the video, even though I\\'m using the same file name?\", \\n\"Why am I getting different output when using a compressed file with wc -l?\", \\n\"I\\'m getting different results when running wc -l on a compressed file, can you explain why?\", \\n\"What is causing the discrepancy in results when comparing wc -l output to the video?\", \\n\"Why does unzipping the file and rerunning wc -l fix the issue and give the correct results?\"',\n",
       " '504b8570': 'Here are the 5 questions:\\n\\n[\"When I try to run spark-submit, I get errors like WARN: Your hostname, <HOSTNAME> resolves to a loopback address.. What does this message mean?\",\\n\"Can I just copy and paste the code as it is in the FAQ or I need to make changes?\",\\n\"Why do I get a SparkException: Master must either be yarn or start with spark, mesos, k8s, or local at ... error when running spark-submit?\",\\n\"What do I replace with `--master=\"{$URL}\"` to fix the error?\", \"How do I change the log level in spark-submit if I need to set it to something other than \\'WARN\\'?\"',\n",
       " '42e933c5': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What should I do if I encounter an exception \\'java.lang.UnsatisfiedLinkError: org.apache.hadoop.io/nativeio/NativeIO$Windows.access0(Ljava/lang/String;I)Z\\' when trying to write to parquet?\"\\n\"How do I fix this issue if I\\'m running on Windows?\"\\n\"Why am I getting this error and what are the solutions?\"\\n\"What are some additional tips to resolve this error, besides setting the HADOOP_HOME variable?\"\\n\"What should I do if I\\'m still having trouble after setting the HADOOP_HOME variable and adding it to the PATH variable?\"',\n",
       " 'fe9240b0': 'Here are the 5 questions the student might ask:\\n\\n[\\n\"How do I resolve the Java.io.IOException error when running \\'C:\\\\hadoop\\\\bin\\\\winutils.exe\\'?\", \\n\"Why do I get a CreateProcess error=216 when trying to run \\'C:\\\\hadoop\\\\bin\\\\winutils.exe\\'?\", \\n\"What special steps do I need to take to resolve this version compatibility issue between Hadoop and Windows?\", \\n\"Should I replace all files in the local Hadoop bin folder, or just some specific files?\", \\n\"Where can I find more information about this compatibility issue and potential solutions?\"',\n",
       " 'c0a46e5d': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I fix the error that says the project property is not set when using gcloud dataproc jobs submit pyspark?\", \\n\"What is the required property that is not currently set when using gcloud dataproc jobs submit pyspark?\", \\n\"How do I set the project ID when using gcloud dataproc jobs submit pyspark?\", \\n\"What is the --project flag used for when using gcloud dataproc jobs submit pyspark?\", \\n\"Could I still submit my pyspark job without specifying the project ID?\"]',\n",
       " '943c2466': '[\"How do I run a local cluster Spark in Windows 10 with COMMAND PROMPT?\", \"What is the purpose of running spark-class org.apache.spark.deploy.master.Master and spark-class org.apache.spark.deploy.worker.Worker?\", \"What should I enter as the URL when running spark-class org.apache.spark.deploy.worker.Worker?\", \"How do I create a new Jupyter notebook to interact with the local Spark cluster?\", \"Can I see the Spark UI and what information can I find there?\"]',\n",
       " 'f41ef231': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\"Why am I getting an LServiceException error saying I don\\'t have storage.objects.list access to my Google Cloud Storage bucket?\", \"How do I resolve the error \\'Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket\\'?\", \"What\\'s the reason behind the \\'Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket\\' error?\", \"How do I log in to Google Cloud to resolve the \\'Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket\\' error?\", \"Why do I need to set the project ID before uploading the pq dir to a GCS Bucket?\"]',\n",
       " '6b26d73c': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"Can you explain why submit a job in Dataproc might throw a Py4JJavaError in the log panel?\",\\n\"Why did choosing a specific Versioning Control when creating a cluster help to resolve the Py4JJavaError?\",\\n\"What was the reason for switching from Debian-Hadoop-Spark to Ubuntu 20.02-Hadoop3.3-Spark3.3 for the cluster?\",\\n\"Why is it important to choose a Versioning Control option that matches the version used on their laptop?\",\\n\"Can you provide documentation or resources on how to use Versioning Control to resolve Py4JJavaError issues?\"',\n",
       " '830e2936': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\"Why did my data not get partitioned into 6 parts when I used df.repartition(6)?\", \\n\"How can I partition my DataFrame and also control the number of shards in the output?\", \\n\"What happened when I only used df.repartition(6) instead of both repartition and coalesce?\", \\n\"In what order should I use repartition and coalesce when controlling partitioning and coalescing in PySpark?\", \\n\"Does the coalesce method always make the final DataFrame have fewer partitions than repartition?\"]',\n",
       " '02007b7c': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\"Why does Jupyter Notebook not load properly at localhost when port forwarding from VS code?\", \"What is the difference between running ssh command from VS code and from the local machine\\'s cli?\", \"How do I add the forwarding instructions to .ssh/config file?\", \"Why does the connection break after I logout from the session?\", \"Why does my Spark session sometimes fail to run at port 4040 and switch to 4041?\"]',\n",
       " '1ebb9a47': '[\"What are the steps to install Java 11 on Codespaces?\", \"How do I check for available Java SDK versions?\", \"What command should I use to install a specific Java version?\", \"What happens if I\\'m prompted to change the default Java version?\", \"How do I confirm that Java is installed and working properly?\"]',\n",
       " '80125745': '[\"What happens when we encounter the error \\'Insufficient \\'SSD_TOTAL_GB\\' quota. Requested 500.0, available 470.0.\\' while creating a dataproc cluster on GCP?\", \"Why is it common for this error to occur in certain GCP regions?\", \"How can we temporarily resolve the error when it occurs?\", \"Is there a permanent solution to this error in addition to waiting for resources to become available?\", \"Can we change the boot disk type to avoid encountering this error in the future?\"]',\n",
       " 'f01df45b': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I convert the time difference of two timestamps to hours using pyspark?\", \"Can you elaborate on how to manually convert the timedelta object into hours?\", \"What is datediff and how does it help in converting time difference to hours?\", \"Can I use datediff with pyspark\\'s TimestampType values?\", \"How do I get the output in terms of hours when using datediff?\"]',\n",
       " '06014eec': 'Here are 5 questions based on the FAQ record:\\n\\n[\"How do I resolve the PicklingError: Could not serialize object: IndexError: tuple index out of range issue in PySpark?\", \"What are the recommended versions of PySpark and Pandas for resolving the PicklingError error?\", \"What could be the next step if the recommended versions still don\\'t work for me?\", \"Can you provide more information about why the PicklingError occurs in PySpark?\", \"How can I troubleshoot the PicklingError error if it still persists even with the recommended versions?\"]',\n",
       " '54653ca9': '[\\n  \"Why am I getting a Py4JJavaError when trying to run a SparkSession, saying Task 0 in stage 6.0 failed 1 times?\",\\n  \"What is causing the Python worker to fail to connect back to the SparkSession?\",\\n  \"I\\'m trying to fix an org.apache.spark.SparkException: Job aborted due to stage failure error, what\\'s the solution?\",\\n  \"Why do I need to set environment variables for SparkSession beforehand, and how do I do it?\",\\n  \"What are the specific environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON for, and how do they help with connecting to the SparkSession?\"',\n",
       " 'f95304db': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\\n  \"How do I resolve the error when trying to run PySpark with different Python versions between the worker and the driver?\",\\n  \"What is the minimum Python version required to run PySpark? Is it 3.10?\",\\n  \"Can I run PySpark on Dataproc? Are there any pricing considerations?\",\\n  \"How do I ensure that PySpark uses the correct Python version when running on Google Cloud Dataproc?\",\\n  \"What are the correct values for the environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON?\"',\n",
       " '591df4e6': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Is it necessary to have a Google Cloud Platform (GCP) Virtual Machine (VM) running for submitting PySpark jobs to Dataproc?\", \"How do I set up gsutil to submit PySpark jobs to Dataproc from my local computer?\", \"What is the purpose of the cluster name in the Dataproc job submission command?\", \"Can I specify multiple input files for my PySpark job when submitting it to Dataproc?\", \"Is the output of the PySpark job saved to the specified output location on GCS?\"]',\n",
       " '5cb7f597': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"Can I use the iteritems method in pyspark? It seems to be deprecated and giving me an error.\",\\n\"I\\'m trying to set up a Dataproc Cluster in DE Zoomcamp 5.6.3 but I keep getting an insufficient SSD TOTAL GB quota error. What specifications should I use for the cluster configuration?\",\\n\"How much memory can I allocate for the worker nodes in the Dataproc Cluster configuration? I don\\'t want to exceed the maximum limit of 250 GB.\",\\n\"What\\'s the recommended machine type for the master and worker nodes in the Dataproc Cluster configuration?\",\\n\"Why do I need to use specific machine types and primary disk sizes for the master and worker nodes in the Dataproc Cluster configuration?\"',\n",
       " 'c5de1f96': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"How do I configure JAVA_HOME on my Apple Silicon Mac since the MacOS setup instruction is for Intel-based Macs?\", \"Why is there a difference in JAVA_HOME configuration between Apple Silicon and Intel-based Macs?\", \"Will I need to update my .bashrc or .zshrc file to set JAVA_HOME on Apple Silicon Mac?\", \"What is the correct format for setting JAVA_HOME on Apple Silicon Mac, and what should I do after setting it?\", \"How can I verify that my JAVA_HOME path is correctly set, and what output should I expect to see when running the command \\'which java\\'?\"]',\n",
       " '70ac8e80': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"How do I ensure that my docker-compose.yaml file is correctly configured for the \\'control-center\\' service?\",\\n\"What details do I need to check in my docker-compose.yaml file to fix the issue with starting the \\'control-center\\' container?\",\\n\"I\\'m using Mac OS Monterey; how did others on the same OS resolve the issue with starting the Kafka Control Center?\",\\n\"Why are there still running Docker images from week 4 that I don\\'t see when I type \\'docker ps\\'?\",\\n\"What else should I check in my configuration apart from the service name, image name, ports, volumes, and environment variables to resolve the issue with starting the Kafka Control Center?\"',\n",
       " 'f6551ffb': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What do I need to do when \\'Module “kafa” not found\\' error occurs when trying to run producer.py?\", \"How do I ensure that I have all the necessary packages installed?\", \"What are the steps to create a virtual environment from scratch?\", \"Do I need to run `python -m venv env` every time I need to use the virtual environment?\", \"Is there a difference in the activation process for Windows users compared to MacOS and Linux users?\"]',\n",
       " '0ec021de': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\"What is the cause of the error I\\'m encountering when importing the cimpl DLL while running Avro examples?\", \"How do I ensure I\\'m using a compatible Python version with the Avro library?\", \"Why do I have to manually load a specific DLL before importing Avro?\", \"What is the relationship between my OS and Python version installed and the error I\\'m experiencing?\", \"Is there an alternative solution to fixing the DLL load failed error while importing cimpl?\"]',\n",
       " '1edd4630': '[\"Why do I encounter a ModuleNotFoundError when trying to use avro in my Kafka streaming project?\", \"What is the correct way to install confluent-kafka[avro] using pip?\", \"Why doesn\\'t Conda include confluent-kafka[avro] when installing confluent-kafka via pip?\", \"Are there any known issues with Anaconda and confluent-kafka installations?\", \"How can I resolve the \\'cannot import producer from confluent-kafka\\' error?\"]',\n",
       " '4664ae28': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What is the solution to an error while running the python3 stream.py worker command?\",\\n\"Why do I need to reinstall Kafka-python if I encounter an error?\",\\n\"What is the purpose of uninstalling and reinstalling Kafka-python?\",\\n\"Can you explain the difference between Redpanda and Kafka?\",\\n\"What advantages does Redpanda have over Kafka that make it a good alternative?\"\\n]',\n",
       " '676e1b76': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"What are the common reasons for the \\'SIGKILL\\' error while converting data files to parquet format?\",\\n\"Why does the \\'SIGKILL\\' error occur when a docker container runs out of memory?\",\\n\"How can I resolve the \\'SIGKILL\\' error when processing large data files in a docker container?\",\\n\"What is the recommended approach to handle large data files when converting them to parquet format?\",\\n\"Can you provide an example of how to use Pandas to load large data files in chunks and create multiple parquet files?\"\\n]',\n",
       " 'a3c84279': '[\"Why is the rides.csv file missing in the Python example?\", \"Can we use the same rides.csv file from the Java example in the Python example?\", \"How do we get the rides.csv file for the Python example?\", \"Is rides.csv file specific to Java or can it be used in Python too?\", \"Can I use a different file instead of rides.csv for Python?\"]',\n",
       " '119c917d': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"How can I improve the audio quality of the Kafka-Python video tutorials?\", \\n\"What is the purpose of the rides.csv file used in the producer.py python programs?\", \\n\"Can you explain the rides.csv data in more detail?\", \\n\"Is there a way to increase the audio volume of the videos without downloading them?\", \\n\"Why are the audio quality of the Kafka-Python video tutorials poor and how to fix them?\"]',\n",
       " 'f1284c1f': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why do I get the kafka.errors.NoBrokersAvailable error?\", \"How do I know if my kafka broker docker container is working?\", \"What can I do if my kafka broker docker container is not working?\", \"Can I use a different command to start the instances?\", \"Is there a specific folder where I should run the docker compose command?\"]',\n",
       " '49a7db28': '[\\n\"Can you elaborate on how to think about scaling from the consumer end for Kafka?\",\\n\"What are the options that support horizontal scaling more than the others in Kafka?\",\\n\"In the context of scaling, what does Ankush mean by \\'consume message via horizontal scaling\\'?\",\\n\"Is horizontal scaling a common approach for consuming messages in Kafka?\",\\n\"How does horizontal scaling differ from other scaling concepts in Kafka?\"',\n",
       " '196cb0f2': '[\"What are the steps to fix the Docker Compose error when trying to pull the spark-3.3.1 image?\", \"Why do I get the error pull access denied for spark-3.3.1 when running Docker Compose?\", \"How can I resolve the error Error response from daemon: pull access denied for spark-3.3.1, repository does not exist or may require \\'docker login\\': denied?\", \"What could be the cause of the error response from daemon when running Docker Compose with spark-3.3.1?\", \"Why do I need to build the sparks and juypter images before running Docker Compose?\"]',\n",
       " '1e50eab7': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I resolve the permission issue when running ./build.sh in Python Kafka?\", \"What is the command I can use to fix the permission error for the build.sh file?\", \"Can you help me with the chmod command to resolve the error?\", \"Why do I get a Permission denied error when running ./build.sh in Kafka?\", \"How to resolve the error by changing the permissions of the build.sh file?\"]',\n",
       " 'a7a6d0d7': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"How to resolve \\'KafkaTimeoutError: Failed to update metadata after 60.0 secs.\\' issue when running stream-example/producer.py?\",\\n\"What are the steps to take when faced with \\'KafkaTimeoutError: Failed to update metadata after 60.0 secs.\\' error\",\\n\"What should I do if I encounter \\'KafkaTimeoutError: Failed to update metadata after 60.0 secs.\\' while running the stream-example/producer.py?\",\\n\"Can you provide a solution for the \\'KafkaTimeoutError: Failed to update metadata after 60.0 secs.\\' issue in the Python Kafka stream example\",\\n\"Why do I get \\'KafkaTimeoutError: Failed to update metadata after 60.0 secs.\\' when running the stream-example/producer.py in the Kafka module\"',\n",
       " '0996213a': 'Here are 5 questions this student might ask:\\n\\n[\\n\"What is the reason for getting \\'Application has been killed. Reason: All masters are unresponsive! Giving up\\' error while running the ./spark-submit.sh streaming.py command?\", \\n\"Why did the error occur when running the ./spark-submit.sh streaming.py command in tutorial 13.2?\", \\n\"What is the importance of the Spark version in this error and how can I check my local machine\\'s Spark version?\", \\n\"What is the solution to resolve the issue of \\'Cannot call methods on a stopped SparkContext\\' error in Py4JJavaError?\", \\n\"Why is it necessary to downgrade PySpark to 3.3.1 to resolve the connection issue in spark-master?\"',\n",
       " '311bf368': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What should I do if Spark master connection fails when running ./spark-submit.sh streaming.py?\", \"How can I check the logs of spark-master when Spark master connection fails?\", \"Can I use Docker to troubleshoot Spark master connection issues?\", \"What is the CONTAINER ID in the docker ps command?\", \"Where can I find more information about an error message I found in the spark-master.out log file?\"]',\n",
       " 'c1551650': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How do I check my Java version?\", \"What could be the problem if I\\'m using Java 8 but need Java 11?\", \"How do I select a specific Java version as default if I already have it installed?\", \"Can I use other versions of Java 11 besides the one mentioned in the answer?\", \"Why do I get an error when trying to use Kafka with Spark?\"]',\n",
       " 'f9b673cf': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"Why do I get the error \\'package xyz does not exist\\' when running <project_name>-1.0-SNAPSHOT.jar, even after I\\'ve built the project using gradle?\",\\n\"Why are the dependencies in my build.gradle not being included in the final jar file?\",\\n\"I\\'ve added all the necessary dependencies in my build.gradle, but I still get errors. What\\'s the issue?\",\\n\"I don\\'t understand why I need to add a \\'shadowJar\\' block in my build.gradle file. Can you explain?\",\\n\"How do I run my Java Kafka project after adding the \\'shadowJar\\' block in my build.gradle file?\"',\n",
       " '5479dce2': 'Here are the 5 questions the student might ask:\\n\\n[\"Can I install confluent-kafka and fastavro using conda instead of pip?\", \"Is it possible to install Faust Library for Python Version due to dependency conflicts?\", \"Do I need to know Java to complete this module, and are there any alternative videos?\", \"Can I skip the Java videos and just watch the Python videos?\", \"Is the Faust repository and library still maintained?\"]',\n",
       " '02cf2317': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I run the Kafka producer in the terminal? Can I run it in the same way as the consumer? Is it necessary to have a specific directory structure for running these programs? Are there any specific package or module dependencies for running Kafka with Java? Can I run multiple Kafka components simultaneously in the terminal?\"]',\n",
       " '947c07a6': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why do I get 0 results when running my Kafka consumer?\", \"How do I troubleshoot authentication issues with my Kafka producer?\", \"Why am I not receiving any messages with my Kafka consumer?\", \"What could be causing my Kafka producer to fail with a \\'SaslAuthenticationException\\'?\", \"How can I fix issues with my Kafka bootstrap server URL?\"]',\n",
       " 'bea22953': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"Can you show me how to find the triangle icon next to each test in VS Code?\",\\n\"What do I need to do to see the tests being picked up by VS Code?\",\\n\"I\\'m having trouble finding the \\'JAVA PROJECTS\\' collapsable, can you tell me where to look?\",\\n\"Why do I need to clean the workspace in VS Code to make the tests visible?\",\\n\"Can I also add classes and packages in this window or only in the project directory?\"',\n",
       " 'a1603359': '[\"What is the URL for schema registry in Confluent Cloud, and how do I access it?\", \"In Confluent Cloud, where can I find the Stream Governance API?\", \"To access Stream Governance API, which navigation option should I choose in the default environment?\", \"What should I do to create credentials for schema registry URL?\", \"Under what section can I find the schema registry URL in Confluent Cloud?\"]',\n",
       " 'a85a6a91': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"How can I verify that my local Spark version is compatible with the one used in the container?\",\\n\"What command do I need to run to check my local Spark version?\",\\n\"Why do I need to match the SPARK_VERSION in the build.sh file to my local Spark version?\",\\n\"Are there any specific instructions for installing pyspark to ensure compatibility with Spark versions?\",\\n\"How do I confirm that my locally-installed pyspark matches the Spark version used in the container?\"',\n",
       " '343864f5': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n  \"How do I fix the error \\'ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\"?\",\\n  \"I\\'m getting a \\'ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\\' error. Can you help me resolve it?\",\\n  \"Is there a specific way to solve the \\'No module named \\'kafka.vendor.six.moves\\'\\' error in the project?\",\\n  \"What is the solution to the \\'ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\\' issue when using the Kafka module?\",\\n  \"How can I resolve the \\'No module found named \\'kafka.vendor.six.moves\\'\\' error that is stopping my project?\"',\n",
       " '6cb3b4a9': '[\"How do the 3 students that will be evaluating my capstone project get assigned?\", \"Will I be grading projects from the same students that will be evaluating mine?\", \"What if I don\\'t comply with the peer review criteria?\", \"How is the final grade determined, exactly?\", \"What happens if my median score is really low?\"]',\n",
       " '5959ea3c': '[\"What do you mean by there being only one project in this Zoomcamp?\", \"Why are we given two chances to pass the course?\", \"Can you explain the reason for two separate attempts?\", \"Do we have to submit two separate projects?\", \"Are the two attempts used in different ways?\"]',\n",
       " '202af70b': '[\"Can someone recommend a list of nice and relatively large datasets for the project?\", \"How do I access a list of datasets for the project?\", \"Is it possible to find nice and large datasets for the project?\", \"Are there any datasets available that are suitable for the week 7 project?\", \"Where can I find a list of relatively large datasets for the week 7 project?\"]',\n",
       " 'f2705fe7': '[\\n\"How do I ensure that my python script runs as a start-up program on my computer?\",\\n\"What is the correct way to configure my python environment variable for the user account?\",\\n\"I am trying to use python as a start-up script. Can you provide guidance on how to make this work?\",\\n\"In order to make python run as a start-up script, do I need to redefine the environment variable for my user account?\",\\n\"How do I make python run at start-up without having to specify the path to the python executable?\"',\n",
       " '74f412c4': 'Here are 5 questions based on the FAQ record:\\n\\n[\\n\"How do I initiate a Spark Session in Spark Streaming?\", \\n\"What are the differences between query1.start() and query3.start().awaitTermination()?\", \\n\"Can I use query1, query2, and query3 as separate variables in the same Spark Session?\", \\n\"How do I make query1, query2, and query3 read from the same Spark Session?\", \\n\"Will awaitAnyTermination() also stop the Spark Session if all queries receive a kill signal or error failure?\"',\n",
       " '5214eb93': '[\\n\"How do I transfer data from Databricks to Azure SQL DB?\", \\n\"What is the recommended approach for moving transformed data from Databricks to Azure?\", \\n\"Can we load data directly from Databricks to Azure SQL DB?\", \\n\"What are the intermediate steps involved in moving data from Databricks to Azure SQL DB?\", \\n\"Is it possible to bypass Azure SQL DB and move data somewhere else first?\"',\n",
       " '3cfd16a7': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"How do I add a dbt job manually to the trial dbt account?\",\\n\"What do I need to provide to run the dbt job using Airflow?\",\\n\"Can I use Airflow\\'s dbt operator without setting up a dbt Cloud job?\",\\n\"Why do I need to be careful not committing the API key to Github?\",\\n\"What is the purpose of the example code available on GitHub for dbt Cloud integration?\"',\n",
       " 'a7cecdf9': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What is the best resource for learning how to orchestrate DataProc with Airflow?\", \"Can I use the same service account for both DataProc Administrator and Service Account User roles?\", \"What operators are recommended for working with DataProc in Airflow?\", \"Why do I need to add specific jars when submitting a PySpark job to DataProc?\", \"Is it possible to get project output in a format other than Parsable JSON from Airflow?\"]',\n",
       " '2aad1011': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\"How do I get my dbt Cloud API key?\", \"Why do I need to add the dbt_api_trigger to my .env file?\", \"Can I trigger a dbt job from any job page in dbt Cloud?\", \"What is the purpose of the \\'cause\\' parameter in the HTTP request body?\", \"How do I parse the response from the API to validate the job trigger was successful?\"]',\n",
       " 'cb478996': '[\"How will the project evaluation for reproducibility be carried out taking into account that even with thorough documentation, the reviewer may not be able to follow up, considering the complexity of the project?\", \"What kind of effort is expected in terms of trying to re-run the entire project, and are there any alternatives for those who cannot dedicate that amount of time?\", \"Will sufficient understanding of the code and its functionality be considered a satisfactory evaluation of reproducibility, or will re-running the entire project still be necessary?\", \"To what extent will the reviewer be expected to review the project and identify potential errors or missing instructions?\", \"If we cannot reproduce the results due to lack of resources or time, will it still be considered a successful project evaluation?\" ]',\n",
       " 'b4ef8ca7': '[\"How does Azure key vault store and manage credentials or passwords\",\\n\"What types of tech stacks are supported in Azure key vault\",\\n\"Can I store a password in Azure key vault to use in a SQL database\",\\n\"In what scenarios should I use Azure key vault instead of hardcoding credentials\",\\n\"Is Azure key vault a secure solution to store sensitive information\"]',\n",
       " '8e74f943': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What does ModuleNotFoundError: No module named \\'py4j\\' mean while executing import pyspark?\", \"How do I get the version of py4j from inside a spark docker?\", \"Is there a command to list the files in a specific path within a container?\", \"How do I access a specific container within a docker exec command?\", \"Can I use the docker exec command to perform multiple actions or is it limited to one command?\"]',\n",
       " 'a73ed357': '[\\n\"Can you tell me why I\\'m getting an error with psycopg2, saying it\\'s incompatible with my environment?\",\\n\"What is the difference between using conda and pip for managing virtual environments?\",\\n\"I\\'m using conda for my virtual environment, how can I install psycopg2 without getting the compatibility error?\",\\n\"Why am I getting an error when I\\'m using both conda and pip together for my virtual environment?\",\\n\"What is the normal install command for psycopg2 when using pip?\"',\n",
       " 'd5b6ef5d': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\"How do I set up dbt locally with Docker and Postgres?\",\\n\"How can I link my dbt_project.yml file with the profile name in profiles.yml?\",\\n\"What command should I use if I encounter troubles while running dbt?\",\\n\"Can I use the same Docker instance from week 2 for dbt setup?\",\\n\"How do I clone dbt-starter-project and set it up in my local project?\"]',\n",
       " 'b406d90e': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"How do I get started with Pyspark for my project? Can you show me an example of how to initialize a SparkSession?\", \"Can I use Pyspark with other big data tools besides BigQuery? How do I configure them?\", \"What is the significance of including a specific configuration in the SparkSession initialization?\", \"How do I handle errors or exceptions when working with Pyspark and BigQuery data?\", \"Can I use multiple BigQuery datasets with Pyspark, and how do I specify which ones to use?\"]',\n",
       " '0002ab8b': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"How do I install the astronomer-cosmos package as a dependency to run my dbt-core project as an Airflow Task Group on Google Cloud Composer?\", \"How do I create a new folder for my dbt-core project inside the dags/ folder of my Composer GCP bucket?\", \"How do I configure my profiles.yml to authenticate with a service account key?\", \"How do I create a new DAG using the DbtTaskGroup class and ProfileConfig?\", \"How do I ensure my dbt lineage graph appears as tasks inside a task group like this:\"]',\n",
       " '138b55c7': '[\\n  \"Can I edit the name that appears on the leaderboard, and if so, how do I do it?\",\\n  \"Do I need to use my real name on the Leaderboard?\",\\n  \"What is the Certificate name used for, and should it be the same as the Display name?\",\\n  \"Can I use data from external sources, like the NY Taxi data website, in BigQuery?\",\\n  \"Are there other data stores supported by BigQuery besides Bigtable, Cloud Storage, and Google Drive?\"',\n",
       " '154d7705': 'Here are the 5 questions the student might ask:\\n\\n[\\n\"What command do I need to execute to install the necessary dependencies?\",\\n\"Can I install the dependencies online or is it necessary to do it locally?\",\\n\"Why do I need to install duckdb pip before the dlt[duckdb] package?\",\\n\"What if I encounter issues during the installation process?\",\\n\"Is there a specific order in which I should install the packages?\"',\n",
       " 'f96517d9': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\"What other packages do I need to download for the workshop?\", \"Why do I need to install Jupyter separately?\", \"What do I do if I\\'m not using Codespace or a Virtual Environment?\", \"Will I need to install Jupyter for every workshop?\", \"Is installing Jupyter required only at the beginning of the course?\"]',\n",
       " '773587dd': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How do I integrate DuckDB In-Memory database with our DLT project?\", \\n\"Can I use DuckDB In-Memory database for storage in DLT?\", \\n\"Is it possible to replace the default DLT storage with DuckDB In-Memory database?\", \\n\"Can I use a different storage option than the default one in DLT, like DuckDB In-Memory database?\", \\n\"What is the best way to switch from one storage option to another in DLT, specifically to DuckDB In-Memory database?\"',\n",
       " '73aff710': 'Here are 5 questions based on the FAQ record:\\n\\n[\"Why is the total sum of ages of all people loaded in Exercise 3 too big to be in the given choices?\"]; \\n\"How do I filter out the people whose occupation is equal to None to get an answer that is close to or present in the given choices?\"]; \\n\"What does the raw string `r\\'file:///content/.dlt/my_folder\\'` mean in the `os.environ` command and what is its purpose?\"]; \\n\"Why is the \\'Load to Parquet file\\' section saving DLT files directly to my C drive, despite changing the file path?\"]; \\n\"How do I get the list of all Parquet files in the specified folder, and what does the `glob.glob` command do in this context?\"]',\n",
       " '0728ca67': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What should I do if I get a \\'no such file or directory: command.sh\\' error when running command.sh?\", \"How can I troubleshoot if I don\\'t find command.sh in the root folder of my repository?\", \"What if I\\'m not sure if I cloned the correct repository? Can you verify it for me?\", \"I tried using \\'ls\\' to check the contents of my repository, but command.sh is missing. What\\'s my next step?\", \"Can you provide more information on the repository URL I can clone to get the correct files?\"]\\n\\nNote: I\\'ve tried to rephrase the questions while keeping them concise and complete, using as few words as possible from the original FAQ record.',\n",
       " '49a51e24': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\"Why are we using psql instead of pgcli?\", \"How do I install usql on my macOS using homebrew?\", \"Is usql compatible with PostgreSQL?\", \"Can you provide more information on how to install usql?\", \"How do I use usql to run a sql script into the DB?\"]',\n",
       " 'f0d552a7': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What should I do if I encounter an error saying \\'docker-compose\\' not found when running the \\'source command.sh\\' file?\"]\\n[\"How do I install or fix the \\'docker-compose\\' command?\"]\\n[\"Why do I need to run \\'docker-compose\\' without a hyphen?\"]\\n[\"Can I keep the hyphen in the \\'docker-compose\\' command in the command.sh file?\"]\\n[\"How do I provide the output in parsable JSON when running the \\'start-cluster\\' function?\"]',\n",
       " '9c750080': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why do I keep getting an \\'Invalid top-level property x-image\\' error when trying to start my cluster?\", \"What are the valid top-level sections for a Compose file?\", \"How do I fix this \\'x-image\\' error in my Compose file?\", \"Is it possible to use a different Compose file version?\", \"Can I use a different operating system to avoid this issue on Ubuntu?\"]',\n",
       " '6f4998e6': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Is it expected that the records are being ingested 10 at a time into the RisingWave database?\", \"How does the script change the date timestamp in the records?\", \"Why do we need to observe changes in real-time while working on the queries?\", \"Is it possible to increase the number of records ingested at a time?\", \"How can we copy the queries while the stream-kafka script is running in the background?\"]',\n",
       " '97170587': '[\\n  \"Are Kafka installations necessary for the workshop?\",\\n  \"Do we need to set up Kafka for the RisingWave workshop?\",\\n  \"Is Kafka setup a requirement for the RisingWave workshop?\",\\n  \"Can I start this workshop without installing Kafka?\",\\n  \"Do I need to have Kafka installed to follow this tutorial?\"\\n]',\n",
       " '4def6541': '[\"What is the recommended amount of free disk space required for setting up the RisingWave Workshop 2, given that all containers need to be provisioned and psql needs to run and ingest taxi data?\", \"What minimum amount of free disk space is necessary for the psql to run and ingest taxi data in RisingWave Workshop 2?\", \"Can you explain why 7GB is not enough for setting up the containers and running psql in RisingWave Workshop 2?\", \"How much free disk space is required in total for setting up the RisingWave Workshop 2, considering all the containers and psql?\", \"Can we set up RisingWave Workshop 2 with only 7GB of free disk space, considering it\\'s not enough for all the containers to be provisioned and psql to run and ingest taxi data?\"]',\n",
       " '66e117dd': '[\"What are some common issues with running the stream-kafka script and how can I resolve them? How do I specify the correct library for interacting with PostgreSQL databases in the requirements.txt file?\", \"Why do I need to use psycopg2-binary instead of psycopg2? What is the difference between the two?\", \"When working with RisingWave, do I always need to do the source command.sh step after opening a new terminal session? Is there an exception to this rule?\", \"How does the source command.sh step help when running the psql command? What does it do exactly?\", \"Are there any specific libraries or dependencies required for using RisingWave, apart from psycopg2-binary?\" ]',\n",
       " '94fd2476': '[\\n  \"What is the solution to the \\'Could not build wheels for psycopg2\\' error in an Anaconda installation?\",\\n  \"Why is the Conda base not sufficient and GCC needs to be installed at the system level?\",\\n  \"What is the purpose of the GNU Compiler Collection (GCC) in the software development process?\",\\n  \"Can GCC be used for compiling source codes written in languages other than C, C++, Objective-C, and Fortran?\",\\n  \"How does the installation of GCC make it available to other Python environments?\"\\n]',\n",
       " '70d83d78': '[\"What is the terminal I should use when running the seed-kafka command after initial setup?\", \"Is Python venv necessary to be activated before running the seed-kafka command?\", \"What modification needs to be done to the seed_kafka.py file to resolve the Psycopg2 InternalError?\", \"How can I connect to the RisingWave cluster from Powershell?\", \"What is the equivalent of source commands.sh in Powershell?\"]',\n",
       " 'accb7285': 'Here are 5 questions based on the FAQ record:\\n\\n[\\n\"Can you explain why my stream-kafka script keeps stuck in a loop with a \\'Connection Refused\\' error and how to resolve it?\",\\n\"Why is my message_queue service in Docker constantly restarting and how can I fix it?\",\\n\"What do I need to do when running psql -f risingwave-sql/table/trip_data.sql after starting services with docker-compose up to avoid getting a syntax error?\",\\n\"What is the solution when I get the error \\'Could not initialize seastar: std::runtime_error (insufficient physical memory: needed 4294967296 available 4067422208)\\' in the message_queue container?\",\\n\"Why am I getting the error \\'psql:risingwave-sql/table/trip_data.sql:61: ERROR:  syntax error at or near \".\" LINE 60:       properties.bootstrap.server=\\'message_queue:29092\\'\\' when running my script?\"]',\n",
       " 'cbca4495': '[\"What is the exact number of records that I need to process for each homework question?\", \"Do I need a specific number of processed records for the homework answers?\", \"How do I ensure I\\'m processing the correct number of records for the homework?\", \"Is there a minimum or maximum number of records required for the homework?\", \"Will the correct answer be achieved if I process varying numbers of records for each question?\"]',\n",
       " '78fce6ad': '[\"What do I need to do to guarantee consistent results when working with a materialized view that uses a warning about not having order by?\", \\n\"Why don\\'t the answers in the homework match the provided options, and what steps can I take to resolve this issue?\", \\n\"In Homework 1, what does it mean to \\'reset the cluster\\' using the command \\'clean-cluster\\'?\", \\n\"Can I use \\'stream-kafka\\' to seed the data for the homework, or do I need to use \\'seed-kafka\\'?\", \\n\"How can I ensure that I have 100,000 records in my data set for the homework?\"]',\n",
       " '68842c02': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"How do I install PostgreSQL on a Linux-like operating system?\", \"What are the specific commands to add the PostgreSQL repository?\", \"Why do I need to update my APT package index after adding the repository?\", \"How do I check if the PostgreSQL service is running?\", \"What do I do if the PostgreSQL service is not running and I need to start it manually?\"]',\n",
       " '71b1984b': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What should I do if xdg-open doesn\\'t open any browser when I try to open the dashboard?\", \\n\"Why is xdg-open used instead of directly opening the index.html file in the browser?\", \\n\"If I\\'m on WSL, can I still use xdg-open to open the index.html file?\", \\n\"Can I use any other browser instead of w3m to open the dashboard?\", \\n\"Is there an alternative solution to the one mentioned in the provided Stack Overflow thread?\"',\n",
       " 'd452b490': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What does the error indicates when trying to execute a Python script in a Unix-like environment?\", \"Why does the presence of `\\\\r` character suggest a problem with the Python interpreter path?\", \"How can I find the correct Python interpreter path in Unix-like environments?\", \"Why do line endings need to be converted from Windows-style to Unix-style?\", \"How can I use the dos2unix command-line tool to convert the line endings of a script?\"]',\n",
       " '707cae8f': '[\"What kind of boundaries can I define for windowing in SQL?\", \"Is windowing in SQL used for analyzing and aggregating data only?\", \"How does windowing in SQL help with managing and organizing streaming data?\", \"Can I use windowing in SQL with time intervals or only with the number of events received?\", \"Is windowing in SQL limited to specific types of data processing?\"]',\n",
       " 'ffbf3311': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\"How do I resolve the \\'ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\\' when running \\'from kafka import KafkaProducer\\' in Jupyter Notebook for Module 6 Homework?\", \"Why are individual blocks in the Mage pipeline running successfully, but failing when executed as a whole?\", \"How do I resolve the issue of failed blocks in the Mage pipeline when GCP credentials are not being recognized?\", \"How do I set the path to profiles.yml and dbt_project.yml in Mage pipeline?\", \"Can I use dbt pipeline to perform data partitioning and clustering in BigQuery?\"]',\n",
       " '3916f4a9': '[\"How do I create a Docker image from a base image and attach it to a stopped or running container?\", \"What are the basic Docker commands for listing and managing Docker images and containers?\", \"How do I install Docker on Google Cloud VM Instance and run it without SUDO permissions?\", \"Can I use Docker to copy files between my local machine and a container or a GCP VM?\", \"How do I create a Docker image using a Dockerfile and what are the best practices for Dockerfile development?\"]',\n",
       " '0227b872': '[\"How do I find the information for signing up for the course?\", \"Is the link to the GitHub repository provided for the data engineering course\", \"Is the purpose of the FAQ to capture frequently asked technical questions\", \"How can I access the GitHub repository that has a link\", \"What document did the course structure its questions and answers like\"]',\n",
       " '39fda9f0': '[\"Will the course videos be live?\", \"When can I start watching the course?\", \"Are the office hours sessions recorded as well?\", \"How do I access the course videos and office hours?\", \"When will the office hours take place?\"]',\n",
       " '5170565b': '[\"What if I am unable to attend a live session?\", \"Can I still ask questions even if I miss a session?\", \"How can I catch up if I miss a session?\", \"Can I request topics to discuss during live sessions?\", \"How can I ask questions when I miss a session?\"]',\n",
       " 'ecca790c': '[\"What is the minimum amount of theory you will cover in the course?\", \"Is it possible to derive the gradient update rule for logistic regression in this course?\", \"Will you cover the theory of Python in this course?\", \"Will we be focusing on understanding the results of the theory?\", \"Will there be other courses that will derive the gradient update rule for logistic regression?\"]',\n",
       " 'c25b3de4': '[\"Can I still take the course even if I don\\'t know math and what resources will help me catch up with linear algebra?\", \"Will there be a lot of math formulas in the course?\", \"Are the math topics really complex?\", \"How will I know when to ask the community for help with a math question?\", \"Is it possible to learn linear algebra on my own before taking the course?\"]',\n",
       " '6ba259b1': '[\\n\"I filled the course registration form but haven\\'t received a confirmation email, what should I do?\",\\n\"I didn\\'t receive a confirmation email after filling up the registration form, is it because of my email provider\",\\n\"What if I haven\\'t received a confirmation email even after checking my promotions tab and spam?\",\\n\"Why haven\\'t I received a confirmation email despite filling the registration form, could it be because I unsubscribed from the newsletter?\",\\n\"I haven\\'t received a confirmation email after filling the form, why is this normal and what should I do instead?\"',\n",
       " '67e2fd13': '[\\n\"How long is the course and is it a fixed duration?\",\\n\"Can I take longer than the recommended 4 months to complete the course?\",\\n\"What does \\'approximately 4 months\\' mean and how much extra time can I take?\",\\n\"Does the extra time affect my grade or certification?\",\\n\"Is there a deadline or a cutoff date after which I cannot finish the course?\"',\n",
       " 'a6897e8c': '[\"What is the ideal weekly time commitment for students in this course?\", \"Can I adjust the pace of the course to fit my schedule?\", \"How does the course provider determine the recommended weekly time commitment?\", \"Will I need to set aside dedicated time for studying each week?\", \"Is it possible to spend more time on certain aspects of the course?\", \"What if I have a full-time job, can I still complete the course?\"]',\n",
       " '2eba08e3': '[\"Will I get a certificate if I complete only two projects?\", \"Do I need to finish all three projects to get the certificate?\", \"What happens if I don\\'t review enough peer projects?\", \"Is there a template for the certificate?\", \"Do you offer different types of certificates?\"]',\n",
       " '1d644223': '[\"Will I receive a certificate if I miss the midterm project?\", \"Do I need to submit the course project if it\\'s not required for my tasks?\", \"Can I request an extension for the project if I need more time?\", \"Does the final project grade replace the midterm project grade if I missed it?\", \"If I miss the project, can I still get a passing grade for the course?\"]',\n",
       " '14890cd2': '[\"How much Python experience should I have before joining the course?\", \"Do I need to know how to run a Jupyter notebook before taking the course?\", \"Do I need to know how to import libraries before taking the course?\", \"Do I need to know how to declare a variable before taking the course?\", \"What are some important operations related to data analysis that I should know before taking the course?\"]',\n",
       " 'a4fad482': '[\\n  \"What are the specific hardware requirements for the Machine Learning part of the course, as I only see a general mention of a \\'working laptop\\'?\",\\n  \"Is it necessary to have a powerful laptop for the Deep Learning part, or can a cloud-based solution suffice?\",\\n  \"Can I use any cloud other than Saturn cloud for the Deep Learning part?\",\\n  \"Will I need to have a specific operating system or software installed on my laptop for the course?\",\\n  \"Are there any general hardware recommendations that I should consider when preparing my laptop for the course?\"\\n]',\n",
       " '34b7fd35': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[{\"What are the general steps to set up TensorFlow with GPU support?\", \"How do I enable GPU support in TensorFlow on Ubuntu?\", \"Can I get a tutorial on how to set up TensorFlow on Ubuntu with GPU support?\", \"Is there a specific resource that worked for someone in setting up TensorFlow with GPU support?\", \"How do I make sure my TensorFlow installation utilizes my GPU for computations?\"]',\n",
       " '4930aa19': '[\"How do I find the course channel in Slack if I am new to the platform and cannot locate it?\", \"Do I necessarily have to provide a GitHub link for only my homework code, or do I need to provide a link to my entire repository?\", \"How do I join a channel in Slack if the \\'All channels\\' option is not available?\", \"What should I do if I\\'m having trouble finding the course channel in Slack?\", \"Do we have to provide the GitHub link for our entire repository or only for the specific code corresponding to homework questions?\"]',\n",
       " 'ee58a693': '[\"What happens if I still want to join the course after it has already started?\", \"Can I submit all homeworks if I join the course now?\", \"Will I still be eligible for a certificate if I join the course late?\", \"How many course projects do I need to submit to get a certificate?\", \"Do I need to review all peers\\' projects to get a certificate?\"]',\n",
       " '636f55d5': '[\"What is the next available start date for the course?\", \"Can I take the course at any time or is there a specific schedule?\", \"When is the next cohort start date?\", \"Is the course only available during the cohort start dates?\", \"Can I do the course with a fixed group of students or is it always self-paced?\"]',\n",
       " 'c839b764': '[\"Can I submit my homework late for an exception?\", \"What happens if I miss the homework deadline?\", \"Why is it not possible to submit homework after the due date?\", \"Are there any alternative assignments if I miss the homework deadline?\", \"Is homework required to pass the course?\"]',\n",
       " '0a278fb2': '[\\n\"How do I access the course materials now that I\\'ve joined? Are there any specific steps I should follow?\",\\n\"What websites or channels should I visit to start watching videos and tutorials?\",\\n\"What is the purpose of the cohort folder, and why should I read everything in it?\",\\n\"Are there any recommended additional resources or playslists I should be aware of beyond the course materials?\",\\n\"What is the ML Zoomcamp timeline I should follow to stay on track with the course?\"',\n",
       " '8de4fefd': '[\"What are the deadlines in this course specifically for the 2023 cohort?\", \"Can we view the deadlines from the previous cohorts\\' pages?\", \"Are the deadlines flexible or fixed for each assignment?\", \"How will we be notified of any changes to the deadlines?\", \"How do we access the 2023 cohort\\'s deadline information?\"]',\n",
       " '94e86808': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What is the special module BentoML that was part of the previous iteration of the course?\", \\n\"What were the modules in the previous iteration of the course (2022) that are the same as this year\\'s?\", \\n\"Why did the homework change this year compared to last year?\", \\n\"Are there any specific topics that were covered in BentoML that will not be covered in this year\\'s course?\", \\n\"How significant is the change in the homework in terms of complexity, number of assignments, etc.?\"]',\n",
       " 'e7ba6b8a': '{\"questions\": [\"Will the course videos be updated from 2021 or will we continue to use the same ones?\", \"Are the course videos still relevant to the current iteration or outdated from 2021?\", \"Can I still learn from the course videos if I haven\\'t taken part in the previous iteration?\", \"Are there any differences in the Python versions used in the course videos?\", \"Are the course videos re-recorded for the current iteration or used from the previous one?\"]}',\n",
       " 'f7bc2f65': '[\"Can I earn extra scores by posting the same content on multiple social media platforms and sharing the URLs in my homework form?\", \"Is there a limit to the number of points I can earn for submitting public links in my homework?\", \"How do I separate multiple links in the homework form?\", \"Will I get points if I share more than 7 links in my homework form?\", \"Can I use the same content to earn points for my midterm or capstone projects?\"]',\n",
       " 'ae52a907': '[\\n\"How can I start creating my own community notes in the course?\",\\n\"What is the purpose of forking the original course repo?\",\\n\"Can I only add notes or can I also add other course-related content to my repository?\",\\n\"Is it necessary to sync my fork with the original course repo?\",\\n\"How do I create a pull request to sync my fork with the original course repo?\"',\n",
       " 'dab5a24a': 'Here are the 5 questions:\\n\\n[\"How can I compute the hash for my email to submit it to the leaderboard?\",\\n\"What if I forget my email? Can I find it somewhere in the course materials?\",\\n\"Can I use the website provided instead of the Python code?\",\\n\"How will I know if my hash is correct?\",\\n\"What is the purpose of the hash when submitting to the leaderboard or for project review?\"]',\n",
       " '49f9bda9': 'Here are the 5 questions a student might ask based on the FAQ record:\\n\\n[\\n\"How do I install wget on Ubuntu?\",\\n\"What are the alternative ways to download files from a URL without using wget?\",\\n\"How do I install wget on Mac if brew isn\\'t working?\",\\n\"Can I use the built-in Python functionality to download files from a URL?\",\\n\"Why do I need to bypass https checks when downloading a file from a URL?\"',\n",
       " 'd44de7d1': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How can I download a csv file from a GitHub repository directly in my notebook?\",\\n\"Are there any other common shell commands besides !wget and !mkdir that I can use in my notebook?\",\\n\"Can I use !wget to download any type of file, or just csv files?\",\\n\"What is the directory structure like after running the !mkdir and !mv commands?\",\\n\"Can I use !mv to move multiple files at once, or do I need to use it separately for each file?\"',\n",
       " '314ebe32': '[\\n\"How do I set up a WSL development environment on my Windows 11 device?\", \\n\"What is the Microsoft Learn link used for?\", \\n\"Can I use VS Code without installing anything extra?\", \\n\"Will using the \\'WSL\\' extension make WSL Ubuntu instance act like a virtual machine?\", \\n\"Are there any prerequisites to download the \\'WSL\\' extension?\"',\n",
       " '98cff602': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What are the commands I need to run to fix the \\'src refspec master does not match any\\' error when uploading my homework to Github?\",\\n\"How do I initialize my Github repository before pushing my code?\",\\n\"Is there a way to upload my homework to Github without using the command line?\",\\n\"Can I use the \\'upload file\\' functionality on Github to share my code with the instructor?\",\\n\"If I\\'ve written my code on Google Colab, how can I share it on Github?\"',\n",
       " '54ec0de4': '[\"Why do I get a singular matrix error even when I\\'m trying to invert a matrix?\", \"Can you explain why matrix multiplication is not commutative?\", \"How do I correctly perform matrix multiplication using the dot method?\", \"Why does the order of multiplication matter when using .dot?\", \"What are the consequences of ignoring the order of multiplication in .dot?\"]',\n",
       " 'f81f4ecb': '[\"Why do I have a problem with my terminal and the command conda create -n ml-zoomcamp python=3.9 doesn\\'t work?\", \"Why does the command conda create -n ml-zoomcamp python=3.9 only work if any of the versions 3.8/3.9/3.10 are used?\", \"How can I use the Anaconda Prompt if I\\'m on Windows and just installed Anaconda?\", \"What should I do if I don\\'t have Anaconda or Miniconda?\", \"Do I need to install anything before I can use the terminal and conda command?\"]',\n",
       " 'be760b92': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What is the correct way to read a CSV file in Windows OS?\", \"Why does my code not work when reading a file in Windows?\", \"What is the purpose of using a backslash in Python?\", \"How do I treat a string as a literal string in Python?\", \"Why do I need to add \\'r\\' before the file path when reading a file in Windows?\"]',\n",
       " 'a2cfa1c9': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"What\\'s the correct syntax for fixing the \\'403 Forbidden\\' error when pushing to a GitHub repository?\",\\n\"Why do I need to change the remote URL to fix the error, and what\\'s the purpose of the \\'origin\\' keyword?\",\\n\"Can I just copy-paste the new URL format into the GitHub repository settings instead of using the command `git remote set-url origin`?\",\\n\"What does the output of `git config -l | grep url` exactly show me, and why does it look like that?\",\\n\"Is it possible to change the URL for a specific branch or commit, or does it only change the default URL for all future pushes?\"',\n",
       " '7b907071': 'Here is the list of questions as requested:\\n\\n[\\n\"What do I do when I get a \\'fatal: Authentication failed\\' error in Git Bash?\",\\n\"Can you explain the security changes on GitHub and how they affect our course?\",\\n\"Why did my password authentication stop working on GitHub?\",\\n\"How do I create a personal access token on GitHub?\",\\n\"Is it safe to use my GitHub account username and password for authentication?\"',\n",
       " 'fc2e0a61': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"Why do I keep getting the wget unable to resolve host address \\'raw.githubusercontent.com\\' error when trying to download datasets from GitHub?\", \"How do I resolve the wget: unable to resolve host address \\'raw.githubusercontent.com\\' error on Kaggle?\", \"Can you explain why I need to turn on the Internet in my Kaggle notebook settings?\", \"Why does Kaggle require phone verification to access the Internet in the notebook settings?\", \"What happens if I don\\'t turn on the Internet in my Kaggle notebook settings?\"]',\n",
       " 'd43e5742': '[\"How do I set up a virtual environment for Python using VS Code?\", \"Can I use jupyter notebooks directly within VS Code without a web browser?\", \"Do I need to have port forwarding enabled to run jupyter notebooks remotely in VS Code?\", \"How do I work with Github from within VS Code?\", \"Is Git support available in VS Code to ease staging and committing of files?\"]',\n",
       " '32bc0538': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\"Do we only need to type \\'conda create -n .....’ once, and then we can use that environment afterwards?\", \"Do we need to run \\'conda create -n .....’ and \\'conda activate ml-zoomcamp\\' every time we open VS Code?\", \"What happens if we try to create the environment again after it\\'s already been created?\", \"Can we save the environment configuration somehow, so we don\\'t have to set it up again if we delete the environment by mistake?\", \"How do we ensure we can reproduce the exact same environment on another machine or if we accidentally delete it?\"]',\n",
       " 'b6730228': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"Why does the result of multiplying the inverse of the original matrix not yield an exact Identity matrix, even though each step was performed correctly?\",\\n\"What causes the slight difference in the results when I multiplied the inverse with the original matrix?\",\\n\"Why is it that floating point math doesn\\'t work well on computers?\",\\n\"Can you provide more information about the limitations of floating point precision?\",\\n\"How can I accurately verify the results in my calculations, given the limitations of floating point precision?\"',\n",
       " '3ce9bbb8': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What is the purpose of the pandas.DataFrame.info() function?\", \"How does pandas.DataFrame.info() provide information about the dataset?\", \"What information does pandas.DataFrame.info() provide about the columns in the dataset?\", \"Can you give an example of how to use pandas.DataFrame.info()?\", \"Is there a way to get the information output in parsed JSON format?\"]',\n",
       " '4e584d06': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What should I check if I get a NameError \\'np\\' is not defined?\", \"Why am I getting a NameError when trying to use \\'nd\\'?\", \"What\\'s causing the error when I\\'m using libraries like numpy?\", \"How can I prevent getting a NameError \\'pd\\' is not defined?\", \"Do I need to import libraries at the beginning of my code?\"]',\n",
       " 'ff4da2b6': '[\\n\"How do I quickly obtain the columns in a pandas DataFrame with numeric data?\",\\n\"How can I get the columns with object data in a pandas DataFrame?\",\\n\"What command do I use to filter a DataFrame based on the data type?\",\\n\"Can I use the select_dtypes method to get the object columns in a DataFrame?\",\\n\"Is there a concise way to get the columns with numeric data in a DataFrame?\"',\n",
       " '58c1c168': 'Here are the 5 questions:\\n\\n[\"How do I get the overall shape of my dataset in Pandas? Can you show me a specific way to do this?\", \"How can I get the number of rows in my dataset using the .shape attribute?\", \"How do I identify the number of columns in a dataset in Pandas? Is it using the .shape attribute as well?\", \"Is there a specific way to identify the shape of a dataset in Pandas, aside from using .shape?\", \"Is df.shape[0] only used to get the number of rows or can I use it for something else?\"]',\n",
       " '96076a1a': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\\n\"What precautions should I take to avoid value errors when performing array operations in homework?\",\\n\"How can I avoid dimension mismatch when multiplying matrices?\",\\n\"What is the correct order for matrix multiplication to avoid errors?\",\\n\"Why do I get a value error when my array shapes don\\'t match?\",\\n\"How can I ensure the correct array shapes for matrix multiplication?\"',\n",
       " '3218389a': '[\"Why do we impute NaN values in a column and what is the process of imputation?\",\"How do we get the average of a column in Python?\",\"Can we replace NaN values with any value or only with the average of the column?\",\"Why can\\'t we just remove rows with NaN values? Why do we want to keep them?\",\"What is the term for replacing NaN values with the average of a column?\"]',\n",
       " '183a1c90': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What is a Matrix X in the context of linear regression?\",\\n\"What is the role of unknown weights w in linear regression equations?\",\\n\"How can I solve the initial problem in question 7?\",\\n\"What is the difference between OLS and other linear regression methods mentioned in the extra resources?\",\\n\"Is the pseudoinverse solution to OLS a common approach in machine learning applications?\"]',\n",
       " 'f0bc1c19': '[\"What could be the reason why my FINAL MULTIPLICATION is not having 5 columns, most likely I interchanged the first step of the multiplication?\", \"Can you confirm that I mistakenly used instead of and that\\'s causing the issue with FINAL MULTIPLICATION?\", \"I accidentally did something with FINAL MULTIPLICATION and now it\\'s not showing 5 columns, how can I fix this problem?\", \"Why is my multiplication not working correctly and not giving me the expected 5 columns result?\", \"I made a mistake while setting up my FINAL MULTIPLICATION and now it\\'s not having 5 columns, how can I correct it?\"]',\n",
       " '735e6c78': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What is the difference between the * operator and the @ operator in matrix multiplication?\", \"Can I use the * operator for matrix-matrix multiplication?\", \"Is it recommended to use np.matmul() for matrix-matrix multiplication?\", \"Can I use the * operator to perform multiplication with a scalar?\", \"What are the preferred methods to perform element-wise multiplication?\"]',\n",
       " 'b8ca1cd3': '[\"When I try to launch a new Jupyter notebook, I get an error saying \\'cannot import name \\'contextfilter\\' from \\'jinja2\\'\\'. What could be causing this?\",\\n\"Is there a way to resolve this ImportError without having to reinstall Anaconda?\",\\n\"How can I fix this issue if I\\'m using a brand new environment?\",\\n\"I\\'m getting this error when launching a notebook for a brand new environment. How can I troubleshoot it?\",\\n\"Normally, launching a Jupyter notebook works fine for me, but today I got this error. What could have caused this and how can I resolve it?\"]',\n",
       " 'efdb235f': '[\"What happens if wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv hangs on MacOS Ventura M1 and how to resolve it?\", \"Is there a specific setting in macOS Ventura that needs to be adjusted for wget to work properly?\", \"How do I resolve the issue with IPv6 addresses in the terminal while using wget?\", \"Why does wget sometimes hang on MacOS Ventura M1 when accessing a GitHub dataset?\", \"Is there a specific command or option that needs to be used to make wget run efficiently on MacOS Ventura M1?\"]',\n",
       " '355348f0': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\"What alternatives do I have to WGET if I\\'m having trouble on Mac OS?\", \"How do I use curl to download a file?\", \"Can you explain what the -o option does in the curl command?\", \"Where can I learn more about using curl?\", \"Do I need to install additional software to use curl?\"]',\n",
       " '67afabf5': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How can I use the round() function to round a variable up to a certain number of decimal places?\", \"What are f-strings and how can I use them to limit the number of decimal places in my output?\", \"Can I use the round() function to round a pandas Series up to a certain number of decimal places?\", \"How many decimal places can I use with round() function?\", \"What is the main difference between using round() function and f-strings to limit the number of decimal places in my output?\"]',\n",
       " '50d737e7': '[\"What links are crucial for starting Week 2 of the machine learning regression course?\", \"How do I submit my homework for Week 2?\", \"Where can I find the calendar for weekly meetings?\", \"Can I find all homeworks for this course in one place?\", \"Can I find the theory material for the course on GitHub?\"]',\n",
       " 'bbc0fca3': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How do I visualise the long tail of my data using Python?\",\\n\"What is the purpose of a histogram in checking the long tail of my data?\",\\n\"Can you provide more information on how to calculate the skewness and kurtosis of my data?\",\\n\"How do I load a CSV file into a Pandas DataFrame in Python?\",\\n\"What is a possible alternative to using a histogram to check the long tail of my data?\"',\n",
       " '6f3bdd20': '[\"What are the reasons for getting a Singular Matrix error when following the machine learning for regression videos?\", \"Why do we get this error despite doing everything as shown in the video tutorials?\", \"Is it normal to encounter the Singular Matrix error?\", \"Can you explain why we encountered this error in the Regularization video?\", \"Can the Singular Matrix error occur due to incorrectly implementing code in the regression problem?\"]',\n",
       " '27c2d90a': '{\"questions\": [\\n\"How can I find a detailed description of the California housing dataset?\\r\\nCan I find the California housing dataset with its description somewhere?\\r\\nWhat is a good source of information for the California housing dataset?\\r\\nWhere can I learn more about the characteristics of the California housing dataset?\\r\\nIs the information about the California housing dataset available online?\"}]',\n",
       " '88e9600a': '[\"What is the cause of NaN values after applying the .mean() method?\", \"Why does applying .mean() on a list of y_val and y_pred result in NaN values?\", \"How did you identify the source of the NaN values?\", \"Why did applying .fillna(0) on only the train data not solve the issue?\", \"What fix did you apply to resolve the NaN value problem?\"]',\n",
       " 'd59d8df7': '[\"What is the purpose of transforming the target variable to a logarithm distribution?\", \"Do we always need to transform the target variable, or is it only necessary in certain cases?\", \"How can we determine if we need to transform the target variable?\", \"Is it true that all machine learning projects require target variable transformation?\", \"What are some common manifestations of skewness that can affect our dataset?\"]',\n",
       " '0b3eaf92': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"How do I directly read the dataset from GitHub?\", \"What programming language is used to read the dataset?\", \"Can I use other data formats besides CSV?\", \"How do I specify the link to read the dataset?\", \"What is the purpose of Krishna Anand in providing the dataset?\"]',\n",
       " '8fe56032': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"Can I use Kaggle Notebooks to load the dataset directly?\", \"What is the command to load the dataset?\", \"Why do we use ! before wget in the command?\", \"How do I read the loaded dataset?\", \"Can I use other datasets from GitHub in Kaggle Notebooks?\"]',\n",
       " 'af833e0a': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"How do I filter a dataset using specific values if I have multiple conditions?\", \\n\"Can I filter a dataset using AND operator instead of OR?\", \\n\"What if I want to filter a dataset based on multiple values instead of single values?\", \\n\"Is there a shorter way to filter a dataset instead of using if conditions?\", \\n\"What type of variable should the column \\'ocean_proximity\\' be, for the filtering method to work?\"',\n",
       " '8d209d6d': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I load the dataset directly from GitHub?\", \"What is the purpose of the `requests` library?\", \"Can I use the `requests` library to load other datasets?\", \"Why do I need to check the status code if the download is failed?\", \"Can I directly use the loaded data or do I need to save it to a file?\"]',\n",
       " '0bc4c3da': 'Here is the list of questions in JSON format:\\n\\n[\"Why is the null column still appearing even if I used .fillna() method? How can I avoid it?\", \"What\\'s the difference between X_train = df_train and X_train = df_train.copy() when creating a new dataset?\", \"Is it possible that the original dataframe is being referenced even after creating a duplicate?\", \"Why do I need to create a deep copy of the dataframe using .copy() method?\", \"What happens if I don\\'t create a deep copy and just reference the original dataframe?\"]',\n",
       " 'c0ee2665': '[\"Can I use Scikit-Learn\\'s functions before implementing train_test_split manually?\", \"Must we manually implement train_test_split in this assignment?\", \"Is it allowed to use Scikit-Learn\\'s train_test_split in this course?\", \"Should I only use Scikit-Learn\\'s train_test_split function or the manual implementation?\", \"Are there any cases where we cannot use Scikit-Learn\\'s train_test_split?\"]',\n",
       " '3f60871d': '{\"questions\": [\"What is the best way to solve the regression problem in this week\\'s course?\", \"Can we use scikit-learn for next week\\'s task too?\", \"How does the LinearRegression work?\", \"Do we have alternative ways to solve the regression problem?\", \"Will we be using the same algorithm for both this week and next week?\"]}',\n",
       " 'f30217a7': '[\"What are the Scikit-Learn equivalent functions for the linear regression model used in week 2 with and without regularization?\", \"How do I implement a linear regression model without regularization in Scikit-Learn?\", \"What is the Scikit-Learn function for regularized linear regression?\", \"Can I use Scikit-Learn\\'s linear regression function for both regularized and unregularized models?\", \"Where can I find more information about Scikit-Learn\\'s linear model functions?\"]',\n",
       " '91fc573d': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"What is the difference between the regularization parameter r in the lesson\\'s notebook and alpha in sklearn.Ridge()?\",\\n\"Does increasing r or alpha in a model lead to stronger regularization?\",\\n\"How does r in the lesson\\'s notebook prevent multicollinearity?\",\\n\"Can you show me the equation including both r and alpha to compare the two?\",\\n\"In terms of the regularization strength, how do r and alpha affect the model in the same way?\"',\n",
       " 'fe3139f6': 'Here are 5 questions based on the FAQ record:\\n\\n[\"Why is it important that linear regression doesn’t provide a \"perfect\" fit?\", \"Can a model fit all the dots in a scatter plot?\", \"How does a linear model draw a line to fit all the dots?\", \"What happens if we overfit the data?\", \"How would a model that fits all the dots perform on new, unseen data?\"]',\n",
       " '48aac030': '[\"What happens when using a random seed of 42 in the machine learning regression homework, despite the instructions assigning missing values to training, validation, and test dataframes? Why did all my missing values end up in the training dataframe?\",\"How does a specific random seed value, like 42, affect the way missing values are distributed in the dataframes when splitting the data? Why do missing values not appear in the validation and test dataframes when using this seed value?\",\"I noticed that when I used a random seed of 42 in the homework, my model performed better than expected, but when I changed the seed to 9, the performance decreased. Is it possible that different seed values can affect the model\\'s ability to learn from the data?\",\"What is the purpose of using a specific random seed value, like 42, when working with machine learning regression, and how does it ensure consistency in the results?\",\"Are there any best practices for choosing a random seed value, such as 42, in machine learning regression, and how can it be used to improve the reliability of the results?\" ]',\n",
       " '28321bc2': '[\\n\"How can I shuffle the entire initial dataset using pandas?\",\\n\"What does setting \\'frac=1\\' do when using pandas.DataFrame.sample?\",\\n\"Can I get the same randomization results as used in the course resources?\",\\n\"How do I reset the index after shuffling the dataset?\",\\n\"Can I use pandas.DataFrame.sample with other built-in functions for machine learning?\"\\n]',\n",
       " 'edb92d22': '[\"What if the answer I get for one of the homework questions doesn\\'t match any of the options?\", \"Why do we have different environments on our computers that affect the answer? Can this be minimized?\", \"Is it common for students to get different answers for the same homework question?, \"If I get a different answer, how do I know it\\'s accurate?\", \"Why are different versions of OS and libraries important in machine learning? Can they cause significant differences?\"]',\n",
       " 'f488ce85': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"What does \\'use the training only\\' mean in the context of calculating the mean in homework 2, question 3?\",\\n\"Can I use the validation or test data set to calculate the mean in homework 2, question 3?\",\\n\"What is another way to calculate the mean besides using the function \\'mean()\\'?\",\\n\"Should I use the \\'describe()\\' function to calculate the mean in homework 2, question 3?\",\\n\"In the context of homework 2, question 3, what is the purpose of calculating the mean using the training data set?\"',\n",
       " 'bf395099': '[\"What are the conditions under which we should transform the target variable to logarithm distribution?\", \"Why is it necessary to use np.log1p() method in certain cases?\", \"What happens if the target variable has negative values after transformation?\", \"How do we ensure our target variable has a long tail distribution?\", \"Can we transform the target variable even if it doesn\\'t have a wide range?\"]',\n",
       " '01cd3b35': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"What are the scenarios under which broadcasting can occur and also fail?\",\\n\"I am getting an error due to operands not being broadcastable together with shapes, can I always use the * operator instead of the dot() method to solve the issue?\",\\n\"Why does the error ValueError: shapes not aligned occur, is it always due to performing an arithmetic operation between arrays of different shapes?\",\\n\"How can I ensure that the shapes of my arrays are aligned before performing an arithmetic operation?\",\\n\"Can you provide an example of when the * operator can be used instead of the dot() method to solve the ValueError: shapes not aligned error and why it would work?\"',\n",
       " '5551c92e': '[\"How do I create a real copy of a dataframe without just referencing it?\", \"What if I accidentally change the original dataframe, are there any consequences?\", \"Can I use X_copy = X to create a copy?\", \"Why does X_copy = X not create a deep copy?\", \"How can I ensure that changes to X_copy do not affect the original dataframe?\"]',\n",
       " '94f928d2': '[\\n\"What does it mean when the distribution of a dataset has a long tail? Can you explain it using an example?\",\\n\"How does the \\'long tail\\' affect the mean, median, and mode of the dataset?\",\\n\"Why is the mean no longer representative when the distribution has a long tail?\",\\n\"What exactly is meant by the \\'area under the curve\\' in the context of the \\'long tail\\'?\",\\n\"Can you provide an example of a dataset where the \\'long tail\\' would occur, and how it would be useful in machine learning for regression?\"]',\n",
       " '266faa6d': '[\"What does it mean when a standard deviation is low or high in machine learning context, and how does it affect the model\\'s performance?\", \\n\"What is the difference between standard deviation and mean in terms of the spread of values?\", \\n\"Can you provide an example of a scenario where a low standard deviation would be beneficial in regression problems?\", \\n\"Is standard deviation only used for statistical analysis, or can it be used in machine learning algorithms?\", \\n\"In what way does the standard deviation impact the accuracy of a regression model?\"]',\n",
       " 'c21f99f5': '[\"Do we always need to apply regularization techniques when training machine learning models?\", \"Under what circumstances do you recommend using regularization?\", \"Will regularization techniques affect the model\\'s performance?\", \"How do you determine if regularization is necessary for a specific problem?\", \"Can regularization help prevent overfitting even with large datasets?\"]',\n",
       " '13702957': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What are some ways to speed up execution in our machine learning code for regression? Can you provide examples of function definitions for faster execution? What is the purpose of the seed parameter in the prepare_df function? How can I reuse existing functions in the class notebook, such as rmse and train_linear_regression, in my own code? Can I do data preprocessing before splitting the initial_df into training and testing sets?\"]',\n",
       " '7cd652c5': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"If pandas can be used to find the standard deviation, can we also use it to calculate the mean?\", \"How do I create a pandas series from a list of data?\", \"If I have multiple series, can I use pandas to find the standard deviation for each?\", \"How do I know if the standard deviation is skewed or normal?\", \"Can pandas be used to find other statistical measures like variance or skewness?\"]',\n",
       " 'e1f93d10': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What is the difference between the standard deviation equations used by Numpy and Pandas?\",\\n\"Why does Numpy use population standard deviation while Pandas uses sample standard deviation by default?\",\\n\"Can you explain why pandas computes its default standard deviation using a different degree of freedom than Numpy?\",\\n\"How can I change the degree of freedom to get an unbiased estimator using Numpy?\",\\n\"Are there any scenarios where pandas\\' default standard deviation would give a different result compared to Numpy?\"',\n",
       " '36b9d1b7': '[\"How do I calculate the standard deviation of a column in Pandas?\", \"How do I use the std function to find the standard deviation of multiple columns in my DataFrame?\", \"Is there a specific way to get the standard deviation of a subset of columns?\", \"Can I use the std function to find the standard deviation of a single value?\", \"How can I calculate the standard deviation of specific rows in my DataFrame?\"]',\n",
       " '3c8b32a1': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\\n\"How can I merge the train and validation datasets into one dataframe?\",\\n\"How do I combine two datasets with different lengths?\",\\n\"What is the best way to concatenate two numpy arrays?\",\\n\"Can I use \\'numpy.concatenate\\' function on DataFrames as well?\",\\n\"How do I use \\'pandas.concat\\' function to merge train and validation datasets?\"',\n",
       " '05fb3a16': '[\\n\"Can you explain in simpler terms what RMSE calculates and why it\\'s an important metric for evaluating a regression model\\'s performance?\",\\n\"How do I calculate the RMSE score if I have a dataset with multiple features and a regression target variable?\",\\n\"In the example, you mentioned using the sklearn library to calculate RMSE. Are there any alternatives to this library for calculating RMSE?\",\\n\"Since RMSE takes the square root of the MSE, is there a way to use MSE instead of RMSE as a metric for model evaluation?\",\\n\"What is the purpose of using both the actual observed values and the model\\'s predicted values in the RMSE calculation, and how do these values impact the final RMSE score?\"',\n",
       " '225506b9': 'Here are the 5 questions the student might ask based on the provided FAQ record:\\n\\n[\"How do I use multiple conditions in Pandas when I want to combine two conditions with a logical AND?\",\\n\"What is the correct syntax for combining multiple conditions with OR in Pandas?\",\\n\"I\\'m getting an error when trying to combine multiple conditions in Pandas? What\\'s wrong?\",\\n\"What is the difference in how I use AND and OR in Pandas?\",\\n\"How can I correctly combine multiple conditions using logical operators in Pandas?\"]',\n",
       " 'bd4a1395': '[\"What is the purpose of the normal equation in linear regression?\",\\n\"Can you summarize the steps for deriving the normal equation?\",\\n\"Why is the video you provided focused on linear regression?\",\\n\"Can you explain how the normal equation is used in machine learning?\",\\n\"Is there a way to visualize the normal equation to better understand its application?\"',\n",
       " '81b8e8d0': '[\\n\"How can I properly treat missing data values in my Python project for regression analysis, especially if I\\'m new to this topic?\",\\n\"Is there a good resource that explains how to handle missing data in Python, like a tutorial or a step-by-step guide?\",\\n\"What is the most effective method for dealing with missing values in my regression machine learning model, and why?\",\\n\"Are there any Kaggle notebooks or tutorials that provide an in-depth explanation of how to handle missing data, and which ones would you recommend?\",\\n\"How can I identify and remove duplicate rows in a dataset that contains missing values, and what are some potential drawbacks of doing so?\"',\n",
       " 'a7f6a33c': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What is the instruction for applying log transformation to the \\'median_house_value\\' variable?\",\\n\"Why was it absent in the subsequent questions of the homework?\",\\n\"What should I do if I forgot to apply log transformation to the target variable?\",\\n\"Can you remind me to apply log transformation to the target variable for each question?\",\\n\"Will forgetting to apply log transformation to the target variable affect my RMSE?\"',\n",
       " '129b4ac0': '[\"What sklearn version is Alexey using in the YouTube videos about machine learning for classification?\", \"Is it okay to use the same Python version as Alexey in the course?\", \"Will using a different Python version cause any issues with the machine learning algorithms?\", \"How do I check my own Python version and ensure it\\'s compatible with sklearn?\", \"Can I use a newer version of sklearn than the one Alexey uses in the videos?\"]',\n",
       " 'b8cca8b7': '[\"What is the link to the Week 3 homework?\", \"Where do I submit my Week 3 homework?\", \"Where can I find all previous homeworks?\", \"How do I access the evaluation matrix?\", \"Is there a YouTube video related to this week\\'s material?\"]',\n",
       " '1091b10f': 'Here are the 5 questions the student might ask:\\n\\n[\"What does the error message \\'could not convert string to float: ‘Nissan’\\' mean?\",\"Why is the machine learning model expecting numerical input?\",\"How can I encode categorical variables like car brands into numerical values?\",\"Is one-hot encoding the only way to encode categorical variables?\",\"How do I implement one-hot encoding using pandas?\"]',\n",
       " '0c7715a1': '[\"What was the reason for changing the target variables to binary format when calculating the mutual information score in the homework assignment?\", \"How does the mutual information score calculate the relationship between categorical variables or discrete variables?\", \"Why was the target variable median_house_value converted to binary format when calculating mutual information score?\", \"What would happen if we left the continuous target variable median_house_value in its original format for calculating mutual information score?\", \"Why is it not suitable to use continuous variables for calculating mutual information score?\"]',\n",
       " 'd2043cf5': '[\"What data should we use for our correlation matrix, specifically for Pastor Soto\\'s answer on converting median_house_value from numeric to binary?\", \"What is the purpose of using only df_train and not df_train_full at this stage, as mentioned in the answer?\", \"How do we go about making conclusions without using the validation data in Pastor Soto\\'s answer?\", \"Can we still make predictions with df_train_full even though we aren\\'t supposed to at this stage?\", \"What is the significance of keeping df_train_full and df_train separate in this context?\"]',\n",
       " '44d22817': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I color the background of a pandas DataFrame in general?\", \"Can I use other values in the DataFrame to color the background?\", \"Is there a specific method to use for coloring the background of a DataFrame?\", \"Can I choose a different colormap for coloring the background?\", \"Are there any specific requirements for the DataFrame values before applying the background_gradient method?\"]',\n",
       " '1f76dbeb': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\"How can I use the unstack function to easily identify highly correlated feature pairs in my dataset?\", \"Can I use seaborn to create a heatmap for the correlation matrix, and if so, how?\", \"Can I create a triangular heatmap with a blue to red color gradient to show correlations between numerical variables without redundant information?\", \"How do I modify the code to plot only a triangle in the heatmap?\", \"What was the output of the function provided by Mélanie Fouesnard for the churn dataset?\"]',\n",
       " 'b8071a54': '[\"What should we consider when deciding which dataset to use for exploratory data analysis in classification machine learning?\", \"Can we include the validation dataset when performing exploratory data analysis?\", \"Is it okay to slightly touch the test dataset during exploratory data analysis?\", \"Why is it important to only rely on the train dataset for exploratory data analysis?\", \"What do you mean by \\'pretending\\' that the test dataset is future unseen data?\"]',\n",
       " 'b8da9037': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What is the purpose of fitting a DictVectorizer on a training dataset?\", \"Why would applying the `fit` method to the validation set not be a good idea?\", \"What happens to the DictVectorizer\\'s knowledge from the train set if I fit it on the train set?\", \"Is there any benefit to applying `fit_transform` to the train set?\"]',\n",
       " '467e0cec': '[\"What do we mean by \\'smallest\\' in the context of feature elimination, especially when the difference in accuracy is negative?\", \"Can we determine the difference in accuracy in real values or do we need to convert it to absolute values?\", \"How do we know if a model becomes better when we remove a feature?\", \"In the context of feature elimination, what exactly do we mean by \\'lowest\\'?\", \"Do I need to calculate the absolute values of the difference in accuracy for all features before selecting the smallest one?\"]',\n",
       " 'b69f32f6': '[\"What is the reason for the FutureWarning regarding the get_feature_names function?\", \"Is the warning a cause for concern?\", \"What should I do to resolve the warning?\", \"Is there an alternative method to get_feature_names?\", \"Will 1.2 version remove this function?\"]',\n",
       " '3b3b1989': '[\"What happens when my logistic regression takes a long time to fit and crashes my Jupyter kernel?\", \"Why does the kernel crash when I try to make predictions with my fitted logistic regression model?\", \"Is the target variable I\\'m using for logistic regression supposed to be binary?\", \"Will my kernel always crash if my logistic regression takes a long time to fit?\", \"Can I do anything to prevent my kernel from crashing when calling predict() with a logistic regression model?\"]',\n",
       " 'eb5771a0': '[\"What does Ridge regression do to mitigate the problem of multicollinearity and prevent overfitting?\", \"How does the sag solver optimize the regularization term in Ridge regression?\", \"What does the alpha parameter control in Ridge regression?\", \"What happens to the model\\'s coefficient values if the alpha value is increased?\", \"How do I implement Ridge regression using scikit-learn?\"]',\n",
       " 'bca10281': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What is the main difference between DictVectorizer with sparse=False and DictVectorizer with sparse=True?\", \\n\"How does using sparse format affect the performance of machine learning models for classification tasks with a high number of classes?\", \\n\"Why does DictVectorizer with sparse=True store non-zero values and indices while pandas.get_dummies() adds a column for each class of each feature?\", \\n\"What are the convergence warnings in Linear/Ridge Regression when using sparse format with a high number of classes?\", \\n\"While DictVectorizer with sparse=True is more memory efficient, are there any other benefits or drawbacks to consider when using it for classification tasks?\"]',\n",
       " '34a8edb0': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What do I need to check if I\\'m getting a ConvergenceWarning when using Ridge with sag solver in W3Q6?\",\\n\"Why do I need to play with different scalers when using Ridge with sag solver?\", \\n\"Is there a specific scaler that I should use for the feature values in W3Q6?\", \\n\"Can I use the same scaler for both numeric and categorical features, or do I need separate scalers?\", \\n\"What happens to the coefficient when it doesn\\'t converge, does it get stuck or does it continue to change?\"',\n",
       " 'f625307b': '[\"What steps can I take to address convergence errors when training a Ridge regression model?\", \"How do I normalize my numerical features for Ridge regression?\", \"What encoding method is commonly used for categorical variables in Ridge regression?\", \"Can you provide more information about feature combination for Ridge regression?\", \"Why is it important to use a suitable encoding method for categorical features in this context?\"]',\n",
       " '7fa98526': '[\"What are the differences between sparse and dense matrices, especially in terms of memory efficiency?\", \"How does a sparse matrix store non-zero values and their positions?\", \"Why is the default DictVectorizer configuration a sparse matrix?\", \"Can sparse matrices handle large datasets with many zero or missing values?\", \"Are there any performance advantages to training a model using sparse matrices instead of dense ones?\"]',\n",
       " '0807f0f3': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I get rid of the warning messages in my Jupyter Notebooks? Are there any special comments I need to add?\", \"Can I disable warnings in Jupyter Notebooks entirely?\", \"How do I avoid getting warnings in my classification task?\", \"Are there any specific steps I need to take to suppress warnings in my code?\", \"What is the purpose of \\'Import warnings\\' in avoiding warnings in Jupyter Notebooks?\"]',\n",
       " '6d0fb418': '[\"How do I decide on the correct score to choose in the study group discussion?\", \"How do I know which alpha to select when RMSE scores are equal?\", \"Can I use the same alpha value for multiple iterations in the classification task?\", \"Is it possible to compare the results of different alpha values?\", \"How does the choice of alpha parameter affect the accuracy of the model?\"]',\n",
       " 'fbda1f40': '[\"What is the second variable that we need to use to calculate the mutual information score for the HW3 Q3?\",\"Can you provide more context about ocean_proximity and how it relates to pricing?\",\"How do I binarize the price variable above_average?\",\"What does it mean to use the training set only when calculating the mutual information score?\",\"Is there an example of a mutual information score calculation including the binarized price and ocean_proximity?\"]',\n",
       " '0f88b7ac': 'Here is the list of questions:\\n\\n[\"Do we evaluate the model one time with all available features to obtain the original accuracy, or do we train the model separately with each feature to make a comparison?\", \"How do we calculate the differences between accuracy scores when training on the whole model versus dropping one feature at a time?\", \"What if the difference between accuracy scores is negative? Do we consider the smallest difference or the smallest absolute difference?\", \"What happens if some of the features are correlated with each other?\", \"Should we compare the absolute values of the differences between accuracy scores, or the absolute values of the differences between the differences?\"]',\n",
       " '9ffcc895': '[\"How does OneHotEncoder convert categorical features compared to DictVectorizer, since both aim to transform categorical features into numerical variables for model training?\", \\n\"What are the advantages and disadvantages of using OneHotEncoder over DictVectorizer for categorical feature conversion?\", \\n\"Can both OneHotEncoder and DictVectorizer produce the same output, and if so, how do they differ in terms of sorting and stacking features?\", \\n\"How does the input format affect the output of OneHotEncoder and DictVectorizer in transforming categorial features?\", \\n\"In what situations would I prefer to use OneHotEncoder instead of DictVectorizer, or vice versa, for categorical feature conversion?\"]',\n",
       " '94a3b2fb': '[\"What is the difference between getting dummies with pandas versus one-hot-encoding with scikit-learn?\", \"Can I use pandas get_dummies in a machine learning pipeline?\", \"Do both methods handle missing values the same way?\", \"Which one should I use when building a scikit-learn-based machine learning pipeline?\", \"Are the results identical between pandas get_dummies and sklearn OnehotEncoder?\"]',\n",
       " 'fb9a45d8': '[\"What is the recommended random seed for the machine learning models in our project?,\", \"Should we use the same seed for both the training and test sets in the project?,\", \"Why are we required to use a specific random seed for the test_train_split question in HW3?,\", \"Can I use any random seed value I want for the test_train_split question in HW3?,\", \"Do I need to use the same random seed for all the questions in HW3?\"]',\n",
       " 'e31051f7': '[\"What is the correct timing to calculate correlation for machine learning model?\", \"How do I determine the two most correlated features in my dataset?\", \"Do I need to calculate correlation before or after splitting my dataset? How do I get the correlation coefficients?\", \"Can you elaborate on the concept of absolute values in correlation coefficient?\", \"Should I also consider correlation between target variable and other features in my dataset?\"]',\n",
       " '493b7b59': '[\"What types of features should I use in a Ridge Regression model?\", \"Can categorical features be used in a Ridge Regression model?\", \"How should I handle categorical features that I want to include in the model?\", \"Is there a way to include categorical features without converting them to one-hot encoding?\", \"Why do I need to set sparse=True when one-hot encoding categorical features?\"]',\n",
       " '4a55c510': 'Here are 5 questions the student might ask:\\n\\n[\\n\"How do I handle all features including the \\'price\\' column for Homework 3 Question 6?\",\\n\"Should I include the average variable we created before when using the features for this question?\",\\n\"What is the purpose of setting \\'sparce=True\\' when using DictVectorizer, and why is it necessary to avoid convergence errors?\",\\n\"Is it necessary to use StandardScalar for the numerical variables in this question, or can I use it without it?\",\\n\"Can I get more clarification on what features I should use for Homework 3 Question 6, especially regarding the \\'average variable\\'?\"]',\n",
       " '3ca0b489': '[\"What are some common encoding methods for non-numerical columns in machine learning for classification tasks\",\\n\"Are there any specific modules in scikit-learn that I should use for encoding categorical variables\",\\n\"How can I apply these encoding techniques to Ordinal data\",\\n\"Is StandardScaler suitable for encoding non-numerical data\",\\n\"What is the main difference between OneHotEncoder and OrdinalEncoder in scikit-learn\"]',\n",
       " '690d97f1': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What are the differences between FeatureHasher and DictVectorizer for categorical features?\", \"Is FeatureHasher suitable for high cardinality features?\", \"Does DictVectorizer store the vocabulary and take more memory?\", \"Can you recommend a method for categorical features with a small number of unique values?\", \"What are the implications for feature names in the transformed data?\"]',\n",
       " 'eb5a25cb': '[\"Why is it generally not recommended to do feature scaling with transformers like DictVectorizer or get dummies before splitting the data into train, validation, and test sets?\", \"Can using DictVectorizer or get dummies before splitting the data into train, validation, and test sets lead to data leakage?\", \"Is avoiding data leakage the main reason for doing feature scaling with transformers like DictVectorizer or get dummies after splitting the data into train, validation, and test sets?\", \"Would splitting the data into train, validation, and test sets before feature scaling with transformers like DictVectorizer or get dummies still prevent data leakage?\", \"Is it the same whether to do feature scaling with transformers like DictVectorizer or get dummies before or after splitting the data into train, validation, and test sets?\"]',\n",
       " '6d9e0a6f': '[\\n\"What happens when I get 1.0 as accuracy in HW3Q4?\", \\n\"Why do I usually get high accuracy even if I haven\\'t balanced my classes?\", \\n\"Can high accuracy mean my model is overfitting?\", \\n\"Is overfitting something I should worry about?\", \\n\"Will dropping a column help me if I\\'m overfitting in HW3Q4?\"',\n",
       " '618ad97a': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"How do I use sklearn and numpy to calculate Root Mean Squared Error for classification tasks?\",\\n\"Can I use another library to calculate Root Mean Squared Error instead of sklearn?\",\\n\"How do I implement Root Mean Squared Error in Python without using sklearn?\",\\n\"Can I refer to a specific notebook for Week 2 to understand Root Mean Squared Error better?\",\\n\"What is the important part of the code for calculating Root Mean Squared Error in the given example?\"]',\n",
       " '683495d2': '[\"How do I solve the AttributeError \\'DictVectorizer\\' object has no attribute \\'get_feature_names\\'?\", \"Why am I getting an error when using \\'get_feature_names\\' for \\'DictVectorizer\\'?\", \"What alternative should I use to \\'get_feature_names\\' for \\'DictVectorizer\\'?\", \"What is the correct method to get the feature names for \\'DictVectorizer\\'?\", \"Why wasn\\'t \\'get_feature_names\\' working for me with \\'DictVectorizer\\'?\"]',\n",
       " 'dc1897b5': 'Here are 5 questions based on the FAQ record:\\n\\n[\"How can I calculate the Root Mean Squared Error (RMSE) without using math or numpy?\", \"How do I use the mean_squared_error function in \\'sklearn.metrics\\'? \", \"What does the squared kwarg in the mean_squared_error function do?\", \"What is the default value of the squared kwarg in the mean_squared_error function?\", \"How can I get the mean_squared_error function to return the Root Mean Squared Error (RMSE) instead of Mean Squared Error?\"]',\n",
       " '826098f2': '[\"What are the different encoding techniques used in machine learning for categorical variables?\", \"Can you explain how to handle missing values in categorical variables after encoding?\", \"What are the advantages and disadvantages of label encoding?\", \"How do I implement one-hot encoding for nominal categorical variables?\", \"Can I use this encoding technique for ordinal categorical variables?\"]',\n",
       " '821dfc08': 'Here are the 5 questions this student might ask:\\n\\n[\"What is the correct way to use accuracy_score from sklearn in Jupyter?\", \"Why am I getting a TypeError when I use accuracy_score?\", \"Can you explain why I need to use metrics.accuracy_score instead of accuracy_score?\", \"What is causing the \\'numpy.float64\\' object is not callable error?\", \"How can I fix the TypeError when using accuracy_score in Jupyter?\"]',\n",
       " '27c8d5da': '[\\n\"How do I access the Week 4 homework? I\\'m having trouble finding the right link.\",\\n\"Where can I find all the homework assignments for the course?\",\\n\"What\\'s the best way to understand the evaluation metrics for classification?\",\\n\"Can you provide more information on machine learning with Sci-Kit Learn?\",\\n\"How do I find the YouTube video associated with Evaluation Matrix?\"\\n]',\n",
       " 'a52d4739': '[\"Can I use the same variable to score my model instead of a classification ratio?\", \"Is it possible to use a metric on a single data point from a series or a dataframe?\", \"How can I calculate a metric on a series or a dataframe in Python?\", \"What types of variables are acceptable for scoring in machine learning models?\", \"Can I apply different metrics to different subsets of a dataset?\"]',\n",
       " 'dc55359c': '[\\n\"What is the purpose of using random_state in some cases but not others?\",\\n\"Why do we need both random_state and shuffle parameters to be set accordingly?\",\\n\"Why is it essential to ensure the randomness used to shuffle the dataset is reproducible?\",\\n\"What makes some cases require random_state while others do not?\",\\n\"Why is it necessary to refer to the sklearn docs for more information about random_state?\"',\n",
       " '2ab49e43': '[\"How can I view all classification metrics, such as precision, recall, and accuracy, simultaneously in my project?\", \"Can you explain how to get all classification metrics at one go?\", \"What is the process to get multiple metrics like precision, recall, and f1 score?\", \"How to calculate multiple evaluation metrics simultaneously, like accuracy, precision, recall, and f1 score?\", \"What is the most efficient way to view multiple classification metrics, including precision, recall, f1 score, and accuracy?\"]',\n",
       " 'b431e7eb': 'Here are 5 questions based on the FAQ record:\\n\\n[\"Why did I get multiple thresholds with the same F1 score?\",\\n\"Is it correct to just pick the lowest threshold if I have multiple options?\",\\n\"Is there a specific method to choose the best threshold when I have the same F1 score?\",\\n\"Can I use the \\'classification_report\\' function to verify my results?\",\\n\"What libraries/packages are recommended to verify my classification results?\"',\n",
       " 'c5fdeba9': 'Here are the 5 questions:\\n\\n[\\n\"What does the error message \\'ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\\' mean?\",\\n\"Why does the code \\'df.churn = (df.churn == \\'yes\\').astype(int)\\' cause the data to only contain 0\\'s in the churn column?\",\\n\"What should I do to fix the issue of having only one class (0) in the data, according to Humberto Rodriguez?\",\\n\"Can I delete one of the cells and get the accuracy without using the duplicated code?\", \\n\"How does the solution provided by Humberto Rodriguez related to the error message and the data?\"',\n",
       " 'b8c9eaf1': '[\"How do I get a beautiful classification report?\", \"Can I use any library to generate a classification report?\", \"Is there a way to produce visualizations for my models?\", \"Can I combine scikit-learn with another library to make a classification report?\", \"Does Yellowbrick support colorful classification reports?\"]',\n",
       " 'c54058a1': '[\"What happens if I don\\'t get the exact result in the homework? Will I be penalized?\", \"Is it acceptable to get an answer that\\'s not exactly perfect in the homework?\", \"Can I get a slight variation in my answer and still be correct?\", \"Will I be able to do well in the course even if I don\\'t get exact results all the time?\", \"Can I use a value that\\'s close to the correct answer in the homework?\"]',\n",
       " 'b4b85c4b': '[\"How do I evaluate feature importance of numerical variables in the 2021 course iteration?\", \"Is roc_auc_score the correct metric to use in the 2021 course iteration solutions?\", \"What are the evaluation metrics used in the classification section of the 2021 course iteration?\", \"Can I use AUC to evaluate feature importance for non-numerical variables in the 2021 course iteration?\", \"Are the solutions to the 2021 course iteration available online for reference?\"]',\n",
       " '7d40f6f6': '[\"What do we mean by \\'for each numerical value\\' in the context of calculating the ROC AUC score?\", \"How should we interpret the parameters \\'y_true\\' and \\'y_score\\' in sklearn.metrics.roc_auc_score?\", \"Can we use the same numerical value for \\'y_score\\' across the entire dataframe?\", \"How does sklearn.metrics.roc_auc_score handle the calculation of AUC when we pass it multiple \\'y_true\\' target variables?\", \"Can we use this technique to calculate the AUC for multiple classes in a multiclass classification problem?\"]',\n",
       " 'f5dc446c': '[\"What is the best approach to evaluate the performance of a binary classification model in this course?\", \"What specific tasks do I need to consider when computing the evaluation metrics?\", \"Why do I need to use a specific dataset for computing the metrics from Question 3 onwards?\", \"What datasets are provided in this course and their use cases?\", \"Can I use any other dataset besides `dt_val` to compute the metrics?\"]',\n",
       " 'd30fc29d': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"What is the purpose of the shuffle=True parameter in the KFold function?\",\\n\"Why does using a different random_state value in KFold affect the results?\",\\n\"How does KFold generate separate pairs of datasets (train+val)?\",\\n\"Can I generate the KFold object inside or outside the loop, and what\\'s the effect on the results?\",\\n\"Is there a specific way to loop through the different values of C with KFold, as mentioned in the video?\"',\n",
       " '8eca9f73': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What does ValueError: multi_class must be in (\\'ovo\\', \\'ovr\\') mean when using roc_auc_score?\", \"How can I resolve a ValueError when using roc_auc_score?\", \"What are the correct parameters to pass to roc_auc_score for numerical variables?\", \"Why am I getting a ValueError when trying to evaluate feature importance?\", \"What is the significance of the \\'multi_class\\' parameter in roc_auc_score?\"]',\n",
       " '7b9eb7f7': '[\"What library do I need to import to display a terminal progress bar while monitoring wait times and progress of the code execution?\", \"How can I utilize tqdm to display the progress of the code execution in the terminal?\", \"What is the primary purpose of using tqdm in classification evaluation metrics?\", \"Can I use another library besides tqdm to display progress in my code execution?\", \"Will the tqdm library slow down my code execution when I\\'m monitoring progress in the terminal?\"]',\n",
       " 'c4aaeed9': '[\"What is the purpose of negating variables with ROC AUC scores below the threshold?\", \"Can you explain why feature importance improves when variables are inverted or negated?\", \"Can you give an example of when to use this technique?\", \"How does this technique impact model performance?\", \"What problems can this technique help solve when dealing with negatively correlated features?\"',\n",
       " '3af31e2a': '[\"What is the difference between using predict(X) and predict_proba(X)[:, 1] for binary classification?\", \"Why does predict(X) lead to incorrect evaluation values, while predict_proba(X)[:, 1] does not?\", \"What are the probabilities obtained from predict_proba(X)[:, 1]? Are they per class or something else?\", \"Why do we need to get the probability that the value belongs to one of the classes?\", \"Can you elaborate on what predict(X) returns when we\\'re dealing with a binary classification task?\"]',\n",
       " '746342ff': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"Why do I need to interpret a specific scenario when understanding FPR and TPR when threshold = 1.0?,\"\\n\"What happens when the threshold is 1.0 and FPR is 0.0, and what does this mean for our classification problem?\",\\n\"Why are there no objects that satisfy the churn-condition when the threshold is 1.0, and what does this imply for our predictions?\",\\n\"Can you explain why g(x) never takes on values of 0 and 1, and what implications this has for our sigmoid function?\",\\n\"In a binary classification problem, what does it mean when there are no true positives or false positives when the threshold is 1.0, and why does this occur?\"]',\n",
       " 'bda2c9b3': '[\"How do I use Matplotlib to annotate a graph with an arrow and text?\", \\n\"What is the purpose of xy, xytext, and textcoords in plt.annotate?\", \\n\"Can I customize the appearance of the annotated text and arrow?\", \\n\"What are the other options available in the arrowprops dictionary?\", \\n\"Can I use plt.annotate to annotate multiple points in a graph?\"',\n",
       " '41521c92': '[\"What is the importance of the ROC AUC metric in binary classification models\", \"How can I really understand the ROC curve despite it being a complex topic\", \"Can I skip the ROC curve and move on if I don\\'t fully understand it\", \"What other videos or notes can I refer to for better understanding of the ROC curve\", \"Can I assume that the ROC curve is the only important metric for classification models\"]',\n",
       " '25481ce5': '[\"What is the main reason for having different values of accuracy than the options in the homework?\", \"Why is splitting data with different ratios causing variation in accuracy?\", \"What is the significance of using a consistent method for splitting data?\", \"How can I ensure consistent results in my classification model?\", \"What is the recommended method for splitting data in this course?\"]',\n",
       " '1427d567': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\"How do I prepare my df_scores to use it in the numpy code?\", \"What if my recall values are not decreasing smoothly in the curve, will this method still work?\", \"How to interpret the index \\'idx\\' returned by the np.argwhere function?\", \"Can I use other methods instead of numpy diff and sign to find the intercept?\", \"Can I plot the precision and recall curves before finding the intercept for better visualization?\"]',\n",
       " '76c91dfb': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How do I calculate precision and recall manually in the demonstration video?\",\\n\"How can I use the Scikit Learn library to calculate the precision and recall?\",\\n\"What is the average parameter in precision_score, recall_score, and f1_score functions?\",\\n\"Can I use the Scikit Learn library to calculate the true positive, true negative, false positive, and false negative values?\",\\n\"Are there any specific conditions or requirements to use the average=\\'binary\\' parameter in the precision_score, recall_score, and f1_score functions?\"',\n",
       " 'e4dd91cf': '[\\n\"Why do we use cross validation to evaluate the performance of a model and choose the best hyperparameters? How does it achieve this?\", \\n\"How does splitting the dataset into multiple parts (folds) help in evaluating the performance of a model and choosing the best hyperparameters?\", \\n\"What are the typical values of the hyperparameter C in models like Support Vector Machines (SVM) and logistic regression?\", \\n\"What is the relationship between the value of the hyperparameter \\'C\\' and the regularization effect in models like Support Vector Machines (SVM) and logistic regression?\", \\n\"How does a smaller value of \\'C\\' affect the model\\'s decision boundary and the overfitting vs underfitting trade-off?\"',\n",
       " 'cc53ae94': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Is scikit-learn library the only option to compute model evaluation metrics? Can\\'t we use numpy and pandas libraries instead?\", \\n\"What is the advantage of using scikit-learn metrics over our own calculations?\", \\n\"Can you provide more information about the \\'y_val\\' and \\'y_pred\\' variables used in the code example?\", \\n\"What are the common scenarios where the F1-score is not sufficient on its own to evaluate a model\\'s performance?\", \\n\"Is it possible to use scikit-learn metrics to evaluate models that are not classification problems, such as regression problems?\"]',\n",
       " '403bbdd8': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What is the primary way to compute Precision, Recall and F1 score in scikit-learn?\", \"Is there an alternative to calculating precision, recall, and F1 score?\", \"Can I use precision_recall_fscore_support only for calculating precision, recall, and F1 score?\", \"Are there any other parameters than y_val, y_val_pred and zero_division=0 in precision_recall_fscore_support?\", \"How do I access the precision, recall, fscore, and support values returned by precision_recall_fscore_support?\"]',\n",
       " '7c68ace0': '[\"What is the main difference between using ROC curves and precision-recall curves for evaluating classification models?\", \"When does a dataset\\'s class imbalance affect the accuracy of its representation on a ROC curve?\", \"Why do true negatives play a significant role in the calculation of False Positive Rate in ROC curves?\", \"Can the performance of a model on a test set change if the class distribution changes, according to ROC curves?\", \"What metrics are typically used in addition to ROC curves to evaluate a model\\'s performance in cases of class imbalance?\"]',\n",
       " '147577f5': '[\\n\"How can we evaluate the feature importance for numerical variables with respect to AUC scores?\",\\n\"How do I calculate the AUC score for feature importance?\",\\n\"What is the best way to assess the importance of numerical variables in classification using AUC?\",\\n\"Can I use roc_auc_score function to evaluate feature importance for numerical variables?\",\\n\"What function from sklearn.metrics module can I use to calculate AUC score for feature importance?\"\\n]',\n",
       " 'd3ffb802': '[\"How does the F-score depend on the class imbalance in a classification problem?\", \"Why do I have to be careful when comparing F-scores across different classification problems with different class ratios?\", \"What are some common issues with comparing the F-score in different classification problems?\", \"Isn\\'t the F-score already adjusted for class imbalance in some way?\", \"Can I use the F-score to compare the performance of different classification models on problems with different class ratios?\"]',\n",
       " 'cc04d27a': 'Here are the 5 questions a student might ask based on the FAQ record:\\n\\n[\"How do I import precision_recall_curve from scikit-learn?\", \"What should I pass as arguments to precision_recall_curve?\", \"Can I plot Precision-Recall Curve directly from scikit-learn?\", \"Why are there threshold values in the plot?\", \"How do I label the Precision and Recall curves in the plot?\"]',\n",
       " '927b5e09': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\"What does Stratified k-fold do to keep class balance?\", \"Can I use Stratified k-fold for multiclass classification?\", \"Why is it important to keep class balance when splitting data sets?\", \"How does the Stratified k-fold approach in scikit-learn library work?\", \"Why is it necessary to split the data set in a way that maintains class balance?\"]',\n",
       " 'd22efea7': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[ \"How do I get access to Week 5 homework?\", \"Where can I find all homework assignments?\", \"Can I see an example of the solution to HW 3?\", \"What is the evaluation matrix for this week\\'s assignment?\", \"Are there any video lectures or resources available on the course GitHub page?\" ]',\n",
       " 'd1409f67': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I prepare my homework environment for week 5 since it introduces several layers of abstraction and dependencies?\", \"Can I use my default environment such as WSL or Ubuntu for week 5?\", \"What are the requirements for the default environment to complete the homework?\", \"Is it necessary to install pipenv for week 5?\", \"Can I run small AWS instances for free and are there any other alternatives to set up my homework environment?\"]',\n",
       " 'e07759e9': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I get started with the Kaggle API?\", \"What\\'s the purpose of the `kaggle.json` file?\", \"How do I set the environment variable `KAGGLE_CONFIG_DIR`?\", \"Can I download the CSV file in a different location?\", \"Why do I need to uncompress the downloaded file?\"]',\n",
       " '620fb76e': '[\"What are some basic Ubuntu commands I need to know for deploying my machine learning models?\", \"What does \\'cd ..\\' do in Ubuntu?\", \"Can you give me an example of how to use the \\'cd\\' command with a specific path?\", \"How do I check what the current directory is using Ubuntu?\", \"How do I edit a text file in Ubuntu using the \\'cat\\' command?\"]',\n",
       " '957280d8': '[\\n\"Can I check the Python version on my laptop before installing a new one?\", \\n\"How do I download and install Python 3.10 or higher on Windows?\", \\n\"What happens if I don\\'t check the \\'Add Python to PATH\\' box during installation on Windows?\", \\n\"Can I upgrade my current Python version to 3 or higher using pip?\", \\n\"If I\\'m having trouble installing Python 3.10 or higher, where can I get help?\"\\n]',\n",
       " '185096ad': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[{\"How do I activate the \\'Virtual Machine Platform\\' feature in Windows?\", \"Can I install any Linux distribution in Ubuntu, or are there specific ones recommended?\", \"Why do I need to move back two times with \\'cd\\' to go back to the Windows system in WSL?\", \"How do I disable bell sounds in WSL?\", \"What\\'s the alternative to pipenv if it\\'s not working due to a known issue?\"]}',\n",
       " 'ec88d101': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n    \"Why am I experiencing errors when building Docker images on my Mac with M1 silicon during the machine learning model deployment process?\",\\n    \"What is the complete error message I get when trying to build Docker images on a Mac with M1 silicon?\",\\n    \"Is there a specific fix for errors building Docker images on Mac M1 chipset, and does it involve replacing a line in the Dockerfile?\",\\n    \"What platform change should I make in the Dockerfile to resolve the error on Mac M1 silicon?\",\\n    \"Why did it take over 2 hours to build the Docker image after making the platform change?\"\\n]',\n",
       " '7156679d': '[\\n\"Can I use a single line of code to find the version of a python library installed in my Jupyter Notebook?\",\\n\"How do I print the version of a specific python library in Jupyter Notebook?\",\\n\"What is the simplest way to identify the version of an installed python library in Jupyter Notebook?\",\\n\"Is there a straightforward method to find the version of a python library using Jupyter Notebook?\",\\n\"In Jupyter Notebook, what is the recommended way to find the version of an installed python library?\"',\n",
       " '4b2a3181': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What do I do if I\\'m getting an error when running hello-world?\", \"Why can\\'t Docker connect to the daemon?\", \"Is the Docker daemon not running?\", \"How can I reinstall Docker on WSL?\", \"How can I start the Docker daemon on Linux?\"]',\n",
       " '73bd7fa1': '[\"What should I do when the command \\'/bin/sh -c pipenv install --deploy --system && rm -rf /root/.cache\\' returns a non-zero code: 1, and my Docker image is not created?\", \"How do I find the correct Python version to replace in the Dockerfile?\", \"Why is the Python version in the Dockerfile important?\", \"What happens when the Python version in the Dockerfile does not match the one installed in the system?\", \"What will happen if I don\\'t change the Python version in the Dockerfile?\"]',\n",
       " 'a4d3b1e5': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What happens when I run pipenv install sklearn==1.0.2 and it gives errors?\",\\n\"How do I install a specific version of sklearn without errors?\",\\n\"Can I use the same sklearn version used in the lectures for the homework?\",\\n\"What is the correct command to install sklearn for the homework?\",\\n\"Why did the facilitator use a different sklearn version in the lectures?\"]',\n",
       " '1d462fe0': '[\"Why do we need to remove the docker containers from our system\", \"What happens when I don\\'t specify a version when building an image\", \"Can I keep the docker images in my system and only remove the containers\", \"How do I remove the docker images manually\", \"Why do we need to use the `docker images` command during development and testing\"]',\n",
       " '366d7563': '[\"What should I name my Dockerfile to avoid an error when building the docker image?\", \"Why does creating a Dockerfile with an extension (.dockerfile) cause problems?\", \"What is one common mistake that leads to the \\'Failed to read Dockerfile\\' error?\", \"Can I have an example of a correct Dockerfile name to avoid this issue?\", \"What is the solution to avoid extension errors when creating a Dockerfile?\"]',\n",
       " 'cef156d1': '[\\n\"How do I go about setting up Docker on my Mac if I have an Apple chip?\",\\n\"Is it possible to install Docker on a MacBook with an Intel chip?\",\\n\"What is the recommended process for deploying machine learning models during this course?\",\\n\"Can you provide a direct link to install Docker on MacOS?\",\\n\"What are the installation steps for Docker on a Mac according to the Docker documentation?\"',\n",
       " 'b632d2ea': 'Here are the 5 questions that might be asked by a student based on the FAQ record:\\n\\n[\"What does the error message \\'manifest unknown: manifest unknown\\' mean when trying to pull an image with docker pull command?\", \"Why do I get an error message when I use the default \\'latest\\' tag to pull an image?\", \"How do I avoid getting an error message when pulling an image with docker pull command?\", \"Can I use any tag while pulling an image, or do I have to use a specific one?\", \"How can I find the correct tag to use when pulling an image with docker pull command?\"]',\n",
       " '514e27bb': '[\\n  \"How do I get a list of all local Docker images, including their sizes, from the command line?\",\\n  \"Can I retrieve information for a specific Docker image instead of all images?\",\\n  \"Is it possible to customize the information displayed for a given Docker image?\",\\n  \"How can I get the size of a Docker image without seeing other information?\",\\n  \"Do I need to install any additional software or plugins to use these Docker commands?\"\\n]',\n",
       " '5c67e086': 'Here is the list of questions based on the FAQ record:\\n\\n[\\n\"Where does pipenv create machine learning model environments exactly when I run pipenv commands?\",\\n\"When deploying machine learning models, how does pipenv name the created environments in different operating systems?\",\\n\"How does pipenv determine the environment name when I run pipenv commands in a specific folder?\",\\n\"Can you show me an example of the directory path where pipenv creates the machine learning model environment on Linux?\",\\n\"How do I activate the machine learning model environment created by pipenv after navigating to the project folder?\"',\n",
       " '63a81b57': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What steps should I take to debug a Docker container if it\\'s not working correctly?\", \"Can I still debug a running Docker container?\", \"How do I find the ID of a Docker container?\", \"Can I run a bash command directly in a Docker container?\", \"How do I execute a command inside a running Docker container?\"',\n",
       " '047f57fb': 'Here are the 5 questions:\\n\\n[\"What does it mean when the input device is not a TTY when running docker in interactive mode?\", \"Why do I get the error \\'the input device is not a TTY\\' when using mintty?\", \"Can I run multiple commands at once with the fix for the TTY error?\", \"What is the purpose of a TTY interface?\", \"Can I find more information about terminal, shell, and console applications?\"]',\n",
       " '11f7371c': '[\\n\"Why do models fail to load when I copy them from the original Docker image, and what\\'s the temporary solution for this issue?\",\\n\"Why did the original model1.bin and dv.bin not work when copied directly?\",\\n\"What are the implications of using \\'COPY [*] ./\\'] in MINGW64 (git bash) on Windows?\",\\n\"Can we assume combining all files with \\'COPY [*] ./\\'] will solve the issue in all environments?\",\\n\"How can I ensure the model is properly loaded when using \\'COPY [*] ./\\']?\"',\n",
       " '45f39b76': '[\"What is the correct command to create a virtual environment?\", \"How can I write the requirements in a text file using the pip freeze command?\", \"Is the pipfile needed for writing dependencies?\", \"Can I use the piplock file for writing dependencies instead of the pipfile?\", \"Will using a virtual environment specify the dependencies for the machine learning models?\"]',\n",
       " '94e17563': '[\"What are the rules for using f-strings in Python and what could be the consequences of not following them?\", \\n\"How do f-strings interact with variables like model_C and dv?\",\\n\"Why do we need to use {} instead of () when formatting strings with f-strings?\", \\n\"What happens when we forget to add parentheses when using pickle.dump()?\", \\n\"What is the relationship between f-strings and pickle.dump() and how do they impact the deployment of machine learning models?\"]',\n",
       " '9dd8efd2': '[\\n\"Can you explain why I get the error \\'pipenv\\' is not recognized as an internal or external command, operable program or batch file when I try to install pipenv?\",\\n\"What is causing the issue when I run \\'pipenv --version\\' and \\'pipenv shell\\'?\",\\n\"I\\'m using Windows, how can I solve the \\'pipenv\\' is not recognized error?\",\\n\"What are the two locations I need to add to my PATH for pipenv to work?\",\\n\"Why might using Anaconda be a better choice for Windows users compared to using pipenv?\"',\n",
       " '9531dc92': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why do I get an AttributeError when following the instructions from week-5.6?\", \"What happens if I\\'m not using the correct Python version?\", \"How can I fix the AttributeError when using pipenv to install Python libraries?\", \"I\\'m following the Zoomcamp lessons, but I still got the error, what am I doing wrong?\", \"Why is it mentioned in the very first lesson that I should use a specific Python version?\"]',\n",
       " '14e0e697': '[\"What is the command to exit the shell after running pipenv shell?\", \"Can using pipenv shell without exiting cause errors when installing packages?\", \"How do I fix the PATH issue if it gets messed up after running pipenv --rm?\", \"Why is it important to manually recreate the removed folder?\", \"What are the terminal commands to reset the VIRTUAL_ENV variable on different systems?\"]',\n",
       " '6189375f': '[\"What can I do if I encounter a ConnectionError: (\\'Connection aborted.\\', RemoteDisconnected(\\'Remote end closed connection without response\\')) when deploying my machine learning model? Should I set the host address to a specific IP or can I use a different solution?\", \"How do I resolve the RemoteDisconnected error when running my URL using localhost? Is there a way to modify the behavior of the connection without changing the actual server?\", \"Can I use any other setting instead of \\'0.0.0.0\\' for the host, or is this the recommended solution for handling connection issues? Is there a specific scenario where \\'0.0.0.0\\' might not work?\", \"Why do I need to set the host to \\'0.0.0.0\\' in my Flask app and Dockerfile to resolve the ConnectionError? Is there a particular benefit to this approach over others?\", \"Can you provide more information on the implications of setting the host to \\'0.0.0.0\\' in my application? Are there any potential trade-offs or considerations I should be aware of?\"]',\n",
       " '3419ee27': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"Why do I get a \\'docker build ERROR\\' when I run a command?\",\\n\"I keep getting an error when I try to \\'COPY...\\' something, what\\'s wrong?\",\\n\"I\\'ve tried to build a Docker image, but I get an error. Can someone help me?\",\\n\"I\\'m trying to deploy a machine learning model with Docker, but I keep getting an error. What\\'s the solution?\",\\n\"I\\'m having trouble with Docker and I get an error that says \\'COPY...\\', what\\'s going on?\"]',\n",
       " '8b8c1603': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What do I do if I encounter an error during the installation of Pipfile inside a Docker container?\",\\n\"Can you suggest a method to resolve issues during Pipfile installation inside Docker?\",\\n\"I tried a solution on Stackoverflow that recommended running a certain command, but it didn\\'t work. What can I try next?\",\\n\"How can I successfully install Pipfile inside a Docker container?\",\\n\"Does switching to a different installation method, such as running a specific pipenv command, resolve the issue?\"',\n",
       " 'e54d5411': 'Here are 5 questions formulated based on the FAQ record:\\n\\n[\"How do I troubleshoot an issue with Docker after running the \\'docker run\\' command?\", \"What error message is caused by an existing gunicorn instance?\", \"How do I stop a Docker container?\", \"Why can\\'t I remove an orphaned Docker container?\", \"What steps should I take to resolve a Docker image issue?\"]',\n",
       " 'f7b38587': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What is the error message when rebuilding a docker image?\", \"Why is the error message not accurate?\", \"How can I resolve this issue?\", \"What is the command to resolve port conflict?\", \"Is this issue specific to Docker for Windows?\"]',\n",
       " 'be86b333': 'Here are the 5 questions:\\n\\n[\"What causes a bind for 127.0.0.1:5000 to show errors?\", \"What is the error message on the client side?\", \"What is the error on the server side when using gunicorn?\", \"Why does binding to 0.0.0.0:8000 or 0.0.0.0:9696 work?\", \"How can I resolve the RemoteDisconnected error?\"]',\n",
       " '4ea80460': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"How do I install the md5sum tool on my MacOS?\", \\n\"What is the correct command to check the hash values for the provided files?\", \\n\"Can I install the md5sum tool using a different command than mentioned in the answer?\", \\n\"What step do I need to take after installing the md5sum tool?\", \\n\"Are the commands provided in the answer the only way to verify the hash values?\"',\n",
       " '8006b496': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"What should I do if I want to run another python script that makes requests to a web-server I started in the same terminal?\",\\n\"How do I launch a new script in addition to the one running a web-server?\",\\n\"Can I run multiple python scripts at the same time when using a command window?\",\\n\"Is it necessary to open a new terminal to run a script that interacts with a running web-server?\",\\n\"Can I run the second script in the same terminal where the web-server is running or do I need a new one?\"',\n",
       " '704f95d8': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"What happens when I get a version-conflict warning while running pipenv?\",\\n\"Why am I getting a warning when using pipenv with Scikit-Learn?\",\\n\"What should I check to avoid version-conflict problems in Scikit-Learn?\",\\n\"What\\'s the recommended way to create virtual environments for machine learning projects?\",\\n\"What happens if I don\\'t use the same Scikit-Learn version for training and running the model?\"',\n",
       " 'a5b3296b': 'Here are the 5 questions:\\n\\n[\"What does the error message \\'python_version: \\'python_version\\' must not be present with \\'python_version\\'\\' mean?\", \"Why do I have to remove either python_version or python_full_version?\", \"How do I open the Pipfile in the nano editor?\", \"What does pipenv lock do?\", \"Can I continue installing packages if I remove either python_version or python_full_version?\"]',\n",
       " 'a23b276a': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"What happens if my Pipfile.lock is out of date during the docker build command?\",\\n\"Why does my Pipfile.lock contain an expected value that\\'s different from what I have?\",\\n\"Can I just rebuild the lock without deleting the old one?\",\\n\"What\\'s the command to remove the pipenv environment and delete the Pipfile and Pipfile.lock files?\",\\n\"What\\'s the advantage of creating a new pipenv environment instead of just rebuilding the lock?\"',\n",
       " '3537eeee': '[\"What happens when using waitress instead of gunicorn in a Windows environment with a conda environment, and why do I suddenly experience issues with running the MLflow server?\", \"When I uninstall waitress and reinstall MLflow, what are the steps I need to take?\", \"Why do I receive an error when trying to run the MLflow server after making these changes?\", \"How can I overcome the issues with running the MLflow server after switching to waitress?\", \"What are the general precautions I should take to avoid this problem in the future?\"]',\n",
       " '1d6d5b51': '[\"How do I ensure I\\'m in the correct AWS region when checking environments?\", \"Why can\\'t I find my environment on AWS even though I created it locally?\", \"Is region selection the only issue I should check when deploying my model to AWS?\", \"What could I be missing that\\'s causing my environment to not be visible on AWS?\", \"How do I verify if I\\'m in the eu-west-1 (ireland) region in my AWS console?\"]',\n",
       " '3a98b6b7': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why isn\\'t the \\'waitress-serve\\' command found when I run \\'pip install waitress\\' in GitBash?\", \"How do I get the executable file \\'waitress-serve.exe\\' when installing waitress via pip?\", \"How can I add the path of \\'waitress-serve.exe\\' to GitBash\\'s PATH?\", \"Will running \\'pip install waitress\\' in a Jupyter notebook resolve the issues with \\'waitress-serve\\'?\", \"What is the purpose of the warning \\'The script waitress-serve.exe is installed in...\\' and how can I suppress it?\"]',\n",
       " 'd42eb923': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What does the error \\'Warning: the environment variable LANG is not set!\\' mean?\", \"How can I fix the error \\'Warning: the environment variable LANG is not set!\\'?\", \"Is this error fatal?\", \"Can I proceed with the deployment even if I get this warning?\", \"What does it mean for \\'explicit language specifications\\' not to be set in the bash profile?\"]',\n",
       " '42aebe10': '[\"How do I know which model files to use for this homework question?\", \"What version of the model image do I need to use?\", \"Where exactly do I find the model and dictvectorizer files?\", \"Can I use any version of the model image or do I have to use the exact one mentioned?\", \"Are the model and dictvectorizer files named \\'model2.bin\\' and \\'dv.bin\\' elsewhere in the project or only in this image?\"]',\n",
       " 'e4f62713': '[\"What terminal should I use for the week 5 videos?\", \"Where can I download the terminal used in week 5 videos?\", \"What is the terminal used in week 5 videos?\", \"Can I use any terminal for the week 5 videos?\", \"How do I get the terminal used in week 5 videos?\"]',\n",
       " 'c13d811f': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What could be causing an exception when running pipenv run waitress-serve?\", \"Why do I get a \\'ValueError\\' when importing a Python module?\", \"What is a \\'Malformed application\\' and how does it affect my deploy?\", \"How do I fix the \\'Malformed application\\' error when using Python file names?\", \"What is the recommended way to rename a Python file to avoid this issue?\"]',\n",
       " 'dfb41f7e': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"How do I test HTTP POST requests from the command line using curl on Windows?\", \"Can I use curl to test HTTP POST requests on a Linux or MacOS machine?\", \"What is the purpose of piping in the curl command, and how does it improve testing?\", \"Can I use the echo command with curl to test HTTP POST requests?\", \"How do I interpret the output of the curl command when testing HTTP POST requests?\"]',\n",
       " 'd04e77f8': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n{\\n\"question1\": \"What are the limitations of using \\'eb local\\' when running a machine learning model?\",\\n\"question2\": \"Why do I get a NotSupportedError when trying to use \\'eb local\\' with my Docker platform?\",\\n\"question3\": \"How can I fix the NotSupportedError when using \\'eb local\\' with my Docker platform?\",\\n\"question4\": \"What is the difference between re-initializing and editing the \\'.elasticbeanstalk/config.yml\\' file to fix the NotSupportedError?\",\\n\"question5\": \"Are there any potential drawbacks to editing the \\'.elasticbeanstalk/config.yml\\' file instead of re-initializing the Docker platform?\"',\n",
       " '451c067f': '[\"What do I need to fix this Requests error saying \\'No connection adapters were found for localhost:9696/predict\\'?\", \\n\"What is the protocol scheme I need to include in my requests to fix the error?\", \\n\"Can I use HTTP:// instead of http:// in my URL without any issues?\", \\n\"Why doesn\\'t requests understand the connection adapter without the http:// part?\", \\n\"I forgot the http:// part, but why is it all lowercase?\"]',\n",
       " '9fbfcd61': '[\"What do I need to check if I\\'m getting the same result while running the docker image?\", \"How do I ensure I\\'m using the correct model when making predictions?\", \"What can cause inconsistencies in model results?\", \"Can I use multiple models in my file for prediction testing?\", \"How can I adjust my model to get different results?\"]',\n",
       " '1ed8cfde': 'Here are the 5 questions as a parsable JSON:\\n\\n[{\\n\\t\"QUESTION\": \"When trying to run a docker image I built, why does it say it\\'s unable to start the container process?\",\\n\\t\"QUESTION\": \"How do I ensure the necessary modules are included when building a docker image?\",\\n\\t\"QUESTION\": \"What is the main reason my docker image is unable to start the container process?\",\\n\\t\"QUESTION\": \"How can I troubleshoot issues with running a docker image I built?\",\\n\\t\"QUESTION\": \"What command should I use to install all necessary modules before building and running a docker image?\"',\n",
       " '3f97f50f': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How do I copy files from my local machine to a Docker container?\",\\n\"Why do I need to specify the path in the docker cp command?\",\\n\"What if I want to copy a directory instead of a single file?\",\\n\"Can I use the docker cp command to copy files from the container to my local machine?\",\\n\"Is it necessary to specify the container ID in the docker cp command?\"',\n",
       " 'a24a874a': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What command do I use to copy files from my local machine into a Docker container?\", \"In Dockerfile, can I specify a folder containing files to copy, and if so, how?\", \"Are there any specific syntax requirements when copying files from a different folder into Docker container\\'s working directory?\", \"Can I copy multiple files from a different folder into Docker container\\'s working directory at once?\", \"Is it necessary to specify the Docker container\\'s working directory when copying files using the docker cp command?\"]',\n",
       " 'bf563b1f': 'Here are the 5 questions the student might ask:\\n\\n[\\n\"What is the correct command syntax to deploy a machine learning model on AWS Elastic Beanstalk using Docker?\",\\n\"I\\'ve tried running \\'eb local run --port 9696\\' but got an error, what\\'s the issue?\",\\n\"Can I use \\'eb local\\' with any Docker platform or are there specific requirements?\",\\n\"Why did changing the command to \\'eb init -p \"Docker running on 64bit Amazon Linux 2\" ...\\' resolve the problem?\",\\n\"I\\'m still having issues with deploying my model, what are some common pitfalls to avoid when setting up the environment?\"',\n",
       " '21e9facf': 'Here are 5 questions based on the FAQ record:\\n\\n[\\n\"What should I do when I encounter an error that states \\'Dockerfile\\' is missing during AWS ElasticBean environment creation?\",\\n\"Why am I getting an error that says \\'Dockerfile\\' is missing when creating a AWS ElasticBean environment?\",\\n\"What is the problem when I use \\'eb create <env-name>\\' and I don\\'t see a \\'Dockerfile\\' in my project?\",\\n\"Why does the AWS ElasticBean environment require at least one file, either \\'Dockerfile\\' or \\'Dockerrun.aws.json\\', to be present?\",\\n\"During AWS ElasticBean environment creation, how do I resolve the error stating \\'Dockerfile\\' and \\'Dockerrun.aws.json\\' are missing from my source bundle?\"',\n",
       " 'aef786aa': '[\\n\"How can I access the homework for Week 6 of the Machine Learning Zoomcamp course?\", \\n\"Where can I find the solutions to the previous homework assignments?\", \\n\"How do I find the evaluation matrix for the course?\", \\n\"Which GitHub repository should I look at for learning the theory of machine learning?\", \\n\"What YouTube video covers decision trees and ensemble learning in the course?\"]',\n",
       " '68858294': '[\"How do we get the training and validation metrics from XGBoost in a more straightforward way?\", \"Can we store the evals_result in a dataframe and plot it?\", \"What\\'s the purpose of the parser created during the XGBoost lesson?\", \"Are there any other ways to extract the training and validation metrics besides using the evals_result parameters?\", \"Can the evals_result dictionary be updated for each tree in the XGBoost model?\"]',\n",
       " '85ac722e': '[\"How do I use scikit-learn to solve regression problems with random forest?\", \"What is the main difference between Random Forest for classification and regression in scikit-learn?\", \"Is there a specific syntax for creating a random forest regressor in scikit-learn?\", \"Can you provide more details on the parameters and options for the RandomForestRegressor object in scikit-learn?\", \"Is there any tutorial or documentation I can refer to for learning more about RandomForestRegressor in scikit-learn?\"]',\n",
       " 'b61d2e92': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What should I do if I get a ValueError when creating a DMatrix for training and validation?\", \"Why am I getting a ValueError when some feature names contain special characters?\", \"How do I fix the ValueError if feature names contain the \\'<\\' symbol?\", \"Is there an alternative solution to replacing special characters in feature names?\", \"Can I use the \\'=\\' symbol in feature names without causing an error?\"]',\n",
       " '8d7392cb': '[\\n\"Can you explain why I\\'m getting a TypeError when training an xgboost model, and how do I resolve this issue if the feature names returned by dv.get_feature_names_out() are a numpy array instead of a list?\",\\n\"What is causing the TypeError: Expecting a sequence of strings for feature names, got: <class \\'numpy.ndarray\\'> in my xgboost model, and how do I convert the feature names into a list?\",\\n\"Can you provide an example of how to convert the feature names in dv.get_feature_names_out() from a numpy array to a list when training an xgboost model?\",\\n\"When I use dv.get_feature_names_out(), I receive a numpy array instead of a list, which results in a TypeError when training my xgboost model. How do I fix this?\",\\n\"Why does the error TypeError: Expecting a sequence of strings for feature names, got: <class \\'numpy.ndarray\\'> occur when I\\'m training an xgboost model, and how do I prevent this from happening if my feature names are a numpy array?\"',\n",
       " 'c920eef3': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What could be the reason for getting TypeError when setting xgb.DMatrix with feature_names?\", \"How do I fix ValueError when setting xgb.DMatrix with feature_names?\", \"Why am I getting TypeError when trying to convert np.ndarray to list for feature_names?\", \"What are the symbols that XGBoost does not like in feature names?\", \"What are the alternative solutions to specify feature_names when creating xgb.DMatrix?\"]',\n",
       " '5017c9a4': 'Here are the 5 questions a student might ask based on the FAQ record:\\n\\n[\"What are the steps to install Xgboost in a Jupyter notebook?\",\\n\"How can I upgrade my pip version?\",\\n\"What is the minimum version of pip required for installing Xgboost?\",\\n\"Is it possible to install Xgboost directly in a Jupyter notebook?\",\\n\"Where can I find more information about the installation process of Xgboost and xgbboost?\"]',\n",
       " '6ffe101d': '[\"What is eta in XGBoost and how does it relate to the learning rate?\",\"How does XGBoost calculate and update the model using gradient descent?\",\"What does the learning rate control in the training process?\",\"Why is it important to tune the learning rate in gradient descent?\",\"How does XGBoost use the learning rate to update the model weights each time it passes through the features during training?\"]',\n",
       " 'a55b29ff': '[\"Why are multiple samples taken with replacement in bagging?\", \"How do models combine their predictions in bagging?\", \"Can bagging reduce bias?\", \"Is bagging more accurate than boosting?\", \"Can overfitting occur in bagging?\"]',\n",
       " 'eac70ce3': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I capture the stdout for each iteration of a loop separately?\", \"Can you provide an example of how to use the capture_output function in a Jupyter Notebook?\", \"Why wasn\\'t the magic cell command %%capture output sufficient for capturing different outputs from each iteration?\", \"How do I customize the output that is captured by the capture_output function?\", \"Is there a way to capture the output of a loop in a Jupyter Notebook without using the capture_output function?\"]',\n",
       " '5f91f8ca': '[\"What is ValueError: continuous format is not supported, and how does it relate to roc_auc_score()? Why do we need to pass y_actuals as the first argument and y_pred as the second argument?\", \"When would we typically use roc_auc_score(), and what is the purpose of this function?\", \"Can roc_auc_score() be used for any type of data, or are there specific conditions it requires?\", \"How does passing y_actuals as the first argument and y_pred as the second argument solve the ValueError issue?\", \"Who is Hareesh Tummala, and what is their relevance to this solution?\"]',\n",
       " 'a3be507a': '[\"What does it mean when rmse stops improving and how can I identify it?, Is \\'when it stops to decrease or remains almost similar\\' a general rule for other machine learning models, Can I use other metrics besides rmse to determine when to stop training, I was expecting the answer to be the number of n_estimators with the lowest rmse value not the one after which it stopped improving, Is there a specific number of iterations that I should wait before considering the rmse to be stable, What happens if I set a high number of n_estimators, will it always lead to better results\"]',\n",
       " '9a8faa50': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How can I visualize the decision trees in my model?\",\\n\"What are some methods for visualizing decision trees?\",\\n\"Can I use graphviz to visualize decision trees?\",\\n\"Is it possible to export the decision tree as an image?\",\\n\"Are there other ways to plot decision trees besides using graphviz and dot_data?\"',\n",
       " 'a6e384fe': '[\"Why do I get ValueError: Unknown label type: \\'continuous\\' when I use DecisionTreeClassifier?\", \"Can I use DecisionTreeClassifier for regression?\", \"What type of decision tree should I choose for classification?,\", \"What\\'s the difference between DecisionTreeClassifier and DecisionTreeRegressor?\", \"How do I decide which algorithm to use for my problem?\"]',\n",
       " 'ddc14ada': 'Here are the questions the student might ask based on the FAQ record:\\n\\n[\"Why do I get different values of AUC when I re-run my code or restart the kernel?\", \"What could be the reason for the variation in AUC values when I run my DecisionTreeClassifier in Jupyter?\", \"Is it normal to get different AUC scores every time I run my DecisionTreeClassifier code?\", \"Can you explain why my AUC values are not consistent when I re-run my code in Jupyter?\", \"How can I ensure that I get the same AUC value for my DecisionTreeClassifier every time I run the code?\"]',\n",
       " '593f7569': '[\"How do we ensure the server always runs with decision trees and ensemble learning?\", \"Is there any difference in functionality between using Python and gunicorn directly for server creation?\", \"Do we need to use any specific libraries for decision tree implementation?\", \"What is the importance of ensemble learning in machine learning?\", \"Can we combine different machine learning models using ensemble learning?\"]',\n",
       " '6cb56405': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\"How do I import the ping function?\", \"Why can\\'t I use import ping to import the ping function?\", \"What do I need to do to use the ping function?\", \"What is the correct way to import the ping function?\", \"How can I resolve the issue of the \\'ping\\' module not being recognized?\"]',\n",
       " 'a22a93f1': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"How do I get the feature names when using DictVectorizer for one-hot encoding?\",\\n\"What is the format of the output from get_feature_names_out()?\",\\n\"Can I use DictVectorizer for one-hot encoding without fitting the predictor and response arrays first?\",\\n\"How do I convert the ndarray array returned by get_feature_names_out() to a standard list of strings?\",\\n\"What are some common use cases for get_feature_names_out(), apart from analyzing feature importance?\"',\n",
       " 'b6259dea': 'Here are the 5 questions a student might ask based on the FAQ record:\\n\\n[\\n\"What characters in the feature names are causing the ValueError error?\",\\n\"How do I replace these problematic characters in the feature names?\",\\n\"Can I replace other characters in the feature names if I want to use different replacements?\",\\n\"Why would I want to replace the special characters with underscores?\",\\n\"What are the possible solutions to fix the ValueError error in general?\"',\n",
       " 'bcfdc6f4': 'Here are the questions based on the FAQ record:\\n\\n[\"How do I visualize feature importance in a horizontal bar chart?\", \"What kind of chart do we use to show feature importance by value?\", \"How do I sort the feature importances in descending order?\", \"What\\'s the purpose of the \\'Blues_r\\' palette in the chart?\", \"Can we use this approach to visualize feature importance for any machine learning model?\"]',\n",
       " 'a7e7cdd2': '[\\n\"How can I calculate the Root Mean Squared Error (RMSE) using metrics.root_meas_square()?\", \\n\"Is there a way to extract the Root Mean Squared Error (RMSE) without using np.sqrt() as the second step?\", \\n\"Can I calculate RMSE without having to square the predicted and actual values first?\", \\n\"What alternative method can be used to calculate RMSE if not using np.sqrt()?\", \\n\"Is there a common way to calculate RMSE in a certain library?\"',\n",
       " '55477da8': '[\"What is the purpose of the visualization for features importance in scikit-learn library?\", \"How does the features importance graph help with model explainability?\", \"Can you provide more information about the standard errors used in the features importance implementation?\", \"Why is it important to consider the standard errors in features importance?\", \"What are the different parameters of the model that the standard errors are used to trace?\"]',\n",
       " '6a245a05': '[\"What does the xgboost.core.XGBoostError message mean and why did it occur in my code?\", \"Why do I need to install sklearn to use this module?\", \"Is the absence of sklearn the only reason that caused this error?\", \"Can you provide more information about the error message?\", \"What is the solution to this error, and is there a minimal set of requirements needed to use this module?\"]',\n",
       " '4405bfca': '[\"How does the formula for Information Gain account for the entropy of Y? What is the significance of using this formula to measure the amount of information gained by knowing X about Y?\", \"What happens to Information Gain if X is completely uninformative about Y? Is it always 0 or does it depend on the initial entropy of Y?\", \"How does the Information Gain formula handle the case where X is completely informative about Y? Does it return a high value or does it saturate at a certain point?\", \"Is Information Gain a universal measure of suitability for all decision tree algorithms or are there specific considerations for certain algorithms?\", \"What is the relationship between Information Gain and entropy? Can the formula for Information Gain be derived from the formula for entropy?\"]',\n",
       " '3e0acc25': '[\"What happens if I accidentally include all my data in the training set before splitting it into training, testing, and validation sets?\", \"Can this mistake significantly affect my model\\'s performance?\", \"I\\'ve heard that filling in missing values can lead to data leakage, can you explain why?\", \"How can I avoid filling in missing values in a way that causes data leakage?\", \"What are some common scenarios where data leakage can occur?\"]',\n",
       " 'abaecdf8': '[\"Why do I get an error when trying to save a serialized model in Xgboost?\", \"Can I also load a model in Xgboost other than the default way?\", \"How can I save a model in Xgboost and keep it for later use?\", \"What is the correct procedure to save and load a model in Xgboost?\", \"How do I load a saved model in Xgboost?\"]',\n",
       " 'ff40f83b': '[\"What are the requirements for starting Week 8 on neural networks and deep learning?\", \"Do we need to have prior knowledge of another section to start Week 8?\", \"Can we get an overview of the topics we\\'ll cover in Week 8?\", \"Are there any specific readings or resources required for Week 8?\", \"What is the format for the assignments in Week 8 and how will they be graded?\"]',\n",
       " '95a16746': 'Here are the 5 questions:\\n\\n[\"How do I use Kaggle for creating deep learning notebooks?\",\\n\"What are the steps to import my own notebook into Kaggle for deep learning?\",\\n\"What is the process to use the Three dots at the top right hand side in Kaggle?\",\\n\"Can I choose a GPU for my deep learning model in Kaggle? And if so, how?\",\\n\"Is there a specific option called \\'Accelerator\\' that I need to activate in Kaggle for deep learning?\"]',\n",
       " '46acdd18': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How can I create a notebook in Google Colab for Deep Learning?\",\\n\"Why do I need to change the runtime type in Google Colab?\",\\n\"What specific steps do I need to follow to use T4 GPU in Google Colab?\",\\n\"Is it safe to import an existing notebook into Google Colab?\",\\n\"Can I use Google Colab for other purposes beyond Deep Learning?\"',\n",
       " 'f721d54b': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\"How do I have the option to use technology to push my Saturn Cloud project to Github?\", \"Do I need to download the notebook and copy it to the Github folder?\", \"Why is creating an SSH private and public key necessary?\", \"How do I add SSH keys to secrets through the Saturn Cloud terminal?\", \"Can I use the default public key provided by Saturn Cloud instead of the generated one?\"]',\n",
       " '69cd4897': '[\"What is the correct location of the Python TensorFlow template on Saturn Cloud?\", \"Why is the location in video 8.1b Setting up the Environment no longer correct?\", \"Where can I find the \\'python deep learning tutorials\\' on Saturn Cloud?\", \"How do I know which tutorials are on the Saturn Cloud home page?\", \"Is Steven Christolis the creator of the template or just someone who corrected the location?\"]',\n",
       " '346e799a': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why am I getting an error saying module scipy not found during model training in Saturn Cloud tensorflow image?\",\\n\"How do I resolve the scipy not found error in Saturn Cloud tensorflow image model training?\",\\n\"What package needs to be installed for model training in Saturn Cloud tensorflow image if I get a scipy not found error?\",\\n\"Why do I need to install scipy explicitly in Saturn Cloud tensorflow image for model training?\",\\n\"What other python packages can I install for model training in Saturn Cloud tensorflow image, if I need to install scipy explicitly?\"]',\n",
       " '551461b2': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\"What is the better solution to upload kaggle data to Saturn Cloud?\", \"How do I create a new API token on the Kaggle website?\", \"Where do I need to upload the kaggle.json file?\", \"How do I download a dataset using the kaggle command?\", \"What folders do I need to create to unzip the downloaded files?\"]',\n",
       " 'c3ba4459': 'Here are 5 questions based on the FAQ record:\\n\\n{\"question1\": \"Can I install CUDA and cuDNN on Ubuntu 22.04 for running TensorFlow with GPU local machine setup?\", \\n\"question2\": \"Why do I need to setup CUDA and cuDNN for running TensorFlow with GPU on my local machine?\", \\n\"question3\": \"What is the process to install CUDA and cuDNN on Ubuntu 22.04 for running TensorFlow with GPU?\", \\n\"question4\": \"Will the process of setting up CUDA and cuDNN on Ubuntu 22.04 be overwhelming?\", \\n\"question5\": \"Is there a simplified guide available for setting up CUDA and cuDNN on Ubuntu 22.04 for running TensorFlow with GPU?\"}',\n",
       " 'a114ad55': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What is the error that occurs when trying to load a saved model in HDF5 format?\", \"Why am I getting this error when I try to load my model?\", \"Does this error happen when I\\'m trying to use a custom class for my model?\", \"Is there a specific order in which I need to call methods when loading a saved model?\", \"Do I need to prepare my input data in a certain way before loading and evaluating my model?\"]',\n",
       " 'dd3c8000': '[\\n\"Can I get more information on why the ssh command is giving me a permission denied error when setting up git in Saturn Cloud?\",\\n\"How do I generate the SSH key and add it to my git account when setting up git in Saturn Cloud?\",\\n\"Can I access/manage my git through Saturn\\'s jupyter server, or do I need to use the command line?\",\\n\"Can you walk me through the detailed steps of setting up git in Saturn Cloud as described in the provided tutorial?\",\\n\"What is an alternative way to set up git in Saturn Cloud if the original method I tried doesn\\'t work?\"\\n]',\n",
       " '34b0ebfc': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What if I get a \\'Host key verification failed\\' error when cloning a GitHub repository?\",\\n\"Why do I need to configure my SSH key when cloning a repository?\",\\n\"What does \\'fatal: Could not read from remote repository\\' mean in this context?\",\\n\"Can I still clone the repository if I don\\'t have SSH key configured?\",\\n\"Is it recommended to use HTTPS instead of SSH when cloning repositories?\"',\n",
       " '7d11d5ce': '[\"What can be the reasons for the same accuracy on epochs during training in the neural network?\", \"Why is it necessary to set class_mode=\\'binary\\' in the homework while reading the data?\", \"What are the common mistakes that might cause the same accuracy on epochs during training in the neural network?\", \"Is the same accuracy on epochs always related to incorrect usage of the optimizer, batch size, or learning rate in the neural network?\", \"When exactly does the problem of same accuracy on epochs during training in the neural network occur?\"]',\n",
       " 'e4e45f15': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"Why does my neural network model break after applying image augmentation, with high loss and bad accuracy?\", \"How do I resolve the issue of high loss after resuming training after image augmentation?\", \"What could be the cause of my model\\'s accuracy dropping to 0.5 after using an augmented ImageDataGenerator?\", \"Can you suggest a solution to the problem of high loss during the first epoch after restarting training with augmented data?\", \"What precaution should I take when using augmented ImageDataGenerator to prevent model accuracy from decreasing\"]',\n",
       " 'b3997e6f': '[\"What happens if the number of channels is not explicitly specified in the model architecture and how does it affect model reloading?\", \"Why do I get a \\'channel dimension of the inputs should be defined\\' error when loading a saved model?\", \"What are the two things that are saved when a model is saved?\", \"How does having the number of channels as a variable affect the model architecture?\", \"Can you provide an example of how to specify the number of channels explicitly in the model architecture?\"]',\n",
       " 'e414df91': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n1. How do I unzip a folder with an image dataset in a jupyter notebook without seeing all the output messages about the unzipping process?\\n2. What is the purpose of using the!! command in the solution for unzipping a zipped folder in a jupyter notebook?\\n3. Can I also unzip a zip file using the Python library, and if so, how do I do it?\\n4. How do I specify the name of the zipped folder when using the unzip command in a jupyter notebook?\\n5. Is the `%%capture` command only used for suppressing output, and if not, what else can it be used for in a jupyter notebook?',\n",
       " 'f20a3479': 'Here are 5 questions based on the FAQ record:\\n\\n[\\n\"How does keras\\' flow_from_directory recognize the class names of images even when they don\\'t provide a explicit list of class names?\",\\n\"How do I add a new class to the classification problem that is not already represented in the folder structure?\",\\n\"What happens if I create a folder with an invalid class name that is not recognized by flow_from_directory?\",\\n\"Is there a way to manually specify the class names when using flow_from_directory, or does it always rely on the folder names?\",\\n\"What if I want to use a class name that is not the exact same as the folder name where the images are located?\"',\n",
       " 'e7af4968': 'Here are 5 questions based on the FAQ record:\\n\\n[\"I tried to create a new environment in SaturnCloud and install the necessary modules, but I\\'m still getting an error about scipy being missing.\", \"I created a new environment in SaturnCloud using the image corresponding to Saturn with Tensorflow, but why am I getting an error about a missing scipy module?\", \"How do I fix the error about scipy being missing when I try to fit my model in SaturnCloud?\", \"I installed scipy in a new cell using !pip install scipy, but I\\'m still getting the error, why?\", \"I restarted the kernel and tried to fit the model again, but the error persists, what else can I do?\"]',\n",
       " '9fad096e': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How are numeric class labels determined in flow_from_directory when using binary class mode?\", \"What is meant by the single probability predicted by a binary Keras model?\", \"What happens when using sigmoid activation function in the last dense layer with 2 neurons?\", \"How can the probability of class 0 be found out when a Keras model predicts binary labels?\", \"What are the two values returned when using from_logits to get results from a binary Keras model?\"]',\n",
       " 'bcdf7407': 'Here are the 5 questions a student might ask based on the FAQ record:\\n\\n[\\n\"What is considered \\'some small changes\\' in the context of actual values after predicting with a neural network?\",\\n\"Should the predicted values be rounded or are exact values expected?\",\\n\"Do the differences in actual values affect the accuracy of the neural network\\'s classification?\",\\n\"Are there any guidelines for how much the actual values can deviate from the predicted values and still be considered \\'fine\\'\",\\n\"Do the \\'some small changes\\' apply only to numerical values, or is this also the case for categorical predictions?\"',\n",
       " '8d1e7e20': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why is the accuracy higher on my Mac than on the HW?\", \"Why does the std deviation of training loss differ between my Mac and HW?\", \"What is the legacy version of the SGD optimizer and how does it impact accuracy?\", \"Why is the model\\'s runtime faster in T4 compared to CPU?\", \"How does running the code on Google Colab affect the accuracy and std deviation of training loss?\"]',\n",
       " '2023a9dc': 'Here are the 5 questions the student might ask:\\n\\n[\"What is the default value for the workers parameter in model.fit()?\", \"Can you explain how using multi-threading helps with data generation?\", \"How do I determine the optimal value for the cpu count on my system?\", \"Does using a higher value for workers always result in faster data loading/generation?\", \"Can I specify multiple values for the workers parameter at once?\"]',\n",
       " '468f69ff': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\\n\"How can I make sure my results are reproducible when training a neural network with TensorFlow?\",\\n\"What does it mean by reproducibility in the context of TensorFlow and how can I achieve it?\",\\n\"What is the purpose of setting a random seed when working with TensorFlow?\",\\n\"Will this method for achieving reproducibility work for all types of scripts I run using TensorFlow?\",\\n\"How can I ensure that the enabling of op determinism and setting of a random seed actually work, and why is it important to verify this?\"',\n",
       " 'c4ff26e5': '[\"Can I use PyTorch for the assignments and lessons?\", \"Can I use PyTorch instead of Keras for the course?\", \"Is PyTorch a good alternative to Keras?\", \"How greatly does the syntax differ between PyTorch and Keras?\", \"Can I submit a PyTorch equivalent for homework?\"]',\n",
       " '62722d72': 'Here is the list of questions based on the FAQ record:\\n\\n[\"What is the difference between passing an image generator and a dataset to a Keras model during training?\", \"Why does the Keras model fail during training with the error \\'Failed to find data adapter\\'?\", \"What is the issue with using an image generator directly in the model.fit() function?\", \"Why do we need to use the training and validation datasets instead of image generators during model training?\", \"What is the solution to the \\'Failed to find data adapter\\' error when training a Keras model?\"]',\n",
       " 'd1419be1': '[\\n   \"How can I run the \\'nvidia-smi\\' command continuously without using the \\'watch\\' command?\",\\n   \"Can I specify a time interval for the \\'nvidia-smi\\' command to update without \\'watch\\'?\",\\n   \"What would happen if I interrupt the \\'nvidia-smi\\' command using CTRL+C?\",\\n   \"Does the \\'nvidia-smi\\' command have a built-in function to run it in a loop?\",\\n   \"Can I customize the time interval for the \\'nvidia-smi\\' command to update?\"\\n]',\n",
       " 'a5f6f439': 'Here are 5 questions based on the FAQ record:\\n\\n[\"How do I use nvitop to check GPU and CPU utilization?\", \"Is nvitop similar to htop for CPU?\", \"Can I use nvitop to monitor GPU only?\", \"How does nvitop display GPU processes?\", \"Can I install nvitop using pip?\"]',\n",
       " '879c1ec0': '[\"What are the steps to calculate the number of parameters in a Conv2d layer, as shown in the example?\", \"How are the \\'features\\' or values derived after the Flatten layer, as illustrated in the model.summary() output?\", \"What does the \\'Param #\\' column in the model.summary() table represent?\", \"Can you explain how the number of parameters in the Conv2d layer is calculated using the formula (3*3*3 +1) * 32?\", \"How is the output shape computed for the Flatten layer, and what does the None value represent in the output shape?\"]',\n",
       " '3ac604c3': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"What is the difference between Sequential and Functional Model Modes in Keras?\", \"Can I start building a model by adding layers one by one in a Sequential way?\", \"Is the Sequential Model API easier to work with than the Functional Model API?\", \"Do I need to use the Functional Model API when doing Transfer Learning?\", \"Is it possible to reuse a base model in Transfer Learning when using the Sequential Model API?\"]',\n",
       " '0315aa96': '[\\n\"That code snippet fixed OOM errors for users with an Nvidia GPU, but what about users who don\\'t have a GPU? Are there any alternative methods for resolving OOM errors on CPUs?\",\\n\"Why does tf.configlist_physical_devices(\\'GPU\\') return a list with a single element, and what is the implication of passing True to tf.config.experimental.set_memory_growth?\",\\n\"What are the limitations of modifying virtual devices once they are initialized, as indicated in the exception message?\",\\n\"What are the typical error messages we can expect when OOM errors occur, and how can we determine that this is the cause of the error?\",\\n\"Is it generally recommended to use set_memory_growth for all neural network applications, or are there specific scenarios where this might not be suitable?\"',\n",
       " 'daf84bc3': 'Here are the 5 questions:\\n\\n[\"How do I improve the performance of model training in Google Colab with a T4 GPU?\", \"Can I increase the number of workers/threads while training my model?\", \"Is the number of threads used in the fit function the default for both CPU and GPU training?\", \"Why did increasing the number of workers/threads improve performance in Google Colab\\'s free T4 GPU?\", \"Can I find more information about the number of cores in Google Colab\\'s GPU?\"]',\n",
       " '1e956ca7': '[\\n\"How can I load images in the new recommended way if I\\'m used to using ImageDataGenerator?\",\\n\"What are preprocessing layers and how do they relate to image disease when using image_dataset_from_directory?\",\\n\"Why are ImageDataGenerator and tf.keras.preprocessing.image deprecated and what are the consequences if I don\\'t switch to image_dataset_from_directory?\",\\n\"What tutorials should I look at for learning how to load and augment images with image_dataset_from_directory?\",\\n\"Can I still use ImageDataGenerator for old projects or should I start migrating as soon as possible?\"',\n",
       " '3ee083ab': '[\\n\"How do I begin with the Week 9 materials on Serverless Deep Learning?\",\\n\"What topics will I learn about in this week\\'s content?\",\\n\"Can I get a quick overview of what\\'s included in the Week 9 package?\",\\n\"Are there any prerequisites for starting Week 9\\'s deep learning lessons?\",\\n\"What kind of resources will I need to complete the Week 9 assignments?\"\\n]',\n",
       " 'f826cba4': '[\"Where can I find the model for this week\\'s task?\", \"Why is the original link to the model not working?\", \"How can I get the model if I\\'m not familiar with GitHub?\", \"Is there an alternative way to access the model?\", \"How can I confirm that I\\'m using the correct model referred to in the resources?\"]',\n",
       " '60fa95ed': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What should I do if the command `echo ${REMOTE_URI}` returns nothing?\", \"Why do I need to set a local variable for the URI address?\", \"Can I use curly brackets in the echo command?\", \"What is the correct syntax for executing the command in the terminal?\", \"Will the variable setting be persisted if I close the terminal session?\"]',\n",
       " '53f3ee10': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What are the differences between aws ecr get-login and aws ecr get-login-password?\", \"Why do I get an invalid choice error when using aws ecr get-login?\", \"How do I combine aws ecr get-login-password with docker login?\", \"What should I replace with my values when using docker login?\", \"How can I simplify the login process for aws ecr?\"]',\n",
       " '93aa4278': '[\"What is the suggested way to pass many parameters in a model at once, especially in a CNN?\", \"How can I use the keras.models.Sequential() function to pass multiple parameters?\", \"Is it possible to pass many parameters to a model using the Sequential function?\", \"Can the Sequential function be used to pass multiple parameters to a CNN in a serverless setting?\", \"How does the Sequential function\\'s ability to pass many parameters at once impact the overall architecture of a serverless deep learning model?\"]',\n",
       " '0edeb016': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What can I do if I encounter the ERROR [internal] load metadata for public.ecr.aws/lambda/python:3.8 when building my docker image?\",\\n\"What is causing this ERROR [internal] load metadata for public.ecr.aws/lambda/python:3.8 error when using the Amazon python base image?\",\\n\"Should I update my docker desktop to resolve the ERROR [internal] load metadata for public.ecr.aws/lambda/python:3.8 issue?\",\\n\"What are the potential solutions to resolve this ERROR [internal] load metadata for public.ecr.aws/lambda/python:3.8 error when building a docker image?\",\\n\"What is the significance of the DOCKER_BUILDKIT=0 command when resolving the ERROR [internal] load metadata for public.ecr.aws/lambda/python:3.8 problem?\"]',\n",
       " 'ba186de6': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What can I do if I encounter an error while running the command \\'!ls -lh\\' in Jupyter Notebook?\", \"Why is the \\'ls\\' command not recognized in my Jupyter Notebook?\", \"Can I use an alternative command to get the same output as \\'!ls -lh\\'?\", \"What is the difference between the \\'ls\\' and \\'dir\\' commands?\", \"How do I troubleshoot errors when running commands in Jupyter Notebook?\"]',\n",
       " 'da2f1cf4': '[\"Why do I get an error message saying ImportError: generic_type: type \"InterpreterWrapper\" is already registered! when I run import tflite_runtime.interpreter as tflite?\", \"What causes the error \"ImportError: generic_type: type \"InterpreterWrapper\" is already registered!\" when importing tflite_runtime.interpreter and tensorflow?\", \"Why do I need to restart the kernel to fix the issue, can\\'t I just edit the import statements?\", \"Will removing tensorflow import statement fix the issue, or do I need to restart the kernel?\", \"Is this error specific to using tflite_runtime.interpreter or can it happen with other imports as well?\"]',\n",
       " '7fd648ca': 'Here are the 5 questions:\\n\\n[\\n  \"What happens when I run $ docker build -t dino_dragon in command line and how does it relate to the Windows version being up-to-date?\",\\n  \"Why do I get an error during connect with Docker and what could it indicate about the Docker daemon?\",\\n  \"What is the purpose of checking if Docker is being stopped by a third-party program?\",\\n  \"How do I make sure Docker is not stopped by a third-party program?\",\\n  \"What is the impact of using an outdated version of Windows on Docker performance or functionality?\"\\n]',\n",
       " '42c09143': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How can I fix the \\'WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available\\' error when building my Docker image?\",\\n\"Why do I get this error when trying to build my Docker image, and what\\'s the most common cause of it?\",\\n\"What is the difference between downloading the wheel file and copying the link, and why does it matter?\",\\n\"Why do I need to find a wheel with the version that I am working on, rather than just using a different version?\",\\n\"When building my Docker image, why do I get this error even when I download the raw format of the wheel file?\"',\n",
       " 'd6d534fc': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"How do I install awscli, is it included in AWS Serverless Deep Learning?\"\\n\"What are the necessary inputs to provide when configuring aws configure after installing awscli?\"\\n\"What are the default settings for Default Region Name and Default output format in aws configure?\"\\n\"Is it okay to leave the Default output format as None during aws configure?\"\\n\"What are the things that you shouldn\\'t do while configuring aws configure after installing awscli?\"',\n",
       " 'b2c0c554': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What does \\'Object of type float32 is not JSON serializable\\' mean in the context of serverless deep learning?\",\\n\"Can you explain why this error occurs only when testing the lambda function with a running docker instance?\",\\n\"How do I convert numpy float32 values to base-Python floats?\",\\n\"Can I use other formats besides JSON to serialize the output instead of using Python?\",\\n\"What are some additional resources I can refer to for further understanding of this issue?\"',\n",
       " '819afebc': 'Here are the 5 questions the student might ask:\\n\\n[\\n\"Can you explain why the error is occurring when using the line \\'interpreter.set_tensor(input_index, X\\' in the video 9.3 around 12 minutes?\",\\n\"Why is the error showing \\'ValueError: Cannot set tensor: Got value of type UINT8 but expected type FLOAT32 for input 0, name: serving_default_conv2d_input:0\\'?\",\\n\"What does \\'UINT8\\' and \\'FLOAT32\\' mean in this context?\",\\n\"Why did converting \\'X\\' to float32 (using \\'X = np.float32(X)\\') solve the issue?\",\\n\"Does this solution only work for TensorFlow version 2.15.0 or is it also applicable to other versions?\"',\n",
       " '74551c54': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"How do I get the file size in PowerShell terminal? Can I use a command similar to \\'dir\\' or \\'ls\\'?\",\\n\"Why do I need to use \\'Get-Item\\' twice when checking the file size?\",\\n\"I\\'m trying to get the file size in MB, but the command doesn\\'t work as expected. What\\'s wrong with my code?\",\\n\"Can I use a variable to store the file path instead of hardcoding it like \\'path_to_file\\'?\",\\n\"Is there a way to get the file size in PowerShell without having to calculate it manually as shown in the example?\"',\n",
       " '4d98cd09': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n{\"question1\": \"Can you explain how Lambda initializes the container images?\"},\\n{\"question2\": \"What are the requirements for creating custom Lambda container images?\"},\\n{\"question3\": \"Is it possible to use different runtimes in my Lambda functions?\"},\\n{\"question4\": \"How do I manage multiple dependencies in my Lambda container images?\"},\\n{\"question5\": \"Are there any specific best practices for optimizing Lambda container images?\"}',\n",
       " '59a81fd5': 'Here are 5 questions the student might ask based on the FAQ record in parsable JSON format:\\n\\n[\\n\"How can I create a docker image for AWS Lambda?\",\\n\"What is the process to deploy the docker image on AWS Lambda?\",\\n\"Can I expose my AWS Lambda function as a REST API using APIGatewayService?\",\\n\"How do I push my docker image to AWS ECR?\",\\n\"Can I use AWS Serverless Framework to deploy my machine learning model on AWS Lambda?\"',\n",
       " '35dbd6e2': 'Here are the 5 questions that the student might ask based on the FAQ record:\\n\\n[\"Why do I get a pip install error for the tflite runtime whl when building the docker image?\", \\n\"Why does the error occur only on an M1 Mac and not on other operating systems?\", \\n\"What alternative do I have if I don\\'t want to run the code on a PC or Ubuntu OS?\", \\n\"How do I modify the docker build command to build the image successfully on an M1 Mac?\", \\n\"What does the \\'--platform linux/amd64\\' option do in the docker build command?\"]',\n",
       " 'e5fe9efe': '[\"What happens if I can\\'t find the deployed API URL for the specific path I\\'m invoking while testing API Gateway locally?\", \"Why is the error message \\'Missing Authentication Token\\' appearing when I try to test the API Gateway?\", \"How do I resolve the error \\'Missing Authentication Token\\' when testing API Gateway locally?\", \"What are the steps to fix the issue of missing authentication token when invoking API Gateway locally?\", \"Can I test API Gateway locally without getting the deployed API URL for the specific path I\\'m invoking?\"]',\n",
       " '5c043c62': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How do I resolve the error \\'could not find a version that satisfies the requirement tflite_runtime\\' when trying to install tflite_runtime?\",\\n\"What are the supported os-python version combinations for tflite_runtime?\",\\n\"Can you provide more information on how to install tflite_runtime using pip after checking the available versions?\",\\n\"Is there a way to run a virtual machine within the cloud service to perform tasks, or do I need a separate VM like VirtualBox?\",\\n\"Are there any specific Dockerfile commands that need to be modified to install tflite_runtime correctly?\"',\n",
       " 'af0739da': '[\\n\"What can we do when we encounter the \\'docker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system.\\' error while running a Docker command?\"]\\n[\\n\"Why do we need to restart the Docker services to resolve the \\'docker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system.\\' error?\"]\\n[\\n\"What is the reason behind the \\'docker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system.\\' error message when running a Docker command?\"]\\n[\\n\"How can we prevent the \\'docker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system.\\' error when running a Docker command?\"]\\n[\\n\"What does the \\'read-only file system\\' part of the \\'docker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system.\\' error message indicate?\"]\\n',\n",
       " '451bc25d': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"How do I save a Docker image to my local machine?\",\\n\"Can I export a Docker image to a specific file type?\",\\n\"What is the command to save a Docker image and what does it save?\",\\n\"Can I view the contents of a Docker image layer by layer?\",\\n\"How do I access the individual layers of a Docker image?\"',\n",
       " 'ea2e7458': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why doesn\\'t my Jupyter notebook recognize packages after installing them?\", \"How do I troubleshoot package import issues with my Jupyter notebook?\", \"What is the best way to restart a Jupyter notebook session for packages to take effect?\", \"Can you provide an example of a package that may cause issues with Jupyter notebook imports?\", \"Why do I need to restart my Jupyter notebook after installing packages using pip?\"]',\n",
       " '6ce8e875': '{\"questions\": [\\n\"How do I manage storage space on my AWS instance, as I\\'m running low?\",\\n\"Why doesn\\'t deleting Docker images automatically free up space on my instance?\",\\n\"How can I ensure I have enough storage on my instance for running multiple experiments?\",\\n\"What happened in my case where I only had 30-GB instance and I ran out of space?\",\\n\"What are the steps I need to take in addition to deleting Docker images to free up space?\"\\n]}',\n",
       " 'b50e9e2b': '[\\n  \"What is the recommended Tensorflow version for AWS deployment?\",\\n  \"Will using Tensorflow 2.15 with AWS work fine?\",\\n  \"Are there any alternative Tensorflow versions for AWS if 2.14 doesn\\'t work?\",\\n  \"What are the supported Python versions for installing Tensorflow 2.4.4?\",\\n  \"Will installation of tf==2.4.4 on any Python version work without issues?\"\\n]',\n",
       " '29311ef5': '[/JSON]\\n\\n[\"What does the \\'aws: error: argument operation: Invalid choice\\' error message mean when using the aws ecr get-login command?\", \"Why does the aws ecr get-login command not work with the --no-include-email option?\", \"Can the aws ecr get-login command be used without the --no-include-email option?\", \"How do I resolve the \\'aws: error: argument operation: Invalid choice\\' error when using the aws ecr get-login command?\", \"Is there a workaround to use the aws ecr get-login command with both the --no-include-email and other options?\"]',\n",
       " '1e0dc11c': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What IAM permission policy is needed to complete Week 9: Serverless?\", \"How do I navigate to the IAM service in the AWS Console?\", \"How do I create a new policy with the specific ECR actions?\", \"What is the error message when trying to solve the problem using the Docker credential?\", \"How do I solve the issue with the Docker credential in WSL2 system?\"]',\n",
       " '1078aeb7': '[\"What is causing the temporary failure in name resolution with Docker? I\\'m trying to use a serverless deep learning model. Can I solve this issue by configuring docker\\'s daemon?\", \"Why do I need to add specific DNS servers to the docker daemon configuration? How does this impact my deep learning model?\", \"What file should I edit to add the DNS servers to the docker daemon configuration, and what is the format of the configuration?\", \"How do I restart the docker service after adding the DNS servers to the daemon configuration? Will this impact my running deep learning model?\", \"Is Ibai Irastorza the same person who provided the answer to this Docker configuration issue, or is this a different person who contributed to the FAQ?\"]',\n",
       " '7daaca73': '[\"What should I do if my Keras model *.h5 doesn\\'t load and I get an error saying \\'weight_decay is not a valid argument, kwargs should be empty for optimizer_experimental.Optimizer\\'?\",\\n\"Is \\'compile=False\\' necessary when loading a Keras model?\",\\n\"How do I use \\'compile=False\\' with the load_model function in Keras?\",\\n\"Can I use \\'compile=False\\' with other Keras functions besides load_model?\",\\n\"Why does Keras require \\'compile=False\\' when loading a model if it\\'s not necessary during training?\"]',\n",
       " '0cfbe2e2': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I set up the AWS RIE (runtime interface emulator) in my local environment?\", \"Why do I need to use a certain port and localhost link when testing AWS Lambda + Docker locally?\", \"What is the correct format for the event data when posting a request to the endpoint using curl?\", \"Can I use a similar command like \\'docker run\\' to test my AWS Lambda function without using AWS RIE?\", \"How do I resolve the error \\'Object of type float32 is not JSON serializable\\' when testing my AWS Lambda function?\"]',\n",
       " '1460fb65': '[\"When I\\'m trying to import the lambda_function module, I get the error \\'No module named tensorflow\\' even though I\\'ve installed TensorFlow. Why is this happening?\", \\n\"How can I resolve the \\'No module named tensorflow\\' error when I\\'m trying to import the lambda_function module?\", \\n\"I\\'m still getting the \\'No module named tensorflow\\' error even after uninstalling and reinstalling TensorFlow. What else could be causing this issue?\", \\n\"What are some common reasons why I might be getting the \\'No module named tensorflow\\' error when running my Python script?\", \\n\"Is there a way to use the tensorflow library in my Python code without running into the \\'No module named tensorflow\\' error?\"]',\n",
       " 'd4f9efdc': 'Here are the 5 questions the student might ask:\\n\\n[\"How do I install Docker (udocker) in Google Colab and run a simple Docker container?\", \"What can I do if I encounter errors when calling the Lambda API Gateway?\", \"Can I use pip install tflite_runtime with wheel links from GitHub?\", \"How do I overcome issues installing tflite_runtime from GitHub wheel links?\", \"What are common errors when working with API Gateway and how can I resolve them?\"]',\n",
       " '6a417bfe': '[\"Can you explain how to get started with the Week 10 module on Kubernetes and TensorFlow Serving?\", \"What software or tools do I need to install to follow along with the Week 10 exercises?\", \"How do I configure TensorFlow Serving with Kubernetes as part of the Week 10 assignment?\", \"What are the system requirements for running TensorFlow Serving with Kubernetes?\", \"Is it possible to use a containerized environment to run TensorFlow Serving with Kubernetes?\"]',\n",
       " 'ed8b300d': '[\\n\"How can I run a CNN on my local machine if I don\\'t have a GPU?\",\\n\"Can I use TensorFlow with CUDA support on WSL2 and if so, how?\",\\n\"Which resources can I use to install TensorFlow with pip?\",\\n\"What are the essential steps to install TensorFlow with CUDA support on my local machine?\",\\n\"How does selecting the computer platform affect the installation process of TensorFlow?\"',\n",
       " 'a64aed6b': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What can I do if I\\'m getting Allocator ran out of memory errors while running TensorFlow?\", \"How can I prevent memory allocation issues when using TensorFlow on my machine?\", \"Can I still get memory errors even after using the suggested solution?\", \"What is the exact purpose of the code snippet provided in the FAQ?\", \"Is it normal to still experience memory issues occasionally even after implementing the suggested fix?\"]',\n",
       " '727238ee': 'Here are the 5 questions based on the FAQ record:\\n\\n[\"What happens when I get a TypeError: Descriptors cannot not be created directly in session 10.3?\",\"Why do I need to regenerate my protos with protoc >= 3.19.0?\",\"Can I downgrade the protobuf package to 3.20.x or lower as a workaround?\",\"Is there a performance impact when setting PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python?\",\"What exact version of protobuf and other packages do I need to install to resolve this issue?\"]',\n",
       " '85d4901d': '[\\n\"How can I resolve the issue of not being able to connect to the Docker daemon in WSL?\", \\n\"What could be causing the error message \\'Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\\'?\", \\n\"Can you provide a solution to fix the Docker daemon connection issue in WSL?\", \\n\"Why does enabling additional distros in Docker Desktop settings solve the problem?\", \\n\"What should I do if enabling additional distros does not resolve the issue and I\\'m still getting the error message?\"',\n",
       " 'df023a13': 'Here are 5 questions based on the FAQ record:\\n\\n[\"Why doesn\\'t my HPA instance run correctly after installing the latest version of Metrics Server?\", \"What do I do if my targets still appear as <unknown> after updating Metrics Server?\", \"How can I troubleshoot issues with the metrics-server deployment?\", \"What configuration line do I need to add to the metrics-server deployment for it to work correctly?\", \"How do I save the changes I make to the metrics-server deployment?\"]',\n",
       " '48e92d65': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What can I do if my HPA instance doesn\\'t run properly after installing the latest version of Metrics Server?\", \"Why do my targets still appear as <unknown> in my HPA instance?\", \"I\\'ve tried installing the latest version of Metrics Server, but my HPA instance still isn\\'t working, what\\'s the next step?\", \"Is there an alternative way to resolve the issue with my HPA instance that\\'s running incorrectly?\", \"Why is it necessary to use a specific deployment file for Metrics Server to resolve the issue with my HPA instance?\"]',\n",
       " '1685cae4': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What happens when I try to install packages in a Windows machine and get an OSError: [WinError 5] Access is denied error?\", \"Why do I get an error when trying to install grpcio and tensorflow-serving-api libraries?\", \"How can I resolve the Access is denied error when installing packages in Windows?\", \"Is there a way to avoid getting the Access is denied error when installing packages in Windows?\", \"Can you give an example of a command that works for installing packages in Windows when you get an Access is denied error?\"]',\n",
       " '4fb7b21e': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How can I resolve the \\'TypeError: Descriptors cannot not be created directly.\\' error when running gateway.py?\",\\n\"What is the minimum version of protoc required for generated code?\",\\n\"What are the possible workarounds if I cannot immediately regenerate my protos?\",\\n\"Why do I need to downgrade the protobuf package to resolve this issue?\",\\n\"What command do I need to use to install a specific version of protobuf using pipenv?\"',\n",
       " '8bd3bfc2': '[\"How can I easily install kubectl on Windows?\", \"What is the correct way to download and use kubectl with curl on Windows?\", \"How do I add the kubectl executable to the PATH environment variable on Windows?\", \"Can I install TensorFlow Serving on Windows?\", \"Are there any specific requirements or tutorials for installing kubectl using the VSCode terminal on Windows?\"]',\n",
       " '03b5fc59': '[\\n\"How do I install an initial power shell terminal with administrator privilege?\",\\n\"How do I install the choco library if I already have PowerShell?\",\\n\"What is the syntax to install choco library in PowerShell?\",\\n\"What permissions do I need to have to install choco library?\",\\n\"What does the iex command do in the PowerShell script?\" ]',\n",
       " '7c31bc9a': 'Here are 5 questions based on the FAQ record:\\n\\n[\\n\"How do I install Kind if the Windows Powershell installation via website and Choco Library is not working?\",\\n\"Can I install Go and Kind from the same Command Prompt?\",\\n\"How do I know if Go is correctly installed in my system?\",\\n\"Is it necessary to install a specific version of Kind, like v0.20.0?\",\\n\"Can I confirm Kind installation by just typing \\'kind\\' in Command Prompt?\"',\n",
       " '605efc12': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What are some common issues with kubectl and how can I troubleshoot them?\", \"How do I clean up my cluster configuration when I\\'m having issues?\", \"Can someone explain why creating an admin.config file in my home directory didn\\'t fix the issue?\", \"What are some quick steps I can take to start over with my cluster?\", \"Why does running kind delete cluster and kind create cluster fix the connection issue with localhost:8080?\"]',\n",
       " 'c5cde96c': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How can I avoid running out of storage space on my AWS instance when building many Docker images?\", \"Can I just delete Docker images to free up space?\", \"Why didn\\'t running `docker rmi` on my duplicate models free up any space?\", \"Do I need to do something in addition to deleting Docker images to free up storage space?\", \"What happens when I run `docker system prune` to free up space?\"]',\n",
       " 'd45d2da6': '[\"What does it mean by \\'correct value for CPU and memory\\' in HW10 Q6? Isn\\'t it arbitrary? Can I use any values I want?\", \"Don\\'t they have a standard or defined range for these values?\", \"What is the specific port I should be referring to in the yaml file? Is it for CPU or memory?\", \"How do I determine the correct value for these parameters in the yaml file?\", \"Is there a specific format or rule for specifying the CPU and memory values in the yaml file?\"]',\n",
       " '59823c72': '[\"What does the \\'m\\' symbol mean in the cpu values for Kubernetes deployment.yaml files, specifically when the values look like \\'100m\\' and \\'500m\\'?\", \"Why are cpu values in Kubernetes deployment.yaml files in milliCPU units?\", \"Can you give an example of how \\'100m\\' and \\'500m\\' differ in terms of cpu core allocation?\", \"How does specifying cpu values in milliCPUs allow for fine-grained control over cpu resources?\", \"Why would someone want to specify cpu requirements and limits in more granular units like milliCPUs instead of in full CPU cores?\"]',\n",
       " '665f7b27': '[\\n\"How can I resolve the error when trying to load a docker image into a cluster with Kind, where the error message says \\'no nodes found for cluster\\'?\",\\n\"Why do I need to specify the cluster name when loading a docker image with Kind\",\\n\"Can I just run \\'kind load docker-image\\' without specifying a cluster name\",\\n\"Do I need to create a cluster before loading a docker image with Kind\",\\n\"What happens if I don\\'t specify the correct cluster name when running \\'kind load docker-image\\'? Will it default to a specific cluster or show an error \"\\n]',\n",
       " '0a406fe0': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"Why do I get an error when I try to download kind from the next command?\", \"Why is kind not recognized as an internal or external command?\", \"What does \\'kind\\' not recognized as an internal or external command mean?\", \"How do I download and install kind on my Windows machine?\", \"What should I do if kind is not recognized after installation?\"]',\n",
       " '64b209b0': '[\"How do I use kind with Rootless Docker on Linux?\", \"What changes do I need to make to my Linux system to run kind with Rootless Docker?\", \"What Linux changes are required to use kind with Rootless Podman?\", \"How does the Linux system need to be set up for kind to work with Rootless Docker?\", \"Can you provide more information on the kind configuration with Rootless Docker on Linux?\"]',\n",
       " '518c4cb8': '{\"questions\": [\"How do I deploy the Kubernetes dashboard?\", \"Is the Kubernetes dashboard accessible from anywhere?\", \"What is the purpose of the Kubernetes dashboard?\", \"Can I use the Kubernetes dashboard to monitor my TensorFlow Serving deployment?\", \"How do I access the Kubernetes dashboard once it\\'s deployed?\"]}',\n",
       " '00882c83': '[\"What is the correct version of AWS CLI that I should be using for eksctl to work correctly?\", \"How can I check the version of AWS CLI I\\'m currently running?\", \"What happens if I\\'m still using AWS CLI v1?\", \"Is there a change I need to make in my code to use the new AWS CLI version?\", \"Where can I find more information about migrating to AWS CLI v2?\"]',\n",
       " 'd6d483ce': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What causes TypeError: __init__() got an unexpected keyword argument \\'unbound_message\\' when importing Flask?\", \"Can I resolve this issue by just importing Flask and not with from flask import Flask?\", \"How can I figure out why I\\'m getting this error when importing Flask?\", \"Can I use pip freeze to troubleshoot issues with Flask and Werkzeug?\", \"Why do I need to pin the version of Flask and not just update it to the latest version?\"]',\n",
       " 'f9711723': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"How do I fix the error when running the command aws ecr get-login?\", \"Why do I get an error when running the command aws ecr get-login --no-include-email?\", \"What is the correct command to use with aws ecr get-login?\", \"How do I know which region to use with aws ecr get-login?\", \"Can I use a default region with aws ecr get-login or do I need to specify it?\"]',\n",
       " '5bda3b94': '[\"What is the issue when trying to run tensorflow/serving:2.7.0 on an Apple M1 Mac, and how can it be resolved?\",\"Why do I get a \\'libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/generated_message_reflection.cc:2345\\' error when trying to run tensorflow/serving:2.7.0 on my M1 Mac?\",\"How to fix the \\'terminate called after throwing an instance of \\'google::protobuf::FatalException\\'\\' error when running tensorflow/serving:2.7.0?\",\"Can you provide an alternative Docker image for tensorflow/serving that is compatible with Apple M1 Macs?\",\"Is there any specific instruction or configuration required for running tensorflow/serving on an Apple M1 Mac, other than switching to a compatible Docker image?\"]',\n",
       " 'cccd31cf': 'Here are the 5 questions the student might ask:\\n\\n[\\n\"Can I still use the tensorflow/serving image on Mac M1 Apple Silicon or M2 Apple Silicon, or is it outdated?\",\\n\"Why do I get an Illegal instruction error when running tensorflow/serving image on Mac M2 Apple Silicon?\",\\n\"Are there any alternative base images if the latest tensorflow/serving image is from 2 years ago?\",\\n\"Can you explain why using bitnami/tensorflow-serving base image is a solution to the Illegal instruction error?\",\\n\"What is the difference between launching tensorflow-serving using docker run and using docker-compose.yaml?\"',\n",
       " '57f49999': 'Here are 5 questions based on the FAQ record:\\n\\n[\\n\"How does HPA know what CPU metrics are unknown?\",\\n\"Why is the CPU metrics value showing as Unknown in HPA?\",\\n\"What is the role of REFERENCE column in the table provided in the solution?\",\\n\"What are the consequences of multiple instances of \\'FailedGetResourceMetric\\' error in HPA?\",\\n\"Can we create HPA for other resources besides CPU, such as memory?\"\\n]',\n",
       " '5cb58698': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What are the possible causes of errors during installation of KServe with Istio?\", \"Why do I get errors when running the quick_install.sh script?\", \"How can I troubleshoot the istio update resource failure issue?\", \"What specific versions of Istio and Knative can I install for KServe?\", \"How do I fix the errors with Istio during installation of KServe?\"]',\n",
       " 'de650b41': '[\"What if I don\\'t like my randomly assigned problem title in the midterm or capstone project?\", \"Can I clone someone else\\'s problem title if I\\'m stuck on my own?\", \"How will my problem title affect my project\\'s complexity?\", \"Can I change my problem title if I\\'m not satisfied with my original assignment?\", \"Is the problem title only for the purpose of identification or does it influence the project\\'s content?\"]',\n",
       " '9ffacaac': '[\"What are the project deadlines and how do I access the 2022 cohort page?\", \"Do I really have to look at the 2022 cohort page for the project deadlines?\", \"How can I find my own cohort\\'s deadline if the 2022 cohort page doesn\\'t make sense to me?\", \"Why can\\'t you just provide the project deadlines instead of sending us to another page?\", \"Is there a reason why the project deadlines are the same for all cohorts?\"]',\n",
       " '4dfb5d4f': '[\"What type of projects are we doing for the midterm and capstone assignments?\", \"Are we allowed to work with others on the projects?\", \"Can we work in groups for the midterms and capstones?\", \"Is it possible to partner with someone else for the projects?\", \"Will there be any collaboration required for the midterm and capstone projects?\"]',\n",
       " '0b8739b7': '[\"What modules are typically covered in a midterm project?\", \"Can my midterm project cover topics not mentioned in the course syllabus?\", \"What modules should I focus on for the capstone project?\", \"Can I include extra topics in my capstone project?\", \"How can I identify relevant materials to include in my project?\"]',\n",
       " '9eb52679': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What do I need to submit for my Midterm Project?\", \\n\"Are these links to datasets or something else?\", \\n\"Can I deploy my model to any cloud service or are there specific requirements?\", \\n\"What should I focus on for the problem description and model explanation?\", \\n\"How will I know which model to tune for performance and how will I evaluate its performance?\"]',\n",
       " '7a1fcfd9': '[\"What is the source of instructions for conducting peer reviews for projects? Will it be available on YouTube?\", \"How will the projects submitted by students be organized and accessed for peer review?\", \"Who will compile the Google Sheet with project links and hashed emails for peer review?\", \"Is the compiled Google Sheet updated in real-time like the leaderboard for homework?\", \"What is the deadline for evaluating the projects in the Google Sheet?\"]',\n",
       " '1cfa62c5': '[\\n\"Can you explain the process of computing the hash for the project review, and is it a part of the submission requirements?\",\\n\"How do I know if I\\'m getting the correct hash value for my project submission?\",\\n\"What is the purpose of computing the hash for project review, and how does it benefit the reviewers?\",\\n\"Is there a specific tool or software required to compute the hash for project review, or can I use any hashing algorithm?\",\\n\"Can you provide an example of how to compute the hash for project review, and what are the typical hash functions used in this context?\"',\n",
       " '2a78f52e': '[\"What do we need to do for the learning in public for the midterm project, specifically regarding the number of posts? Can you clarify the calculation mentioned in the total value of 14? Do we need to make a total of 14 posts? \", \"How are the regular seven posts for each module defined, and what is their value? Can you break down the calculation for us? \", \"Are we required to make a total of 14 posts for the learning in public, or can we do one with a total value of 14? Should we aim for a single post with the highest value? \", \"Can you confirm that the learning in public requires a minimum of 14 posts, one for each day? Is this the correct understanding? \", \"Is the total value of 14 for the midterm project referring to a specific type of post, such as a blog post or a social media post? Should we focus on a specific format?\" ]',\n",
       " '68aeab64': '[\\n\"Can I use GitHub to store my large dataset when loading it on the project submission platform?\",\\n\"What can I do if I\\'m having trouble uploading my large dataset to GitHub because of its size limitations?\",\\n\"I\\'m struggling to load my dataset into GitHub because it\\'s too large, what are some effective solutions I can try?\",\\n\"Can I use Git Large File System (LFS) to store and manage my large dataset on GitHub?\",\\n\"Why does my dataset exceed the file size limits of GitHub, and how can I resolve this issue?\"',\n",
       " '9a7c26e0': '[\"What if I have only submitted two projects and failed to submit the third one? Do I still get the certificate?\", \"Can I still get the certificate if I submit only two projects?\", \"What happens if I submit just two projects instead of three? Will I get the certificate?\", \"If I have submitted two projects and missed the third, can I still get a certificate?\", \"Do I need to submit all three projects to get the certificate?\"]',\n",
       " '1fd83eb9': '[\"What happens if I don\\'t complete the last capstone project?\", \"What\\'s the impact on the next two projects if I skip one of them?\", \"How does peer review work if I submit my project early?\", \"Is it necessary to finish the last project to complete the course?\", \"Can I skip a project and not have to review others afterwards?\"]',\n",
       " 'fbaa5b20': '[\"How do I determine when to stop training more models in order to avoid overfitting?\", \"Can I only train one extra model to exceed the requirement?\", \"How do I know if training more models improves the overall project\\'s quality?\", \"Are there any specific limitations on the number of models I can train?\", \"Does the \\'multiple\\' in \\'multiple models\\' imply only training two models?\"]',\n",
       " '37eab341': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"How do I find the list of all submitted projects to be evaluated?\", \"What is the link provided for example where I can find the list of all submitted projects?\", \"What if I have trouble deriving my hash value of my email address?\", \"How do I run the python code to calculate my hash value?\", \"What do I do if I have trouble finding my peer projects in the spreadsheet based on my hash value?\"]',\n",
       " '57754faf': '[\"Do you pass a project based on the average of everyone else\\'s scores or based on the total score you earn that you earn?\", \"Can I apply a course-related question that doesn\\'t fall into any specific category or module?\", \"Can I pass more than one project in our course?\", \"What happens if I miss any course-related deadlines?\", \"Can I use information from other courses in this course?\"]',\n",
       " '6979c5d1': '[\"Why do I need to provide a separate train.py file if I already have a notebook.ipynb file? Is there a specific reason it needs to be in a different file?\", \"Why isn\\'t the notebook.ipynb file enough for my peers to review my training process?\", \"Do I need to include the train.py file in my conda environment?\", \"Will the train.py file be used by my peers for training their own projects?\", \"Can the train.py file contain any additional information that isn\\'t in the notebook.ipynb file?\"]',\n",
       " 'a1bd8c34': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\"How do I install the Pillow library to use it with images?\", \"How do I load an image using the Pillow library?\", \"How do I convert a PIL image to a numpy array?\", \"What libraries are required to use the Pillow library for image loading?\", \"How do I import the necessary libraries to use the Pillow library?\"]',\n",
       " 'b2ab0fc1': '[\"What are the implications of having both a train.py file and a train.ipynb file in the midterm project directory? Can a train.ipynb file be used as a replacement for a train.py file? Is there a difference in simplicity of running a python script for training a model versus running a notebook? Are there any real-life scenarios where training a model typically involves running a train.ipynb file?\"]',\n",
       " '80c439a9': '[\"Can I create a way for users to enter data for the model to process?\", \"Is there a way to validate the data before processing it?\", \"How do I create a mobile app or interface for users to enter data?\", \"What tools or technologies can I use to create this form and validation system?\", \"Should I also ensure backend validation in addition to frontend validation?\"]',\n",
       " 'ff93b86e': '[\"What does AttributeError: \\'Booster\\' object has no attribute \\'feature_importances_\\' mean in the context of getting feature importance for XGBoost model?\", \"How do I fix the error when trying to get feature importance for XGBoost model after training it?\", \"Is there an alternative way to obtain feature importance for XGBoost model if get_score() is used for training?\", \"Why can\\'t I use model.feature_importances_ when getting feature importance for XGBoost model?\", \"What\\'s the difference between training XGBoost model with xgb.train and using model = XGBoost() to get feature importance?\"]',\n",
       " 'fcd86c8f': '[\"What could be the possible cause of the error \\'[Errno 12] Cannot allocate memory\\' in the Elastic Container Service task log?\", \"How can I resolve the \\'[Errno 12] Cannot allocate memory\\' error in my AWS Elastic Container Service task?\", \"What is the most effective way to increase the memory allocation for my Elastic Container Service task?\", \"Is increasing the CPU and RAM in the task definition the best approach to resolve the error?\", \"Can you provide more details on how to modify the task definition to increase the RAM and CPU?\"]',\n",
       " '236864c2': '[\"Why does Pickle throw an error when running a docker container with waitress serving the app.py for making predictions?\", \"Why doesn\\'t this error occur when Flask is used directly?\", \"Why does Pickle refer to the class in the global namespace (__main__)?\", \"What is the solution to this problem and why?\", \"Why does running Flask directly in predict.py without waitress make it work?\"]',\n",
       " 'efc4a04f': '[\"What are common techniques to handle datasets with outliers?\", \"How can I apply dataset transformation to handle outliers?\", \"Will clipping high values always remove outliers?\", \"What is the impact of dropping outlier observations on my analysis?\", \"Is there a specific method that is recommended over others?\"]',\n",
       " '15f361b7': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What can I do if I encounter an error message \\'Failed to import module \"service\": No module named \\'sklearn\\'\\' when creating a docker image with bentoml?\",\\n\"What\\'s the common cause of the \\'Failed to import module \"service\": No module named \\'sklearn\\'\\' error?\",\\n\"I\\'m getting an error when loading a Bento from a directory /home/bentoml/bento. How can I fix this?\",\\n\"I\\'ve written sklearn in my bentofile.yaml. Is this causing the error I\\'m seeing?\",\\n\"When creating a Bento with bentoml, I got an error saying \\'Failed to import module \"service\": No module named \\'sklearn\\'\\'. How do I resolve this issue?\"]',\n",
       " 'dbbce78b': 'Here are the 5 questions that the student might ask based on the FAQ record:\\n\\n[{\"What happens when I use the –production flag with BentoML and encounter errors?\"), \\n{\"Why do I see a long error message with something about sparse matrices when using the –production flag?\"), \\n{\"What causes inconsistent length errors when converting each input to a different sized sparse matrix?\"), \\n{\"How can I resolve code 500 errors with an empty string output in the swagger UI when using the –production flag?\"), \\n{\"How can I set the batchable flag to False in bentoml model signatures for production?\")]',\n",
       " 'f3a00e15': '[{\"question\": \"Do we have to run everything? What if we can\\'t?\", \"question2\": \"How do I know if I can run all the files?\", \"question3\": \"Are there any ways to skip running some files?\", \"question4\": \"Can I see what I need to reproduce everything?\", \"question5\": \"What if I find an obvious error?\"}]',\n",
       " '9102b3c0': '[\"What should I do if the model I\\'m working with is too big to upload to GitHub?\", \"How can I compress my model to make it smaller?\", \"Can you give an example of how to compress a model using joblib?\", \"Is compressing a model using joblib a fast process?\", \"Will compressing my model still allow me to use it in the same way?\"]',\n",
       " '70d89fdf': 'Here are 5 questions this student might ask:\\n\\n[\"What happens if I get an \"unauthorized\" message when trying to push a docker image to Google Container Registry?\", \"Is there a specific console command to fix permission issues when pushing docker images?\", \"Do I need to install gcloud on my local machine to use gcloud auth configure-docker in the console?\", \"Can I use gcloud auth configure-docker if I don\\'t have invalid credentials?\", \"How do I fix the issue of not having the needed permissions to push a docker image to Google Container Registry?\"]',\n",
       " 'c5d6a804': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why can\\'t I install tflite_runtime using pipenv?\", \"What specific issue am I encountering by using Python 3.10?\", \"Can I find all versions of tflite_runtime from a single place?\", \"Is there an alternative way to install tflite_runtime other than installing a specific version?\", \"What do I do if none of the installation methods work, and I still want to use tflite_runtime?\"]',\n",
       " '8c7f089f': '[\"Why do I get the error \\'ImageDataGenerator name \\'\\'scipy\\'\\' is not defined\\' when running ImageDataGenerator.flow_from_dataframe?\", \"How can I fix the error \\'ImageDataGenerator name \\'\\'scipy\\'\\' is not defined\\' when running ImageDataGenerator.flow_from_dataframe?\", \"Is scipy required for running ImageDataGenerator.flow_from_dataframe?\", \"What could be the cause of the error \\'ImageDataGenerator name \\'\\'scipy\\'\\' is not defined\\' when running ImageDataGenerator.flow_from_dataframe?\", \"How do I resolve the error \\'ImageDataGenerator name \\'\\'scipy\\'\\' is not defined\\' when running ImageDataGenerator.flow_from_dataframe?\"]',\n",
       " '739bcccf': '{\"question1\": \"How do I pass BentoML content/ docker container to Amazon Lambda?\", \"question2\": \"Can I use a video tutorial to learn how to pass BentoML content/ docker container to Amazon Lambda?\", \"question3\": \"What use case is the video tutorial specifically addressing?\", \"question4\": \"Who is the creator of the video tutorial addressing the BentoML/Amazon Lambda use case?\", \"question5\": \"Is there only one format for video tutorials available on the provided link?\"}',\n",
       " '4603e4e5': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\"Why am I getting the error UnidentifiedImageError: cannot identify image file when trying to test my model locally?\", \\n\"Why do I need to modify the URL to fix this error?\", \\n\"Can you provide an example of the correct URL format?\",\\n\"What is the purpose of adding ?raw=true after the .jpg in the URL?\", \\n\"Why didn\\'t I get this error when running the model on a different image?\"]',\n",
       " '0a7c328e': '{\"questions\": [\"What happens when I run pipenv install and get a [pipenv.exceptions.ResolutionFailure] error warning?\", \"Why is my dependency mismatch possibly causing this issue?\", \"How do I manually resolve the mismatch in Pipfile and Pipfile.lock?\", \"What is the process to fix this issue and update the dependency files?\", \"What is the command to run in order to resolve the dependency problem?\"]}',\n",
       " '77efd069': '[\\n\"What is the problem with the dv.get_feature_names() function not working despite working in the course? Can it be related to library versions?\",\\n\"Why will the get_feature_names function be deprecated soon and what are the implications for our project?\",\\n\"What is the reason for the change from dv.get_feature_names() to list(dv.get_feature_names_out)?\",\\n\"What is the recommended solution for accessing the features in the dictVectorizer instance?\",\\n\"Can I still use dv.get_feature_names() in my project or do I need to update it as soon as possible?\"',\n",
       " 'cc60f7bc': 'Here are the 5 questions:\\n\\n[\"What should I do when I get an error decoding JSON response expecting value line 1 column 1 char 0?\", \"Why is my data not in the correct shape when contacting the server for predict-test?\", \"Can you explain how to fix the format input to the model when sending predict-test data?\", \"How do I change my data from dictionary to numpy arrays to send to the server?\", \"What should I be aware of when converting my data to the correct shape for the model?\"]',\n",
       " 'aa13dd66': '[\\n\"Are there any free cloud alternatives besides Render that I can use for deploying my docker image?\",\\n\"Is there a cloud provider that offers free instances with more RAM than .5GB?\",\\n\"Can I use Saturn for GPU instances without paying for it?\",\\n\"Are there any free microinstances available from AWS or GCP?\",\\n\"What other free benefits can I get from AWS or GCP besides free microinstances?\"',\n",
       " 'c41e479c': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\"How do I convert the day_of_the_month column from int to string format?\",\\n\"What is the quickest way to transform the month_of_the_year column from strings like \\'jan\\' to numbers 1-12?\",\\n\"How do I combine the day and month columns to create a single date column?\",\\n\"Once I have the date column, how do I extract the day of the year from it?\",\\n\"Can I avoid adding a year to the date column if it\\'s not necessary for the calculation?\"',\n",
       " '2f28dcf1': 'Here are the 5 questions the student might ask:\\n\\n[\"How does the FAQ explain how to visualize the predictions per classes after training a neural net?\",\\n\"What does the \\'zip\\' function do in the solution description?\",\\n\"Can I adjust the size of the plot created by plt.figure()?\",\\n\"What are the \\'classes\\' and \\'predictions\\' variables in the solution description?\",\\n\"In the plt.bar() function, what does the \\'classes\\' parameter represent?\"]',\n",
       " '7a69cccf': '[\"What if I want to convert multiple dictionary values to Dataframe tables?\", \"How do I sort my Dataframe after converting dictionary values?\", \"Can I use the same function to convert multiple dictionaries?\", \"How do I modify the column names in my Dataframe?\", \"What if my dictionary values contain lists or nested dictionaries?\"]',\n",
       " '20174c95': '[\"Why was the image dataset for the competition in a different layout compared to the dino vs dragon lesson?\", \"How did some folks feel about this different layout?\", \"Was there a problem encountered with the new layout that the script solved?\", \"Who wrote the script to generate the kitchenware classification competition dataset?\", \"Can I find the script to generate the dataset anywhere online?\"]',\n",
       " 'f2cd48b6': 'Here are 5 questions this student might ask:\\n\\n[\"How do I install the CUDA toolkit for Tensorflow?\", \\n\"In the Miscellaneous section, what does \\'Install the Tensorflow way\\' refer to and how do I do it?\", \\n\"Can I install the CUDA toolkit on WSL/Linux?\", \\n\"In the answer, what does \\'make sure to follow step 4 to install CUDA by environment\\' mean?\", \\n\"Do I need to create a post on LinkedIn and share my progress with others, or is it sufficient to post a comment about completing my first assignment?\"]',\n",
       " '59b4324f': '[\"When multiplying matrices, how do I ensure I get the correct result?\", \"Why does the order of multiplication affect the outcome?\", \"Can you give an example of matrices where the order of multiplication changes the result?\", \"Are there any general rules I should follow when multiplying matrices?\", \"How does the size of the matrices impact the outcome?\"]',\n",
       " 'e1dc1ed9': '[\"What are the steps to install the environment on a Mac, particularly one with an M1 chip?\", \"Are there any specific instructions for installing the environment on a Mac equivalent to the ones provided for other platforms?\", \"Can you provide more information on how to install the environment for Mac users?\", \"Are there any tutorials or guides specifically designed for installing the environment on a Mac with an M1 chip?\", \"I couldn\\'t find any instructions for installing the environment on a Mac after reviewing the available videos, can you point me in the right direction?\"]',\n",
       " 'fc60bf3b': '[\"What happens if I end up submitting my assignment late?\", \"If I\\'m late, can I still submit my homework?\", \"Will my assignment be evaluated if I submit it late?\", \"Will my late submission get evaluated if the form is still open?\", \"What if I\\'m too late and the form is closed?\"]',\n",
       " '1e60e888': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Does the github repository need to be shared publicly, or will a private repository also work? I want to ensure my corrections are accessed correctly.\", \"How do I establish a Conda environment on my local machine, step by step? I\\'m new to this whole process.\", \"Can I install an additional Integrated Development Environment for machine learning, or is one pre-selected for the course?\", \"Can the Conda environment be public or anonymous for visibility or sharing purposes?\", \"Will the chosen IDE influence the outcome of the machine learning project?\"]',\n",
       " '44552c2e': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\\n\"How do I use wget with Google Colab?\",\\n\"Can I use wget with Google Colab?\",\\n\"I want to know how to download data using wget with Google Colab, what are the steps?\",\\n\"How do I access Google Drive files in Google Colab when using wget?\",\\n\"What is the correct syntax for using wget in Google Colab?\"',\n",
       " '7116b3be': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\"What kind of data structure are features in scikit-learn expected to be?\", \\n\"How do I reshape a 1D array to a 2D array in Python?\", \\n\"Is it necessary to filter the dataset before selecting the desired columns?\", \\n\"What are the desired columns to select in the filtered DataFrame?\", \\n\"Is there a way to display the first few rows of the DataFrame in a more readable format?\"]',\n",
       " '5d4d206e': 'Here are the questions:\\n\\n[\\n\"What do I do if I get an error when plotting with Matplotlib to check if a median has a tail?\",\\n\"Why am I getting a warning when using is_categorical_dtype in Matplotlib?\",\\n\"Can I still use is_categorical_dtype though?\",\\n\"Is there a problem with my data if I get a FutureWarning in Matplotlib?\",\\n\"What should I replace is_categorical_dtype with in my code?\"',\n",
       " '387093cc': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"What happens when trying to run the docker file in Windows instead of WSL/Linux?\",\\n\"Why do I get an error when trying to rerun the docker file in Windows?\",\\n\"What is the cause of the \\'Python 3.11 was not found on your system...\\' error?\",\\n\"How do I fix the \\'Neither \\'pipenv\\' nor \\'asdf\\' could be found to install Python.\\' error?\",\\n\"What steps do I need to take to make the docker file run again after the error is resolved?\"',\n",
       " 'd12a2657': '[\\n\"How do I deploy my project to DigitalOcean App Cloud if the deployment process seems unclear?\",\\n\"What are the costs associated with deploying my project to DigitalOcean?\",\\n\"Do I need to modify the Dockerfile path in DigitalOcean settings?\",\\n\"What if my project is not located in the root directory of my GitHub repository?\",\\n\"What files do I need to add to my container build process, if model files are not built automatically?\"',\n",
       " 'eb7a57a6': ' [{\"question\": \"What is the best approach after determining the most important features in a model? Should we only train the model on these features?\"}, {\"question\": \"What happens if some of the important features are highly correlated? Can we drop some of them?\"}, {\"question\": \"Is it recommended to train a model on only the most important features or is it better to include all features for better predictive value?\"}, {\"question\": \"How can we determine if leaving out a feature would have a significant impact on the model\\'s performance?\"}, {\"question\": \"Are there any specific feature selection algorithms that we\\'ve learned in this course that can help with selecting the most important features?\"}',\n",
       " 'd6f0c6ea': ' Here is the list of questions:\\n[\\n\"How can I handle large datasets with over a million rows like New York Yellow Taxi dataset?\",\\n\"Can I reduce memory usage in Pandas\\' automatic data type inference?\",\\n\"Are there any ways to work with a dataset that doesn\\'t fit in memory?\",\\n\"Is there a Python project that can parallelize Numpy and Pandas for large datasets?\",\\n\"How can I use random samples of a large dataset in the exploratory phase?\"',\n",
       " '9f261648': '[\"Can I submit my work if it\\'s written in R or Scala?\", \"Why would I get penalized for using different languages?\", \"Will my answers match if I use a different language?\", \"Will my peers be familiar with the other languages I use?\", \"Can I use a different language for learning, but not for submissions?\"]',\n",
       " 'aa7ff0f7': '[\"What are the guidelines regarding the use of libraries like fast.ai or huggingface in the capstone and competition, considering their potential to provide extensive assistance?\", \"Are there any limitations on using libraries like fast.ai or huggingface in the capstone and competition, or can we fully utilise their resources?\", \"Are fast.ai or huggingface considered to be \\'too much help\\' in the context of the capstone and competition, and if so, why?\", \"Can we use fast.ai or huggingface libraries alongside our own code in the capstone and competition, or should we solely rely on these libraries?\", \"Is there a specific threshold or level of assistance that would be considered excessive in the use of libraries like fast.ai or huggingface in the capstone and competition?\"]',\n",
       " '387bdc5f': '[\"What are the specific version requirements for building and testing a TensorFlow Serving image?\", \"Why can\\'t I test my TensorFlow Serving image after building it successfully?\", \"How do I ensure I have matching TF and TF Serving versions?\", \"What happens if I don\\'t match the TF and TF Serving versions?\", \"Can you provide an example of how to resolve the discrepancy between TF and TF Serving versions?\"]',\n",
       " 'c6a22665': '[\"What are the recommended titles to use when adding the Machine Learning Zoomcamp experience to my LinkedIn profile?\", \"Can I consider this experience as an official job or internship on LinkedIn?\", \"How can I incorporate the Machine Learning Zoomcamp experience in the \\'Organizations\\' section of my LinkedIn profile?\", \"Are there any other suitable sections in LinkedIn where I can showcase my Machine Learning Zoomcamp experience?\", \"Can I share the link to my project on LinkedIn as part of showcasing my progress?\"]',\n",
       " '0560e827': '[\\n\"What is the format for questions in this MLOps Zoomcamp FAQ?\",\\n\"How can I participate in the course if I have a busy schedule?\",\\n\"What additional resources will be provided outside of the course materials?\",\\n\"Are there any requirements for installing software or tools for this course?\",\\n\"What if I get stuck or need help with something in the course?\"\\n]',\n",
       " '59812e77': '[\"How long does it typically take to complete each module in this course, considering there might be deadline extensions?\", \"Can you give me a more detailed breakdown of the time I\\'ll spend on each module and the capstone project?\", \"What is the overall duration of the course, and how is it divided between modules and the capstone project?\", \"Are there any specific dates or timelines I should keep in mind for completing each module?\", \"Can I request an extension on the module deadlines if I need extra time?\"]',\n",
       " 'dce0bb09': '[\"What is the main difference between the 2023 and 2022 course versions?\", \"Will all the changes in the 2023 course be significant?\", \"What specific modules are being re-recorded in the 2023 course?\", \"Will the homeworks in the 2023 course be mostly identical to those in the 2022 course?\", \"Will the rest of the course content be the same in 2023 as it was in 2022?\"]',\n",
       " '4920d4e9': '[\"Will there be a 2024 Cohort? Can it start in May 2024?\", \"What is the expected starting time for the 2024 Cohort?\", \"Will the course have a 2024 Cohort with a specific start date?\", \"Can we know the start date of the 2024 Cohort?\", \"When does the 2024 Cohort of the course start?\"]',\n",
       " '0f1d2765': '[\"What if I provide an answer that is not exactly the same as the choices given in the question?\", \"What if my answer is slightly different from the available options but closest to one of them?\", \"Can I share my answer with the class through the course Slack channel?\", \"If I\\'m unsure about the closest answer, how will that affect my score?\", \"Is it okay if my answer is not similar to the provided options at all, but still correct?\"]',\n",
       " '4eef2f81': '[\"What are some reliable sources for finding datasets for the final project?\", \"How do I go about selecting a dataset to work with?\", \"Can we modify or combine datasets from different sources for the project?\", \"What is the scope for creativity in choosing the topic for the final project?\", \"Are there any specific requirements for the datasets used in the project?\"]',\n",
       " '7f93c032': '[\"What happens to my graduation chances if I missed a week\\'s homework?\", \"Must I have completed every single homework to get my certificate?\", \"Can I still graduate if I didn\\'t finish the week 5 homework?\", \"Do I have to finish all homework assignments to pass the course?\", \"Will missing one homework assignment affect my final grade or ranking?\"]',\n",
       " 'ee6f7c89': '[\"What are the cloud options for the final project, specifically what are the possibilities if I don\\'t deploy it on the cloud?\", \"Is it necessary to use cloud infrastructure for the final project, or can I still get points by mimicking something like AWS locally?\", \"Are there any specific requirements for the final project concerning cloud deployment?\", \"Can I get points for using a local stack for the final project rather than cloud infrastructure?\", \"Are there any benefits for using cloud infrastructure versus deploying locally for the final project?\"]',\n",
       " 'b63b12e0': '[\"How do I set up port-forwarding for Jupyter Notebook without using Visual Studio?\", \"Can I use this method for other IDEs besides VSCode?\", \"Do I need to restart my computer after updating the ~/.ssh/config file?\", \"Why do I need to use --no-browser with the jupyter notebook command?\", \"Can I use a different port number instead of 8899?\"]',\n",
       " '892c22c1': '[\"How can I open Jupyter notebooks in VSCode? Can I use the standard Jupyter interface to edit my notebooks?\", \"Why do I need to install a specific extension to open Jupyter in VSCode? Is there a different way to do it?\", \"Is the Jupyter extension the only way to edit my Jupyter notebooks in VSCode? Can I use other extensions or tools?\", \"What are the benefits of opening Jupyter notebooks in VSCode instead of using the standard Jupyter interface?\", \"Can I still access the original Jupyter interface if I install the VSCode extension? Can I use both in parallel?\"]',\n",
       " '13d38e8d': '[\\n\"How do I set up a GitHub repository for Homeworks?\",\\n\"What are the tutorials I can follow to configure GitHub on AWS instance?\",\\n\"Can I push to my GitHub repository from the remote VM?\",\\n\"What role does AWS instance play in setting up GitHub?\",\\n\"How do I set up keys on the AWS instance for GitHub?\"',\n",
       " '7d64e9e0': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How do I resolve issues while setting up Jupyter Notebook on AWS?\", \\n\"What are the specific steps to access Jupyter Notebook from my desktop?\", \\n\"What are the prerequisites for accessing Jupyter Notebook on AWS?\", \\n\"How do I edit the Jupyter Notebook configuration file?\", \\n\"What specific line do I need to add to the configuration file to access Jupyter Notebook from my desktop?\"',\n",
       " '645f0a55': 'Here are 5 questions based on the FAQ record:\\n\\n[\\n  \"How do I install and set up WSL on my Windows machine?\",\\n  \"Do I need to install Anaconda separately, or is it included in another package?\",\\n  \"Can I use Docker Desktop with WSL on my Windows machine?\",\\n  \"How do I clone a GitHub repository using WSL?\",\\n  \"Is there a single package that includes all the necessary software for the course, or do I need to install each one separately?\"\\n]',\n",
       " '7297b7fc': '[\"How do I prevent specific files from being tracked by Git when I commit my changes?\", \"Can I ignore an entire folder instead of individual files?\", \"Should I include all rules in the .gitignore file when I create it?\", \"How do I add rules to the .gitignore file?\", \"Is there a way to learn more about the syntax and rules for the .gitignore file?\"]',\n",
       " '68154f64': '[\"What does it mean when an EC2 instance is stopped but still incurs charges?\", \"How do I make sure an EC2 instance is actually stopped?\", \"What happens to data uploaded to a stopped EC2 instance?\", \"How can I set up billing alerts for my AWS usage?\", \"Why is it important to refresh the page when stopping an EC2 instance?\"]',\n",
       " 'dc7b6f51': '[\"What does it mean that IBM Cloud has different characteristics compared to AWS?\", \"How do I get an invitation code for IBM Cloud?\", \"What is the main difference between IBM Cloud and AWS?\", \"Can I use the invitation code to create an account on IBM Cloud?\", \"What is the video link provided in the FAQ if not provided by the instructor?\"]',\n",
       " 'b25c6ca3': '[\"How can I estimate the cost of running an AWS instance during the course if I don\\'t want to remember to stop the instance every day?\", \"Will I have a different public IP address every time I restart the instance and do I need to edit my ssh Config file?\", \"Can I set up an automatic email alert if my AWS budget exceeds a certain amount?\", \"What is the cost of using an instance with 2 hours of usage per day and 30 GB EBS per month?\", \"Can I calculate the cost of using AWS without logging in and is there a link to a tool that can help me with this?\"]',\n",
       " '9f69ca26': '[\\n\"Can we use the AWS free tier for most parts of the course, considering some things like Kinesis are not included and need to be done locally instead?\",\\n\"Is the AWS free tier sufficient for our needs in this course?\",\\n\"Will the AWS free tier allow us to complete the majority of course activities, excluding certain tools like Kinesis that require additional setup?\",\\n\"Can we make use of the AWS free tier for most aspects of the course, except for maybe tools like Kinesis that are not included?\",\\n\"Are there any limitations in the AWS free tier that would hinder our ability to complete the majority of the course activities, like the case with Kinesis?\"',\n",
       " '0f1ddc9e': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What should I do if I get an error \\'This site can\\'t be reached\\' when I click an open IP address in an AWS EC2 instance?\",\\n\"Why can\\'t I access my AWS EC2 instance by opening its IP address in a browser?\",\\n\"I don\\'t understand why I need to connect to the instance via terminal, can\\'t I just use my browser?\",\\n\"Can I use a different username than \\'ubuntu\\' when connecting to the instance via terminal?\",\\n\"How do I know if the key I downloaded is the correct one for my instance?\"',\n",
       " '01f61154': 'here are the 5 questions this student might ask:\\n\\n[\\n\"What does \\'unprotected private key file\\' error mean?\",\\n\"Can you explain why the key file needs to be changed?\",\\n\"What is the purpose of the \\'chmod 400\\' command?\",\\n\"Why do I need to use the specific directory \\'~/.ssh/\\' for the key file?\",\\n\"Is the \\'https//99robots.com...\\' link provided for troubleshooting general SSH errors?\"',\n",
       " 'd43c32ba': 'Here are 5 potential questions based on the FAQ record:\\n\\n[\\n  \"Why do my SSH connections to my AWS EC2 instance keep dropping?\",\\n  \"Is it normal to have to restart the instance with a new public IPv4 address after it disconnects?\",\\n  \"I\\'ve tried the troubleshooting steps on the AWS support page, but my issue remains. What can I do next?\",\\n  \"How can I prevent my EC2 instance from running out of memory and killing my SSH connection?\",\\n  \"Can I use a higher compute VM with more RAM to resolve these connection issues, or are there other workarounds?\"',\n",
       " 'a044d267': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What happens to my EC2 instance\\'s IP address when I restart it?\", \"Do I need to manually update the config file every time I restart my EC2 instance?\", \"Why do I get different IP addresses for my EC2 instance after restarting?\", \"Can I automate the process of updating the config file after restarting my EC2 instance?\", \"How can I avoid having to manually update the config file after restarting my EC2 instance?\"]',\n",
       " 'abf8ccdc': '[\"How to prevent VS Code from crashing when connecting to Jupyter?\", \"Can I use any instance to connect to Jupyter?\", \"What are some common problems I might encounter when connecting to Jupyter?\", \"How do I troubleshoot issues with connecting to Jupyter?\", \"What are some general guidelines for selecting an instance to use with Jupyter?\"]',\n",
       " '26918af3': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What causes the ValueError in my Linear Regression Model when running on the validation dataset?\", \"Why does my Linear Regression Model expect a specific number of features?\", \"How does the DictVectorizer handle features when transforming the validation dataset?\", \"What is the difference between fit_transform and transform methods of the DictVectorizer?\", \"Why do I need to use fit_transform and transform separately when working with the DictVectorizer?\"]',\n",
       " 'a5234ac0': '[\"What happens when you encounter a missing dependency error and how do you resolve it? How do you install the required packages if it occurs when using pandas? What if you\\'re using Conda instead of pip? Can you explain the differences in installing pyarrow and fastparquet? Why are fastparquet and pyarrow used together?\"]\\n\\nLet me know if I can help you with anything else!',\n",
       " 'af22c52a': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"How can I get a RMSE value that matches the options when I evaluate my model on the entire February data?\", \"Why does the RMSE value I get not match any of the options?\", \"How do I handle deprecated functions in my code and prevent the warnings from appearing?\", \"Can I filter null values and still expect to get a RMSE value that matches the options?\", \"Why do I need to convert the columns data types to str before using DictVectorizer?\"]',\n",
       " '2aaac94c': '[\"What changes can I make to a distplot to convert it to a histplot?\", \"Can I use kde=True with histplot?\", \"Is it necessary to specify stat=\\'density\\' when using histplot?\", \"How can I customize the appearance of the histogram bars in histplot?\", \"What is the default behavior of histplot in terms of binning and edgecolor?\"]',\n",
       " '9d15c9e9': '[\"What might be causing a KeyError with \\'PULocationID\\' or \\'DOLocationID\\' in Module 1?\", \"Can you give me a hint about fixing the KeyError issue in Module 1?\", \"I\\'m getting a KeyError when trying to access \\'PULocationID\\' or \\'DOLocationID\\' in Module 1, what should I do?\", \"Why am I encountering a KeyError with \\'PULocationID\\' or \\'DOLocationID\\' when running my code in Module 1?\", \"How can I resolve the KeyError in Module 1 related to \\'PULocationID\\' or \\'DOLocationID\\'?\"]',\n",
       " '79b88d0b': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What errors do you typically encounter when reading large Parquet files in Jupyter?\", \"How do you resolve an \\'IndexError: index 311297 is out of bounds for axis 0 with size 131743\\' error?\", \"Can I use Jupyter to perform the homework directly as a Python script?\", \"Is there a reliable solution for reading large Parquet files in Jupyter?\", \"Can the Pyspark library help me overcome issues with reading large Parquet files in Jupyter?\"]',\n",
       " '45485322': '[\"Why does my distplot take so much time to render?\", \"I have a lot of outliers in my data, can I remove them before plotting?\", \"I removed some outliers, but the plot is still slow, is there anything else I can do?\", \"Can I remove only some trips with unusual duration and not all?\", \"Can you explain why removing outliers helps to speed up the plotting?\"]',\n",
       " 'd5eab395': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\"Why is it important to use the correct encoder for one-hot encoding the validation set?\", \\n\"What are the consequences of using the wrong encoder for one-hot encoding the validation set?\", \\n\"How can I ensure that my one-hot encoding of categorical features is correct?\", \\n\"Why does the handle_unknown=\\'ignore\\' parameter cause issues when one-hot encoding?\", \\n\"Are there any general guidelines for choosing between OneHotEncoder and DictVectorizer for encoding categorical features?\"]',\n",
       " '282957fb': '[\"Why are pd.get_dummies and OneHotEncoder sometimes used to get different results? Can you explain how they handle missing data in the train and validation sets? What is the difference in their behavior? Are there any situations where one is preferred over the other? Are there other alternative solutions to encoding categorical variables? Can you provide resources for more information on this topic?\"]',\n",
       " '39ad14fd': 'Here are the 5 questions based on the FAQ record:\\n\\n[\\n\"Why did we choose DictVectorizer over OneHotEncoder for one-hot encoding in pandas?\",\\n\"Can we use get_dummies in pandas library for one-hot encoding?\",\\n\"Why not use scikit-learn\\'s OneHotEncoder for one-hot encoding?\",\\n\"Can DictVectorizer also be used to output a row column tuple matrix?\",\\n\"Did you consider using NumPy\\'s eye or identity components for one-hot encoding?\"',\n",
       " 'e34df2a5': \"Here are 5 questions the student might ask based on the FAQ record:\\n\\n1. How do I ensure that I'm effectively removing outliers from my dataset when using a boolean expression?\\n2. What is the best way to verify the data distribution after clipping outliers, and what statistics should I look for?\\n3. Can you show me an example of how to use the describe() function to get statistics on the data distribution after outlier removal?\\n4. Why is it important to use the describe() function to check on the data distribution after clipping outliers, and what potential issues might occur if I don't?\\n5. How can I easily convert the output of the describe() function to a parsable JSON format, as mentioned in the answer?\",\n",
       " 'c91b6b57': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why do I need to convert PUlocationID and DOlocationID to string values for one-hot encoding?\", \"How do pd.get_dummies and DictVectorizer create a one-hot encoding on string values?\", \"Will the RMSE change if I use a different string representation for NaN values in DictVectorizer?\", \"Can I use pd.get_dummies for converting numeric columns to categorical?\", \"Do I need to convert PUlocationID and DOlocationID to string before running DictVectorizer or pd.get_dummies?\"]',\n",
       " '4aa8eafc': 'Here are the 5 questions this student might ask:\\n\\n[\"Why is my LinearRegression RSME slightly different from the expected answer?\", \"How do I ensure I\\'m treating outliers properly in both train and validation sets?\", \"Can you explain what\\'s wrong with one hot encoding if the shape of the one hot encoded feature matrix shows 2 features?\", \"How can I properly format the drop off and pick up codes to fix the one hot encoding issue?\", \"Why does the LinearRegression model not output the same results when given the same inputs?\"]',\n",
       " 'a9daaab0': 'Here are 5 questions based on the FAQ record:\\n\\n[\\n\"What can I do when my RMSE score is extremely low, such as 4.3451e-6?\",\\n\"I\\'m getting a extremely low RMSE score, what should I check in my code?\",\\nWhy is my model scoring extremely low and what can I do to fix it?\",\\n\"Is it normal to get a low RMSE score when passing the target variable as a parameter while fitting the model?\",\\n\"What should I check to ensure my model predicts the data correctly and not just the target variable?\"',\n",
       " '931f9626': 'Here is the list of questions based on the FAQ record:\\n\\n[\"What is the problem people are trying to solve when it comes to enabling auto-completion in Jupyter notebook?\",\\n\"How do I install the necessary software to enable auto-completion in Jupyter notebook?\",\\n\"What version of the software should I install to enable auto-completion in Jupyter notebook?\",\\n\"Why is the tab button not working for auto-completion in Jupyter notebook?\",\\n\"How do I upgrade the Jedi package to enable auto-completion in Jupyter notebook?\"]',\n",
       " '782e1723': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why do I get a 403 error when trying to download the NY Taxis datasets?\", \"Can I download the dataset directly from the link provided in the videos?\", \"What should I do if the links change or are no longer accessible?\", \"Why is the download not working and what\\'s the error I\\'m encountering?\", \"Is there a way to get the data without using wget?\"]',\n",
       " '4e08c86a': 'Here are the 5 questions the student might ask based on the FAQ record in JSON format:\\n\\n[\"How do I get PyCharm to recognize my conda environment on a remote server?\", \"What is the issue with PyCharm not seeing my conda environment on a remote server?\", \"How do I activate my conda environment on a remote server?\", \"What is the correct path I should use when adding a local interpreter in PyCharm on a remote server?\", \"How do I add a new interpreter to PyCharm using the python execution path on a remote server?\"]',\n",
       " '34bcad27': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"How can I resolve memory issues when running the DictVectorizer?\", \"Why does setting the \\'sparse\\' parameter to False cause memory issues?\", \"What is the default value of the \\'sparse\\' parameter in the DictVectorizer example?\", \"Is it possible to use \\'sparse=False\\' and still view the results?\", \"What other memory-saving strategies can I use when working with large datasets in scikit-learn?\"]',\n",
       " '96144e66': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I activate my Anaconda environment in my .bashrc file?\", \"Why didn\\'t installing Anaconda modify my .bashrc profile?\", \"Can you give an example of how to initiate conda in the bash environment?\", \"How do I reload my .bashrc file after modifying it?\", \"What is the benefit of initiating conda in the bash environment?\"]',\n",
       " '840f739d': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why are the feature sizes different for the training set and validation set in HW1?\",\\n\"Why do we need to use fit_transform instead of transform on the dictionary vectorizer?\",\\n\"What\\'s the purpose of the `fit` pipeline in the model?\",\\n\"Why did the author go down the rabbit hole to figure this out?\",\\n\"What\\'s the difference between `fit` and `transform` methods in the dictionary vectorizer?\"]',\n",
       " 'bf006ff9': '{\"questions\": [\\n\"How do I regain access to my AWS machine after removing my public key?\",\\n\"What is the primary cause of the \\'Permission denied (publickey)\\' error in AWS?\",\\n\"Is there a specific guide for resolving permission denied errors in the AWS Session Manager?\",\\n\"Can I use the \\'ssh-keygen\\' command to recover my old public key?\",\\n\"How can I retrieve the public key of an existing AWS key pair?\"]}',\n",
       " 'f178d4a0': '[\"Why do we get absurdly high RMSE on the validation dataset?\", \"Can we use February dataset as validation/test dataset?\", \"What\\'s the issue with the DictVectorizer and ndarray conversion?\", \"Why did we need to remove the rows for duration between 1 and 60 for both train and validation dataset?\", \"How does removing the sparsematrix result from DictVectorizer help in fixing the issue?\"]',\n",
       " 'b80401a2': '[\"How do I solve the issue of not being able to import sklearn when following along with the course?\", \"Why did the instructor suggest reinstalling scikit-learn using pip when they couldn\\'t import it earlier?\", \"What command did Joel Auccapuclla use to install scikit-learn successfully?\", \"What is the purpose of using !pip install scikit-learn for fixing import issues?\", \"What specific feature is being imported from sklearn.feature_extraction with the code from DictVectorizer?\"]',\n",
       " '88002d35': '[\"What do I do when I get Access Denied at Localhost:5000 and the error message is You don\\'t have authorization to view this page (127.0.0.1:5000)?\", \"Why do I need to flush socket pools when accessing Localhost:5000 in chrome?\", \"How can I resolve authorization issues on Localhost:5000?\", \"What specific command should I enter to flush socket pools when using chrome? Answer:\", \"Is there a way to avoid authorization issues when accessing Localhost:5000 in the future?\", \"Will flushing socket pools resolve all authorization issues on Localhost:5000?\"]',\n",
       " 'fe61aa5b': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What does the error message \"Connection in use: (\\'127.0.0.1\\', 5000)\" mean?\", \"How do I stop a process that\\'s using port 5000 on my Mac?\", \"Is there a way to stop all processes using port 5000 in one command?\", \"What happens if I\\'m running multiple processes using port 5000? How do I stop them all at once?\", \"Can I change the port that the mlflow UI is using, and how do I do that?\"]',\n",
       " 'b9adeb39': 'Here are the 5 questions the student might ask:\\n\\n[\"What does \\'could not convert string to float\\' error mean in this context?\", \\n\"Can you explain why I\\'m getting \\'could not convert string to float\\' error in register_model.py?\", \\n\"Why is my hyperparameter tuning search_space dictionary being treated as a string?\", \\n\"Can you provide more information about why I need to log parameters in the hyperparameter tuning objective function?\", \\n\"How do I prevent my parameters from being logged in groups and causing the \\'could not convert string to float\\' error?\"]',\n",
       " 'ebc13686': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"How do I ensure the mlflow UI appears when running experiments\",\\n\"Why can\\'t I see my experiment in the MLflow UI\",\\n\"What is the correct way to specify the tracking URI in the mlflow ui\",\\n\"Can I use an absolute path to the mlflow.db file\",\\n\"How do I make the mlflow ui appear from a different directory\"\\n]',\n",
       " '939f9c33': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"When I try to install mlflow using pip, I get an ERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. What does this error mean?\",\\n\"Why does this error occur only when I install numpy as part of mlflow and not when I install it separately?\",\\n\"What causes the sha256 hash mismatch error during mlflow installation?\",\\n\"Is it possible to consistently simulate this issue during pip install of mlflow?\",\\n\"How do I resolve this \\'Hash Mismatch Error\\' to successfully install mlflow and numpy?\"',\n",
       " 'b5c3e6af': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"How do I delete an experiment that\\'s still persisting in the database after deleting it from the MLFlow UI?\",\\n\"What does it mean to \\'delete an experiment permanently\\'?\",\\n\"Can you give more details on how to use SQL magic scripts in Jupyter Notebook?\",\\n\"What is the purpose of loading the SQL scripts with `%load_ext sql`?\",\\n\"How do I ensure that I\\'m using the correct database file name when running the SQL script?\"',\n",
       " '80554fc2': 'Here are 5 questions that a student might ask based on the FAQ record:\\n\\n[\"How do I make a fork on GitHub instead of cloning a repository?\", \"I cloned the public repo and made changes, but now I want to merge recent commits without losing my own changes. What are the exact commands I use?\", \"What are the Git configuration settings that are important for this scenario?\", \"How do I fetch recent commits from a public repository without overwriting my own changes?\", \"What is the difference between fetching and merging upstream changes on GitHub?\"]',\n",
       " '943df153': 'Here are 5 questions based on the FAQ record:\\n\\n[\"Why does the image size of 460x93139 pixels exceed the allowed limit of 2^16 pixels in each direction?\", \"What happens when using mlflow.xgboost.autolog() with xgboost version 1.6.1?\", \"How can we avoid the issue of large image size in our experiment tracking?\", \"What is the alternative for installing xgboost version 1.6.0?\", \"Why did the author, Nakul Bajaj, add this solution?\" ]',\n",
       " 'b8d3c55e': '[\"What happen to the list_experiments method in MlflowClient?\", \"Is there an alternative method to use instead of list_experiments for searching experiments?\", \"Why was list_experiments method deprecated and what is the new recommended way to do it?\", \"Are there any specific reasons why it was removed from the later versions of Mlflow?\", \"Can you give an example of how to use search_experiments method to list experiments?\"]',\n",
       " '67bf60c6': '[\"How do I ensure that mlflow.autolog() is written before with mlflow.start_run() when using MLflow Autolog?\", \"What dependencies should I make sure are installed for the autologger to work?\", \"Are there any warnings raised if dependencies are not installed?\", \"Can you provide an example of how to properly use mlflow.autolog() and mlflow.start_run()?\", \"Why is it important to use mlflow.autolog() and mlflow.start_run() in a specific order?\"]',\n",
       " '336f5e36': 'Here are 5 questions based on the FAQ record:\\n\\n[\"Why is the MLflow URL not opening when I run it on a remote VM?\", \"Do I need to forward the port for MLflow like I did for Jupyter notebook?\", \"How do I navigate to the MLflow URL if I\\'m running it locally and it shows a blank page?\", \"Why is the screenshot of MLflow port forwarding important?\", \"What is the difference between \\'127.0.0.1:5000\\' and \\'localhost:5000\\'?\"]',\n",
       " 'fd2b9972': '[\"What happens when I use mlflow.xgboost.autolog() and get the same warning message as Warrie Warrie?\", \"Why do I get a warning message when tracking my model?\", \"Will I still be able to track my model even with this warning message?\", \"How do I check if my model is being tracked in the MLflow UI?\", \"Should I worry about this warning message when running mlflow.xgboost.autolog()?\"\"]',\n",
       " '75cd9b7a': '[\"What is the error message when trying to set a deleted experiment as active?\", \"Why is an experiment considered deleted?\", \"Can I just restore the deleted experiment and set it as active?\", \"What are some common reasons why an experiment is marked as deleted?\", \"I have a deleted experiment, but the link provided does not work, what can I do?\"]',\n",
       " '51c99586': '[\"What can I do if I run out of disk space when installing requirements in Experiment tracking?\", \"How do I increase the base EBS volume to install requirements?\", \"Can I use an external disk to store conda installation?\", \"How do I configure conda installation to happen on an external disk?\", \"Is it possible to install a separate instance of conda on an external disk?\"]',\n",
       " '089c8c18': '[\"What causes parameters mismatch in Homework Q3?\", \"Is this issue specific to random forest regressor?\", \"Why is min_impurity_split deprecated in the latest version of sklearn?\", \"How do I know if I\\'m using an old version of sklearn?\", \"What happens when I upgrade to the latest version of sklearn?\"]',\n",
       " 'f4b82056': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What causes the protobuf error when installing MLflow?\", \"Why do I get a protobuf error when running mlflow from my terminal?\", \"How do I fix the protobuf error when installing MLflow?\", \"Is it necessary to downgrade the version of \\'protobuf\\' module to install MLflow?\", \"Can I install a different version of \\'protobuf\\' to resolve the error?\"]',\n",
       " 'dd2e7dc9': '[\\n  \"What directory should I run the mlflow ui command in?\",\\n  \"Do I need to run mlflow server command instead of mlflow ui?\",\\n  \"What\\'s the difference between mlflow ui and mlflow server commands?\",\\n  \"How can I verify that I\\'m running the command in the correct directory?\",\\n  \"Why am I getting an error when running mlflow ui or mlflow server command?\"\\n]',\n",
       " '3fcbd80e': '[\"How can I set up MLflow experiment tracker on GCP if I\\'m having trouble?\", \"What are some common issues with setting up MLflow for GCP experiment tracking?\", \"Are there any additional resources I can use to troubleshoot MLflow setup on GCP?\", \"Can I use MLflow with other workflow orchestration tools like Zenml?\", \"What are the specific steps I need to follow to set up MLflow experiment tracker on GCP?\"]',\n",
       " '924fcf47': '[\"What solutions were provided for the problem of setuptools replacing Distutils and MLflow Autolog warning?\", \"How can I resolve the MLflow Autolog warning issue?\", \"Is it recommended to downgrade setuptools when resolving MLflow Autolog warning?\", \"Can you give me an example of a specific version downgrade for setuptools?\", \"What was the specific version downgrade solution provided for the issue?\"]',\n",
       " '58240887': '[\"How do I sort runs in the MLflow UI?\", \"Why can\\'t I sort runs in the UI initially?\", \"What if I\\'m already in table view, but still can\\'t sort?\", \"Can I sort runs in list view as well?\", \"How to ensure I\\'m in the correct view to sort runs?\"]',\n",
       " '67d343f2': '[\"What is the most common issue encountered during MLflow UI launch?\", \"Why do I get a TypeError when trying to open the MLflow UI?\", \"How do I resolve the TypeError issue after launching MLflow UI?\", \"Can you explain why reinstalling Flask solves the problem?\", \"What are the potential causes of a stuck old version of Flask?\"]',\n",
       " '6de95c2a': '[\"Why do I get a FileNotFoundError when trying to run mlflow ui on Windows?\", \"How do I resolve the issue with mlflow ui not being able to find the necessary file?\", \"Can you help me troubleshoot the problem with mlflow ui not finding the specified file?\", \"What is the solution to the mlflow ui error on Windows?\", \"Why is it necessary to add something to the PATH environment variable for mlflow ui to work?\"]',\n",
       " '2ff28e5b': 'Here are the 5 questions the student might ask:\\n\\n[\"What caused the TypeError: unsupported operand type(s) for -: \\'str\\' and \\'int\\' error in hpo.py?\",\"Why did the --max_evals argument implicitly convert to string?\",\"What should I do to fix the TypeError in hpo.py?\",\"How can I ensure the --max_evals argument is correctly inferred as an integer?\",\"What is the purpose of the --max_evals argument in hpo.py?\"]',\n",
       " '29c6bbf1': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\\n\"What if I encounter errors while running mlflow.sklearn with my current version of scikit-learn?\",\\n\"Why is my scikit-learn version not supported by MLflow?\",\\n\"What are the supported versions of scikit-learn for use with MLflow?\",\\n\"Can you provide a reference for the supported scikit-learn versions?\",\\n\"How do I upgrade/downgrade scikit-learn to a supported version?\"',\n",
       " 'bd09df94': 'Here are 5 questions based on the FAQ record:\\n\\n[\"Why doesn\\'t the mlflow experiments list command return any experiments?\", \"I ran the mlflow experiments list command but it\\'s not showing my experiments. What\\'s wrong?\", \"How do I make the mlflow experiments list command work?\", \"The mlflow experiments list command doesn\\'t display my experiments. What am I doing wrong?\", \"Why is the mlflow experiments list command not showing any of my experiments?\"]',\n",
       " 'af887c59': '[\\n  \"How can I set the MLFLOW_TRACKING_URI environment variable for my tracking server so I can view my experiments from the command line?\",\\n  \"What is the correct syntax for setting the MLFLOW_TRACKING_URI environment variable, and how do I determine the URI of my sqlite database?\",\\n  \"Why do I need to set MLFLOW_TRACKING_URI to view my experiments, even after starting the tracking server?\",\\n  \"Can I make it so that MLFLOW_CLI commands like \\'mlflow gc\\' automatically use my tracking URI, or do I always need to specify it?\",\\n  \"How do I use \\'mlflow experiments search\\' to view my experiments from the command line after setting MLFLOW_TRACKING_URI?\"\\n]',\n",
       " 'ee7c59ea': '[\\n\"How do I access the SQLite database provided by initiating the mlflow UI command?\",\\n\"How can I use PyCharm\\'s Database tab to inspect the tables in the SQLite database?\",\\n\"Can I query and inspect the tables using regular SQL after creating the connection?\",\\n\"Is the same process applicable for any SQL-backed database, such as Postgres?\",\\n\"How can I use the SQLite database to improve understanding of the entity structure of the data being stored within mlflow?\"',\n",
       " 'a2531c75': '[\"What is another way to start an mlflow server for remote hosting?\",\"How is launching the tracking server locally useful when working with multiple colleagues?\",\"Can I run the tracking server on only one laptop when working with multiple colleagues?\",\"What is the purpose of a mlflow server when working with colleagues?\",\"Why is launching the tracking server locally a preferred option when hosting mlflow server?\"]',\n",
       " 'bc4b2320': '[\"What happens when a parameter is not recognized during the model registry process in Experiment tracking in Module 2?\", \"How can I ensure that my parameters are recognized during the model registry?\", \"Why is it necessary to add parameters before the model registry?\", \"Can you show an example of how to add parameters with mlflow.log_params(params)?\", \"How do I append the parameters to the data.run.params dictionary?\"]',\n",
       " 'f69fb077': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What should I do if mlflow.log_params doesn\\'t recognize the Max_depth parameter even after adding it?\",\\n\"Why does mlflow.log_params append new models to previous runs that don\\'t contain the parameters?\",\\n\"How can I modify the hpo.py script to include mlflow.log_params with Max_depth?\",\\n\"What should I do if I want to remove or change previous experiments?\",\\n\"How does modifying the hpo.py script with mlflow.log_params affect my experiment tracking?\"',\n",
       " 'e223524c': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What should I do when my week_2 homework in the register_model.py script fails and I get the error AttributeError: \\'tuple\\' object has no attribute \\'tb_frame\\' in Jupyter Notebook?\", \\n\"What is the issue with the click decorators in my register_model.py script that\\'s causing the error?\", \\n\"Can you explain why I\\'m getting an AttributeError in my Jupyter Notebook when I run my week_2 homework?\", \\n\"What changes should I make to my register_model.py script to resolve the error and allow it to run in Jupyter Notebook?\", \\n\"Why do I need to remove click decorators in my script to fix the error and get it working in Jupyter Notebook?\"]',\n",
       " '0f08bec7': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\"How do I resolve the \\'api_key not configured\\' error when running preprocess_data.py?\", \"What is the \\'Danger Zone\\' in my WandB profile and why do I need to access it?\", \"Why do I need to run a bash cell in my notebook before running preprocess_data.py?\", \"What is the purpose of logging in to WandB before running preprocess_data.py?\", \"How will I know if I have successfully configured my WandB API key?\"]',\n",
       " '8b4b1685': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"How do I enable autologging for XGBoost? I want to ensure that my dataset is in the correct format before constructing it.\",\\n\"What are the necessary steps to follow when dealing with WARNING mlflow.xgboost: Failed to infer model signature? I want to avoid this issue in the future.\",\\n\"What is the format compatible with XGBoost that I should check my data against? I don\\'t want to encounter any errors.\",\\n\"Can you provide an example of how to construct the dataset and train the XGBoost model in the correct order? I\\'m struggling with the syntax.\",\\n\"Why am I getting this warning message despite following the order suggested in the FAQ? Can I get some additional troubleshooting tips?\"',\n",
       " 'ecfc5c07': '[\"What do you mean by \\'it did not recognize the pip command\\'?\", \"How do I use the pip command in Python?\", \"Is there a specific reason why pip doesn\\'t work as usual?\", \"Can I use pip for other commands too?\", \"How does a virtual environment affect pip commands?\"]',\n",
       " 'a1b68c52': '[\"How do I open a public GitHub notebook (.ipynb) directly in Google Colab without having to download it first?\", \"Why is it that Wandb UI sometimes becomes difficult to navigate?\", \"Can I open a notebook from a private GitHub repository in Google Colab?\", \"Is there a way to directly run a GitHub notebook (.ipynb) that is not public?\", \"I am having trouble finding the correct option in Wandb UI, can someone guide me on how to navigate it?\"]',\n",
       " '483e7d61': 'Here are 5 questions a student might ask based on the FAQ record:\\n\\n[\"Why do we use specific dates like Jan/Feb/March for Train/Test/Validation purposes and how do we avoid seasonality in our data?\", \"Can you provide more details on how to check for seasonality in our data and how it affects our model performance?\", \"Why does the mlflow server on AWS CLI with S3 bucket and POSTGRES database encounter an error and how can we resolve it?\", \"What is the purpose of validation in reporting model metrics to leadership, regulators, and auditors?\", \"How can we handle data leakage and overfitting in our model when predicting future outcomes?\"]',\n",
       " 'e5c33f50': '[\\n\"What is the purpose of orchestration in a microservices architecture?\", \"How do I troubleshoot issues with my Kubernetes service?\", \"How will I know if I am experiencing a delay in my application\\'s response?\", \"Why is there a difference in the response times for my two services?\", \"What is the best practice for setting the timeout value in my service?\"',\n",
       " 'cbf13b19': '[\"What does module 4 focus on?\", \"Can I find the Prefect FAQ here?\", \"What is Deployment in Module 4?\", \"Why are there no Prefect questions in this FAQ?\", \"Is the Prefect FAQ link correct?\"]',\n",
       " '39861d6e': '[\\n\"What does it mean if the AWS CLI version is not matching the correct version?\",\\n\"I\\'m using Windows with AWS CLI already installed, why is the Docker unable to login to ECR?\",\\n\"What is the exact command to run with the AWS CLI version I have, and why did the previous one fail?\",\\n\"In the provided command, what does \\'--username AWS\\' and \\'--password-stdin\\' flags do?\",\\n\"Why is it recommended to use \\'aws ecr get-login-password\\' instead of \\'aws ecr get-login\\'?\"]',\n",
       " '3dac15ff': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I use Windows Powershell for multiline commands?\", \"Can I use backticks for multiline string in Windows Powershell?\", \"How do I escape quotes in Windows Powershell?\", \"What is the difference between persistent and non-persistent environment variables in Windows Powershell?\", \"How do I format the output of the aws kinesis put-record command in JSON?\"]',\n",
       " '32686722': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"Why am I getting an AttributeError when trying to install pipenv?\",\"How do I resolve conflicting Python versions on my system?\",\"What is the purpose of removing pipenv installed via apt-get?\",\"How do I ensure pipenv is installed correctly?\",\"What alternative Python environments can I use instead of system Python?\"]',\n",
       " '22521751': '[\\n\"What could be the causes of the \\'module is not available\\' error when trying to connect to an HTTPS URL?\", \\n\"Why is the SSL configuration command \\'Python -m ssl\\' necessary in this case?\", \\n\"How do I know if the SSL configuration is correct if the output of this command is empty?\", \\n\"What is the purpose of upgrading the pipenv package in the current environment to resolve the \\'Can\\'t connect to HTTPS URL\\' issue?\", \\n\"Can the \\'module is not available\\' error occur if the SSL configuration is not the problem?\"',\n",
       " '81ad4784': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"What can I do if I get a \\'ModuleNotFoundError: No module named \\'pip._vendor.six\\' when installing scikit-learn via pipenv?\",\\n\"Why do I need to install \\'python-six\\' when using pipenv to install scikit-learn?\",\\n\"What does \\'pipenv --rm\\' do, and why do I need to run it?\",\\n\"What\\'s the correct sequence of commands to install scikit-learn using pipenv, and why do I need to follow this sequence?\",\\n\"I installed \\'python-six\\' and ran the correct sequence of commands, but I still get the same error. What could be the reason?\"',\n",
       " '29b5651e': '[\\n\"How can we integrate Jupyter notebooks with a Pipenv environment?\",\\n\"In order to use Jupyter notebooks with a Pipenv environment, do I need to install jupyter and ipykernel separately?\",\\n\"Can I install jupyter and ipykernel using pipenv or do I need to install them separately?\",\\n\"How do I register the kernel with python -m ipykernel install --user --name=my-virtualenv-name inside the Pipenv shell?\",\\n\"If I\\'m using Jupyter notebooks in VS Code, what will happen if I register the kernel in the Pipenv shell?\"\\n]',\n",
       " 'ca79bbe8': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What are common issues with deploying Jupyter notebooks using Pipenv?\", \"Why did I encounter no output in my Jupyter notebook after running it on Pipenv?\", \"How does Tornado relate to Jupyter and what might cause issues?\", \"How can I troubleshoot printing output in a Jupyter notebook running on Pipenv?\", \"What is the suggested solution to fix the no output issue with Jupyter and Tornado?\"]',\n",
       " '668f1ad9': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What causes the \\'Invalid base64\\' error when running \\'aws kinesis put-record\\' on my local machine?\",\\n\"Why is it a problem to use AWS CLI version 2 for \\'aws kinesis put-record\\'?\",\\n\"How can I solve the \\'Invalid base64\\' error when using AWS CLI version 2?\",\\n\"Will the encoding issue still occur if I upgrade to a newer version of AWS CLI?\",\\n\"What is the purpose of the \\'--cli-binary-format raw-in-base64-out\\' argument in AWS CLI?\"]',\n",
       " '7a6f23eb': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What is the error index 311297 and why is it out of bounds for axis 0?\", \"Why does the error appear when loading a parquet file?\", \"What is the relationship between the error and running the starter.ipynb in homework\\'s Q1?\", \"How can I resolve the error if pandas version is already the latest?\", \"What other dependencies need to be updated for the error to be fixed?\"]',\n",
       " '232e5557': '[\"Why doesn\\'t Pipfile.lock get created automatically during the initial project setup?\", \"What do I need to do if my Pipfile is created but not the Pipfile.lock?\", \"Can I use any command to generate Pipfile.lock?\", \"I created my Pipfile, but Pipfile.lock is missing. What\\'s going on?\", \"How can I ensure Pipfile.lock is generated when I set up a new project in Pipenv?\"]',\n",
       " 'e44ec04a': '[{\"question\": \"Why am I getting a Permission Denied error when trying to use Pipenv?\"}, {\"question\": \"Can you explain what\\'s causing the Permission Denied issue in Pipenv?\"}, {\"question\": \"How can I resolve this Permission Denied error when using Pipenv?\"}, {\"question\": \"What is the pythonfinder module in Pipenv and how does it relate to the Permission Denied issue?\"}, {\"question\": \"Where can I find more information on how to manually change the scripts to resolve the Permission Denied error?\"}]',\n",
       " '55fdb8b9': 'Here are the 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"Can I get an example of how to convert the inputs from the command line to numeric/integer before using them in f-strings when parsing arguments?\"\\n\"What is the cause of the ValueError: Unknown format code \\'d\\' for object of type \\'str\\' when passing arguments via command line?\"\\n\"In the example provided using click library, what is the purpose of the decorator and the option parameters?\"\\n\"How can I fix the error when using the sys.argv method to read inputs from the command line?\"\\n\"Why do I need to add a decorator to define the data type of the input argument in the script?\"',\n",
       " 'bf9082a2': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What should I do to ensure the correct image is being used to derive from when Dockerizing my application?\", \\n\"What is the best practice for copying data from local to the docker image?\", \\n\"Can I use absolute paths within the docker image for copying data?\", \\n\"What directory should I start paths from when copying data to the docker image?\", \\n\"What is the difference between the `docker build` and `docker run` commands?\"]',\n",
       " 'e7906e44': 'Here are 5 questions based on the FAQ record:\\n\\n[\"What should I do if I\\'m trying to run multiple services like Flask gunicorn & MLFlow server from the same container?\",\"How do I define the commands to run multiple services in my Dockerfile?\",\"What\\'s the purpose of a wrapper script in this scenario?\",\"How do I add executable permissions to the script files?\",\"Do I need to expose all ports defined by services when running multiple services in a Docker container?\"',\n",
       " '76d8892e': '[\\n  \"What is the exact error message that appears when trying to generate pipfile.lock?\",\\n  \"How do I fix the issue when pipfile.lock cannot be generated and an InstallationError occurs?\",\\n  \"Why does the error code 1 appear in the command \\'python setup.py egg_info\\'?\",\\n  \"What is the reason behind the requirement to upgrade pipenv and wheel?\",\\n  \"What is the sequence of commands to be executed to resolve the pipfile.lock generation issue?\"\\n]',\n",
       " 'c5c2c82a': '{\"questions\":\\n[\"How can I connect an S3 bucket to MLLow using boto3 and AWS CLI?\", \\n\"How do I ensure that I have the right to access the S3 bucket if I am not the owner?\", \\n\"Are access keys necessary to connect to the S3 bucket using boto3?\", \\n\"Can I set the S3 bucket as public to access it without using access keys?\", \\n\"What are the possible consequences if I do not have access keys?\"}',\n",
       " '82b6c143': '[\\n  \"I\\'m trying to deploy to S3 but I keep getting an error saying that my AWS Access Key Id doesn\\'t exist. I can upload files just fine using the AWS CLI and boto3 in Jupyter Notebook, so I\\'m not sure what\\'s going on.\",\\n  \"When I upload to S3, I get an error that says my AWS Access Key Id doesn\\'t exist, even though it works fine in Jupyter Notebook and with AWS CLI.\",\\n  \"I\\'m getting an error saying that my AWS Access Key Id doesn\\'t exist when trying to upload to S3, and I used to be able to do it just fine.\",\\n  \"Why do I keep getting an error saying that my AWS Access Key Id doesn\\'t exist when I try to upload to S3? I\\'ve double-checked my credentials and it works in Jupyter Notebook.\",\\n  \"I\\'m having trouble uploading to S3 because it\\'s saying that my AWS Access Key Id doesn\\'t exist, but uploading works from Jupyter Notebook and with AWS CLI - what should I do?\"\\n]',\n",
       " '77d9a742': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[{\"How can I fix the problem of \\'lib_lightgbm.so not being found\\' when I\\'m trying to deploy my LightGBM model using Docker?\", \"I\\'m using a specific operating system for my Docker image, will installing a different package affect my deployment?\", \"Why am I getting an \\'image not found\\' error when I run my Dockerized LightGBM application?\", \"What exactly does the \\'libgomp1\\' package do in the context of deploying LightGBM with Docker?\", \"Is installing \\'libgomp1\\' a one-time solution or do I need to include it in every Docker build?\"]',\n",
       " '1667e95d': '[\"What happens when I\\'m trying to execute mlflow\\'s pyfunc.load_model in a lambda function?\", \"Why is the mlflow library raising an error when processing requests in a lambda function?\", \"What does the phrase \\'AttributeError(\"module \\'dataclasses\\' has no attribute \\'__version__\\'\"\\') mean in the error message?\", \"How can I increase the memory of a lambda function?\", \"Would setting the logging level to DEBUG help resolve this error?\"]',\n",
       " '624a3525': '[\"What are the end states of the video in terms of MLFlow pipelines?\", \"Why is the notebook the end state of the video?\", \"What is the purpose of the MLFlow pipelines in this context?\", \"Will everything work as long as I am patient and follow the video?\", \"Do I need to use the repo\\'s notebook to reach the end state of the video?\"]',\n",
       " '1db86601': 'Here are 5 questions that the student might ask based on the FAQ record:\\n\\n[\"Why do I need to pass env vars before specifying the name of the image when running my Docker container?\",\\n\"How can I set environment variables in my shell for my AWS credentials?\",\\n\"What if the AWS credentials file is not found, what alternatives are available?\",\\n\"Does setting the AWS credentials files to volumes mean that my container will have access to my local machine\\'s credentials?\",\\n\"In the Docker run command, what does the --rm flag do?\"',\n",
       " '047baefe': '[\"What is the purpose of creating a Dockerfile and building it? How do I use it to see the model in the docker container in app/?\",\\n\"When I run the \\'docker run -it zoomcamp_test ls /app\\' command, why do I need to specify \\'zoomcamp_test\\'? Can\\'t I just use \\'docker run -it ls /app\\'?\",\\n\"What is the difference between RUN and CMD commands in Dockerfile? Why can\\'t I use RUN to run my script?\",\\n\"How do I run a script in the Docker container without having to run \\'docker run -it\\' every time? Can I configure the Dockerfile to do this?\",\\n\"If I change the Dockerfile, do I need to rebuild the image every time? Is there a way to avoid this or automate the process?\"]',\n",
       " '4f240372': 'Here are the 5 questions the student might ask:\\n\\n[\\n\"What does \\'platform tag\\' mean exactly in the context of building a Docker image?\", \\n\"How do I determine the detected host platform?\", \\n\"Why do I need to specify the platform tag when building the image?\", \\n\"Can I build the image using the Docker GUI instead of the command line?\", \\n\"What happens if I forget to include the platform tag when building the image?\" \\n]',\n",
       " '7aef625b': '[\"When calling apply_model() in score.ipynb, I encounter an HTTPError 403: Forbidden, how do I resolve this issue?\",\"What is the difference between the current input_file path and the suggested one when accessing trip data?\",\"Is there a specific reason why the original input_file path is causing an HTTPError 403: Forbidden?\",\"Why do we need to change the input_file path in order to successfully call apply_model() in score.ipynb?\",\"How does the revised input_file path help to resolve the HTTPError 403: Forbidden when calling apply_model() in score.ipynb?\"]',\n",
       " 'a3aa3a7d': '[{\"question\": \"What is causing the ModuleNotFoundError: No module named \\'pipenv.patched.pip._vendor.urllib3.response\\'?\"},  \\n{\"question\": \"How can I fix the error ModuleNotFoundError: No module named \\'pipenv.patched.pip._vendor.urllib3.response\\'?\"},  \\n{\"question\": \"What is the difference between pip install pipenv --force-reinstall and pip install -U pip and pip install requests?\"},  \\n{\"question\": \"Which command can be used to resolve the error site-packages\\\\pipenv\\\\patched\\\\pip\\\\_vendor\\\\urllib3\\\\connectionpool.py with regards to pipenv?\"},  \\n{\"question\": \"Who provided the solution to resolving the ModuleNotFoundError: No module named \\'pipenv.patched.pip._vendor.urllib3.response\\'?\"}]',\n",
       " 'd2719204': '[\"When running docker-compose up in Module 5, how do I access the login window in Grafana?\", \"What is the default username and password for the login window?\", \"Why do I need to enter a new password after logging in with the default credentials?\", \"Is the default username and password the same for all users or just for the first user?\", \"Where can I find more information about troubleshooting login issues in Grafana?\"]',\n",
       " '30b8e8e6': '[\\n\"What happens when I try to start services using docker compose up --build in Linux?\",\\n\"Why is the --build flag not recognized when I use \\'docker compose up --build\\'?\",\\n\"Is there a difference between running \\'docker-compose up --build\\' and \\'docker compose up --build\\'?\",\\n\"When I\\'m installing docker-compose separately in Linux, what modification do I need to make to start services?\",\\n\"What is the solution when the services won\\'t start and I get an \\'unknown flag: --build\\' message?\"',\n",
       " 'f33fc6e9': 'Here are 5 questions based on the FAQ record:\\n\\n[\\n\"Why do I keep getting a KeyError \\'content-length\\' when running prepare.py?\",\\n\"Why doesn\\'t the link in prepare.py work to download taxi data?\",\\n\"What should I do if the link in prepare.py is not working?\",\\n\"Is there an alternative to the link in prepare.py for downloading taxi data?\",\\n\"What was the solution suggested by Emeli Dral for the KeyError \\'content-length\\' issue?\"',\n",
       " 'd828de2a': 'Here are five questions that the student might ask based on the FAQ record:\\n\\n[\"What does the error message \"Max retries exceeded with url: /api\" mean when I run docker-compose up –build?\", \"Why does the evidently service exit with code 2 when I run docker-compose up –build?\", \"How do I fix the issue where the evidently service cannot import \\'from pyarrow import parquet as pq\\'? \", \"What are the possible solutions to the problem when the evidently service exits with code 2?\", \"Is there a way to check if app.py of evidently service really uses the pyarrow module before commenting it out?\"]',\n",
       " '03f20ec1': '[\"When I use evidently in Module 5, how do I avoid getting a ValueError if an incorrect item is passed to Report?\", \"In Report, what happens when you pass an incorrect item instead of a metric or metric preset?\", \"What is the correct format for passing items to Report in Module 5?\", \"What is the correct syntax to use if I want to avoid getting the ValueError in Module 5?\", \"Why do I get the ValueError \\'Incorrect item\\' when I run a report in Module 5?\"]',\n",
       " '249726fe': '[\"What happens if I forgot to add the \\'target\\' argument in the RegressionQualityMetric() report?\", \"Do I need to add a target for RegressionQualityMetric() to work?\", \"What specific value should I use for the target in RegressionQualityMetric()?\", \"Why did the documentation mention \\'duration_min\\' as the target for RegressionQualityMetric?\", \"Can I use RegressionQualityMetric() with other targets besides \\'duration_min\\'?\"]',\n",
       " '4e492af0': 'Here are 5 questions the student might ask based on the FAQ record:\\n\\n[\"What happens when I get a ValueError: Found array with 0 sample(s)?\", \"Why does LinearRegression require a minimum of 1 sample?\", \"Can you explain why the training dataset would be empty?\", \"How do I adjust the code to fix the issue?\", \"What date should I use for the \\'begin\\' datetime in the generated data?\"]',\n",
       " '10011dc1': '[\\n\"What are the requirements and optional parameters when adding a metric to avoid getting \\'target columns\\' and \\'prediction columns\\' not present errors?\"\\n\"Why do I get \\'target columns\\' and \\'prediction columns\\' not present errors after adding a new metric?\"\\n\"Can I add a metric that doesn\\'t require any parameters and how does it work?\"\\n\"I\\'ve added a metric and I\\'m still getting errors, what am I doing wrong?\"\\n\"What documentation should I read to understand the process of adding a new metric?\"',\n",
       " '92fb909a': '[\\n  \"When trying to log in to Grafana with the standard admin/admin credentials, I get an error message. How can I resolve this issue?\",\\n  \"What should I do if my standard login to Grafana doesn\\'t work as expected?\",\\n  \"Why do I receive an error when trying to log in to Grafana using the default admin/admin username and password?\",\\n  \"What is the correct method for resolving login issues with Grafana\\'s standard admin credentials?\",\\n  \"When trying to access Grafana with the standard admin/admin login, I encounter an error. How do I fix this problem?\"',\n",
       " '2b8cb640': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How do I get my chart in Grafana to update in real-time?\", \"Why doesn\\'t my metric chart get updated in Grafana?\", \"What settings do I need to configure to get updates in Grafana?\", \"How do I set the refresh interval for my chart in Grafana?\", \"Is it necessary to set my local timezone in pytz.timezone to get updates in Grafana?\"]',\n",
       " 'd4ceab0b': '[\"What happens when I run the Prefect server start command locally?\", \"Why did my Prefect server stop running immediately after starting?\", \"Can I use Prefect cloud to run my scripts instead?\", \"How do I troubleshoot issues with Prefect\", \"Is there a specific issue that I should report on GitHub when encountering problems with Prefect?\"]',\n",
       " '482e575f': '[\\n  \"When using Docker Compose up, why do I get the no disk space left error and how can I fix it?\",\\n  \"What does \\'docker system prune\\' do and how does it relate to the no disk space left error?\",\\n  \"Can you provide more details about the build cache, containers, and images that are being removed when I run \\'docker system prune\\'?\",\\n  \"How do I determine what\\'s taking up space on my disk before running \\'docker system prune\\'?\",\\n  \"Are there any other methods to free up disk space in Docker besides running \\'docker system prune\\'?\"\\n]',\n",
       " '33e775eb': '[\"What is the common error that occurs when running docker-compose up --build?\", \"What triggers the error \\'Failed to listen on :::8080\\'?\", \"How can I add a command to the adminer block in the yml file?\", \"What is the role of the Adminer image in the context of this error?\", \"Can I run docker-compose up --build without seeing this error?\"]',\n",
       " '19a3d34a': 'Here are the 5 questions:\\n\\n[\"How can we create a stat panel in Grafana?\", \"Does Grafana have a built-in way to create Evidently-like dashboards?\", \"Can we export Evidently output in JSON and use it in an external visualization layer?\", \"How do we ensure we have all the relevant information logged to our Grafana data source?\", \"Is it easier to access the under- and over-performance segments using Evidently or Grafana?\"]',\n",
       " '55c68f23': 'Here is the list of questions:\\n\\n[\"What happens when I run localstack with kinesis and get an error \\'Unable to locate credentials\\'?\",\\n\"How do I fix the error \\'Unable to locate credentials\\' when running localstack with kinesis?\", \\n\"What are the additional variables needed in the docker-compose.yaml file to resolve the \\'Unable to locate credentials\\' error?\",\\n\"Is it necessary to provide valid AWS credentials to resolve the error \\'Unable to locate credentials\\' when running localstack with kinesis?\", \\n\"Are there alternative methods to resolve the \\'Unable to locate credentials\\' error when running localstack with kinesis?\"]',\n",
       " '54020f0a': '[\"When creating a bucket with localstack and boto3, why do I get an error \\'unspecified location constraint is incompatible\\'?\",\\n\"Why am I getting an error when trying to create a bucket via s3_client.create_bucket(Bucket=\\'nyc-duration\\')?\",\\n\"Can you explain why the error occurs when creating a bucket to a region specific endpoint?\",\\n\"What is the solution to fix the error when trying to create a bucket with localstack and boto3?\",\\n\"How do I create a bucket with a specific location constraint using boto3 and s3_client?\"]',\n",
       " 'b6249d2c': 'Here are the 5 questions a student might ask based on the FAQ record:\\n\\n[\"How do I fix the error I get when I run an AWS CLI command?\", \"What is causing the error \\'<botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>\\' when I execute an AWS CLI command?\", \"Is it necessary to fill in the actual values for AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY?\", \"Can I just leave the AWS_DEFAULT_REGION variable blank?\", \"Why do I need to set these environment variables to resolve the error?\"]',\n",
       " '31543d95': '[\"What do I need to check when I get an error at every commit that says \\'mapping values are not allowed in this context\\'?\",\"Why isn\\'t my pre-commit hook running when I commit my code in the module?\",\"Can I use certain values in the pre-commit-config.yaml file?\",\"How can I fix the error when my pre-commit config file has incorrect indentation?\",\"What is the usual mistake when the pre-commit hook is not running?\"',\n",
       " 'e147bbb6': '[\\n\"What if I couldn\\'t reconfigure pytest from zero after completing a previous folder?\",\\n\"How do I completely remove pytest configurations?\",\\n\"Can I delete the .vscode folder from a previous testing folder?\",\\n\"How do I remove the pytest test folder from a previous week?\",\\n\"Why is pytest not configurable after testing a previous folder?\"',\n",
       " 'dc55657f': '[\"What are the possible reasons for getting empty records when using the Kinesis get records command with LocalStack?\", \"Why do I need to add the --no-sign-request parameter to the Kinesis get records command?\", \"Can I use the --no-sign-request parameter with other Kinesis commands?\", \"How does adding --no-sign-request affect the output of the Kinesis get records command?\", \"In the case of an empty record set, is there an alternative way to troubleshoot the issue?\"]',\n",
       " 'f6979915': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"What are the specific error messages we can expect to see when Git commit raises a utf-8 encoding error?\", \"Why does a pre-commit yaml file create encoding errors when using Git commit in Powershell?\", \"Can you provide a more detailed reason why the error \\'utf-8\\' codec can\\'t decode byte 0xff in position 0: invalid start byte\\' occurs in the pre-commit-config.yaml file?\", \"Should we always set the encoding type when creating pre-commit yaml files, or is this specific to certain scenarios?\", \"Can you provide a step-by-step explanation of how we can solve this encoding error when creating a pre-commit yaml file in Git?\"]',\n",
       " '1076a121': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n  \"When I try to commit with a pre-commit hook, why do I get an error message saying that \\'PythonInfo\\' object has no attribute \\'version_nodot\\'?\",\\n  \"What does the \\'return code: 1\\' and \\'expected return code: 0\\' message mean in the error output?\",\\n  \"What is the benefit of reusing the environment once installed, and how does it affect my workflow?\",\\n  \"Why do I need to clear app-data of the virtualenv, and what exactly does it do?\",\\n  \"Can I use the same virtualenv for multiple projects, or do I need to create a new one for each project?\"',\n",
       " 'aa203ca7': 'Here are 5 questions based on the FAQ record:\\n\\n[\\n\"What should I do if I get a \\'module not found\\' error when using custom packages in my source code with Pytest?\",\\n\"Why do I get a \\'No module named \\'production\\'\\' error when running pytest ./test/unit_tests?\",\\n\"Can you explain why running pytest from the command line doesn\\'t work as expected?\",\\n\"Why does running python -m pytest solve the \\'module not found\\' error?\",\\n\"Why is it necessary to add the current directory to the Python path when running pytest?\"',\n",
       " '8b04605d': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\\n\"How do I resolve the \\'module not found\\' error when using pre-commit hooks with custom packages in my source code?\",\\n\"What is causing the \\'No module named \\'production\\'\\' error when I commit my changes using \\'git commit -t \\'test\\'\\' and call the pytest hook?\",\\n\"Why am I getting this error when using the default pytest hook and not when I modify the hook entry to use a custom script?\",\\n\"Can you provide an example of a custom script that I can use to resolve this issue with pytest and pre-commit hooks?\",\\n\"How can I ensure that my custom script sets the right directory and runs the pytest tests correctly with the pre-commit hook?\"',\n",
       " 'a3b9af04': 'Here are the 5 questions a student might ask based on the FAQ record:\\n\\n[\"What does the \\'Permission denied error\\' mean when executing a script file in Github actions?\", \"Why is the execution permission important in a CI YAML file?\", \"How do I add execution permission to a script file in Github actions?\", \"What command should I use to add execution permission to a script file?\", \"What are the steps to resolve the \\'Permission denied error\\' when running a script in Github actions?\"]',\n",
       " 'b16aae74': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"How can we efficiently manage a large number of Docker containers when running a docker-compose file?\", \"Is there a way to selectively run only a subset of containers defined in a docker-compose file?\", \"What is the purpose of adding profiles in a service definition in a docker-compose file?\", \"Can you provide an example of how to specify a profile when starting a service in a docker-compose file?\", \"How do we ignore irrelevant containers during testing when running a docker-compose file with multiple containers?\"]',\n",
       " '66326a87': '[\"What are the common issues with AWS regions when using docker-compose?\", \"How do I ensure my AWS regions are correctly matched in docker-compose and local config?\", \"Can you provide an example of how to set the correct region in the config file and docker-compose.yaml?\", \"Why do I need to match AWS regions in docker-compose and local config for integration tests?\", \"What happens if I don\\'t match AWS regions in docker-compose and local config?\"]',\n",
       " 'fb3c4150': 'Here are the 5 questions the student might ask based on the FAQ record:\\n\\n[\"What causes the pre-commit command to fail with isort?\", \\n\"When was the issue with isort pre-commit command resolved?\", \\n\"Can you tell me why we need to set a specific version of isort?\", \\n\"So, how do we make sure our isort configuration works correctly?\", \\n\"What were the steps taken to resolve the isort pre-commit command issue?\"]',\n",
       " '886d1617': 'Here are 5 questions this student might ask based on the FAQ record:\\n\\n[\"How do we ensure all infrastructure created via GitHub Actions is properly destroyed?\", \"What are the necessary steps to delete infrastructure created with CD-Deploy Action in AWS?\", \"Can you provide more details on the \\'terraform init\\' command and its role in the infrastructure destruction process?\", \"What is the purpose of the \\'vars/prod.tfvars\\' file and how does it impact the \\'terraform destroy\\' command?\", \"How can we verify that the infrastructure has been successfully destroyed using the provided terraform commands?\"]'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "62165051-523b-4eee-aa3e-b927d0babf9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"What are the prerequisites for this course that I need to take before joining the data engineering zoomcamp course?\", \"What if I don\\'t have experience in data engineering yet?\", \"Can I take this course as a beginner?\", \"Are there any specific skills I need for this course?\", \"Do I need to have GitHub for this course?\"]'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['1f6520ca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "031ecc50-a9a3-4508-8149-cf716e045d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5cae0a85-7138-469b-b778-e6cd7206fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will now write dictionary into a pickle file for retrieval later\n",
    "with open('results_llama3_groq.bin', 'wb') as file:\n",
    "    pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "de64d355-a4c8-4633-b179-952bb38923c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will now use the binary file generated by Alexey from OpenAI\n",
    "with open('results.bin', 'rb') as f_in:\n",
    "    results = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2d5cd8f8-ddfd-4802-908b-504722511a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"Where can I find the prerequisites for this course?\", \"How do I check the prerequisites for this course?\", \"Where are the course prerequisites listed?\", \"What are the requirements for joining this course?\", \"Where is the list of prerequisites for the course?\"]'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['1f6520ca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d265559b-de13-45a3-ba4d-b5f84a3c070a",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Invalid \\escape: line 6 column 59 (char 414)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m parsed_resulst \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc_id, json_questions \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 4\u001b[0m     parsed_resulst[doc_id] \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_questions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Invalid \\escape: line 6 column 59 (char 414)"
     ]
    }
   ],
   "source": [
    "parsed_resulst = {}\n",
    "\n",
    "for doc_id, json_questions in results.items():\n",
    "    parsed_resulst[doc_id] = json.loads(json_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "691b4d59-4cd4-45e0-afb1-5f3cf35a6422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "\"How can I resolve the Docker error 'invalid mode: \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data'?\",\n",
      "\"What should I do if I encounter an invalid mode error in Docker on Windows?\",\n",
      "\"What is the correct mounting path to use in Docker for PostgreSQL data on Windows?\",\n",
      "\"Can you provide an example of a correct Docker mounting path for PostgreSQL data?\",\n",
      "\"How do I correct the mounting path error in Docker for \\\\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data'?\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a84d0e44-5797-43be-8e4d-07391f6b066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above record is fixed manually by chahing the 2 forward slash to 1\n",
    "json_questions = [\n",
    "r\"How can I resolve the Docker error 'invalid mode: \\Program Files\\Git\\var\\lib\\postgresql\\data'?\",\n",
    "\"What should I do if I encounter an invalid mode error in Docker on Windows?\",\n",
    "\"What is the correct mounting path to use in Docker for PostgreSQL data on Windows?\",\n",
    "\"Can you provide an example of a correct Docker mounting path for PostgreSQL data?\",\n",
    "r\"How do I correct the mounting path error in Docker for \\Program Files\\Git\\var\\lib\\postgresql\\data'?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f35e6db6-d319-4957-8273-0249712d53bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58c9f99f\n"
     ]
    }
   ],
   "source": [
    "print(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5be8035d-0837-4693-9c43-17c62c172caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"How can I resolve the Docker error \\'invalid mode: \\\\\\\\Program Files\\\\\\\\Git\\\\\\\\var\\\\\\\\lib\\\\\\\\postgresql\\\\\\\\data\\'?\", \"What should I do if I encounter an invalid mode error in Docker on Windows?\", \"What is the correct mounting path to use in Docker for PostgreSQL data on Windows?\", \"Can you provide an example of a correct Docker mounting path for PostgreSQL data?\", \"How do I correct the mounting path error in Docker for \\\\\\\\Program Files\\\\\\\\Git\\\\\\\\var\\\\\\\\lib\\\\\\\\postgresql\\\\\\\\data\\'?\"]'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(json_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a7d19dd3-2fee-4a61-a9d6-e4da1f15b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[doc_id]=json.dumps(json_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1cdf12a3-7c9f-4dee-bcc4-0e1f1e9d7077",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_resulst = {}\n",
    "\n",
    "for doc_id, json_questions in results.items():\n",
    "    parsed_resulst[doc_id] = json.loads(json_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6c928923-d3d8-4b8f-b093-887dffc8e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_index = {d['id']: d for d in documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1ad018b8-33d7-4b80-85df-de3a115aa2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []\n",
    "\n",
    "for doc_id, questions in parsed_resulst.items():\n",
    "    course = doc_index[doc_id]['course']\n",
    "    for q in questions:\n",
    "        final_results.append((q, course, doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f7e44c9c-c383-4b7a-8c7a-404d9c2ec8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c5adba2e-d628-47e5-a107-0ddad14fd667",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final_results, columns=['question', 'course', 'document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9e4d5ba5-c59c-4e0f-9e79-8f4cd86bb5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ground-truth-data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f88e2b0d-53ba-4766-9f5c-aa5d85eff47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question,course,document\n",
      "When does the course begin?,data-engineering-zoomcamp,c02e79ef\n",
      "How can I get the course schedule?,data-engineering-zoomcamp,c02e79ef\n",
      "What is the link for course registration?,data-engineering-zoomcamp,c02e79ef\n",
      "How can I receive course announcements?,data-engineering-zoomcamp,c02e79ef\n",
      "Where do I join the Slack channel?,data-engineering-zoomcamp,c02e79ef\n",
      "Where can I find the prerequisites for this course?,data-engineering-zoomcamp,1f6520ca\n",
      "How do I check the prerequisites for this course?,data-engineering-zoomcamp,1f6520ca\n",
      "Where are the course prerequisites listed?,data-engineering-zoomcamp,1f6520ca\n",
      "What are the requirements for joining this course?,data-engineering-zoomcamp,1f6520ca\n"
     ]
    }
   ],
   "source": [
    "!head ground-truth-data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46afefd7-2230-4a08-ae31-5600de189d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
